#!/bin/bash
#SBATCH -t 10:00:00
#SBATCH -n 1
#SBATCH --mem=24G
#SBATCH -C TitanX
#SBATCH --gres=gpu:1
#SBATCH --mail-type=END
#SBATCH --mail-user=andrei.apostol@student.uva.nl


ARG1=${1:-0}

module load python/3.6.0
module load cuda10.0/toolkit/10.0.130

#export PYTHONPATH=/var/scratch/asembeni/pysot:$PYTHONPATH
# export PATH="/var/node433/local/andrei_apostol/anaconda3/envs/torch/bin:$PATH"
# source "/var/node433/local/andrei_apostol/anaconda3/envs/torch/bin/activate"

#cd /var/scratch/asembeni/pysot/experiments/siamrpn_r50_l234_dwxcorr
#while [ "$ARG1" -eq 1 ]
#do
#if git pull | grep -v 'up-to-date' ; then
#break
#fi
#done

CUDA_VISIBLE_DEVICES=2
#python -m torch.distributed.launch \
#        --nproc_per_node=1 \
#        --master_port=2333 \
#        ../../tools/train.py --cfg config.yaml

echo "ip address: $SLURM_LAUNCH_NODE_IPADDR"

echo "Slurm job nodelist: $SLURM_JOB_NODELIST"
echo "Running $@"
srun python "$@"
#python -m torch.distributed.launch --nproc_per_node=1 --master_port=2333 ../..
