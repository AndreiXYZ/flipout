Running --model vgg19 --noise --prune_criterion weight_div_flips --seed 43 --prune_freq 32 --prune_rate 0.5 --comment=vgg19_crit=weight_div_flips_pf=32_seed=43 --save_model=pre-finetune/vgg19_weight_div_flips_pf32_s43 --logdir=criterion_experiment_no_bias/vgg19
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_weight_div_flips_pf32_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.105080 loss:        3.010980
Test - acc:         0.123000 loss:        2.359551
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.174860 loss:        2.110594
Test - acc:         0.220300 loss:        1.926493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.228320 loss:        1.910866
Test - acc:         0.260400 loss:        1.855760
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.280100 loss:        1.830354
Test - acc:         0.280900 loss:        1.853899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.337380 loss:        1.708202
Test - acc:         0.334500 loss:        1.776725
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.462520 loss:        1.424452
Test - acc:         0.521400 loss:        1.340117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.580400 loss:        1.178112
Test - acc:         0.606200 loss:        1.185553
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.652880 loss:        1.000811
Test - acc:         0.627500 loss:        1.159720
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.695780 loss:        0.890398
Test - acc:         0.423400 loss:        2.100009
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.726660 loss:        0.822717
Test - acc:         0.596000 loss:        1.244230
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.742400 loss:        0.778924
Test - acc:         0.633200 loss:        1.214685
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.756740 loss:        0.737486
Test - acc:         0.703600 loss:        0.920415
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.765240 loss:        0.717637
Test - acc:         0.699700 loss:        0.949451
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.769120 loss:        0.701513
Test - acc:         0.711200 loss:        0.881064
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.779120 loss:        0.674703
Test - acc:         0.733500 loss:        0.862860
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.783200 loss:        0.666881
Test - acc:         0.703900 loss:        0.881886
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.789080 loss:        0.649335
Test - acc:         0.656300 loss:        1.096627
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.790460 loss:        0.640201
Test - acc:         0.726200 loss:        0.868548
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.793520 loss:        0.634689
Test - acc:         0.695700 loss:        0.978369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.799100 loss:        0.619804
Test - acc:         0.743000 loss:        0.818328
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.798620 loss:        0.618211
Test - acc:         0.759600 loss:        0.749610
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.801900 loss:        0.610053
Test - acc:         0.771800 loss:        0.715111
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.803940 loss:        0.601588
Test - acc:         0.746400 loss:        0.797954
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.806880 loss:        0.595910
Test - acc:         0.749800 loss:        0.763987
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.806680 loss:        0.596324
Test - acc:         0.764100 loss:        0.743477
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.811680 loss:        0.583949
Test - acc:         0.740500 loss:        0.875080
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.813120 loss:        0.581808
Test - acc:         0.771900 loss:        0.738781
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.810200 loss:        0.591224
Test - acc:         0.796400 loss:        0.653327
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.815840 loss:        0.575764
Test - acc:         0.768500 loss:        0.731990
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.818460 loss:        0.565293
Test - acc:         0.714600 loss:        1.062118
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.815740 loss:        0.568747
Test - acc:         0.728600 loss:        0.906000
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.820140 loss:        0.556274
Test - acc:         0.751500 loss:        0.763809
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.828760 loss:        0.523623
Test - acc:         0.767800 loss:        0.682282
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.829080 loss:        0.522252
Test - acc:         0.782400 loss:        0.700823
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.825980 loss:        0.529268
Test - acc:         0.797900 loss:        0.631842
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.826720 loss:        0.527481
Test - acc:         0.772200 loss:        0.754107
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.828960 loss:        0.524742
Test - acc:         0.761000 loss:        0.735787
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.828980 loss:        0.523818
Test - acc:         0.625700 loss:        1.470533
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.830400 loss:        0.520690
Test - acc:         0.791200 loss:        0.625927
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.831460 loss:        0.511983
Test - acc:         0.779000 loss:        0.694955
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.828520 loss:        0.520832
Test - acc:         0.748300 loss:        0.827149
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.831980 loss:        0.514963
Test - acc:         0.784900 loss:        0.682711
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.834040 loss:        0.510578
Test - acc:         0.807400 loss:        0.601358
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.835340 loss:        0.501037
Test - acc:         0.769300 loss:        0.747983
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.834460 loss:        0.505310
Test - acc:         0.771500 loss:        0.797788
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.832960 loss:        0.510603
Test - acc:         0.810200 loss:        0.577877
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.833960 loss:        0.507406
Test - acc:         0.784300 loss:        0.689577
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.836480 loss:        0.499167
Test - acc:         0.799100 loss:        0.606851
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.835760 loss:        0.503520
Test - acc:         0.777100 loss:        0.690496
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.834740 loss:        0.503567
Test - acc:         0.733800 loss:        0.824122
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.837580 loss:        0.498404
Test - acc:         0.702900 loss:        1.018466
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.837320 loss:        0.497404
Test - acc:         0.743600 loss:        0.873090
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.836260 loss:        0.501445
Test - acc:         0.745100 loss:        0.841157
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.840580 loss:        0.491174
Test - acc:         0.794100 loss:        0.635184
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.842020 loss:        0.484448
Test - acc:         0.697800 loss:        1.062391
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.837860 loss:        0.493665
Test - acc:         0.777000 loss:        0.724053
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.839740 loss:        0.494081
Test - acc:         0.823700 loss:        0.579264
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.839340 loss:        0.490409
Test - acc:         0.782900 loss:        0.686674
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.836980 loss:        0.494641
Test - acc:         0.826300 loss:        0.539802
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.841460 loss:        0.485915
Test - acc:         0.798900 loss:        0.606892
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.840420 loss:        0.483654
Test - acc:         0.807000 loss:        0.633067
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.839140 loss:        0.489496
Test - acc:         0.770600 loss:        0.722197
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.841960 loss:        0.488524
Test - acc:         0.772900 loss:        0.721852
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.838000 loss:        0.492385
Test - acc:         0.701700 loss:        0.953285
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.846120 loss:        0.468978
Test - acc:         0.778400 loss:        0.699539
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.845300 loss:        0.469864
Test - acc:         0.793900 loss:        0.631692
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.847520 loss:        0.465225
Test - acc:         0.785100 loss:        0.668840
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.846700 loss:        0.468360
Test - acc:         0.816200 loss:        0.584680
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.842940 loss:        0.477851
Test - acc:         0.758400 loss:        0.749207
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.842820 loss:        0.480316
Test - acc:         0.777400 loss:        0.718909
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.844720 loss:        0.469932
Test - acc:         0.815400 loss:        0.559662
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.844120 loss:        0.471691
Test - acc:         0.805300 loss:        0.589590
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.846740 loss:        0.464725
Test - acc:         0.817000 loss:        0.558121
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.849180 loss:        0.462467
Test - acc:         0.722100 loss:        0.890199
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.847380 loss:        0.464012
Test - acc:         0.688900 loss:        1.055790
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.844020 loss:        0.476775
Test - acc:         0.751400 loss:        0.805222
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.845940 loss:        0.468338
Test - acc:         0.776200 loss:        0.716397
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.843760 loss:        0.468157
Test - acc:         0.816600 loss:        0.592487
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.843800 loss:        0.471267
Test - acc:         0.779800 loss:        0.691179
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.843040 loss:        0.474346
Test - acc:         0.756900 loss:        0.778929
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.843920 loss:        0.471039
Test - acc:         0.747500 loss:        0.791152
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.847340 loss:        0.463547
Test - acc:         0.801200 loss:        0.637410
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.848420 loss:        0.466526
Test - acc:         0.815100 loss:        0.574587
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.845980 loss:        0.464741
Test - acc:         0.787900 loss:        0.639010
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.846600 loss:        0.465835
Test - acc:         0.749900 loss:        0.796029
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.848180 loss:        0.459863
Test - acc:         0.766900 loss:        0.744303
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.844300 loss:        0.472035
Test - acc:         0.780500 loss:        0.680696
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.847360 loss:        0.458731
Test - acc:         0.775700 loss:        0.726679
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.844420 loss:        0.469359
Test - acc:         0.823800 loss:        0.572523
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.847500 loss:        0.462751
Test - acc:         0.795700 loss:        0.655695
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.849580 loss:        0.460014
Test - acc:         0.793400 loss:        0.653814
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.845480 loss:        0.465962
Test - acc:         0.793300 loss:        0.645309
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.847640 loss:        0.454450
Test - acc:         0.751700 loss:        0.805940
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.846360 loss:        0.460154
Test - acc:         0.747900 loss:        0.819605
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.847020 loss:        0.464949
Test - acc:         0.771700 loss:        0.736057
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.846600 loss:        0.462747
Test - acc:         0.821300 loss:        0.587922
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.854720 loss:        0.441689
Test - acc:         0.794000 loss:        0.639918
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.848660 loss:        0.451204
Test - acc:         0.791100 loss:        0.650696
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.853920 loss:        0.439847
Test - acc:         0.735500 loss:        0.860054
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.854000 loss:        0.442078
Test - acc:         0.804900 loss:        0.590161
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.850260 loss:        0.451947
Test - acc:         0.822000 loss:        0.544291
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.852740 loss:        0.444963
Test - acc:         0.830000 loss:        0.534574
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.851700 loss:        0.448426
Test - acc:         0.755800 loss:        0.786505
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.855040 loss:        0.436558
Test - acc:         0.749500 loss:        0.819357
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.849640 loss:        0.453836
Test - acc:         0.823400 loss:        0.567344
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.851340 loss:        0.445162
Test - acc:         0.790700 loss:        0.647407
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.850320 loss:        0.448583
Test - acc:         0.815400 loss:        0.569335
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.851680 loss:        0.447731
Test - acc:         0.766900 loss:        0.715750
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.851920 loss:        0.448343
Test - acc:         0.814300 loss:        0.543193
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.854600 loss:        0.439283
Test - acc:         0.761100 loss:        0.736321
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.849880 loss:        0.451528
Test - acc:         0.777100 loss:        0.699717
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.854640 loss:        0.442521
Test - acc:         0.798700 loss:        0.633106
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.851780 loss:        0.444632
Test - acc:         0.773900 loss:        0.730972
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.853100 loss:        0.447557
Test - acc:         0.803400 loss:        0.592573
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.853440 loss:        0.442088
Test - acc:         0.807800 loss:        0.648374
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.852140 loss:        0.443646
Test - acc:         0.819500 loss:        0.554031
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.850480 loss:        0.451540
Test - acc:         0.732300 loss:        0.918567
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.856340 loss:        0.438730
Test - acc:         0.814500 loss:        0.583601
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.854620 loss:        0.441428
Test - acc:         0.757600 loss:        0.778243
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.851260 loss:        0.447113
Test - acc:         0.771900 loss:        0.761579
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.852540 loss:        0.443678
Test - acc:         0.772800 loss:        0.720105
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.852400 loss:        0.448252
Test - acc:         0.819300 loss:        0.545947
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.853440 loss:        0.441273
Test - acc:         0.790600 loss:        0.689312
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.855340 loss:        0.437175
Test - acc:         0.781200 loss:        0.705493
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.851760 loss:        0.446527
Test - acc:         0.796900 loss:        0.625953
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.852240 loss:        0.443886
Test - acc:         0.781500 loss:        0.706509
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.850000 loss:        0.449886
Test - acc:         0.799100 loss:        0.639279
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.852500 loss:        0.444321
Test - acc:         0.803300 loss:        0.622868
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.862360 loss:        0.414368
Test - acc:         0.762600 loss:        0.873337
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.862080 loss:        0.417088
Test - acc:         0.837500 loss:        0.514017
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.860760 loss:        0.419310
Test - acc:         0.835700 loss:        0.506254
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.859500 loss:        0.428373
Test - acc:         0.779700 loss:        0.720521
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.856100 loss:        0.433635
Test - acc:         0.819900 loss:        0.568331
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.863240 loss:        0.417783
Test - acc:         0.820200 loss:        0.565010
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.859100 loss:        0.424451
Test - acc:         0.804000 loss:        0.588124
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.857960 loss:        0.429514
Test - acc:         0.741000 loss:        0.827972
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.857880 loss:        0.431254
Test - acc:         0.706700 loss:        1.025038
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.856800 loss:        0.426681
Test - acc:         0.794000 loss:        0.645285
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.862660 loss:        0.418581
Test - acc:         0.773500 loss:        0.766809
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.855900 loss:        0.430608
Test - acc:         0.758600 loss:        0.769995
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.860520 loss:        0.421554
Test - acc:         0.821400 loss:        0.557915
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.860320 loss:        0.421865
Test - acc:         0.786900 loss:        0.661592
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.856460 loss:        0.429143
Test - acc:         0.797400 loss:        0.632867
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.857860 loss:        0.428408
Test - acc:         0.756900 loss:        0.740407
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.860360 loss:        0.419882
Test - acc:         0.795600 loss:        0.630410
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.857800 loss:        0.428967
Test - acc:         0.807800 loss:        0.582877
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.858140 loss:        0.428058
Test - acc:         0.779400 loss:        0.691850
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.858760 loss:        0.424046
Test - acc:         0.818800 loss:        0.576377
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.860360 loss:        0.417949
Test - acc:         0.790500 loss:        0.645514
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.858140 loss:        0.433287
Test - acc:         0.817600 loss:        0.550902
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.911160 loss:        0.263885
Test - acc:         0.903100 loss:        0.287302
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.927900 loss:        0.213880
Test - acc:         0.911000 loss:        0.271214
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.934500 loss:        0.193841
Test - acc:         0.914500 loss:        0.263393
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.940960 loss:        0.175103
Test - acc:         0.916800 loss:        0.260963
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.945440 loss:        0.165390
Test - acc:         0.914300 loss:        0.267840
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.947380 loss:        0.156409
Test - acc:         0.913300 loss:        0.262967
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.949780 loss:        0.149181
Test - acc:         0.914800 loss:        0.263729
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.953060 loss:        0.137854
Test - acc:         0.916500 loss:        0.270999
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.954320 loss:        0.136402
Test - acc:         0.917500 loss:        0.268683
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.955740 loss:        0.127706
Test - acc:         0.913400 loss:        0.289586
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.958540 loss:        0.121642
Test - acc:         0.911900 loss:        0.287794
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.960020 loss:        0.118137
Test - acc:         0.912700 loss:        0.280687
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.962160 loss:        0.111977
Test - acc:         0.914700 loss:        0.285660
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.961800 loss:        0.111007
Test - acc:         0.916300 loss:        0.282285
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.963600 loss:        0.104155
Test - acc:         0.910100 loss:        0.295823
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.963620 loss:        0.107917
Test - acc:         0.913600 loss:        0.295714
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.965060 loss:        0.103681
Test - acc:         0.911800 loss:        0.308463
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.965020 loss:        0.103194
Test - acc:         0.912000 loss:        0.295995
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.966360 loss:        0.097617
Test - acc:         0.913800 loss:        0.293294
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.964680 loss:        0.101752
Test - acc:         0.912600 loss:        0.310042
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.966640 loss:        0.097722
Test - acc:         0.910000 loss:        0.307932
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.965380 loss:        0.100232
Test - acc:         0.910700 loss:        0.311406
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.966060 loss:        0.098698
Test - acc:         0.910300 loss:        0.306258
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.965700 loss:        0.098766
Test - acc:         0.908600 loss:        0.318115
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.965620 loss:        0.098891
Test - acc:         0.912500 loss:        0.296852
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.964920 loss:        0.100227
Test - acc:         0.904600 loss:        0.327839
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.966640 loss:        0.097354
Test - acc:         0.910400 loss:        0.306191
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.964940 loss:        0.101184
Test - acc:         0.909800 loss:        0.302952
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.966300 loss:        0.097250
Test - acc:         0.909800 loss:        0.315352
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.965680 loss:        0.100328
Test - acc:         0.896500 loss:        0.366493
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.966120 loss:        0.098426
Test - acc:         0.905100 loss:        0.323839
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.964280 loss:        0.103421
Test - acc:         0.906600 loss:        0.326887
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.967540 loss:        0.095299
Test - acc:         0.903200 loss:        0.334801
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.965860 loss:        0.100848
Test - acc:         0.904100 loss:        0.348252
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.965740 loss:        0.098977
Test - acc:         0.903400 loss:        0.336420
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.966260 loss:        0.098665
Test - acc:         0.905200 loss:        0.330001
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.964800 loss:        0.103370
Test - acc:         0.899800 loss:        0.348731
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.966600 loss:        0.097180
Test - acc:         0.905400 loss:        0.330974
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.964380 loss:        0.103001
Test - acc:         0.895200 loss:        0.360495
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.966360 loss:        0.099697
Test - acc:         0.909800 loss:        0.319955
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.966880 loss:        0.095357
Test - acc:         0.902500 loss:        0.352633
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.964840 loss:        0.103185
Test - acc:         0.901300 loss:        0.355795
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.963460 loss:        0.107407
Test - acc:         0.905300 loss:        0.325626
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.965840 loss:        0.098261
Test - acc:         0.901500 loss:        0.339205
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.967160 loss:        0.095498
Test - acc:         0.908200 loss:        0.341719
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.967120 loss:        0.093468
Test - acc:         0.900600 loss:        0.364298
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.967040 loss:        0.095935
Test - acc:         0.895800 loss:        0.384514
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.967320 loss:        0.095657
Test - acc:         0.889000 loss:        0.390173
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.965500 loss:        0.097574
Test - acc:         0.906200 loss:        0.328270
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.967280 loss:        0.095601
Test - acc:         0.897700 loss:        0.362032
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.966200 loss:        0.097369
Test - acc:         0.903200 loss:        0.342732
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.968920 loss:        0.090388
Test - acc:         0.902700 loss:        0.369548
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.966660 loss:        0.096901
Test - acc:         0.906100 loss:        0.329381
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.965760 loss:        0.099180
Test - acc:         0.906800 loss:        0.333072
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.967560 loss:        0.094874
Test - acc:         0.905000 loss:        0.325744
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.967860 loss:        0.093395
Test - acc:         0.906400 loss:        0.338129
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.969980 loss:        0.089742
Test - acc:         0.901400 loss:        0.362070
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.967280 loss:        0.096723
Test - acc:         0.905100 loss:        0.340893
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.967280 loss:        0.094035
Test - acc:         0.897300 loss:        0.374890
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.966080 loss:        0.098157
Test - acc:         0.902300 loss:        0.330020
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.967480 loss:        0.095327
Test - acc:         0.894500 loss:        0.387108
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.966540 loss:        0.096053
Test - acc:         0.900400 loss:        0.358491
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.968180 loss:        0.094310
Test - acc:         0.902900 loss:        0.355487
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.967140 loss:        0.095755
Test - acc:         0.898100 loss:        0.374323
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.967120 loss:        0.095433
Test - acc:         0.898300 loss:        0.369443
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.965620 loss:        0.097511
Test - acc:         0.901300 loss:        0.359552
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.965680 loss:        0.099188
Test - acc:         0.896600 loss:        0.361591
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.965940 loss:        0.098621
Test - acc:         0.902300 loss:        0.351990
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.966460 loss:        0.098256
Test - acc:         0.893600 loss:        0.366935
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.967300 loss:        0.097190
Test - acc:         0.897300 loss:        0.368696
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.965720 loss:        0.100667
Test - acc:         0.900300 loss:        0.364897
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.968060 loss:        0.092374
Test - acc:         0.897100 loss:        0.373849
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.965680 loss:        0.100402
Test - acc:         0.893600 loss:        0.378963
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.966100 loss:        0.098915
Test - acc:         0.893600 loss:        0.360093
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.952500 loss:        0.139663
Test - acc:         0.897600 loss:        0.333194
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.958580 loss:        0.119673
Test - acc:         0.895300 loss:        0.329612
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.960480 loss:        0.115034
Test - acc:         0.895400 loss:        0.363484
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.959960 loss:        0.117067
Test - acc:         0.895700 loss:        0.361203
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.962040 loss:        0.109318
Test - acc:         0.900400 loss:        0.343701
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.961540 loss:        0.111253
Test - acc:         0.892100 loss:        0.396555
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.963060 loss:        0.110568
Test - acc:         0.904000 loss:        0.335456
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.962340 loss:        0.110374
Test - acc:         0.887500 loss:        0.405373
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.960980 loss:        0.111101
Test - acc:         0.896200 loss:        0.366934
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.963520 loss:        0.105625
Test - acc:         0.896100 loss:        0.369641
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.962360 loss:        0.107761
Test - acc:         0.896700 loss:        0.363852
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.963540 loss:        0.103950
Test - acc:         0.895100 loss:        0.357414
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.962000 loss:        0.110121
Test - acc:         0.909200 loss:        0.313152
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.963480 loss:        0.105001
Test - acc:         0.899600 loss:        0.357080
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.962660 loss:        0.105411
Test - acc:         0.897400 loss:        0.365989
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.962800 loss:        0.108568
Test - acc:         0.892400 loss:        0.361131
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.964020 loss:        0.103627
Test - acc:         0.895400 loss:        0.384220
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.961920 loss:        0.108890
Test - acc:         0.900900 loss:        0.347851
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.963540 loss:        0.105119
Test - acc:         0.896000 loss:        0.371337
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.963900 loss:        0.104206
Test - acc:         0.889900 loss:        0.381426
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.964740 loss:        0.103157
Test - acc:         0.897500 loss:        0.355849
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.964940 loss:        0.104680
Test - acc:         0.900200 loss:        0.347540
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.962000 loss:        0.107350
Test - acc:         0.895500 loss:        0.370313
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.962020 loss:        0.111825
Test - acc:         0.899400 loss:        0.355610
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.964200 loss:        0.102865
Test - acc:         0.897200 loss:        0.356642
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.964420 loss:        0.104160
Test - acc:         0.898100 loss:        0.369323
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.975960 loss:        0.071386
Test - acc:         0.916200 loss:        0.291561
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.982300 loss:        0.054468
Test - acc:         0.917800 loss:        0.289208
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.983820 loss:        0.049271
Test - acc:         0.918100 loss:        0.292495
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.985160 loss:        0.045858
Test - acc:         0.920900 loss:        0.294418
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.986640 loss:        0.042839
Test - acc:         0.918200 loss:        0.297559
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.986820 loss:        0.040280
Test - acc:         0.919600 loss:        0.299905
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.935940 loss:        0.189243
Test - acc:         0.902000 loss:        0.333568
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.948360 loss:        0.150449
Test - acc:         0.902500 loss:        0.327993
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.952240 loss:        0.136292
Test - acc:         0.904300 loss:        0.323806
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.956080 loss:        0.127930
Test - acc:         0.907500 loss:        0.314063
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.956680 loss:        0.122807
Test - acc:         0.905400 loss:        0.314420
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.959540 loss:        0.118139
Test - acc:         0.909000 loss:        0.308732
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.960580 loss:        0.114067
Test - acc:         0.908200 loss:        0.308080
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.960880 loss:        0.113750
Test - acc:         0.910700 loss:        0.303878
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.961580 loss:        0.110375
Test - acc:         0.909900 loss:        0.308438
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.964440 loss:        0.103570
Test - acc:         0.910600 loss:        0.306141
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.965340 loss:        0.101967
Test - acc:         0.910700 loss:        0.306131
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.964160 loss:        0.103161
Test - acc:         0.908500 loss:        0.311350
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.965280 loss:        0.100569
Test - acc:         0.911100 loss:        0.306927
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.967100 loss:        0.096019
Test - acc:         0.913000 loss:        0.307672
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.967300 loss:        0.094095
Test - acc:         0.911700 loss:        0.308175
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.968460 loss:        0.091955
Test - acc:         0.913400 loss:        0.311295
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.967840 loss:        0.093027
Test - acc:         0.912000 loss:        0.308818
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.970020 loss:        0.088923
Test - acc:         0.912600 loss:        0.310266
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.969860 loss:        0.087422
Test - acc:         0.912700 loss:        0.311788
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.970680 loss:        0.084304
Test - acc:         0.913100 loss:        0.310938
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.970840 loss:        0.084001
Test - acc:         0.913500 loss:        0.316659
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.971160 loss:        0.083430
Test - acc:         0.912800 loss:        0.317122
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.971380 loss:        0.081912
Test - acc:         0.914300 loss:        0.310955
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.970140 loss:        0.084537
Test - acc:         0.915000 loss:        0.313047
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.971940 loss:        0.082160
Test - acc:         0.913700 loss:        0.317263
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.972400 loss:        0.079505
Test - acc:         0.914700 loss:        0.317527
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.973500 loss:        0.077823
Test - acc:         0.915000 loss:        0.316732
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.973240 loss:        0.076364
Test - acc:         0.915600 loss:        0.315857
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.973740 loss:        0.076103
Test - acc:         0.912400 loss:        0.318341
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.975220 loss:        0.071982
Test - acc:         0.913600 loss:        0.322370
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.973040 loss:        0.076285
Test - acc:         0.911800 loss:        0.320102
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.974460 loss:        0.074206
Test - acc:         0.911100 loss:        0.329664
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.844160 loss:        0.476178
Test - acc:         0.853400 loss:        0.435264
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.878680 loss:        0.358922
Test - acc:         0.868000 loss:        0.401135
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.889320 loss:        0.327090
Test - acc:         0.872300 loss:        0.389451
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.892680 loss:        0.310060
Test - acc:         0.874300 loss:        0.375161
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.897560 loss:        0.298181
Test - acc:         0.878300 loss:        0.372083
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.903840 loss:        0.283382
Test - acc:         0.880800 loss:        0.362416
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.905560 loss:        0.276953
Test - acc:         0.883200 loss:        0.359346
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.907220 loss:        0.271952
Test - acc:         0.881800 loss:        0.357288
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.908980 loss:        0.269068
Test - acc:         0.882800 loss:        0.350965
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.910740 loss:        0.259853
Test - acc:         0.882400 loss:        0.352697
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.913020 loss:        0.253440
Test - acc:         0.887200 loss:        0.346660
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.913440 loss:        0.251126
Test - acc:         0.885600 loss:        0.352368
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.913680 loss:        0.248239
Test - acc:         0.888300 loss:        0.346912
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.915480 loss:        0.246341
Test - acc:         0.888700 loss:        0.343643
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.917860 loss:        0.239380
Test - acc:         0.887500 loss:        0.338119
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.918420 loss:        0.236400
Test - acc:         0.887600 loss:        0.342757
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.919780 loss:        0.235013
Test - acc:         0.889000 loss:        0.338557
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.919820 loss:        0.233486
Test - acc:         0.889600 loss:        0.338320
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.920200 loss:        0.228570
Test - acc:         0.889400 loss:        0.342286
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.921740 loss:        0.228930
Test - acc:         0.890800 loss:        0.334414
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.920380 loss:        0.229518
Test - acc:         0.890000 loss:        0.330078
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.921320 loss:        0.226013
Test - acc:         0.890000 loss:        0.338420
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.922480 loss:        0.223859
Test - acc:         0.892300 loss:        0.335234
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.922540 loss:        0.222941
Test - acc:         0.889200 loss:        0.340085
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.924420 loss:        0.219694
Test - acc:         0.892500 loss:        0.335902
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.923080 loss:        0.222199
Test - acc:         0.890700 loss:        0.341518
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.926560 loss:        0.213851
Test - acc:         0.891900 loss:        0.334999
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.924800 loss:        0.217045
Test - acc:         0.892000 loss:        0.331865
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.923720 loss:        0.217454
Test - acc:         0.891100 loss:        0.335741
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.927060 loss:        0.210419
Test - acc:         0.892600 loss:        0.335295
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.927400 loss:        0.212729
Test - acc:         0.892200 loss:        0.334238
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.926600 loss:        0.210752
Test - acc:         0.892700 loss:        0.336605
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.761340 loss:        0.717298
Test - acc:         0.807700 loss:        0.574968
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.812280 loss:        0.550214
Test - acc:         0.822800 loss:        0.531160
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.827080 loss:        0.511801
Test - acc:         0.831700 loss:        0.513600
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.836960 loss:        0.483422
Test - acc:         0.835900 loss:        0.493727
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.841480 loss:        0.464733
Test - acc:         0.839400 loss:        0.480912
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.844880 loss:        0.456753
Test - acc:         0.841700 loss:        0.473774
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.848600 loss:        0.446794
Test - acc:         0.846400 loss:        0.465302
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.849820 loss:        0.437358
Test - acc:         0.846100 loss:        0.460103
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.855220 loss:        0.429322
Test - acc:         0.846000 loss:        0.459829
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.853580 loss:        0.426997
Test - acc:         0.847400 loss:        0.455471
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.857500 loss:        0.416699
Test - acc:         0.849200 loss:        0.452856
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.858740 loss:        0.411570
Test - acc:         0.849300 loss:        0.455958
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.859500 loss:        0.411264
Test - acc:         0.849000 loss:        0.449906
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.863500 loss:        0.404630
Test - acc:         0.852200 loss:        0.447802
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.862500 loss:        0.404550
Test - acc:         0.851200 loss:        0.443072
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.863540 loss:        0.399942
Test - acc:         0.852200 loss:        0.445752
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.864580 loss:        0.396281
Test - acc:         0.852100 loss:        0.437185
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.866200 loss:        0.397069
Test - acc:         0.854100 loss:        0.431966
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.867600 loss:        0.389308
Test - acc:         0.858400 loss:        0.433884
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.866860 loss:        0.387641
Test - acc:         0.850300 loss:        0.442163
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.866640 loss:        0.388049
Test - acc:         0.852500 loss:        0.426633
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.869340 loss:        0.381097
Test - acc:         0.858400 loss:        0.426034
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.867920 loss:        0.385248
Test - acc:         0.857400 loss:        0.430440
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.869600 loss:        0.381387
Test - acc:         0.858300 loss:        0.419689
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.870160 loss:        0.378400
Test - acc:         0.856400 loss:        0.426696
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.869080 loss:        0.377569
Test - acc:         0.858100 loss:        0.421792
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.871560 loss:        0.376016
Test - acc:         0.856800 loss:        0.426064
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.872780 loss:        0.371582
Test - acc:         0.860800 loss:        0.414641
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.872460 loss:        0.370590
Test - acc:         0.860900 loss:        0.415046
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.871840 loss:        0.374128
Test - acc:         0.860100 loss:        0.421491
Sparsity :          0.9990
Wdecay :        0.000500
