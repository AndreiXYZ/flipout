Running --model vgg19 --noise --prune_criterion weight_div_flips --seed 42 --prune_freq 39 --prune_rate 0.5 --comment=vgg19_crit=weight_div_flips_pf=39_seed=42 --save_model=pre-finetune/vgg19_weight_div_flips_pf39_s42 --logdir=criterion_experiment_no_bias/vgg19
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_weight_div_flips_pf39_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.104000 loss:        2.718243
Test - acc:         0.130700 loss:        2.376117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.217760 loss:        2.006799
Test - acc:         0.271900 loss:        1.870665
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.297260 loss:        1.794897
Test - acc:         0.362200 loss:        1.644529
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.414420 loss:        1.527382
Test - acc:         0.384000 loss:        1.644484
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.518700 loss:        1.326566
Test - acc:         0.503400 loss:        1.491033
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.615080 loss:        1.108830
Test - acc:         0.505000 loss:        1.498029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.666460 loss:        0.988673
Test - acc:         0.658400 loss:        1.002956
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.709120 loss:        0.874903
Test - acc:         0.636600 loss:        1.287662
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.731720 loss:        0.813348
Test - acc:         0.651400 loss:        1.223787
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.754900 loss:        0.745460
Test - acc:         0.735700 loss:        0.818550
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.764920 loss:        0.715788
Test - acc:         0.657100 loss:        1.178176
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.774980 loss:        0.689548
Test - acc:         0.657900 loss:        1.163287
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.781380 loss:        0.667972
Test - acc:         0.743100 loss:        0.805025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.785520 loss:        0.660618
Test - acc:         0.684300 loss:        1.101409
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.791480 loss:        0.644370
Test - acc:         0.723200 loss:        0.861721
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.793940 loss:        0.635592
Test - acc:         0.744500 loss:        0.907952
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.797740 loss:        0.621285
Test - acc:         0.762100 loss:        0.729169
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.800700 loss:        0.613860
Test - acc:         0.710300 loss:        0.907862
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.803760 loss:        0.610200
Test - acc:         0.732300 loss:        0.853393
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.807740 loss:        0.595498
Test - acc:         0.784000 loss:        0.705477
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.807560 loss:        0.594736
Test - acc:         0.752500 loss:        0.779057
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.810820 loss:        0.585076
Test - acc:         0.727100 loss:        0.870213
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.814800 loss:        0.570571
Test - acc:         0.791400 loss:        0.632741
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.816760 loss:        0.566251
Test - acc:         0.787500 loss:        0.667275
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.812280 loss:        0.579204
Test - acc:         0.770000 loss:        0.714722
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.821540 loss:        0.552373
Test - acc:         0.793600 loss:        0.638860
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.820720 loss:        0.557908
Test - acc:         0.741500 loss:        0.878304
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.821660 loss:        0.553534
Test - acc:         0.713300 loss:        0.984538
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.820440 loss:        0.560757
Test - acc:         0.782900 loss:        0.690656
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.821300 loss:        0.555406
Test - acc:         0.809300 loss:        0.617641
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.825080 loss:        0.546125
Test - acc:         0.715200 loss:        0.889496
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.823620 loss:        0.546504
Test - acc:         0.801200 loss:        0.608614
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.823600 loss:        0.541720
Test - acc:         0.753000 loss:        0.784193
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.825180 loss:        0.543560
Test - acc:         0.720800 loss:        0.915281
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.829380 loss:        0.531755
Test - acc:         0.771600 loss:        0.697956
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.830860 loss:        0.520896
Test - acc:         0.791900 loss:        0.642480
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.827600 loss:        0.528385
Test - acc:         0.752500 loss:        0.794267
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.829860 loss:        0.529083
Test - acc:         0.750200 loss:        0.786398
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.827200 loss:        0.533626
Test - acc:         0.767200 loss:        0.753464
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.836620 loss:        0.503407
Test - acc:         0.781500 loss:        0.690175
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.835240 loss:        0.502953
Test - acc:         0.775700 loss:        0.706361
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.835380 loss:        0.511294
Test - acc:         0.764600 loss:        0.776889
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.835640 loss:        0.507603
Test - acc:         0.691600 loss:        1.075583
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.837340 loss:        0.499911
Test - acc:         0.766600 loss:        0.783064
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.833740 loss:        0.505600
Test - acc:         0.779700 loss:        0.732746
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.835980 loss:        0.510193
Test - acc:         0.785700 loss:        0.675663
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.837680 loss:        0.499375
Test - acc:         0.808400 loss:        0.602315
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.834920 loss:        0.506368
Test - acc:         0.796800 loss:        0.625562
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.838560 loss:        0.502840
Test - acc:         0.821300 loss:        0.578300
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.834420 loss:        0.506504
Test - acc:         0.768700 loss:        0.711571
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.837900 loss:        0.499137
Test - acc:         0.768800 loss:        0.752863
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.839140 loss:        0.495365
Test - acc:         0.726500 loss:        0.871686
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.838060 loss:        0.502222
Test - acc:         0.767900 loss:        0.785511
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.838260 loss:        0.493136
Test - acc:         0.813800 loss:        0.580978
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.838960 loss:        0.492570
Test - acc:         0.818500 loss:        0.555723
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.836780 loss:        0.494408
Test - acc:         0.816400 loss:        0.569497
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.840600 loss:        0.484668
Test - acc:         0.774700 loss:        0.778775
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.837700 loss:        0.503269
Test - acc:         0.809000 loss:        0.584276
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.838760 loss:        0.492898
Test - acc:         0.808700 loss:        0.585040
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.840560 loss:        0.489607
Test - acc:         0.820000 loss:        0.559617
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.841160 loss:        0.487718
Test - acc:         0.780000 loss:        0.735323
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.839540 loss:        0.494136
Test - acc:         0.785900 loss:        0.676557
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.841940 loss:        0.488606
Test - acc:         0.800200 loss:        0.687324
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.843880 loss:        0.479061
Test - acc:         0.760600 loss:        0.751675
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.835880 loss:        0.500309
Test - acc:         0.785900 loss:        0.736943
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.840360 loss:        0.486614
Test - acc:         0.746800 loss:        0.809161
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.844600 loss:        0.479533
Test - acc:         0.780600 loss:        0.682296
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.840040 loss:        0.491482
Test - acc:         0.772000 loss:        0.774368
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.840740 loss:        0.485398
Test - acc:         0.840000 loss:        0.519942
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.839700 loss:        0.489209
Test - acc:         0.744500 loss:        0.829647
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.841500 loss:        0.485036
Test - acc:         0.808600 loss:        0.610659
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.841240 loss:        0.485253
Test - acc:         0.785300 loss:        0.699102
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.840360 loss:        0.489095
Test - acc:         0.801900 loss:        0.632661
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.840480 loss:        0.488593
Test - acc:         0.795300 loss:        0.638103
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.842600 loss:        0.483402
Test - acc:         0.769100 loss:        0.745543
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.842260 loss:        0.483633
Test - acc:         0.765900 loss:        0.726814
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.842540 loss:        0.486686
Test - acc:         0.794300 loss:        0.652377
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.834500 loss:        0.505460
Test - acc:         0.785200 loss:        0.662196
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.846580 loss:        0.475187
Test - acc:         0.807700 loss:        0.600755
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.848200 loss:        0.470010
Test - acc:         0.824300 loss:        0.564601
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.844820 loss:        0.472062
Test - acc:         0.674500 loss:        1.234695
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.844020 loss:        0.476485
Test - acc:         0.798000 loss:        0.655738
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.843840 loss:        0.483038
Test - acc:         0.795000 loss:        0.632238
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.844000 loss:        0.474795
Test - acc:         0.775100 loss:        0.738538
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.845300 loss:        0.469571
Test - acc:         0.771100 loss:        0.704515
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.848540 loss:        0.468266
Test - acc:         0.762200 loss:        0.796365
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.844480 loss:        0.477543
Test - acc:         0.793000 loss:        0.644859
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.847880 loss:        0.465536
Test - acc:         0.796400 loss:        0.642190
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.844740 loss:        0.478221
Test - acc:         0.792900 loss:        0.664857
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.843820 loss:        0.472878
Test - acc:         0.802800 loss:        0.609190
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.846140 loss:        0.470082
Test - acc:         0.796200 loss:        0.636342
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.848420 loss:        0.467090
Test - acc:         0.781200 loss:        0.701155
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.847560 loss:        0.468204
Test - acc:         0.780400 loss:        0.652665
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.845140 loss:        0.471969
Test - acc:         0.775300 loss:        0.752575
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.847700 loss:        0.464446
Test - acc:         0.800300 loss:        0.610405
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.847040 loss:        0.467198
Test - acc:         0.752400 loss:        0.841873
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.846900 loss:        0.467897
Test - acc:         0.639600 loss:        1.447917
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.847400 loss:        0.467933
Test - acc:         0.814200 loss:        0.583315
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.843900 loss:        0.475971
Test - acc:         0.804200 loss:        0.604300
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.846220 loss:        0.466806
Test - acc:         0.804400 loss:        0.593765
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.848500 loss:        0.462863
Test - acc:         0.805800 loss:        0.635411
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.846140 loss:        0.470871
Test - acc:         0.766600 loss:        0.715878
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.846660 loss:        0.468474
Test - acc:         0.789600 loss:        0.662315
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.844580 loss:        0.470689
Test - acc:         0.798100 loss:        0.612484
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.849660 loss:        0.459949
Test - acc:         0.808500 loss:        0.571299
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.848380 loss:        0.467214
Test - acc:         0.774000 loss:        0.763482
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.849880 loss:        0.459196
Test - acc:         0.792000 loss:        0.660042
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.850980 loss:        0.454696
Test - acc:         0.739400 loss:        0.841465
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.846220 loss:        0.473537
Test - acc:         0.785700 loss:        0.705098
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.847140 loss:        0.465698
Test - acc:         0.787100 loss:        0.633186
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.846060 loss:        0.474665
Test - acc:         0.825400 loss:        0.547739
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.848720 loss:        0.466833
Test - acc:         0.821600 loss:        0.572784
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.848080 loss:        0.463643
Test - acc:         0.804500 loss:        0.594250
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.847780 loss:        0.465979
Test - acc:         0.835200 loss:        0.511437
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.845920 loss:        0.467958
Test - acc:         0.796400 loss:        0.629396
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.848480 loss:        0.464260
Test - acc:         0.829300 loss:        0.529196
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.849420 loss:        0.461316
Test - acc:         0.757100 loss:        0.798666
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.849180 loss:        0.456410
Test - acc:         0.750400 loss:        0.831231
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.850980 loss:        0.456161
Test - acc:         0.761400 loss:        0.774741
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.852280 loss:        0.449820
Test - acc:         0.765400 loss:        0.713099
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.847040 loss:        0.465366
Test - acc:         0.829900 loss:        0.531475
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.849280 loss:        0.456899
Test - acc:         0.789500 loss:        0.669986
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.847500 loss:        0.465356
Test - acc:         0.769900 loss:        0.765560
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.852560 loss:        0.450405
Test - acc:         0.725600 loss:        0.855689
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.852440 loss:        0.451212
Test - acc:         0.791500 loss:        0.657894
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.848340 loss:        0.460766
Test - acc:         0.792200 loss:        0.653615
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.852220 loss:        0.453703
Test - acc:         0.822400 loss:        0.543144
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.850200 loss:        0.455960
Test - acc:         0.774200 loss:        0.704395
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.850960 loss:        0.452384
Test - acc:         0.790200 loss:        0.662680
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.850460 loss:        0.459350
Test - acc:         0.821900 loss:        0.562056
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.851600 loss:        0.454073
Test - acc:         0.800900 loss:        0.650403
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.850040 loss:        0.462421
Test - acc:         0.726400 loss:        0.944405
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.853220 loss:        0.449624
Test - acc:         0.753500 loss:        0.787271
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.848800 loss:        0.461926
Test - acc:         0.791100 loss:        0.641491
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.850760 loss:        0.450056
Test - acc:         0.804700 loss:        0.604998
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.849420 loss:        0.456758
Test - acc:         0.821000 loss:        0.548006
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.851540 loss:        0.455349
Test - acc:         0.801200 loss:        0.613284
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.851700 loss:        0.447489
Test - acc:         0.794600 loss:        0.686412
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.849680 loss:        0.455206
Test - acc:         0.785600 loss:        0.681617
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.849360 loss:        0.455973
Test - acc:         0.803500 loss:        0.613445
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.850060 loss:        0.449406
Test - acc:         0.805800 loss:        0.627687
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.851700 loss:        0.451337
Test - acc:         0.739300 loss:        0.867070
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.851400 loss:        0.451701
Test - acc:         0.781800 loss:        0.672562
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.847420 loss:        0.460033
Test - acc:         0.811100 loss:        0.574076
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.852040 loss:        0.453967
Test - acc:         0.806300 loss:        0.609583
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.849860 loss:        0.453026
Test - acc:         0.783500 loss:        0.667997
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.851080 loss:        0.454445
Test - acc:         0.789700 loss:        0.694798
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.849360 loss:        0.456557
Test - acc:         0.779800 loss:        0.669890
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.852100 loss:        0.452822
Test - acc:         0.810800 loss:        0.591441
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.851440 loss:        0.452462
Test - acc:         0.763100 loss:        0.888871
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.909480 loss:        0.273890
Test - acc:         0.899300 loss:        0.316306
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.925880 loss:        0.226303
Test - acc:         0.904700 loss:        0.302310
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.932180 loss:        0.203486
Test - acc:         0.906200 loss:        0.285350
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.936560 loss:        0.189991
Test - acc:         0.909100 loss:        0.286059
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.940740 loss:        0.173462
Test - acc:         0.908800 loss:        0.288731
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.944120 loss:        0.167258
Test - acc:         0.908600 loss:        0.289821
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.949400 loss:        0.152538
Test - acc:         0.910700 loss:        0.290139
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.947840 loss:        0.153095
Test - acc:         0.909800 loss:        0.291800
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.951980 loss:        0.142698
Test - acc:         0.909600 loss:        0.289574
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.954380 loss:        0.136147
Test - acc:         0.911000 loss:        0.291834
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.955920 loss:        0.131577
Test - acc:         0.908600 loss:        0.299089
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.956700 loss:        0.129813
Test - acc:         0.913900 loss:        0.288107
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.958440 loss:        0.123904
Test - acc:         0.910200 loss:        0.296104
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.959300 loss:        0.119129
Test - acc:         0.912200 loss:        0.305219
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.960700 loss:        0.118525
Test - acc:         0.907500 loss:        0.306613
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.959960 loss:        0.118306
Test - acc:         0.909900 loss:        0.301927
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.959960 loss:        0.115216
Test - acc:         0.907100 loss:        0.308912
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.961980 loss:        0.112708
Test - acc:         0.906500 loss:        0.323907
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.962500 loss:        0.111272
Test - acc:         0.910100 loss:        0.308803
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.962880 loss:        0.109823
Test - acc:         0.910100 loss:        0.296260
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.964900 loss:        0.105027
Test - acc:         0.908100 loss:        0.311692
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.965220 loss:        0.104680
Test - acc:         0.903000 loss:        0.344255
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.962660 loss:        0.111294
Test - acc:         0.909200 loss:        0.304924
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.964020 loss:        0.104931
Test - acc:         0.906100 loss:        0.325341
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.961840 loss:        0.108395
Test - acc:         0.902300 loss:        0.339252
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.962400 loss:        0.109730
Test - acc:         0.907600 loss:        0.321546
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.963220 loss:        0.109994
Test - acc:         0.907000 loss:        0.329894
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.962920 loss:        0.110057
Test - acc:         0.906500 loss:        0.327214
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.963020 loss:        0.109220
Test - acc:         0.912400 loss:        0.307793
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.963400 loss:        0.107890
Test - acc:         0.892800 loss:        0.372431
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.965280 loss:        0.103354
Test - acc:         0.910000 loss:        0.316456
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.963080 loss:        0.109036
Test - acc:         0.895100 loss:        0.379412
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.964240 loss:        0.104273
Test - acc:         0.899300 loss:        0.376797
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.962540 loss:        0.109178
Test - acc:         0.906800 loss:        0.320440
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.964840 loss:        0.104915
Test - acc:         0.898400 loss:        0.353957
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.963840 loss:        0.105925
Test - acc:         0.904000 loss:        0.316107
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.965060 loss:        0.104324
Test - acc:         0.902200 loss:        0.334356
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.965520 loss:        0.102142
Test - acc:         0.900100 loss:        0.356735
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.962900 loss:        0.108964
Test - acc:         0.893500 loss:        0.371247
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.963360 loss:        0.107480
Test - acc:         0.906000 loss:        0.339347
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.963320 loss:        0.109132
Test - acc:         0.900900 loss:        0.342661
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.962100 loss:        0.112951
Test - acc:         0.900400 loss:        0.349447
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.962580 loss:        0.111111
Test - acc:         0.904300 loss:        0.334194
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.961640 loss:        0.113496
Test - acc:         0.892200 loss:        0.369924
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.962720 loss:        0.107483
Test - acc:         0.896900 loss:        0.370610
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.967200 loss:        0.098956
Test - acc:         0.902000 loss:        0.340065
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.971220 loss:        0.085407
Test - acc:         0.904800 loss:        0.336236
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.969240 loss:        0.089690
Test - acc:         0.904900 loss:        0.333153
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.968860 loss:        0.092715
Test - acc:         0.906200 loss:        0.341887
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.968540 loss:        0.091668
Test - acc:         0.897500 loss:        0.366580
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.969460 loss:        0.091270
Test - acc:         0.905000 loss:        0.352285
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.969520 loss:        0.092037
Test - acc:         0.907400 loss:        0.339028
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.967320 loss:        0.097601
Test - acc:         0.904900 loss:        0.345451
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.968180 loss:        0.092973
Test - acc:         0.899300 loss:        0.348173
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.967060 loss:        0.098245
Test - acc:         0.907000 loss:        0.362028
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.967860 loss:        0.094199
Test - acc:         0.890500 loss:        0.404484
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.966080 loss:        0.099952
Test - acc:         0.897900 loss:        0.373296
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.968140 loss:        0.094417
Test - acc:         0.897000 loss:        0.362839
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.968620 loss:        0.093519
Test - acc:         0.906800 loss:        0.342764
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.967380 loss:        0.096557
Test - acc:         0.890500 loss:        0.391274
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.966520 loss:        0.096509
Test - acc:         0.900800 loss:        0.364561
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.968360 loss:        0.091569
Test - acc:         0.903100 loss:        0.341607
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.967280 loss:        0.097723
Test - acc:         0.903600 loss:        0.349923
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.969680 loss:        0.090737
Test - acc:         0.900600 loss:        0.361149
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.968100 loss:        0.095379
Test - acc:         0.904500 loss:        0.348090
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.969800 loss:        0.090503
Test - acc:         0.903400 loss:        0.356013
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.966700 loss:        0.097966
Test - acc:         0.882900 loss:        0.429531
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.966880 loss:        0.095862
Test - acc:         0.900100 loss:        0.368620
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.967340 loss:        0.096351
Test - acc:         0.902700 loss:        0.350711
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.967880 loss:        0.095050
Test - acc:         0.897100 loss:        0.373607
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.967820 loss:        0.095023
Test - acc:         0.905900 loss:        0.339997
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.967140 loss:        0.097539
Test - acc:         0.897400 loss:        0.355513
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.966720 loss:        0.097880
Test - acc:         0.905200 loss:        0.342994
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.968300 loss:        0.095063
Test - acc:         0.896200 loss:        0.388498
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.967460 loss:        0.096330
Test - acc:         0.901900 loss:        0.348226
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.968040 loss:        0.093188
Test - acc:         0.904600 loss:        0.338087
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.967060 loss:        0.098517
Test - acc:         0.902700 loss:        0.350516
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.965680 loss:        0.100274
Test - acc:         0.903400 loss:        0.343225
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.969300 loss:        0.092058
Test - acc:         0.898400 loss:        0.364413
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.968140 loss:        0.098356
Test - acc:         0.895500 loss:        0.372885
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.967340 loss:        0.098907
Test - acc:         0.902700 loss:        0.351565
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.965840 loss:        0.099904
Test - acc:         0.901800 loss:        0.345875
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.967000 loss:        0.097074
Test - acc:         0.906300 loss:        0.342349
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.968380 loss:        0.094994
Test - acc:         0.898000 loss:        0.364263
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.968300 loss:        0.096509
Test - acc:         0.901400 loss:        0.348648
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.968560 loss:        0.094106
Test - acc:         0.906300 loss:        0.330534
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.969360 loss:        0.091030
Test - acc:         0.896500 loss:        0.374222
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.970600 loss:        0.088117
Test - acc:         0.901000 loss:        0.356418
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.971060 loss:        0.086437
Test - acc:         0.904500 loss:        0.332811
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.972140 loss:        0.085654
Test - acc:         0.909700 loss:        0.324772
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.971840 loss:        0.084920
Test - acc:         0.902800 loss:        0.368047
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.970320 loss:        0.087864
Test - acc:         0.903300 loss:        0.342920
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.971920 loss:        0.085306
Test - acc:         0.906500 loss:        0.349244
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.970980 loss:        0.085840
Test - acc:         0.904600 loss:        0.350246
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.972420 loss:        0.082850
Test - acc:         0.896700 loss:        0.378362
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.970620 loss:        0.086717
Test - acc:         0.905800 loss:        0.339652
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.969100 loss:        0.091609
Test - acc:         0.902100 loss:        0.346853
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.971820 loss:        0.086518
Test - acc:         0.904900 loss:        0.344024
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.970460 loss:        0.087464
Test - acc:         0.895500 loss:        0.379090
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.970560 loss:        0.088433
Test - acc:         0.905500 loss:        0.341713
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.982660 loss:        0.053010
Test - acc:         0.921000 loss:        0.285563
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.988320 loss:        0.037317
Test - acc:         0.922900 loss:        0.283833
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.990360 loss:        0.031047
Test - acc:         0.923200 loss:        0.284851
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.991140 loss:        0.028213
Test - acc:         0.922300 loss:        0.288854
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.991840 loss:        0.026094
Test - acc:         0.924400 loss:        0.290302
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.992420 loss:        0.024900
Test - acc:         0.923700 loss:        0.293719
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.993020 loss:        0.023551
Test - acc:         0.926100 loss:        0.295844
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.993560 loss:        0.021918
Test - acc:         0.924500 loss:        0.297962
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.993980 loss:        0.020302
Test - acc:         0.926100 loss:        0.293937
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.994620 loss:        0.018741
Test - acc:         0.924800 loss:        0.297980
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.994280 loss:        0.018978
Test - acc:         0.927400 loss:        0.295278
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.994380 loss:        0.019322
Test - acc:         0.925100 loss:        0.298427
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.994580 loss:        0.018131
Test - acc:         0.926200 loss:        0.299952
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.994780 loss:        0.017429
Test - acc:         0.925400 loss:        0.301866
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.995780 loss:        0.015764
Test - acc:         0.925100 loss:        0.305855
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.995400 loss:        0.015457
Test - acc:         0.924700 loss:        0.307707
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.995260 loss:        0.016276
Test - acc:         0.926400 loss:        0.303108
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.995580 loss:        0.016110
Test - acc:         0.926500 loss:        0.311376
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.995660 loss:        0.014879
Test - acc:         0.926200 loss:        0.308822
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.996000 loss:        0.013123
Test - acc:         0.926600 loss:        0.307574
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.996660 loss:        0.012994
Test - acc:         0.926700 loss:        0.309140
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.996060 loss:        0.013488
Test - acc:         0.925800 loss:        0.313430
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.996200 loss:        0.013147
Test - acc:         0.925100 loss:        0.312665
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.972840 loss:        0.081109
Test - acc:         0.915400 loss:        0.335169
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.977780 loss:        0.065053
Test - acc:         0.918500 loss:        0.324769
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.980600 loss:        0.058994
Test - acc:         0.918800 loss:        0.317030
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.981960 loss:        0.054588
Test - acc:         0.920200 loss:        0.317545
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.982360 loss:        0.051988
Test - acc:         0.918800 loss:        0.318330
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.982960 loss:        0.050124
Test - acc:         0.918900 loss:        0.321765
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.985020 loss:        0.046033
Test - acc:         0.918200 loss:        0.319127
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.985520 loss:        0.042553
Test - acc:         0.918400 loss:        0.325239
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.985020 loss:        0.043297
Test - acc:         0.920800 loss:        0.324604
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.986720 loss:        0.040999
Test - acc:         0.918500 loss:        0.327284
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.986820 loss:        0.039813
Test - acc:         0.921300 loss:        0.324814
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.988280 loss:        0.036432
Test - acc:         0.918400 loss:        0.330290
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.988080 loss:        0.036022
Test - acc:         0.920400 loss:        0.327532
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.987800 loss:        0.036709
Test - acc:         0.921800 loss:        0.323863
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.988840 loss:        0.034121
Test - acc:         0.920400 loss:        0.327294
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.989020 loss:        0.032214
Test - acc:         0.921000 loss:        0.331486
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.988380 loss:        0.034734
Test - acc:         0.921200 loss:        0.332347
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.989960 loss:        0.030939
Test - acc:         0.921500 loss:        0.326308
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.989200 loss:        0.033198
Test - acc:         0.921300 loss:        0.329479
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.991060 loss:        0.028524
Test - acc:         0.920400 loss:        0.333779
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.990100 loss:        0.030538
Test - acc:         0.921200 loss:        0.332466
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.990540 loss:        0.029092
Test - acc:         0.920000 loss:        0.333157
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.990420 loss:        0.029605
Test - acc:         0.919300 loss:        0.335277
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.991400 loss:        0.026857
Test - acc:         0.919900 loss:        0.336358
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.991060 loss:        0.028299
Test - acc:         0.920700 loss:        0.331074
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.990460 loss:        0.028697
Test - acc:         0.921700 loss:        0.329435
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.990760 loss:        0.028024
Test - acc:         0.922200 loss:        0.332618
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.991780 loss:        0.025710
Test - acc:         0.922700 loss:        0.333125
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.991280 loss:        0.026933
Test - acc:         0.920900 loss:        0.337258
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.992140 loss:        0.024606
Test - acc:         0.919300 loss:        0.347129
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.991520 loss:        0.025761
Test - acc:         0.921100 loss:        0.338861
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.991680 loss:        0.025506
Test - acc:         0.920200 loss:        0.342023
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.990920 loss:        0.027296
Test - acc:         0.921600 loss:        0.343581
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.991600 loss:        0.025842
Test - acc:         0.921300 loss:        0.344061
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.991120 loss:        0.026559
Test - acc:         0.921900 loss:        0.343085
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.992720 loss:        0.022715
Test - acc:         0.921300 loss:        0.343356
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.992480 loss:        0.022816
Test - acc:         0.918700 loss:        0.348459
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.992760 loss:        0.022988
Test - acc:         0.921000 loss:        0.346080
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.993060 loss:        0.021367
Test - acc:         0.921700 loss:        0.348847
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.916480 loss:        0.265996
Test - acc:         0.894600 loss:        0.339424
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.932940 loss:        0.200773
Test - acc:         0.896200 loss:        0.330964
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.938920 loss:        0.179677
Test - acc:         0.901500 loss:        0.326328
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.942640 loss:        0.169209
Test - acc:         0.899900 loss:        0.323102
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.945320 loss:        0.162419
Test - acc:         0.903800 loss:        0.315781
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.948500 loss:        0.150903
Test - acc:         0.905900 loss:        0.318246
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.950400 loss:        0.147098
Test - acc:         0.903800 loss:        0.320516
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.951780 loss:        0.141681
Test - acc:         0.906400 loss:        0.320537
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.951760 loss:        0.141256
Test - acc:         0.905100 loss:        0.322173
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.953380 loss:        0.137089
Test - acc:         0.904200 loss:        0.323241
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.955320 loss:        0.132637
Test - acc:         0.908100 loss:        0.324081
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.957460 loss:        0.127386
Test - acc:         0.906200 loss:        0.325189
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.957780 loss:        0.124056
Test - acc:         0.907400 loss:        0.328902
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.959000 loss:        0.118981
Test - acc:         0.907100 loss:        0.323753
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.959500 loss:        0.117279
Test - acc:         0.908500 loss:        0.320973
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.960040 loss:        0.117643
Test - acc:         0.907200 loss:        0.325616
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.961120 loss:        0.114777
Test - acc:         0.908000 loss:        0.324864
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.962200 loss:        0.110256
Test - acc:         0.908700 loss:        0.322387
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.963680 loss:        0.108656
Test - acc:         0.906800 loss:        0.333200
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.962560 loss:        0.109563
Test - acc:         0.909000 loss:        0.326496
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.962800 loss:        0.108463
Test - acc:         0.909500 loss:        0.324243
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.963240 loss:        0.104713
Test - acc:         0.910500 loss:        0.328350
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.965460 loss:        0.101763
Test - acc:         0.908500 loss:        0.336508
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.965120 loss:        0.101020
Test - acc:         0.910700 loss:        0.331923
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.964220 loss:        0.103126
Test - acc:         0.908500 loss:        0.334650
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.965520 loss:        0.099408
Test - acc:         0.910300 loss:        0.334838
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.966420 loss:        0.099434
Test - acc:         0.910100 loss:        0.330079
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.966700 loss:        0.097974
Test - acc:         0.910100 loss:        0.332434
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.966700 loss:        0.097785
Test - acc:         0.911200 loss:        0.335372
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.966420 loss:        0.097816
Test - acc:         0.907800 loss:        0.336377
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.967640 loss:        0.093593
Test - acc:         0.907100 loss:        0.335622
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.967800 loss:        0.093254
Test - acc:         0.908600 loss:        0.329904
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.968540 loss:        0.092564
Test - acc:         0.908800 loss:        0.337652
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.967480 loss:        0.094215
Test - acc:         0.909100 loss:        0.336090
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.968520 loss:        0.090484
Test - acc:         0.911800 loss:        0.334842
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.969120 loss:        0.087388
Test - acc:         0.912600 loss:        0.334246
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.969860 loss:        0.086842
Test - acc:         0.910900 loss:        0.336541
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.969420 loss:        0.087381
Test - acc:         0.910100 loss:        0.341438
Sparsity :          0.9961
Wdecay :        0.000500
