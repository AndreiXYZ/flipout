Running --model vgg19 --noise --prune_criterion weight_div_flips --seed 42 --prune_freq 50 --prune_rate 0.5 --comment=vgg19_crit=weight_div_flips_pf=50_seed=42 --save_model=pre-finetune/vgg19_weight_div_flips_pf50_s42 --logdir=criterion_experiment_no_bias/vgg19
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_weight_div_flips_pf50_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.104000 loss:        2.718243
Test - acc:         0.130700 loss:        2.376117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.217760 loss:        2.006799
Test - acc:         0.271900 loss:        1.870665
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.297260 loss:        1.794897
Test - acc:         0.362200 loss:        1.644529
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.414420 loss:        1.527382
Test - acc:         0.384000 loss:        1.644484
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.518700 loss:        1.326566
Test - acc:         0.503400 loss:        1.491033
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.615080 loss:        1.108830
Test - acc:         0.505000 loss:        1.498029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.666460 loss:        0.988673
Test - acc:         0.658400 loss:        1.002956
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.709120 loss:        0.874903
Test - acc:         0.636600 loss:        1.287662
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.731720 loss:        0.813348
Test - acc:         0.651400 loss:        1.223787
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.754900 loss:        0.745460
Test - acc:         0.735700 loss:        0.818550
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.764920 loss:        0.715788
Test - acc:         0.657100 loss:        1.178176
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.774980 loss:        0.689548
Test - acc:         0.657900 loss:        1.163287
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.781380 loss:        0.667972
Test - acc:         0.743100 loss:        0.805025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.785520 loss:        0.660618
Test - acc:         0.684300 loss:        1.101409
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.791480 loss:        0.644370
Test - acc:         0.723200 loss:        0.861721
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.793940 loss:        0.635592
Test - acc:         0.744500 loss:        0.907952
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.797740 loss:        0.621285
Test - acc:         0.762100 loss:        0.729169
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.800700 loss:        0.613860
Test - acc:         0.710300 loss:        0.907862
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.803760 loss:        0.610200
Test - acc:         0.732300 loss:        0.853393
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.807740 loss:        0.595498
Test - acc:         0.784000 loss:        0.705477
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.807560 loss:        0.594736
Test - acc:         0.752500 loss:        0.779057
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.810820 loss:        0.585076
Test - acc:         0.727100 loss:        0.870213
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.814800 loss:        0.570571
Test - acc:         0.791400 loss:        0.632741
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.816760 loss:        0.566251
Test - acc:         0.787500 loss:        0.667275
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.812280 loss:        0.579204
Test - acc:         0.770000 loss:        0.714722
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.821540 loss:        0.552373
Test - acc:         0.793600 loss:        0.638860
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.820720 loss:        0.557908
Test - acc:         0.741500 loss:        0.878304
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.821660 loss:        0.553534
Test - acc:         0.713300 loss:        0.984538
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.820440 loss:        0.560757
Test - acc:         0.782900 loss:        0.690656
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.821300 loss:        0.555406
Test - acc:         0.809300 loss:        0.617641
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.825080 loss:        0.546125
Test - acc:         0.715200 loss:        0.889496
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.823620 loss:        0.546504
Test - acc:         0.801200 loss:        0.608614
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.823600 loss:        0.541720
Test - acc:         0.753000 loss:        0.784193
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.825180 loss:        0.543560
Test - acc:         0.720800 loss:        0.915281
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.829380 loss:        0.531755
Test - acc:         0.771600 loss:        0.697956
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.830860 loss:        0.520896
Test - acc:         0.791900 loss:        0.642480
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.827600 loss:        0.528385
Test - acc:         0.752500 loss:        0.794267
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.829860 loss:        0.529083
Test - acc:         0.750200 loss:        0.786398
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.827200 loss:        0.533626
Test - acc:         0.767200 loss:        0.753464
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.828980 loss:        0.529054
Test - acc:         0.755600 loss:        0.772111
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.831460 loss:        0.516624
Test - acc:         0.744000 loss:        0.883261
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.829180 loss:        0.530973
Test - acc:         0.685600 loss:        1.067758
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.834460 loss:        0.513079
Test - acc:         0.741700 loss:        0.838087
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.832240 loss:        0.517125
Test - acc:         0.701400 loss:        1.078249
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.831760 loss:        0.517794
Test - acc:         0.728200 loss:        0.983483
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.830700 loss:        0.521428
Test - acc:         0.790300 loss:        0.680382
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.833560 loss:        0.514616
Test - acc:         0.774600 loss:        0.733232
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.828540 loss:        0.523976
Test - acc:         0.806700 loss:        0.601056
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.832060 loss:        0.518902
Test - acc:         0.811300 loss:        0.584603
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.833400 loss:        0.516480
Test - acc:         0.713000 loss:        1.028625
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.838900 loss:        0.490100
Test - acc:         0.709300 loss:        1.002711
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.838900 loss:        0.494269
Test - acc:         0.757900 loss:        0.760856
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.841660 loss:        0.488926
Test - acc:         0.738400 loss:        0.889729
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.837300 loss:        0.495248
Test - acc:         0.806200 loss:        0.595137
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.835980 loss:        0.502860
Test - acc:         0.782400 loss:        0.681482
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.839000 loss:        0.496226
Test - acc:         0.815900 loss:        0.578769
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.840740 loss:        0.487637
Test - acc:         0.767800 loss:        0.745132
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.839220 loss:        0.493507
Test - acc:         0.736900 loss:        0.867493
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.842160 loss:        0.487919
Test - acc:         0.797700 loss:        0.660267
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.841320 loss:        0.486881
Test - acc:         0.717800 loss:        0.980898
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.841000 loss:        0.489430
Test - acc:         0.806100 loss:        0.620788
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.840120 loss:        0.491880
Test - acc:         0.789700 loss:        0.665284
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.841980 loss:        0.486714
Test - acc:         0.817200 loss:        0.570535
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.844080 loss:        0.483054
Test - acc:         0.679000 loss:        1.134941
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.840760 loss:        0.488616
Test - acc:         0.746900 loss:        0.919898
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.839640 loss:        0.490109
Test - acc:         0.715100 loss:        0.931428
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.844620 loss:        0.479787
Test - acc:         0.766900 loss:        0.765492
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.838000 loss:        0.493011
Test - acc:         0.705200 loss:        1.024345
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.840220 loss:        0.487844
Test - acc:         0.772200 loss:        0.723369
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.841080 loss:        0.481635
Test - acc:         0.756700 loss:        0.824367
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.840680 loss:        0.489755
Test - acc:         0.793300 loss:        0.647297
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.840480 loss:        0.490990
Test - acc:         0.806300 loss:        0.613293
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.843600 loss:        0.484127
Test - acc:         0.675300 loss:        1.159038
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.842520 loss:        0.483025
Test - acc:         0.815400 loss:        0.565198
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.842140 loss:        0.481245
Test - acc:         0.769200 loss:        0.798710
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.844220 loss:        0.481854
Test - acc:         0.791800 loss:        0.655352
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.842180 loss:        0.485084
Test - acc:         0.715600 loss:        0.939611
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.842340 loss:        0.484806
Test - acc:         0.805200 loss:        0.597946
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.843600 loss:        0.484298
Test - acc:         0.807500 loss:        0.597542
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.840480 loss:        0.486896
Test - acc:         0.831400 loss:        0.528234
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.844100 loss:        0.478334
Test - acc:         0.747200 loss:        0.853876
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.843260 loss:        0.479120
Test - acc:         0.772700 loss:        0.781031
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.842120 loss:        0.484691
Test - acc:         0.792300 loss:        0.652278
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.844560 loss:        0.475000
Test - acc:         0.798600 loss:        0.643488
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.845920 loss:        0.470763
Test - acc:         0.766000 loss:        0.757380
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.844140 loss:        0.475809
Test - acc:         0.797100 loss:        0.647043
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.842700 loss:        0.482281
Test - acc:         0.791400 loss:        0.702544
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.845580 loss:        0.470376
Test - acc:         0.791400 loss:        0.708331
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.840700 loss:        0.485261
Test - acc:         0.807100 loss:        0.625236
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.842480 loss:        0.484133
Test - acc:         0.754800 loss:        0.808552
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.843820 loss:        0.481132
Test - acc:         0.811700 loss:        0.557279
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.844160 loss:        0.474203
Test - acc:         0.811200 loss:        0.627585
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.843660 loss:        0.478166
Test - acc:         0.803200 loss:        0.607772
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.843520 loss:        0.478582
Test - acc:         0.784800 loss:        0.705261
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.845140 loss:        0.471617
Test - acc:         0.782400 loss:        0.692658
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.846460 loss:        0.470817
Test - acc:         0.785300 loss:        0.697221
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.840800 loss:        0.483193
Test - acc:         0.756900 loss:        0.752308
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.848040 loss:        0.466613
Test - acc:         0.801000 loss:        0.608944
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.845000 loss:        0.473380
Test - acc:         0.795300 loss:        0.633374
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.845300 loss:        0.474774
Test - acc:         0.809200 loss:        0.577125
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.849880 loss:        0.455310
Test - acc:         0.790500 loss:        0.674068
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.847200 loss:        0.463488
Test - acc:         0.716300 loss:        0.920622
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.847180 loss:        0.464463
Test - acc:         0.811900 loss:        0.580817
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.849780 loss:        0.460227
Test - acc:         0.768900 loss:        0.723516
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.847820 loss:        0.464049
Test - acc:         0.830400 loss:        0.528608
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.846100 loss:        0.469562
Test - acc:         0.719700 loss:        1.000991
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.849660 loss:        0.460372
Test - acc:         0.801700 loss:        0.602166
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.850340 loss:        0.459187
Test - acc:         0.693200 loss:        1.033638
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.846360 loss:        0.472385
Test - acc:         0.779400 loss:        0.682299
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.847360 loss:        0.463766
Test - acc:         0.677300 loss:        1.229206
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.844440 loss:        0.477561
Test - acc:         0.828500 loss:        0.527706
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.846800 loss:        0.468465
Test - acc:         0.822400 loss:        0.562443
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.849980 loss:        0.460876
Test - acc:         0.731000 loss:        0.833678
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.848380 loss:        0.466532
Test - acc:         0.820600 loss:        0.569617
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.848080 loss:        0.464773
Test - acc:         0.784500 loss:        0.716093
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.845420 loss:        0.472159
Test - acc:         0.785300 loss:        0.677603
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.847340 loss:        0.465191
Test - acc:         0.820700 loss:        0.578072
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.849920 loss:        0.461062
Test - acc:         0.790300 loss:        0.678865
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.850840 loss:        0.457856
Test - acc:         0.838000 loss:        0.507000
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.849000 loss:        0.460180
Test - acc:         0.755800 loss:        0.790057
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.847500 loss:        0.465715
Test - acc:         0.810800 loss:        0.609628
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.847320 loss:        0.468959
Test - acc:         0.781500 loss:        0.658107
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.847940 loss:        0.465928
Test - acc:         0.792300 loss:        0.664324
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.850020 loss:        0.460465
Test - acc:         0.764000 loss:        0.756535
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.849520 loss:        0.459285
Test - acc:         0.797900 loss:        0.637011
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.848740 loss:        0.464013
Test - acc:         0.800700 loss:        0.606879
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.849660 loss:        0.464844
Test - acc:         0.788300 loss:        0.736828
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.848780 loss:        0.465102
Test - acc:         0.696400 loss:        0.980863
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.849380 loss:        0.461323
Test - acc:         0.840100 loss:        0.504331
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.848020 loss:        0.466996
Test - acc:         0.773300 loss:        0.732557
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.847900 loss:        0.463483
Test - acc:         0.784700 loss:        0.689665
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.846840 loss:        0.467476
Test - acc:         0.815900 loss:        0.598939
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.849920 loss:        0.461277
Test - acc:         0.776800 loss:        0.710321
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.846600 loss:        0.470142
Test - acc:         0.763000 loss:        0.752046
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.847100 loss:        0.463967
Test - acc:         0.796000 loss:        0.627586
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.847120 loss:        0.464030
Test - acc:         0.814200 loss:        0.590833
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.844440 loss:        0.472477
Test - acc:         0.768700 loss:        0.765322
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.849180 loss:        0.457506
Test - acc:         0.792000 loss:        0.656807
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.847700 loss:        0.460249
Test - acc:         0.783700 loss:        0.684672
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.848400 loss:        0.466242
Test - acc:         0.713800 loss:        1.107754
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.848800 loss:        0.463012
Test - acc:         0.808200 loss:        0.618137
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.848300 loss:        0.461102
Test - acc:         0.725800 loss:        0.884666
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.848520 loss:        0.458495
Test - acc:         0.796700 loss:        0.625875
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.848420 loss:        0.465431
Test - acc:         0.826600 loss:        0.546922
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.848260 loss:        0.463657
Test - acc:         0.824400 loss:        0.534583
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.849180 loss:        0.460537
Test - acc:         0.832700 loss:        0.513184
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.847300 loss:        0.464779
Test - acc:         0.754100 loss:        0.898458
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.846200 loss:        0.465050
Test - acc:         0.775700 loss:        0.725011
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.850240 loss:        0.455503
Test - acc:         0.828000 loss:        0.535974
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.848020 loss:        0.463242
Test - acc:         0.823600 loss:        0.551723
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.906820 loss:        0.282317
Test - acc:         0.897300 loss:        0.311953
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.924180 loss:        0.232166
Test - acc:         0.906200 loss:        0.297000
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.930540 loss:        0.207956
Test - acc:         0.907300 loss:        0.287029
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.935620 loss:        0.191057
Test - acc:         0.909500 loss:        0.282498
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.940100 loss:        0.180336
Test - acc:         0.903400 loss:        0.296027
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.942680 loss:        0.171060
Test - acc:         0.909100 loss:        0.289620
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.946660 loss:        0.159948
Test - acc:         0.905400 loss:        0.292182
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.947660 loss:        0.157044
Test - acc:         0.910400 loss:        0.287132
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.950900 loss:        0.146118
Test - acc:         0.908400 loss:        0.283712
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.953980 loss:        0.138229
Test - acc:         0.913000 loss:        0.283038
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.955720 loss:        0.132344
Test - acc:         0.911600 loss:        0.293521
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.956200 loss:        0.128818
Test - acc:         0.911200 loss:        0.303268
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.958160 loss:        0.123589
Test - acc:         0.906700 loss:        0.310361
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.959780 loss:        0.119546
Test - acc:         0.911200 loss:        0.304793
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.958960 loss:        0.121329
Test - acc:         0.901500 loss:        0.312762
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.960180 loss:        0.117228
Test - acc:         0.908400 loss:        0.322581
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.961500 loss:        0.114468
Test - acc:         0.909600 loss:        0.304441
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.962160 loss:        0.112793
Test - acc:         0.907400 loss:        0.315045
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.962000 loss:        0.111835
Test - acc:         0.902100 loss:        0.335737
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.964460 loss:        0.108924
Test - acc:         0.905600 loss:        0.314603
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.961740 loss:        0.110893
Test - acc:         0.907000 loss:        0.321365
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.963300 loss:        0.108641
Test - acc:         0.907200 loss:        0.325076
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.962360 loss:        0.108781
Test - acc:         0.905000 loss:        0.325265
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.965200 loss:        0.105986
Test - acc:         0.899300 loss:        0.369948
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.964260 loss:        0.104805
Test - acc:         0.898800 loss:        0.342038
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.962380 loss:        0.108199
Test - acc:         0.904100 loss:        0.339183
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.963900 loss:        0.106689
Test - acc:         0.905900 loss:        0.327847
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.963000 loss:        0.108062
Test - acc:         0.907000 loss:        0.324799
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.964200 loss:        0.104609
Test - acc:         0.900700 loss:        0.343487
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.963900 loss:        0.106608
Test - acc:         0.902800 loss:        0.341519
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.964320 loss:        0.102746
Test - acc:         0.900000 loss:        0.344410
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.964500 loss:        0.107230
Test - acc:         0.900100 loss:        0.356881
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.964300 loss:        0.105655
Test - acc:         0.903800 loss:        0.339058
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.964740 loss:        0.104716
Test - acc:         0.901400 loss:        0.351857
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.963240 loss:        0.109532
Test - acc:         0.892300 loss:        0.392647
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.965280 loss:        0.103951
Test - acc:         0.900800 loss:        0.347784
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.965140 loss:        0.104241
Test - acc:         0.900700 loss:        0.351360
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.962880 loss:        0.108680
Test - acc:         0.897900 loss:        0.356606
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.963280 loss:        0.110038
Test - acc:         0.899300 loss:        0.363150
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.964740 loss:        0.105858
Test - acc:         0.897800 loss:        0.357931
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.963300 loss:        0.109442
Test - acc:         0.904200 loss:        0.340923
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.964660 loss:        0.106292
Test - acc:         0.903300 loss:        0.336959
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.963340 loss:        0.109378
Test - acc:         0.903700 loss:        0.332779
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.963460 loss:        0.107483
Test - acc:         0.895900 loss:        0.357125
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.963740 loss:        0.106645
Test - acc:         0.897000 loss:        0.359126
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.962940 loss:        0.109908
Test - acc:         0.898300 loss:        0.362388
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.963760 loss:        0.106926
Test - acc:         0.899200 loss:        0.354689
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.964680 loss:        0.104607
Test - acc:         0.898200 loss:        0.365036
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.962300 loss:        0.110737
Test - acc:         0.897000 loss:        0.374931
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.963420 loss:        0.107936
Test - acc:         0.903100 loss:        0.348853
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.967420 loss:        0.093275
Test - acc:         0.897500 loss:        0.367056
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.969000 loss:        0.092690
Test - acc:         0.906500 loss:        0.346709
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.969480 loss:        0.091293
Test - acc:         0.904900 loss:        0.356422
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.967520 loss:        0.093487
Test - acc:         0.903700 loss:        0.348258
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.968020 loss:        0.095683
Test - acc:         0.893200 loss:        0.387998
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.968020 loss:        0.093779
Test - acc:         0.898900 loss:        0.360955
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.969200 loss:        0.089328
Test - acc:         0.896000 loss:        0.369157
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.967900 loss:        0.095286
Test - acc:         0.889900 loss:        0.402586
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.968700 loss:        0.091047
Test - acc:         0.895900 loss:        0.385805
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.967560 loss:        0.095424
Test - acc:         0.889500 loss:        0.394681
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.967860 loss:        0.095743
Test - acc:         0.902800 loss:        0.336141
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.967160 loss:        0.095947
Test - acc:         0.899400 loss:        0.352484
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.967800 loss:        0.093249
Test - acc:         0.897300 loss:        0.375283
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.968680 loss:        0.095723
Test - acc:         0.887900 loss:        0.421088
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.970500 loss:        0.091240
Test - acc:         0.895600 loss:        0.375853
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.969220 loss:        0.092146
Test - acc:         0.887700 loss:        0.401864
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.964940 loss:        0.102339
Test - acc:         0.892100 loss:        0.391236
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.967600 loss:        0.093165
Test - acc:         0.893300 loss:        0.389551
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.966980 loss:        0.097623
Test - acc:         0.889700 loss:        0.409511
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.968260 loss:        0.093427
Test - acc:         0.897800 loss:        0.374821
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.967720 loss:        0.094236
Test - acc:         0.901200 loss:        0.358457
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.968620 loss:        0.094755
Test - acc:         0.898400 loss:        0.364682
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.968760 loss:        0.091906
Test - acc:         0.895100 loss:        0.375836
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.966540 loss:        0.100722
Test - acc:         0.902000 loss:        0.347063
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.968520 loss:        0.092968
Test - acc:         0.893300 loss:        0.389787
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.968580 loss:        0.094059
Test - acc:         0.903400 loss:        0.339492
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.967420 loss:        0.098670
Test - acc:         0.904900 loss:        0.343273
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.967440 loss:        0.097595
Test - acc:         0.904300 loss:        0.344546
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.966640 loss:        0.098316
Test - acc:         0.899900 loss:        0.356868
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.969020 loss:        0.094430
Test - acc:         0.903700 loss:        0.351080
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.967300 loss:        0.095031
Test - acc:         0.901000 loss:        0.361898
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.967940 loss:        0.097556
Test - acc:         0.894700 loss:        0.385647
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.965380 loss:        0.100739
Test - acc:         0.896200 loss:        0.366044
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.969640 loss:        0.091973
Test - acc:         0.883000 loss:        0.444013
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.968460 loss:        0.096404
Test - acc:         0.893600 loss:        0.382306
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.969920 loss:        0.092190
Test - acc:         0.896300 loss:        0.383715
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.968100 loss:        0.095153
Test - acc:         0.902700 loss:        0.349565
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.967760 loss:        0.097582
Test - acc:         0.897300 loss:        0.378901
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.968300 loss:        0.095339
Test - acc:         0.891300 loss:        0.401469
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.969340 loss:        0.092348
Test - acc:         0.910000 loss:        0.348232
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.968040 loss:        0.095723
Test - acc:         0.897900 loss:        0.382014
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.970040 loss:        0.089169
Test - acc:         0.901500 loss:        0.371971
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.967980 loss:        0.095462
Test - acc:         0.902500 loss:        0.348035
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.968080 loss:        0.092930
Test - acc:         0.885400 loss:        0.423934
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.968600 loss:        0.091628
Test - acc:         0.901700 loss:        0.352994
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.967220 loss:        0.096089
Test - acc:         0.902700 loss:        0.339052
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.967800 loss:        0.094361
Test - acc:         0.897200 loss:        0.381372
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.968960 loss:        0.093268
Test - acc:         0.900400 loss:        0.356120
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.969340 loss:        0.091185
Test - acc:         0.897900 loss:        0.369742
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.970100 loss:        0.091433
Test - acc:         0.899700 loss:        0.358771
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.981100 loss:        0.060871
Test - acc:         0.918300 loss:        0.283320
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.986800 loss:        0.042816
Test - acc:         0.919600 loss:        0.284671
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.989580 loss:        0.035680
Test - acc:         0.921900 loss:        0.287885
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.990420 loss:        0.030736
Test - acc:         0.922400 loss:        0.287611
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.991620 loss:        0.028520
Test - acc:         0.921300 loss:        0.293831
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.992000 loss:        0.025947
Test - acc:         0.923100 loss:        0.293914
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.992720 loss:        0.023976
Test - acc:         0.924200 loss:        0.294049
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.993020 loss:        0.023208
Test - acc:         0.924300 loss:        0.299904
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.993560 loss:        0.020982
Test - acc:         0.923400 loss:        0.299573
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.994220 loss:        0.019957
Test - acc:         0.923300 loss:        0.298114
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.994640 loss:        0.017828
Test - acc:         0.924300 loss:        0.303367
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.994320 loss:        0.018709
Test - acc:         0.923600 loss:        0.305667
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.994640 loss:        0.018211
Test - acc:         0.925000 loss:        0.308078
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.995720 loss:        0.014906
Test - acc:         0.924700 loss:        0.308332
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.995180 loss:        0.016224
Test - acc:         0.924700 loss:        0.313405
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.995960 loss:        0.014293
Test - acc:         0.924600 loss:        0.313949
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.995660 loss:        0.014830
Test - acc:         0.923500 loss:        0.314025
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.995860 loss:        0.013639
Test - acc:         0.924100 loss:        0.316406
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.996420 loss:        0.013012
Test - acc:         0.925200 loss:        0.320115
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.996920 loss:        0.011794
Test - acc:         0.926200 loss:        0.320921
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.996780 loss:        0.011805
Test - acc:         0.924900 loss:        0.321124
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.996240 loss:        0.012844
Test - acc:         0.925200 loss:        0.322362
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.996920 loss:        0.010903
Test - acc:         0.924500 loss:        0.323246
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.996680 loss:        0.010846
Test - acc:         0.926600 loss:        0.323965
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.997020 loss:        0.010852
Test - acc:         0.923500 loss:        0.330756
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.997100 loss:        0.010900
Test - acc:         0.925400 loss:        0.328868
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.996880 loss:        0.010313
Test - acc:         0.925900 loss:        0.327734
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.997240 loss:        0.010186
Test - acc:         0.925400 loss:        0.328654
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.996740 loss:        0.011286
Test - acc:         0.924300 loss:        0.332089
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.997460 loss:        0.010236
Test - acc:         0.925500 loss:        0.328863
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.997320 loss:        0.009597
Test - acc:         0.924100 loss:        0.333443
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.997480 loss:        0.009407
Test - acc:         0.925600 loss:        0.330221
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.997700 loss:        0.008491
Test - acc:         0.926400 loss:        0.327134
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.997520 loss:        0.009021
Test - acc:         0.926600 loss:        0.330714
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.997840 loss:        0.007917
Test - acc:         0.925700 loss:        0.332610
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.997560 loss:        0.008441
Test - acc:         0.925500 loss:        0.335112
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.997660 loss:        0.008692
Test - acc:         0.924400 loss:        0.339208
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.997660 loss:        0.008210
Test - acc:         0.924000 loss:        0.338188
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.997940 loss:        0.007466
Test - acc:         0.924400 loss:        0.339568
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.998080 loss:        0.007401
Test - acc:         0.925200 loss:        0.340208
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.998000 loss:        0.007313
Test - acc:         0.923800 loss:        0.342642
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.997620 loss:        0.007882
Test - acc:         0.923600 loss:        0.346078
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.997760 loss:        0.007529
Test - acc:         0.924900 loss:        0.343837
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.997940 loss:        0.007684
Test - acc:         0.925100 loss:        0.344010
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.998060 loss:        0.007337
Test - acc:         0.926400 loss:        0.340261
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.997960 loss:        0.006841
Test - acc:         0.924700 loss:        0.344493
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.998060 loss:        0.006916
Test - acc:         0.925600 loss:        0.347461
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.998180 loss:        0.006934
Test - acc:         0.924200 loss:        0.344177
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.998140 loss:        0.006847
Test - acc:         0.926200 loss:        0.345261
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.998160 loss:        0.006910
Test - acc:         0.924900 loss:        0.350439
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.986160 loss:        0.042453
Test - acc:         0.915900 loss:        0.350998
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.989520 loss:        0.031964
Test - acc:         0.920100 loss:        0.343623
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.991200 loss:        0.027923
Test - acc:         0.922500 loss:        0.338049
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.991320 loss:        0.026762
Test - acc:         0.920300 loss:        0.336160
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.992720 loss:        0.023024
Test - acc:         0.921400 loss:        0.336714
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.992200 loss:        0.023157
Test - acc:         0.921900 loss:        0.345511
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.993140 loss:        0.021526
Test - acc:         0.923300 loss:        0.343625
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.994060 loss:        0.019285
Test - acc:         0.921700 loss:        0.344217
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.993960 loss:        0.020298
Test - acc:         0.921300 loss:        0.344198
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.994180 loss:        0.018440
Test - acc:         0.920000 loss:        0.346895
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.994740 loss:        0.016879
Test - acc:         0.921700 loss:        0.343203
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.994240 loss:        0.017700
Test - acc:         0.920200 loss:        0.352046
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.994880 loss:        0.016096
Test - acc:         0.923300 loss:        0.349496
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.994960 loss:        0.016687
Test - acc:         0.920700 loss:        0.348853
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.995140 loss:        0.015863
Test - acc:         0.922400 loss:        0.349890
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.995480 loss:        0.014598
Test - acc:         0.921900 loss:        0.351619
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.995580 loss:        0.014683
Test - acc:         0.921000 loss:        0.356445
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.995540 loss:        0.014230
Test - acc:         0.921900 loss:        0.355835
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.995100 loss:        0.015234
Test - acc:         0.922700 loss:        0.350638
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.995880 loss:        0.013292
Test - acc:         0.923000 loss:        0.355274
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.995600 loss:        0.014023
Test - acc:         0.922400 loss:        0.352203
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.995680 loss:        0.013657
Test - acc:         0.923500 loss:        0.355771
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.995780 loss:        0.013560
Test - acc:         0.923800 loss:        0.353623
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.996200 loss:        0.012928
Test - acc:         0.922200 loss:        0.360266
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.996020 loss:        0.013095
Test - acc:         0.923000 loss:        0.360361
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.996780 loss:        0.011519
Test - acc:         0.922700 loss:        0.363592
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.996480 loss:        0.011494
Test - acc:         0.921600 loss:        0.365784
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.996760 loss:        0.011213
Test - acc:         0.923000 loss:        0.369233
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.995880 loss:        0.013071
Test - acc:         0.922000 loss:        0.364391
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.996360 loss:        0.011520
Test - acc:         0.922100 loss:        0.363050
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.996260 loss:        0.012044
Test - acc:         0.921800 loss:        0.367759
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.996560 loss:        0.011642
Test - acc:         0.923500 loss:        0.367326
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.996600 loss:        0.011354
Test - acc:         0.921600 loss:        0.364751
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.996860 loss:        0.011036
Test - acc:         0.921100 loss:        0.363553
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.997260 loss:        0.009734
Test - acc:         0.921800 loss:        0.372035
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.996580 loss:        0.011449
Test - acc:         0.922000 loss:        0.371494
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.996820 loss:        0.010258
Test - acc:         0.921300 loss:        0.370844
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.996400 loss:        0.011999
Test - acc:         0.921400 loss:        0.367272
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.996760 loss:        0.010987
Test - acc:         0.921200 loss:        0.365868
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.997440 loss:        0.009118
Test - acc:         0.922000 loss:        0.373538
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.997000 loss:        0.009871
Test - acc:         0.921000 loss:        0.373189
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.997060 loss:        0.010021
Test - acc:         0.922500 loss:        0.374415
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.997520 loss:        0.008982
Test - acc:         0.921700 loss:        0.374709
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.997640 loss:        0.008185
Test - acc:         0.923400 loss:        0.377273
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.997300 loss:        0.009797
Test - acc:         0.921800 loss:        0.382915
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.997100 loss:        0.009796
Test - acc:         0.922200 loss:        0.372774
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.997280 loss:        0.009004
Test - acc:         0.922500 loss:        0.371618
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.997040 loss:        0.009801
Test - acc:         0.922800 loss:        0.372478
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.997020 loss:        0.009708
Test - acc:         0.923800 loss:        0.376854
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.997060 loss:        0.010127
Test - acc:         0.923700 loss:        0.375452
Sparsity :          0.9844
Wdecay :        0.000500
