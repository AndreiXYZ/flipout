Running --model resnet18 --noise --prune_criterion weight_div_flips --seed 43 --prune_freq 117 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=117_seed=43 --save_model=pre-finetune/resnet18_weight_div_flips_pf117_s43 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 117,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf117_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.275580 loss:        2.206162
Test - acc:         0.379300 loss:        1.773930
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.448080 loss:        1.503151
Test - acc:         0.529700 loss:        1.350102
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.561660 loss:        1.220748
Test - acc:         0.589900 loss:        1.153149
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.644920 loss:        1.002910
Test - acc:         0.641300 loss:        1.060353
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.705420 loss:        0.837933
Test - acc:         0.662400 loss:        0.967185
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.753280 loss:        0.707195
Test - acc:         0.719800 loss:        0.820684
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.778140 loss:        0.637992
Test - acc:         0.746700 loss:        0.735908
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.797340 loss:        0.584231
Test - acc:         0.764400 loss:        0.731834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.810600 loss:        0.549598
Test - acc:         0.739700 loss:        0.764843
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.816060 loss:        0.533603
Test - acc:         0.747800 loss:        0.758131
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.822860 loss:        0.513958
Test - acc:         0.768500 loss:        0.701588
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.827900 loss:        0.501282
Test - acc:         0.816000 loss:        0.537637
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.833580 loss:        0.483668
Test - acc:         0.810200 loss:        0.568396
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.836280 loss:        0.476076
Test - acc:         0.768400 loss:        0.720073
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.839420 loss:        0.464692
Test - acc:         0.759300 loss:        0.735929
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845000 loss:        0.453000
Test - acc:         0.824500 loss:        0.526457
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.849680 loss:        0.444501
Test - acc:         0.826500 loss:        0.520368
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.849380 loss:        0.438538
Test - acc:         0.798100 loss:        0.637503
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.850040 loss:        0.440032
Test - acc:         0.829700 loss:        0.507356
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.854180 loss:        0.426594
Test - acc:         0.802500 loss:        0.589427
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.855560 loss:        0.423533
Test - acc:         0.786400 loss:        0.632288
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.855960 loss:        0.420786
Test - acc:         0.789600 loss:        0.646576
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.858360 loss:        0.416946
Test - acc:         0.752800 loss:        0.756219
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.860040 loss:        0.415164
Test - acc:         0.812200 loss:        0.558783
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.858620 loss:        0.414178
Test - acc:         0.814800 loss:        0.577834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.859180 loss:        0.410590
Test - acc:         0.813800 loss:        0.559850
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.860320 loss:        0.407069
Test - acc:         0.851000 loss:        0.442927
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.857900 loss:        0.414354
Test - acc:         0.838400 loss:        0.479358
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.861760 loss:        0.403122
Test - acc:         0.813600 loss:        0.552473
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.864300 loss:        0.400279
Test - acc:         0.798500 loss:        0.615302
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.864260 loss:        0.395054
Test - acc:         0.802800 loss:        0.596695
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.862920 loss:        0.397425
Test - acc:         0.813300 loss:        0.539908
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.863160 loss:        0.393080
Test - acc:         0.815100 loss:        0.580103
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.865860 loss:        0.394840
Test - acc:         0.824300 loss:        0.534873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.864560 loss:        0.393687
Test - acc:         0.828400 loss:        0.520443
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.867500 loss:        0.385755
Test - acc:         0.811500 loss:        0.577941
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.864660 loss:        0.392658
Test - acc:         0.724200 loss:        0.831120
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.866840 loss:        0.389900
Test - acc:         0.847400 loss:        0.461297
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.869240 loss:        0.384570
Test - acc:         0.804400 loss:        0.615804
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.868220 loss:        0.392120
Test - acc:         0.835200 loss:        0.483585
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.868400 loss:        0.386378
Test - acc:         0.832600 loss:        0.516400
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.871260 loss:        0.379740
Test - acc:         0.803600 loss:        0.591817
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.868820 loss:        0.384890
Test - acc:         0.824800 loss:        0.553375
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.867840 loss:        0.387072
Test - acc:         0.832100 loss:        0.534749
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.869020 loss:        0.387383
Test - acc:         0.812300 loss:        0.552859
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.868060 loss:        0.383151
Test - acc:         0.788200 loss:        0.640874
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.870100 loss:        0.382418
Test - acc:         0.809300 loss:        0.578768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.871020 loss:        0.381197
Test - acc:         0.852600 loss:        0.426795
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.869460 loss:        0.380910
Test - acc:         0.806300 loss:        0.583816
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.871240 loss:        0.376807
Test - acc:         0.826200 loss:        0.532606
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.870840 loss:        0.376128
Test - acc:         0.741300 loss:        0.833599
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.871800 loss:        0.376930
Test - acc:         0.803300 loss:        0.625899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.872840 loss:        0.372406
Test - acc:         0.805600 loss:        0.615994
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.872480 loss:        0.377339
Test - acc:         0.808300 loss:        0.567164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.872680 loss:        0.372489
Test - acc:         0.817000 loss:        0.557201
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.872240 loss:        0.375717
Test - acc:         0.820600 loss:        0.541011
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.871320 loss:        0.375434
Test - acc:         0.817900 loss:        0.565509
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.871360 loss:        0.374007
Test - acc:         0.827800 loss:        0.531900
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.873820 loss:        0.373272
Test - acc:         0.821500 loss:        0.544774
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.872140 loss:        0.377352
Test - acc:         0.797900 loss:        0.611422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.870440 loss:        0.378227
Test - acc:         0.837500 loss:        0.477486
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.871900 loss:        0.375845
Test - acc:         0.833100 loss:        0.488383
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.874100 loss:        0.367354
Test - acc:         0.826500 loss:        0.539295
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.871080 loss:        0.375922
Test - acc:         0.841800 loss:        0.473066
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.874420 loss:        0.369489
Test - acc:         0.840400 loss:        0.472459
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.874260 loss:        0.373805
Test - acc:         0.819500 loss:        0.551145
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.874260 loss:        0.371657
Test - acc:         0.854800 loss:        0.430678
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.874440 loss:        0.364968
Test - acc:         0.855400 loss:        0.423692
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.870380 loss:        0.378505
Test - acc:         0.794900 loss:        0.651736
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.871280 loss:        0.374141
Test - acc:         0.820000 loss:        0.559046
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.873240 loss:        0.370397
Test - acc:         0.806100 loss:        0.632268
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.874620 loss:        0.367881
Test - acc:         0.857500 loss:        0.429875
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.874280 loss:        0.370817
Test - acc:         0.826000 loss:        0.523220
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.874020 loss:        0.368452
Test - acc:         0.835100 loss:        0.523028
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.873600 loss:        0.374590
Test - acc:         0.826800 loss:        0.531651
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.874400 loss:        0.370043
Test - acc:         0.807200 loss:        0.664722
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.874180 loss:        0.370537
Test - acc:         0.822800 loss:        0.571619
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.874600 loss:        0.366761
Test - acc:         0.836000 loss:        0.495049
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.873540 loss:        0.372187
Test - acc:         0.826100 loss:        0.527888
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.872460 loss:        0.373549
Test - acc:         0.766800 loss:        0.795014
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.874640 loss:        0.366928
Test - acc:         0.828800 loss:        0.519656
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.874720 loss:        0.367976
Test - acc:         0.820000 loss:        0.568690
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.874220 loss:        0.370904
Test - acc:         0.820300 loss:        0.575675
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.874780 loss:        0.367725
Test - acc:         0.844600 loss:        0.476037
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.875320 loss:        0.367132
Test - acc:         0.834100 loss:        0.508926
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.873220 loss:        0.369282
Test - acc:         0.792800 loss:        0.655586
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.876020 loss:        0.363000
Test - acc:         0.809100 loss:        0.580211
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.874320 loss:        0.370175
Test - acc:         0.829300 loss:        0.512752
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.872960 loss:        0.370879
Test - acc:         0.865500 loss:        0.396209
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.873320 loss:        0.371317
Test - acc:         0.835700 loss:        0.485132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.876060 loss:        0.364766
Test - acc:         0.812000 loss:        0.566341
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.873920 loss:        0.370142
Test - acc:         0.821300 loss:        0.544474
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.875960 loss:        0.366758
Test - acc:         0.813800 loss:        0.569448
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.875080 loss:        0.367920
Test - acc:         0.831400 loss:        0.516796
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.873840 loss:        0.368995
Test - acc:         0.762900 loss:        0.744116
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.876640 loss:        0.364468
Test - acc:         0.788400 loss:        0.622598
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.874340 loss:        0.369125
Test - acc:         0.792900 loss:        0.647570
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.873940 loss:        0.369864
Test - acc:         0.741700 loss:        0.853622
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.874500 loss:        0.367637
Test - acc:         0.840600 loss:        0.503573
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.876660 loss:        0.362519
Test - acc:         0.784900 loss:        0.649397
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.875220 loss:        0.365134
Test - acc:         0.849800 loss:        0.460079
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.877000 loss:        0.363290
Test - acc:         0.843100 loss:        0.459065
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.876500 loss:        0.362516
Test - acc:         0.798200 loss:        0.621677
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.876840 loss:        0.367939
Test - acc:         0.783400 loss:        0.719071
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.875520 loss:        0.365693
Test - acc:         0.821600 loss:        0.536567
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.875060 loss:        0.366684
Test - acc:         0.849600 loss:        0.438157
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.875340 loss:        0.364328
Test - acc:         0.821600 loss:        0.533868
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.878300 loss:        0.358434
Test - acc:         0.837800 loss:        0.494301
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.878940 loss:        0.358265
Test - acc:         0.685600 loss:        1.170237
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.874260 loss:        0.366177
Test - acc:         0.845500 loss:        0.471626
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.874340 loss:        0.364436
Test - acc:         0.807000 loss:        0.590794
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.876520 loss:        0.362383
Test - acc:         0.849900 loss:        0.454454
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.876160 loss:        0.367129
Test - acc:         0.717600 loss:        0.997870
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.875200 loss:        0.364394
Test - acc:         0.819600 loss:        0.558280
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.876060 loss:        0.363108
Test - acc:         0.847200 loss:        0.453666
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.876240 loss:        0.363968
Test - acc:         0.833100 loss:        0.505149
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.875420 loss:        0.364747
Test - acc:         0.767000 loss:        0.785326
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.883240 loss:        0.338391
Test - acc:         0.852000 loss:        0.454869
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.882580 loss:        0.342546
Test - acc:         0.798700 loss:        0.637609
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.882560 loss:        0.345149
Test - acc:         0.827400 loss:        0.514371
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.881960 loss:        0.346254
Test - acc:         0.836200 loss:        0.496084
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.884600 loss:        0.341021
Test - acc:         0.834500 loss:        0.491920
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.881920 loss:        0.349473
Test - acc:         0.850700 loss:        0.443752
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.881760 loss:        0.349107
Test - acc:         0.823200 loss:        0.522656
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.880500 loss:        0.348541
Test - acc:         0.854900 loss:        0.445072
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.880580 loss:        0.348892
Test - acc:         0.836800 loss:        0.491638
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.881160 loss:        0.344809
Test - acc:         0.772800 loss:        0.741065
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.880320 loss:        0.350602
Test - acc:         0.849600 loss:        0.452903
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.881220 loss:        0.347308
Test - acc:         0.820500 loss:        0.530637
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.881540 loss:        0.346632
Test - acc:         0.824600 loss:        0.534990
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.880920 loss:        0.346835
Test - acc:         0.783400 loss:        0.671895
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.880520 loss:        0.353015
Test - acc:         0.842400 loss:        0.487955
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.882600 loss:        0.344245
Test - acc:         0.788400 loss:        0.684610
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.880400 loss:        0.349286
Test - acc:         0.832600 loss:        0.507095
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.880140 loss:        0.350351
Test - acc:         0.792800 loss:        0.640005
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.880880 loss:        0.350102
Test - acc:         0.841700 loss:        0.480683
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.880220 loss:        0.349694
Test - acc:         0.826300 loss:        0.525899
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.881820 loss:        0.347347
Test - acc:         0.833600 loss:        0.510363
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.880160 loss:        0.351085
Test - acc:         0.819400 loss:        0.534988
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.880100 loss:        0.347485
Test - acc:         0.814700 loss:        0.556824
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.880700 loss:        0.350231
Test - acc:         0.835500 loss:        0.514561
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.879100 loss:        0.352469
Test - acc:         0.843600 loss:        0.464762
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.881440 loss:        0.350751
Test - acc:         0.854100 loss:        0.428597
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.881160 loss:        0.349412
Test - acc:         0.797600 loss:        0.635932
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.880180 loss:        0.350191
Test - acc:         0.823600 loss:        0.534491
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.880060 loss:        0.348751
Test - acc:         0.834200 loss:        0.495888
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.880400 loss:        0.349378
Test - acc:         0.825800 loss:        0.536803
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.881840 loss:        0.347389
Test - acc:         0.851700 loss:        0.461390
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.879140 loss:        0.351961
Test - acc:         0.825300 loss:        0.546392
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.878980 loss:        0.355512
Test - acc:         0.845000 loss:        0.468756
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.932360 loss:        0.200021
Test - acc:         0.919500 loss:        0.235483
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.947840 loss:        0.155154
Test - acc:         0.923300 loss:        0.224219
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.954020 loss:        0.134392
Test - acc:         0.926600 loss:        0.219553
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.958240 loss:        0.122658
Test - acc:         0.929600 loss:        0.214518
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.961160 loss:        0.114400
Test - acc:         0.928300 loss:        0.214305
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.965100 loss:        0.103664
Test - acc:         0.932700 loss:        0.220241
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.967840 loss:        0.095300
Test - acc:         0.928800 loss:        0.223255
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.970100 loss:        0.089393
Test - acc:         0.927700 loss:        0.232392
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.970760 loss:        0.085385
Test - acc:         0.928700 loss:        0.221673
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.974100 loss:        0.077790
Test - acc:         0.933600 loss:        0.220098
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.974860 loss:        0.074081
Test - acc:         0.927400 loss:        0.235311
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.975880 loss:        0.068248
Test - acc:         0.928800 loss:        0.230729
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.976960 loss:        0.068095
Test - acc:         0.932800 loss:        0.228447
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.979460 loss:        0.059964
Test - acc:         0.926600 loss:        0.244222
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.978600 loss:        0.063128
Test - acc:         0.927800 loss:        0.246639
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.977440 loss:        0.063939
Test - acc:         0.924900 loss:        0.261829
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.978560 loss:        0.061421
Test - acc:         0.933800 loss:        0.240253
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.981100 loss:        0.055938
Test - acc:         0.931400 loss:        0.233896
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.980900 loss:        0.056858
Test - acc:         0.928700 loss:        0.238814
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.982380 loss:        0.051271
Test - acc:         0.930400 loss:        0.249169
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.980880 loss:        0.057502
Test - acc:         0.923800 loss:        0.271843
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.979560 loss:        0.060160
Test - acc:         0.926700 loss:        0.258766
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.979560 loss:        0.058781
Test - acc:         0.926600 loss:        0.242681
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.982420 loss:        0.052812
Test - acc:         0.929000 loss:        0.248462
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.980800 loss:        0.056153
Test - acc:         0.925800 loss:        0.263580
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.980840 loss:        0.056959
Test - acc:         0.925500 loss:        0.259580
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.979940 loss:        0.058101
Test - acc:         0.928500 loss:        0.253145
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.980240 loss:        0.057559
Test - acc:         0.923300 loss:        0.270639
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.981500 loss:        0.054800
Test - acc:         0.926800 loss:        0.257985
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.979720 loss:        0.059411
Test - acc:         0.919100 loss:        0.284198
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.980320 loss:        0.058918
Test - acc:         0.927200 loss:        0.247335
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.979860 loss:        0.058560
Test - acc:         0.922600 loss:        0.269992
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.980080 loss:        0.059662
Test - acc:         0.924500 loss:        0.272011
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.980160 loss:        0.059000
Test - acc:         0.925500 loss:        0.252465
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.978540 loss:        0.063705
Test - acc:         0.924600 loss:        0.254789
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.979660 loss:        0.059991
Test - acc:         0.923600 loss:        0.270613
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.980580 loss:        0.057565
Test - acc:         0.926100 loss:        0.265052
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.977560 loss:        0.064277
Test - acc:         0.915300 loss:        0.307039
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.977620 loss:        0.064168
Test - acc:         0.910500 loss:        0.307953
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.061413
Test - acc:         0.922900 loss:        0.260805
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.977740 loss:        0.064976
Test - acc:         0.922700 loss:        0.270670
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.976940 loss:        0.067103
Test - acc:         0.924000 loss:        0.268608
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.977460 loss:        0.067256
Test - acc:         0.924400 loss:        0.261361
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.978640 loss:        0.062319
Test - acc:         0.921100 loss:        0.274594
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.978000 loss:        0.063354
Test - acc:         0.915000 loss:        0.301696
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.976540 loss:        0.069140
Test - acc:         0.918900 loss:        0.278061
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.978480 loss:        0.065507
Test - acc:         0.920000 loss:        0.271733
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.977600 loss:        0.066210
Test - acc:         0.922800 loss:        0.267881
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.978680 loss:        0.064914
Test - acc:         0.920100 loss:        0.288718
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.978500 loss:        0.065676
Test - acc:         0.904500 loss:        0.355385
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.977600 loss:        0.065671
Test - acc:         0.920400 loss:        0.274626
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.976680 loss:        0.068993
Test - acc:         0.921300 loss:        0.271799
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.977360 loss:        0.066555
Test - acc:         0.918900 loss:        0.286989
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.977840 loss:        0.065907
Test - acc:         0.926200 loss:        0.251695
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.977620 loss:        0.067070
Test - acc:         0.920400 loss:        0.275881
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.977080 loss:        0.066166
Test - acc:         0.921900 loss:        0.269393
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.978140 loss:        0.064950
Test - acc:         0.913500 loss:        0.301644
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.978760 loss:        0.062815
Test - acc:         0.920800 loss:        0.279042
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.977060 loss:        0.066039
Test - acc:         0.920500 loss:        0.290347
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.976980 loss:        0.068662
Test - acc:         0.912900 loss:        0.306645
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.978760 loss:        0.065344
Test - acc:         0.919600 loss:        0.279524
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.978300 loss:        0.064465
Test - acc:         0.922600 loss:        0.262682
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.978220 loss:        0.065798
Test - acc:         0.916600 loss:        0.303472
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.976600 loss:        0.068101
Test - acc:         0.921000 loss:        0.285169
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.978780 loss:        0.063618
Test - acc:         0.918400 loss:        0.290861
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.978900 loss:        0.063203
Test - acc:         0.921900 loss:        0.279779
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.979800 loss:        0.060749
Test - acc:         0.920700 loss:        0.289010
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.974860 loss:        0.072391
Test - acc:         0.917900 loss:        0.281463
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.977340 loss:        0.066038
Test - acc:         0.912900 loss:        0.311378
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.978720 loss:        0.063543
Test - acc:         0.912800 loss:        0.314357
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.977340 loss:        0.065746
Test - acc:         0.920300 loss:        0.292235
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.977440 loss:        0.067587
Test - acc:         0.915800 loss:        0.301449
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.978060 loss:        0.064858
Test - acc:         0.921100 loss:        0.280273
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.978420 loss:        0.062769
Test - acc:         0.921300 loss:        0.279162
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.977300 loss:        0.066286
Test - acc:         0.902700 loss:        0.350259
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.978720 loss:        0.063681
Test - acc:         0.914100 loss:        0.298848
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.980140 loss:        0.061326
Test - acc:         0.921600 loss:        0.273615
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.978400 loss:        0.064608
Test - acc:         0.923400 loss:        0.259269
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.979200 loss:        0.063356
Test - acc:         0.921300 loss:        0.272946
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.980580 loss:        0.059615
Test - acc:         0.921000 loss:        0.287902
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.976900 loss:        0.067302
Test - acc:         0.915100 loss:        0.289632
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.977160 loss:        0.066481
Test - acc:         0.922500 loss:        0.272757
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.978540 loss:        0.062891
Test - acc:         0.920000 loss:        0.288856
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.978400 loss:        0.064413
Test - acc:         0.919600 loss:        0.273521
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.986200 loss:        0.042983
Test - acc:         0.927000 loss:        0.266073
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.986780 loss:        0.040838
Test - acc:         0.923200 loss:        0.278973
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.984620 loss:        0.046482
Test - acc:         0.924100 loss:        0.267537
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.986740 loss:        0.039972
Test - acc:         0.924000 loss:        0.282420
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.984280 loss:        0.048409
Test - acc:         0.918700 loss:        0.289123
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.985440 loss:        0.045021
Test - acc:         0.927900 loss:        0.262723
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.984440 loss:        0.046412
Test - acc:         0.926700 loss:        0.269725
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.985020 loss:        0.047734
Test - acc:         0.921100 loss:        0.290394
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.984160 loss:        0.049763
Test - acc:         0.925000 loss:        0.272558
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.984160 loss:        0.047879
Test - acc:         0.912400 loss:        0.336120
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.983640 loss:        0.049142
Test - acc:         0.923600 loss:        0.278438
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.983500 loss:        0.049337
Test - acc:         0.920700 loss:        0.293286
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.983800 loss:        0.047789
Test - acc:         0.924800 loss:        0.280673
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.982440 loss:        0.051291
Test - acc:         0.919900 loss:        0.302004
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.981340 loss:        0.056596
Test - acc:         0.919600 loss:        0.279650
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.983220 loss:        0.051326
Test - acc:         0.918300 loss:        0.304628
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.992740 loss:        0.024980
Test - acc:         0.939000 loss:        0.226486
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.995920 loss:        0.016167
Test - acc:         0.941600 loss:        0.221546
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.996580 loss:        0.014018
Test - acc:         0.941600 loss:        0.223465
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.997680 loss:        0.011300
Test - acc:         0.944600 loss:        0.221289
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.997760 loss:        0.010167
Test - acc:         0.944300 loss:        0.219398
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.998500 loss:        0.009071
Test - acc:         0.944700 loss:        0.218955
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.998360 loss:        0.008576
Test - acc:         0.944100 loss:        0.218564
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.998780 loss:        0.007662
Test - acc:         0.944000 loss:        0.219883
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.007420
Test - acc:         0.944800 loss:        0.217841
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.007439
Test - acc:         0.942800 loss:        0.220705
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.007037
Test - acc:         0.943900 loss:        0.218947
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.006283
Test - acc:         0.944500 loss:        0.217952
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.005999
Test - acc:         0.945600 loss:        0.219416
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.998940 loss:        0.006447
Test - acc:         0.945700 loss:        0.219314
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.999160 loss:        0.005954
Test - acc:         0.945600 loss:        0.217460
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.999260 loss:        0.005537
Test - acc:         0.946400 loss:        0.214592
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999180 loss:        0.005484
Test - acc:         0.945000 loss:        0.218740
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.005451
Test - acc:         0.944800 loss:        0.216909
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.005391
Test - acc:         0.945000 loss:        0.218348
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.005260
Test - acc:         0.945300 loss:        0.218365
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.004939
Test - acc:         0.946200 loss:        0.218849
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.004780
Test - acc:         0.946300 loss:        0.218201
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.004634
Test - acc:         0.946400 loss:        0.217811
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.005052
Test - acc:         0.946000 loss:        0.217229
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.004622
Test - acc:         0.945600 loss:        0.219948
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.004624
Test - acc:         0.944700 loss:        0.217404
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.004480
Test - acc:         0.946200 loss:        0.216670
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004236
Test - acc:         0.945400 loss:        0.218531
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.004277
Test - acc:         0.945800 loss:        0.216095
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.004176
Test - acc:         0.946100 loss:        0.216483
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.004256
Test - acc:         0.947000 loss:        0.215444
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.003937
Test - acc:         0.947000 loss:        0.215833
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.003699
Test - acc:         0.948200 loss:        0.213516
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004026
Test - acc:         0.947000 loss:        0.214675
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003766
Test - acc:         0.947300 loss:        0.213523
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.003533
Test - acc:         0.946300 loss:        0.214881
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.003738
Test - acc:         0.947100 loss:        0.214524
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003610
Test - acc:         0.947600 loss:        0.215375
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003545
Test - acc:         0.946700 loss:        0.214483
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003549
Test - acc:         0.947000 loss:        0.214330
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003618
Test - acc:         0.945400 loss:        0.217080
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.003658
Test - acc:         0.946600 loss:        0.215709
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.003445
Test - acc:         0.947100 loss:        0.214446
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003393
Test - acc:         0.947700 loss:        0.216124
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.003414
Test - acc:         0.946000 loss:        0.215561
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.003358
Test - acc:         0.947500 loss:        0.213024
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.003690
Test - acc:         0.946800 loss:        0.213366
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003295
Test - acc:         0.947300 loss:        0.213423
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003552
Test - acc:         0.948600 loss:        0.212913
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003407
Test - acc:         0.948100 loss:        0.214094
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.003061
Test - acc:         0.947900 loss:        0.213444
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.003332
Test - acc:         0.948300 loss:        0.214283
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.003211
Test - acc:         0.947500 loss:        0.212820
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.003366
Test - acc:         0.947800 loss:        0.212532
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.003125
Test - acc:         0.947700 loss:        0.213621
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.002945
Test - acc:         0.947100 loss:        0.215079
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.003106
Test - acc:         0.946800 loss:        0.216210
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003374
Test - acc:         0.946800 loss:        0.214994
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.003046
Test - acc:         0.947600 loss:        0.214013
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.003155
Test - acc:         0.946300 loss:        0.213651
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002873
Test - acc:         0.947300 loss:        0.212052
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.002996
Test - acc:         0.946600 loss:        0.213420
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002985
Test - acc:         0.947800 loss:        0.211335
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.003065
Test - acc:         0.947300 loss:        0.211699
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.002893
Test - acc:         0.947500 loss:        0.211857
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002917
Test - acc:         0.947700 loss:        0.212568
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.002890
Test - acc:         0.947300 loss:        0.213288
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002988
Test - acc:         0.946800 loss:        0.212415
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.002867
Test - acc:         0.947900 loss:        0.210506
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002868
Test - acc:         0.948200 loss:        0.211348
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002609
Test - acc:         0.947200 loss:        0.210010
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.002885
Test - acc:         0.948000 loss:        0.210525
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002805
Test - acc:         0.948400 loss:        0.210417
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002870
Test - acc:         0.948000 loss:        0.210773
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002837
Test - acc:         0.948200 loss:        0.210840
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002724
Test - acc:         0.948000 loss:        0.210783
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002670
Test - acc:         0.947000 loss:        0.213564
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002850
Test - acc:         0.947800 loss:        0.211720
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002575
Test - acc:         0.948200 loss:        0.209636
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002942
Test - acc:         0.947900 loss:        0.211988
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002656
Test - acc:         0.948000 loss:        0.210664
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002668
Test - acc:         0.947300 loss:        0.210604
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.002898
Test - acc:         0.947300 loss:        0.210532
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002886
Test - acc:         0.947500 loss:        0.210993
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002598
Test - acc:         0.946700 loss:        0.211718
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002686
Test - acc:         0.946600 loss:        0.210924
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002595
Test - acc:         0.946700 loss:        0.210814
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002777
Test - acc:         0.948400 loss:        0.209519
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002408
Test - acc:         0.948000 loss:        0.209246
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002612
Test - acc:         0.947700 loss:        0.209915
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002566
Test - acc:         0.948000 loss:        0.209554
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002591
Test - acc:         0.948000 loss:        0.209860
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002753
Test - acc:         0.948200 loss:        0.209473
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002658
Test - acc:         0.947500 loss:        0.207503
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002658
Test - acc:         0.947800 loss:        0.207679
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002668
Test - acc:         0.948100 loss:        0.206769
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002494
Test - acc:         0.947000 loss:        0.210555
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002710
Test - acc:         0.947400 loss:        0.211978
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002551
Test - acc:         0.947600 loss:        0.208025
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002530
Test - acc:         0.948500 loss:        0.207791
Sparsity :          0.7500
Wdecay :        0.000500
