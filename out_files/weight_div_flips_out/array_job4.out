Running --model resnet18 --noise --prune_criterion weight_div_flips --seed 42 --prune_freq 39 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=39_seed=42 --save_model=pre-finetune/resnet18_weight_div_flips_pf39_s42 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf39_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.319760 loss:        2.027701
Test - acc:         0.373600 loss:        1.709656
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.483520 loss:        1.421054
Test - acc:         0.526700 loss:        1.263736
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.596320 loss:        1.129661
Test - acc:         0.595600 loss:        1.171379
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.673280 loss:        0.927933
Test - acc:         0.610100 loss:        1.134739
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.723460 loss:        0.790148
Test - acc:         0.705200 loss:        0.872062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.766400 loss:        0.675023
Test - acc:         0.746900 loss:        0.720768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.790000 loss:        0.608257
Test - acc:         0.766300 loss:        0.692052
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.805140 loss:        0.568890
Test - acc:         0.777000 loss:        0.661023
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.812440 loss:        0.540974
Test - acc:         0.748500 loss:        0.765813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.823580 loss:        0.513555
Test - acc:         0.742700 loss:        0.756514
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.831620 loss:        0.490783
Test - acc:         0.778800 loss:        0.671311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.834560 loss:        0.478143
Test - acc:         0.793000 loss:        0.626211
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.841480 loss:        0.462775
Test - acc:         0.792800 loss:        0.604948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838760 loss:        0.468699
Test - acc:         0.789100 loss:        0.624176
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844780 loss:        0.450396
Test - acc:         0.816100 loss:        0.559841
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.846280 loss:        0.448807
Test - acc:         0.764600 loss:        0.719807
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.850540 loss:        0.438978
Test - acc:         0.818300 loss:        0.530640
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.850840 loss:        0.433470
Test - acc:         0.796600 loss:        0.604748
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.853620 loss:        0.425959
Test - acc:         0.841700 loss:        0.488544
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.854620 loss:        0.421329
Test - acc:         0.795500 loss:        0.623324
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.859220 loss:        0.417579
Test - acc:         0.823000 loss:        0.516899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.856440 loss:        0.420470
Test - acc:         0.758600 loss:        0.712926
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.858640 loss:        0.411732
Test - acc:         0.793800 loss:        0.625225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.858100 loss:        0.413444
Test - acc:         0.793100 loss:        0.628817
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.862000 loss:        0.407803
Test - acc:         0.819100 loss:        0.573117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.861660 loss:        0.407108
Test - acc:         0.805500 loss:        0.599522
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.864180 loss:        0.397832
Test - acc:         0.836000 loss:        0.468893
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.863120 loss:        0.402166
Test - acc:         0.833700 loss:        0.486745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.863140 loss:        0.396906
Test - acc:         0.855500 loss:        0.423829
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.867600 loss:        0.394886
Test - acc:         0.810100 loss:        0.579276
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.865620 loss:        0.394207
Test - acc:         0.853400 loss:        0.434255
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.866820 loss:        0.393427
Test - acc:         0.698200 loss:        1.116456
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.866560 loss:        0.390132
Test - acc:         0.833300 loss:        0.506263
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.866820 loss:        0.389891
Test - acc:         0.838100 loss:        0.489114
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.868660 loss:        0.383678
Test - acc:         0.817400 loss:        0.559010
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.384254
Test - acc:         0.822300 loss:        0.562526
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.866740 loss:        0.387204
Test - acc:         0.820700 loss:        0.539781
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.868140 loss:        0.390223
Test - acc:         0.839300 loss:        0.510206
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.866480 loss:        0.388159
Test - acc:         0.846800 loss:        0.458624
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.879780 loss:        0.350265
Test - acc:         0.806600 loss:        0.598913
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.878000 loss:        0.359557
Test - acc:         0.796000 loss:        0.651818
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.876240 loss:        0.360435
Test - acc:         0.794000 loss:        0.619637
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.878860 loss:        0.357716
Test - acc:         0.841700 loss:        0.470049
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.875740 loss:        0.365612
Test - acc:         0.824400 loss:        0.525015
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.873320 loss:        0.368278
Test - acc:         0.838200 loss:        0.491423
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.875380 loss:        0.364585
Test - acc:         0.830500 loss:        0.527031
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.874740 loss:        0.367417
Test - acc:         0.830400 loss:        0.523969
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.874380 loss:        0.366340
Test - acc:         0.791900 loss:        0.628040
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.875840 loss:        0.362732
Test - acc:         0.809200 loss:        0.597018
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.874060 loss:        0.365828
Test - acc:         0.794300 loss:        0.657354
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.874860 loss:        0.362596
Test - acc:         0.835300 loss:        0.507801
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.876060 loss:        0.362162
Test - acc:         0.862100 loss:        0.414540
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.878140 loss:        0.357131
Test - acc:         0.851500 loss:        0.438911
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.879220 loss:        0.354466
Test - acc:         0.814700 loss:        0.553604
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.875680 loss:        0.363197
Test - acc:         0.853900 loss:        0.433712
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.876980 loss:        0.358991
Test - acc:         0.813500 loss:        0.585572
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.876660 loss:        0.359887
Test - acc:         0.837800 loss:        0.481237
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.879440 loss:        0.354272
Test - acc:         0.832200 loss:        0.507592
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.876740 loss:        0.355833
Test - acc:         0.864700 loss:        0.415490
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.875020 loss:        0.365471
Test - acc:         0.830800 loss:        0.499917
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.876900 loss:        0.360581
Test - acc:         0.823200 loss:        0.533838
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.875800 loss:        0.362874
Test - acc:         0.813600 loss:        0.586353
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.877260 loss:        0.360224
Test - acc:         0.814300 loss:        0.568829
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.876400 loss:        0.359267
Test - acc:         0.829600 loss:        0.537334
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.877880 loss:        0.362426
Test - acc:         0.823500 loss:        0.540909
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.877040 loss:        0.358762
Test - acc:         0.829100 loss:        0.514156
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.878660 loss:        0.357828
Test - acc:         0.819500 loss:        0.541911
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.874720 loss:        0.365669
Test - acc:         0.815700 loss:        0.574540
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.878240 loss:        0.357313
Test - acc:         0.839500 loss:        0.492911
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.876280 loss:        0.363165
Test - acc:         0.803100 loss:        0.624457
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.877020 loss:        0.360618
Test - acc:         0.826200 loss:        0.532254
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.876800 loss:        0.354713
Test - acc:         0.818900 loss:        0.541154
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.876600 loss:        0.361106
Test - acc:         0.854000 loss:        0.430403
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.877360 loss:        0.361108
Test - acc:         0.850600 loss:        0.453860
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.879160 loss:        0.350782
Test - acc:         0.835600 loss:        0.533768
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.876480 loss:        0.360741
Test - acc:         0.823600 loss:        0.533071
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.876240 loss:        0.359799
Test - acc:         0.805400 loss:        0.661717
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.878300 loss:        0.356194
Test - acc:         0.846000 loss:        0.445582
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.890360 loss:        0.321402
Test - acc:         0.855000 loss:        0.432866
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.887280 loss:        0.325538
Test - acc:         0.830500 loss:        0.506346
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.885840 loss:        0.334164
Test - acc:         0.819400 loss:        0.559520
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.884300 loss:        0.336532
Test - acc:         0.799200 loss:        0.635027
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.884580 loss:        0.337322
Test - acc:         0.835500 loss:        0.512842
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.885580 loss:        0.332148
Test - acc:         0.813900 loss:        0.605542
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.881340 loss:        0.340902
Test - acc:         0.846200 loss:        0.463425
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.884120 loss:        0.336803
Test - acc:         0.851100 loss:        0.441802
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.886660 loss:        0.333877
Test - acc:         0.840400 loss:        0.477421
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.887700 loss:        0.332616
Test - acc:         0.850100 loss:        0.452661
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.882360 loss:        0.340561
Test - acc:         0.806600 loss:        0.585778
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.884980 loss:        0.333019
Test - acc:         0.832800 loss:        0.524260
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.885600 loss:        0.339512
Test - acc:         0.837000 loss:        0.488423
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.885180 loss:        0.336980
Test - acc:         0.836400 loss:        0.480688
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.885400 loss:        0.333298
Test - acc:         0.836100 loss:        0.489745
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.881980 loss:        0.342973
Test - acc:         0.850400 loss:        0.449818
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.883340 loss:        0.340095
Test - acc:         0.852800 loss:        0.440619
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.883500 loss:        0.341087
Test - acc:         0.835100 loss:        0.515395
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.883780 loss:        0.340712
Test - acc:         0.856100 loss:        0.444638
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.883720 loss:        0.337723
Test - acc:         0.806500 loss:        0.595350
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.881840 loss:        0.343888
Test - acc:         0.841300 loss:        0.478492
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.883100 loss:        0.339640
Test - acc:         0.835600 loss:        0.507187
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.885480 loss:        0.335070
Test - acc:         0.837900 loss:        0.502071
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.882640 loss:        0.343163
Test - acc:         0.820300 loss:        0.577298
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.885160 loss:        0.334176
Test - acc:         0.834100 loss:        0.500814
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.884480 loss:        0.338174
Test - acc:         0.858900 loss:        0.423323
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.885320 loss:        0.336405
Test - acc:         0.834100 loss:        0.510032
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.882780 loss:        0.343617
Test - acc:         0.863500 loss:        0.405942
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.886420 loss:        0.334711
Test - acc:         0.814400 loss:        0.578921
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.883280 loss:        0.339971
Test - acc:         0.824100 loss:        0.554975
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.884020 loss:        0.337526
Test - acc:         0.841400 loss:        0.470041
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.882720 loss:        0.337347
Test - acc:         0.815200 loss:        0.577399
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.882480 loss:        0.339054
Test - acc:         0.828000 loss:        0.521769
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.885840 loss:        0.332006
Test - acc:         0.837500 loss:        0.503319
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.882760 loss:        0.340767
Test - acc:         0.841100 loss:        0.497879
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.884640 loss:        0.336490
Test - acc:         0.845800 loss:        0.465273
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.884660 loss:        0.338447
Test - acc:         0.849500 loss:        0.442910
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.884220 loss:        0.337483
Test - acc:         0.840600 loss:        0.481242
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.887220 loss:        0.333388
Test - acc:         0.859500 loss:        0.417976
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.896380 loss:        0.299623
Test - acc:         0.810000 loss:        0.588509
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.894180 loss:        0.307575
Test - acc:         0.782300 loss:        0.718621
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.892940 loss:        0.312178
Test - acc:         0.831000 loss:        0.532258
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.891020 loss:        0.312859
Test - acc:         0.847900 loss:        0.454824
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.893140 loss:        0.310701
Test - acc:         0.871600 loss:        0.390532
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.892600 loss:        0.313860
Test - acc:         0.859600 loss:        0.408898
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.891060 loss:        0.315155
Test - acc:         0.851700 loss:        0.463268
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.892540 loss:        0.314368
Test - acc:         0.861200 loss:        0.424475
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.889680 loss:        0.316893
Test - acc:         0.845700 loss:        0.472297
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.892660 loss:        0.312968
Test - acc:         0.830300 loss:        0.542722
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.891940 loss:        0.315942
Test - acc:         0.852200 loss:        0.449998
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.891680 loss:        0.316590
Test - acc:         0.837500 loss:        0.500606
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.891900 loss:        0.316865
Test - acc:         0.831300 loss:        0.499309
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.890040 loss:        0.316128
Test - acc:         0.860900 loss:        0.428686
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.891120 loss:        0.317638
Test - acc:         0.848400 loss:        0.457400
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.892560 loss:        0.312864
Test - acc:         0.827400 loss:        0.544775
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.890880 loss:        0.318585
Test - acc:         0.849200 loss:        0.486883
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.891700 loss:        0.315561
Test - acc:         0.840300 loss:        0.489615
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.893040 loss:        0.308984
Test - acc:         0.846900 loss:        0.448631
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.889420 loss:        0.317201
Test - acc:         0.836600 loss:        0.505656
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.891000 loss:        0.318170
Test - acc:         0.858800 loss:        0.423611
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.890900 loss:        0.315322
Test - acc:         0.823400 loss:        0.561626
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.888920 loss:        0.320836
Test - acc:         0.830300 loss:        0.525189
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.892000 loss:        0.317176
Test - acc:         0.841700 loss:        0.475304
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.892520 loss:        0.313371
Test - acc:         0.829600 loss:        0.509754
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.890680 loss:        0.314478
Test - acc:         0.851600 loss:        0.445800
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.892480 loss:        0.312939
Test - acc:         0.847200 loss:        0.463338
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.892020 loss:        0.316676
Test - acc:         0.851100 loss:        0.480795
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.891660 loss:        0.317363
Test - acc:         0.856200 loss:        0.434531
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.891620 loss:        0.316429
Test - acc:         0.839600 loss:        0.511368
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.891860 loss:        0.314705
Test - acc:         0.851000 loss:        0.451509
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.891460 loss:        0.316847
Test - acc:         0.846600 loss:        0.455694
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.892120 loss:        0.313163
Test - acc:         0.860400 loss:        0.420253
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.937460 loss:        0.186703
Test - acc:         0.920800 loss:        0.230383
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.951700 loss:        0.144051
Test - acc:         0.924400 loss:        0.218843
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.957860 loss:        0.125944
Test - acc:         0.927700 loss:        0.209572
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.961940 loss:        0.113782
Test - acc:         0.929700 loss:        0.212908
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.965300 loss:        0.106040
Test - acc:         0.931000 loss:        0.211715
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.966960 loss:        0.096889
Test - acc:         0.928800 loss:        0.213457
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.969600 loss:        0.090577
Test - acc:         0.929100 loss:        0.208604
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.971140 loss:        0.085947
Test - acc:         0.931800 loss:        0.205059
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.972460 loss:        0.082007
Test - acc:         0.927700 loss:        0.219272
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.974880 loss:        0.075336
Test - acc:         0.930200 loss:        0.218140
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.973960 loss:        0.075171
Test - acc:         0.929500 loss:        0.218121
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.975840 loss:        0.071055
Test - acc:         0.929600 loss:        0.225528
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.977700 loss:        0.066650
Test - acc:         0.929500 loss:        0.222181
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.977820 loss:        0.065250
Test - acc:         0.929300 loss:        0.226245
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.980780 loss:        0.058423
Test - acc:         0.927500 loss:        0.227297
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.979900 loss:        0.060055
Test - acc:         0.930800 loss:        0.224645
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.979780 loss:        0.060017
Test - acc:         0.928500 loss:        0.239610
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.980600 loss:        0.058381
Test - acc:         0.927900 loss:        0.235610
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.980300 loss:        0.057831
Test - acc:         0.929500 loss:        0.238651
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.980980 loss:        0.057859
Test - acc:         0.927200 loss:        0.245943
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.981920 loss:        0.054701
Test - acc:         0.926200 loss:        0.240992
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.981860 loss:        0.054550
Test - acc:         0.928000 loss:        0.233766
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.981580 loss:        0.055373
Test - acc:         0.925800 loss:        0.253010
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.981960 loss:        0.053691
Test - acc:         0.927900 loss:        0.244661
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.981080 loss:        0.057207
Test - acc:         0.925400 loss:        0.253171
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.981980 loss:        0.054264
Test - acc:         0.928300 loss:        0.245625
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.982520 loss:        0.052232
Test - acc:         0.927200 loss:        0.248514
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.981900 loss:        0.053209
Test - acc:         0.924000 loss:        0.263935
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.982380 loss:        0.052324
Test - acc:         0.926100 loss:        0.258592
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.980960 loss:        0.057085
Test - acc:         0.926400 loss:        0.254617
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.981420 loss:        0.055480
Test - acc:         0.923900 loss:        0.259679
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.981360 loss:        0.055752
Test - acc:         0.924400 loss:        0.251651
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.981320 loss:        0.055123
Test - acc:         0.926600 loss:        0.252414
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.980100 loss:        0.058202
Test - acc:         0.928600 loss:        0.249428
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.980360 loss:        0.056059
Test - acc:         0.926100 loss:        0.251934
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.981400 loss:        0.056769
Test - acc:         0.920600 loss:        0.281471
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.979780 loss:        0.059912
Test - acc:         0.920200 loss:        0.290026
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.980320 loss:        0.058894
Test - acc:         0.922800 loss:        0.255516
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.979780 loss:        0.060201
Test - acc:         0.926000 loss:        0.249911
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.977140 loss:        0.065507
Test - acc:         0.918900 loss:        0.284648
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.979460 loss:        0.061010
Test - acc:         0.920500 loss:        0.274702
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.979720 loss:        0.060388
Test - acc:         0.930000 loss:        0.258435
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.979260 loss:        0.060600
Test - acc:         0.922400 loss:        0.267915
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.978040 loss:        0.063698
Test - acc:         0.921500 loss:        0.273263
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.977440 loss:        0.065480
Test - acc:         0.916900 loss:        0.288176
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.976960 loss:        0.068172
Test - acc:         0.928300 loss:        0.239631
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.979160 loss:        0.061279
Test - acc:         0.924500 loss:        0.253996
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.981800 loss:        0.056639
Test - acc:         0.923100 loss:        0.266130
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.979280 loss:        0.059363
Test - acc:         0.925300 loss:        0.261803
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.980920 loss:        0.058701
Test - acc:         0.924800 loss:        0.260096
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.981480 loss:        0.056755
Test - acc:         0.921400 loss:        0.268636
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.981080 loss:        0.057171
Test - acc:         0.923900 loss:        0.268154
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.980940 loss:        0.056903
Test - acc:         0.916900 loss:        0.295982
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.057997
Test - acc:         0.924100 loss:        0.256572
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.980060 loss:        0.060282
Test - acc:         0.924400 loss:        0.268073
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.979860 loss:        0.059604
Test - acc:         0.924400 loss:        0.261978
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.978780 loss:        0.062273
Test - acc:         0.920700 loss:        0.285570
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.980760 loss:        0.058696
Test - acc:         0.913200 loss:        0.306451
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.980140 loss:        0.058562
Test - acc:         0.918800 loss:        0.288751
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.978920 loss:        0.060908
Test - acc:         0.924600 loss:        0.265698
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.979080 loss:        0.061647
Test - acc:         0.923800 loss:        0.271404
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.058467
Test - acc:         0.923700 loss:        0.259650
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.979040 loss:        0.062384
Test - acc:         0.921600 loss:        0.277525
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.981040 loss:        0.056954
Test - acc:         0.927000 loss:        0.262991
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.978300 loss:        0.063746
Test - acc:         0.917900 loss:        0.278062
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.979120 loss:        0.062061
Test - acc:         0.916300 loss:        0.299704
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.978140 loss:        0.063667
Test - acc:         0.922200 loss:        0.268892
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.979040 loss:        0.062578
Test - acc:         0.923800 loss:        0.259296
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.979320 loss:        0.061585
Test - acc:         0.924700 loss:        0.263722
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.978400 loss:        0.063695
Test - acc:         0.913500 loss:        0.305711
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.979720 loss:        0.058942
Test - acc:         0.919600 loss:        0.273710
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.979980 loss:        0.058719
Test - acc:         0.921800 loss:        0.282605
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.978520 loss:        0.064856
Test - acc:         0.919600 loss:        0.276099
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.978580 loss:        0.063625
Test - acc:         0.921400 loss:        0.272833
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.978840 loss:        0.061299
Test - acc:         0.916900 loss:        0.303087
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.979760 loss:        0.060120
Test - acc:         0.919100 loss:        0.285511
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.978560 loss:        0.063080
Test - acc:         0.921700 loss:        0.274066
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.978180 loss:        0.063741
Test - acc:         0.917400 loss:        0.277848
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.979940 loss:        0.059919
Test - acc:         0.922500 loss:        0.271210
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.979340 loss:        0.060746
Test - acc:         0.920900 loss:        0.277840
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.979660 loss:        0.058931
Test - acc:         0.923100 loss:        0.275744
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.978920 loss:        0.062399
Test - acc:         0.920200 loss:        0.279375
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.979720 loss:        0.060030
Test - acc:         0.917300 loss:        0.291751
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.059201
Test - acc:         0.915800 loss:        0.290329
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.966480 loss:        0.098185
Test - acc:         0.919400 loss:        0.263798
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.971180 loss:        0.084114
Test - acc:         0.916100 loss:        0.278669
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.972700 loss:        0.080850
Test - acc:         0.926400 loss:        0.256056
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.974840 loss:        0.075095
Test - acc:         0.917800 loss:        0.267379
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.973360 loss:        0.076775
Test - acc:         0.917300 loss:        0.275853
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.975600 loss:        0.071815
Test - acc:         0.921900 loss:        0.279978
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.973840 loss:        0.076423
Test - acc:         0.924000 loss:        0.255814
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.974600 loss:        0.077395
Test - acc:         0.922800 loss:        0.262815
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.975020 loss:        0.074176
Test - acc:         0.921100 loss:        0.266315
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.975220 loss:        0.073346
Test - acc:         0.921700 loss:        0.267242
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.976540 loss:        0.070604
Test - acc:         0.914700 loss:        0.284563
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.976680 loss:        0.069423
Test - acc:         0.917100 loss:        0.284769
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.975920 loss:        0.070786
Test - acc:         0.921200 loss:        0.270824
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.974660 loss:        0.074609
Test - acc:         0.920900 loss:        0.271740
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.974700 loss:        0.074330
Test - acc:         0.924000 loss:        0.256371
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.976580 loss:        0.070358
Test - acc:         0.918100 loss:        0.290697
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.985220 loss:        0.047903
Test - acc:         0.933100 loss:        0.227784
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.989460 loss:        0.036424
Test - acc:         0.936400 loss:        0.222453
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.990360 loss:        0.032626
Test - acc:         0.936400 loss:        0.220885
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.991940 loss:        0.029432
Test - acc:         0.936900 loss:        0.221299
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.992200 loss:        0.028455
Test - acc:         0.937300 loss:        0.218975
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.993380 loss:        0.025540
Test - acc:         0.936400 loss:        0.220727
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.993260 loss:        0.025404
Test - acc:         0.937000 loss:        0.219011
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.994160 loss:        0.024013
Test - acc:         0.937200 loss:        0.220167
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.993680 loss:        0.023414
Test - acc:         0.936800 loss:        0.220714
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.994580 loss:        0.021522
Test - acc:         0.937200 loss:        0.221391
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.995000 loss:        0.021026
Test - acc:         0.937000 loss:        0.220380
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.994320 loss:        0.021204
Test - acc:         0.938100 loss:        0.221733
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.995360 loss:        0.019932
Test - acc:         0.939200 loss:        0.220584
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.995300 loss:        0.020352
Test - acc:         0.939300 loss:        0.222580
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.995380 loss:        0.019350
Test - acc:         0.937600 loss:        0.222093
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.995180 loss:        0.019910
Test - acc:         0.939000 loss:        0.222333
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.995680 loss:        0.018145
Test - acc:         0.938700 loss:        0.221641
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.995340 loss:        0.018483
Test - acc:         0.939300 loss:        0.222267
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.995940 loss:        0.017648
Test - acc:         0.940300 loss:        0.220618
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.995860 loss:        0.018057
Test - acc:         0.939200 loss:        0.222377
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.996460 loss:        0.016154
Test - acc:         0.940000 loss:        0.223530
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.996080 loss:        0.016957
Test - acc:         0.938500 loss:        0.223043
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.996440 loss:        0.016077
Test - acc:         0.939100 loss:        0.224329
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.950600 loss:        0.144330
Test - acc:         0.915800 loss:        0.274048
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.962320 loss:        0.110623
Test - acc:         0.918000 loss:        0.263097
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.965820 loss:        0.100651
Test - acc:         0.921800 loss:        0.257724
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.968840 loss:        0.093605
Test - acc:         0.921300 loss:        0.255453
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.970320 loss:        0.088560
Test - acc:         0.923600 loss:        0.252060
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.971820 loss:        0.084788
Test - acc:         0.923300 loss:        0.250471
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.971900 loss:        0.082883
Test - acc:         0.925100 loss:        0.248031
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.975140 loss:        0.076876
Test - acc:         0.928000 loss:        0.247102
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.975220 loss:        0.075485
Test - acc:         0.926200 loss:        0.246195
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.976200 loss:        0.073776
Test - acc:         0.926800 loss:        0.249405
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.976540 loss:        0.071872
Test - acc:         0.925300 loss:        0.245773
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.977040 loss:        0.070908
Test - acc:         0.927700 loss:        0.247895
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.977180 loss:        0.068983
Test - acc:         0.927700 loss:        0.245756
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.979220 loss:        0.065927
Test - acc:         0.926200 loss:        0.246929
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.978620 loss:        0.064632
Test - acc:         0.929200 loss:        0.245205
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.978980 loss:        0.064735
Test - acc:         0.927300 loss:        0.246792
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.979360 loss:        0.063075
Test - acc:         0.928100 loss:        0.245447
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.980520 loss:        0.061253
Test - acc:         0.927900 loss:        0.244676
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.981360 loss:        0.059125
Test - acc:         0.929300 loss:        0.243907
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.981400 loss:        0.058029
Test - acc:         0.929700 loss:        0.244698
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.980740 loss:        0.059270
Test - acc:         0.928000 loss:        0.248263
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.980900 loss:        0.059004
Test - acc:         0.928700 loss:        0.248843
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.982560 loss:        0.054792
Test - acc:         0.929900 loss:        0.246768
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.981480 loss:        0.057741
Test - acc:         0.929200 loss:        0.247575
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.982180 loss:        0.056023
Test - acc:         0.928400 loss:        0.248619
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.981800 loss:        0.055920
Test - acc:         0.929800 loss:        0.244730
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.982860 loss:        0.053859
Test - acc:         0.930600 loss:        0.245888
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.983900 loss:        0.052717
Test - acc:         0.930300 loss:        0.247152
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.983740 loss:        0.051544
Test - acc:         0.931000 loss:        0.245501
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.983820 loss:        0.051319
Test - acc:         0.930100 loss:        0.248335
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.984060 loss:        0.050251
Test - acc:         0.930100 loss:        0.249238
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.983860 loss:        0.050043
Test - acc:         0.929300 loss:        0.245333
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.984840 loss:        0.049040
Test - acc:         0.929300 loss:        0.250338
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.983960 loss:        0.051176
Test - acc:         0.931000 loss:        0.243632
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.983840 loss:        0.049542
Test - acc:         0.930900 loss:        0.249025
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.984720 loss:        0.048365
Test - acc:         0.929500 loss:        0.252810
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.985160 loss:        0.047178
Test - acc:         0.931900 loss:        0.249303
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.985420 loss:        0.046524
Test - acc:         0.929100 loss:        0.251798
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.986260 loss:        0.045218
Test - acc:         0.928600 loss:        0.247778
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.867440 loss:        0.384086
Test - acc:         0.877000 loss:        0.372400
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.902780 loss:        0.281094
Test - acc:         0.889600 loss:        0.334451
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.910840 loss:        0.257705
Test - acc:         0.893900 loss:        0.323625
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.919260 loss:        0.236932
Test - acc:         0.896700 loss:        0.315806
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.921820 loss:        0.225752
Test - acc:         0.901200 loss:        0.306129
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.924800 loss:        0.216911
Test - acc:         0.902400 loss:        0.299683
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.927840 loss:        0.210550
Test - acc:         0.902900 loss:        0.299270
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.928500 loss:        0.205544
Test - acc:         0.903200 loss:        0.299002
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.931400 loss:        0.199814
Test - acc:         0.904000 loss:        0.298814
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.932600 loss:        0.197833
Test - acc:         0.901300 loss:        0.297744
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.931820 loss:        0.195021
Test - acc:         0.904700 loss:        0.295802
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.934700 loss:        0.189379
Test - acc:         0.906200 loss:        0.291278
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.936020 loss:        0.186218
Test - acc:         0.905700 loss:        0.293329
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.935800 loss:        0.187717
Test - acc:         0.904300 loss:        0.291383
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.937860 loss:        0.182948
Test - acc:         0.905400 loss:        0.292617
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.939400 loss:        0.178742
Test - acc:         0.905800 loss:        0.289514
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.939460 loss:        0.176854
Test - acc:         0.904600 loss:        0.286791
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.939640 loss:        0.175089
Test - acc:         0.906300 loss:        0.291561
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.940440 loss:        0.174580
Test - acc:         0.903300 loss:        0.291286
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.940340 loss:        0.172297
Test - acc:         0.900400 loss:        0.298952
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.940420 loss:        0.173159
Test - acc:         0.904600 loss:        0.286853
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.942960 loss:        0.168093
Test - acc:         0.904900 loss:        0.287366
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.943640 loss:        0.164919
Test - acc:         0.908600 loss:        0.284921
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.942120 loss:        0.166128
Test - acc:         0.909300 loss:        0.287049
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.943420 loss:        0.165015
Test - acc:         0.909500 loss:        0.282456
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.943340 loss:        0.165045
Test - acc:         0.907600 loss:        0.286702
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.942680 loss:        0.166005
Test - acc:         0.906800 loss:        0.289200
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.943640 loss:        0.161658
Test - acc:         0.905500 loss:        0.290612
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.944740 loss:        0.160167
Test - acc:         0.909900 loss:        0.285999
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.945120 loss:        0.158462
Test - acc:         0.909400 loss:        0.285284
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.944560 loss:        0.160607
Test - acc:         0.906100 loss:        0.290348
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.946940 loss:        0.156165
Test - acc:         0.910400 loss:        0.283530
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.946940 loss:        0.156409
Test - acc:         0.907400 loss:        0.286164
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.946940 loss:        0.156511
Test - acc:         0.907300 loss:        0.289561
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.946900 loss:        0.155012
Test - acc:         0.906900 loss:        0.292159
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.946420 loss:        0.154629
Test - acc:         0.908600 loss:        0.288532
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.945480 loss:        0.156038
Test - acc:         0.909500 loss:        0.285388
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.946120 loss:        0.152460
Test - acc:         0.908900 loss:        0.288324
Sparsity :          0.9961
Wdecay :        0.000500
