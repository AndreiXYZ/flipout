Running --model resnet18 --noise --prune_criterion weight_div_flips --seed 43 --prune_freq 39 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=39_seed=43 --save_model=pre-finetune/resnet18_weight_div_flips_pf39_s43 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf39_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.275580 loss:        2.206162
Test - acc:         0.379300 loss:        1.773930
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.448080 loss:        1.503151
Test - acc:         0.529700 loss:        1.350102
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.561660 loss:        1.220748
Test - acc:         0.589900 loss:        1.153149
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.644920 loss:        1.002910
Test - acc:         0.641300 loss:        1.060353
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.705420 loss:        0.837933
Test - acc:         0.662400 loss:        0.967185
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.753280 loss:        0.707195
Test - acc:         0.719800 loss:        0.820684
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.778140 loss:        0.637992
Test - acc:         0.746700 loss:        0.735908
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.797340 loss:        0.584231
Test - acc:         0.764400 loss:        0.731834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.810600 loss:        0.549598
Test - acc:         0.739700 loss:        0.764843
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.816060 loss:        0.533603
Test - acc:         0.747800 loss:        0.758131
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.822860 loss:        0.513958
Test - acc:         0.768500 loss:        0.701588
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.827900 loss:        0.501282
Test - acc:         0.816000 loss:        0.537637
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.833580 loss:        0.483668
Test - acc:         0.810200 loss:        0.568396
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.836280 loss:        0.476076
Test - acc:         0.768400 loss:        0.720073
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.839420 loss:        0.464692
Test - acc:         0.759300 loss:        0.735929
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845000 loss:        0.453000
Test - acc:         0.824500 loss:        0.526457
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.849680 loss:        0.444501
Test - acc:         0.826500 loss:        0.520368
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.849380 loss:        0.438538
Test - acc:         0.798100 loss:        0.637503
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.850040 loss:        0.440032
Test - acc:         0.829700 loss:        0.507356
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.854180 loss:        0.426594
Test - acc:         0.802500 loss:        0.589427
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.855560 loss:        0.423533
Test - acc:         0.786400 loss:        0.632288
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.855960 loss:        0.420786
Test - acc:         0.789600 loss:        0.646576
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.858360 loss:        0.416946
Test - acc:         0.752800 loss:        0.756219
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.860040 loss:        0.415164
Test - acc:         0.812200 loss:        0.558783
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.858620 loss:        0.414178
Test - acc:         0.814800 loss:        0.577834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.859180 loss:        0.410590
Test - acc:         0.813800 loss:        0.559850
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.860320 loss:        0.407069
Test - acc:         0.851000 loss:        0.442927
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.857900 loss:        0.414354
Test - acc:         0.838400 loss:        0.479358
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.861760 loss:        0.403122
Test - acc:         0.813600 loss:        0.552473
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.864300 loss:        0.400279
Test - acc:         0.798500 loss:        0.615302
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.864260 loss:        0.395054
Test - acc:         0.802800 loss:        0.596695
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.862920 loss:        0.397425
Test - acc:         0.813300 loss:        0.539908
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.863160 loss:        0.393080
Test - acc:         0.815100 loss:        0.580103
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.865860 loss:        0.394840
Test - acc:         0.824300 loss:        0.534873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.864560 loss:        0.393687
Test - acc:         0.828400 loss:        0.520443
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.867500 loss:        0.385755
Test - acc:         0.811500 loss:        0.577941
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.864660 loss:        0.392658
Test - acc:         0.724200 loss:        0.831120
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.866840 loss:        0.389900
Test - acc:         0.847400 loss:        0.461297
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.869240 loss:        0.384570
Test - acc:         0.804400 loss:        0.615804
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.879180 loss:        0.356247
Test - acc:         0.845800 loss:        0.466713
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.876020 loss:        0.362714
Test - acc:         0.850100 loss:        0.441680
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.875560 loss:        0.362267
Test - acc:         0.828500 loss:        0.507909
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.873340 loss:        0.367292
Test - acc:         0.839600 loss:        0.492546
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.873240 loss:        0.366667
Test - acc:         0.835600 loss:        0.496460
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.874300 loss:        0.367670
Test - acc:         0.844200 loss:        0.465952
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.876000 loss:        0.366620
Test - acc:         0.793400 loss:        0.633127
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.876420 loss:        0.367326
Test - acc:         0.820900 loss:        0.569462
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.875400 loss:        0.364236
Test - acc:         0.841900 loss:        0.476416
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.874440 loss:        0.369369
Test - acc:         0.818700 loss:        0.535480
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.878020 loss:        0.357077
Test - acc:         0.841000 loss:        0.473295
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.876100 loss:        0.361532
Test - acc:         0.826600 loss:        0.531906
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.877100 loss:        0.360258
Test - acc:         0.789800 loss:        0.667672
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.876520 loss:        0.361812
Test - acc:         0.773000 loss:        0.749694
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.879500 loss:        0.358107
Test - acc:         0.837100 loss:        0.485922
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.877260 loss:        0.362834
Test - acc:         0.833000 loss:        0.519675
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.876180 loss:        0.361375
Test - acc:         0.809000 loss:        0.579068
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.873500 loss:        0.367560
Test - acc:         0.838500 loss:        0.495875
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.875240 loss:        0.366362
Test - acc:         0.806400 loss:        0.584532
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.878420 loss:        0.356640
Test - acc:         0.829400 loss:        0.522929
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.875040 loss:        0.363885
Test - acc:         0.825200 loss:        0.521756
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.874640 loss:        0.361850
Test - acc:         0.800400 loss:        0.627315
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.877560 loss:        0.358274
Test - acc:         0.825100 loss:        0.512537
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.878300 loss:        0.352688
Test - acc:         0.775400 loss:        0.711205
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.878560 loss:        0.358713
Test - acc:         0.804500 loss:        0.648083
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.878440 loss:        0.360314
Test - acc:         0.829200 loss:        0.526443
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.878300 loss:        0.358801
Test - acc:         0.814600 loss:        0.548827
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.878320 loss:        0.358656
Test - acc:         0.832200 loss:        0.502687
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.876020 loss:        0.358519
Test - acc:         0.821100 loss:        0.569305
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.877980 loss:        0.358505
Test - acc:         0.809000 loss:        0.567474
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.875700 loss:        0.363927
Test - acc:         0.816200 loss:        0.565565
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.876200 loss:        0.361771
Test - acc:         0.799400 loss:        0.645655
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.877700 loss:        0.356560
Test - acc:         0.843400 loss:        0.458912
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.876500 loss:        0.363070
Test - acc:         0.854900 loss:        0.423348
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.879660 loss:        0.354584
Test - acc:         0.807400 loss:        0.608888
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.876920 loss:        0.362353
Test - acc:         0.840400 loss:        0.486150
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.877200 loss:        0.360580
Test - acc:         0.822600 loss:        0.587441
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.877780 loss:        0.358522
Test - acc:         0.817700 loss:        0.555510
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.880440 loss:        0.353347
Test - acc:         0.838400 loss:        0.485715
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.888520 loss:        0.323666
Test - acc:         0.821600 loss:        0.581947
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.885400 loss:        0.332928
Test - acc:         0.805400 loss:        0.609221
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.886220 loss:        0.331243
Test - acc:         0.843100 loss:        0.459196
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.883360 loss:        0.336164
Test - acc:         0.845500 loss:        0.469308
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.883080 loss:        0.341901
Test - acc:         0.844500 loss:        0.476767
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.883860 loss:        0.337130
Test - acc:         0.853200 loss:        0.438695
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.884220 loss:        0.340059
Test - acc:         0.832600 loss:        0.513710
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.885480 loss:        0.334618
Test - acc:         0.835500 loss:        0.507889
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.884720 loss:        0.336633
Test - acc:         0.804400 loss:        0.632688
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.883900 loss:        0.339888
Test - acc:         0.806500 loss:        0.596982
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.882240 loss:        0.341845
Test - acc:         0.836100 loss:        0.499893
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.883880 loss:        0.338899
Test - acc:         0.814900 loss:        0.556495
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.881560 loss:        0.341608
Test - acc:         0.797100 loss:        0.611935
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.882520 loss:        0.341117
Test - acc:         0.793300 loss:        0.617141
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.884140 loss:        0.338374
Test - acc:         0.824500 loss:        0.540871
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.885360 loss:        0.336259
Test - acc:         0.799000 loss:        0.630305
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.884100 loss:        0.335209
Test - acc:         0.785800 loss:        0.655545
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.883200 loss:        0.340783
Test - acc:         0.814100 loss:        0.581575
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.884840 loss:        0.335412
Test - acc:         0.795600 loss:        0.642335
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.883140 loss:        0.339602
Test - acc:         0.757600 loss:        0.785431
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.883980 loss:        0.336319
Test - acc:         0.838000 loss:        0.518157
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.883240 loss:        0.338285
Test - acc:         0.806700 loss:        0.599570
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.884840 loss:        0.332907
Test - acc:         0.852100 loss:        0.451365
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.885780 loss:        0.334067
Test - acc:         0.823800 loss:        0.526190
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.884440 loss:        0.336240
Test - acc:         0.822400 loss:        0.546525
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.883400 loss:        0.341530
Test - acc:         0.791300 loss:        0.696681
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.884200 loss:        0.338408
Test - acc:         0.851500 loss:        0.454431
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.886380 loss:        0.333282
Test - acc:         0.863900 loss:        0.418491
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.883140 loss:        0.338444
Test - acc:         0.822200 loss:        0.532044
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.883860 loss:        0.337588
Test - acc:         0.846600 loss:        0.459229
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.884440 loss:        0.337772
Test - acc:         0.792400 loss:        0.675208
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.884760 loss:        0.334257
Test - acc:         0.830800 loss:        0.521259
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.882440 loss:        0.338992
Test - acc:         0.824400 loss:        0.537311
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.885800 loss:        0.339248
Test - acc:         0.807600 loss:        0.592676
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.885620 loss:        0.335948
Test - acc:         0.851000 loss:        0.477109
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.882620 loss:        0.338963
Test - acc:         0.830500 loss:        0.513786
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.884760 loss:        0.336013
Test - acc:         0.810300 loss:        0.579136
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.882980 loss:        0.342658
Test - acc:         0.837100 loss:        0.478363
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.885240 loss:        0.335553
Test - acc:         0.829000 loss:        0.532775
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.896100 loss:        0.304019
Test - acc:         0.848700 loss:        0.476705
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.895020 loss:        0.306038
Test - acc:         0.842400 loss:        0.485960
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.890220 loss:        0.316241
Test - acc:         0.844000 loss:        0.489491
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.891020 loss:        0.313462
Test - acc:         0.820500 loss:        0.575267
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.893080 loss:        0.315059
Test - acc:         0.846200 loss:        0.465678
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.893100 loss:        0.314240
Test - acc:         0.847000 loss:        0.473216
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.890880 loss:        0.315423
Test - acc:         0.800500 loss:        0.612361
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.892820 loss:        0.314405
Test - acc:         0.859400 loss:        0.424029
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.892460 loss:        0.316171
Test - acc:         0.832400 loss:        0.507981
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.892580 loss:        0.312775
Test - acc:         0.819100 loss:        0.548112
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.890960 loss:        0.316233
Test - acc:         0.856000 loss:        0.417707
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.891560 loss:        0.313346
Test - acc:         0.826500 loss:        0.531040
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.891480 loss:        0.316695
Test - acc:         0.835100 loss:        0.484116
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.892680 loss:        0.311899
Test - acc:         0.839800 loss:        0.459675
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.890320 loss:        0.316274
Test - acc:         0.824700 loss:        0.559121
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.891340 loss:        0.313101
Test - acc:         0.848200 loss:        0.465722
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.894760 loss:        0.310062
Test - acc:         0.850200 loss:        0.456510
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.890520 loss:        0.319384
Test - acc:         0.829400 loss:        0.531942
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.893160 loss:        0.315544
Test - acc:         0.865800 loss:        0.399372
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.888060 loss:        0.320971
Test - acc:         0.841200 loss:        0.456888
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.892620 loss:        0.312398
Test - acc:         0.852000 loss:        0.446004
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.892340 loss:        0.313990
Test - acc:         0.837400 loss:        0.510185
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.891200 loss:        0.317433
Test - acc:         0.841500 loss:        0.489037
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.890600 loss:        0.315560
Test - acc:         0.839900 loss:        0.475046
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.891780 loss:        0.316010
Test - acc:         0.824800 loss:        0.540752
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.890420 loss:        0.320063
Test - acc:         0.871900 loss:        0.378481
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.890120 loss:        0.316487
Test - acc:         0.848400 loss:        0.479449
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.891840 loss:        0.314594
Test - acc:         0.833900 loss:        0.484692
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.890080 loss:        0.318766
Test - acc:         0.858700 loss:        0.409466
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.889600 loss:        0.319677
Test - acc:         0.859900 loss:        0.423762
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.889760 loss:        0.315119
Test - acc:         0.845700 loss:        0.460323
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.889700 loss:        0.318431
Test - acc:         0.859900 loss:        0.431888
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.888960 loss:        0.321082
Test - acc:         0.828200 loss:        0.520893
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.937480 loss:        0.185059
Test - acc:         0.919600 loss:        0.229309
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.950600 loss:        0.146031
Test - acc:         0.924200 loss:        0.223158
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.957280 loss:        0.126527
Test - acc:         0.925700 loss:        0.212673
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.959500 loss:        0.117280
Test - acc:         0.926800 loss:        0.215190
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.963680 loss:        0.109422
Test - acc:         0.927600 loss:        0.209320
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.966680 loss:        0.099916
Test - acc:         0.928700 loss:        0.210623
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.968760 loss:        0.093818
Test - acc:         0.929200 loss:        0.209086
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.971460 loss:        0.085388
Test - acc:         0.928100 loss:        0.213989
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.972320 loss:        0.082419
Test - acc:         0.929900 loss:        0.211616
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.973220 loss:        0.078596
Test - acc:         0.931200 loss:        0.208539
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.974400 loss:        0.076030
Test - acc:         0.928600 loss:        0.219424
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.974960 loss:        0.073023
Test - acc:         0.930200 loss:        0.215681
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.975940 loss:        0.070871
Test - acc:         0.929400 loss:        0.215148
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.977540 loss:        0.067037
Test - acc:         0.928800 loss:        0.221470
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.978660 loss:        0.065592
Test - acc:         0.926000 loss:        0.230775
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.978280 loss:        0.063398
Test - acc:         0.929100 loss:        0.221314
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.978560 loss:        0.062681
Test - acc:         0.931200 loss:        0.221732
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.059062
Test - acc:         0.930300 loss:        0.226724
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.981180 loss:        0.057056
Test - acc:         0.925700 loss:        0.237713
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.981060 loss:        0.056912
Test - acc:         0.926100 loss:        0.243016
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.980880 loss:        0.057656
Test - acc:         0.930100 loss:        0.229665
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.981620 loss:        0.057252
Test - acc:         0.924500 loss:        0.255946
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.980480 loss:        0.057100
Test - acc:         0.924300 loss:        0.252934
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.982380 loss:        0.052963
Test - acc:         0.928700 loss:        0.242167
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.981100 loss:        0.054044
Test - acc:         0.927600 loss:        0.245518
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.981280 loss:        0.056704
Test - acc:         0.924500 loss:        0.254429
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.980640 loss:        0.057607
Test - acc:         0.924200 loss:        0.265077
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.980700 loss:        0.055815
Test - acc:         0.922900 loss:        0.264930
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.982740 loss:        0.052644
Test - acc:         0.929000 loss:        0.246858
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.980400 loss:        0.056941
Test - acc:         0.924100 loss:        0.255800
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.980840 loss:        0.056654
Test - acc:         0.924400 loss:        0.262542
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.981520 loss:        0.056959
Test - acc:         0.928000 loss:        0.242986
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.981940 loss:        0.055228
Test - acc:         0.925400 loss:        0.272723
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.981440 loss:        0.055021
Test - acc:         0.924000 loss:        0.264301
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.979920 loss:        0.061568
Test - acc:         0.923600 loss:        0.265108
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.981300 loss:        0.056738
Test - acc:         0.921600 loss:        0.283307
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.979660 loss:        0.059657
Test - acc:         0.917700 loss:        0.280574
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.978680 loss:        0.061694
Test - acc:         0.922100 loss:        0.265318
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.979560 loss:        0.061541
Test - acc:         0.926300 loss:        0.258860
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.980280 loss:        0.059414
Test - acc:         0.919100 loss:        0.267961
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.979460 loss:        0.061226
Test - acc:         0.924100 loss:        0.256741
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.978020 loss:        0.062798
Test - acc:         0.921600 loss:        0.281907
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.978560 loss:        0.062376
Test - acc:         0.923400 loss:        0.273814
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.978960 loss:        0.062652
Test - acc:         0.924600 loss:        0.257143
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.978780 loss:        0.064711
Test - acc:         0.923900 loss:        0.256547
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.977920 loss:        0.065665
Test - acc:         0.923300 loss:        0.260089
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.978840 loss:        0.061189
Test - acc:         0.924300 loss:        0.252859
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.980620 loss:        0.057896
Test - acc:         0.921400 loss:        0.267988
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.980920 loss:        0.057708
Test - acc:         0.929600 loss:        0.240980
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.980460 loss:        0.057158
Test - acc:         0.927400 loss:        0.248924
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.979960 loss:        0.060042
Test - acc:         0.925600 loss:        0.259626
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.981300 loss:        0.056679
Test - acc:         0.917800 loss:        0.284843
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.980300 loss:        0.057356
Test - acc:         0.923400 loss:        0.272258
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.978660 loss:        0.063406
Test - acc:         0.922800 loss:        0.274019
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.980100 loss:        0.059250
Test - acc:         0.922400 loss:        0.285940
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.979360 loss:        0.060317
Test - acc:         0.926100 loss:        0.263621
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.980360 loss:        0.058195
Test - acc:         0.919100 loss:        0.288471
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.980420 loss:        0.057583
Test - acc:         0.918000 loss:        0.284254
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.980080 loss:        0.060562
Test - acc:         0.927000 loss:        0.249572
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.978240 loss:        0.064223
Test - acc:         0.919900 loss:        0.287929
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.977500 loss:        0.065918
Test - acc:         0.925800 loss:        0.267798
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.979380 loss:        0.060090
Test - acc:         0.922200 loss:        0.261730
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.978480 loss:        0.063014
Test - acc:         0.925100 loss:        0.271743
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.977680 loss:        0.064543
Test - acc:         0.917500 loss:        0.298732
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.979820 loss:        0.061263
Test - acc:         0.927700 loss:        0.256072
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.979440 loss:        0.061745
Test - acc:         0.923200 loss:        0.270813
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.980960 loss:        0.056627
Test - acc:         0.921100 loss:        0.291788
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.978320 loss:        0.064356
Test - acc:         0.927300 loss:        0.260702
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.978540 loss:        0.063551
Test - acc:         0.923800 loss:        0.276056
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.980040 loss:        0.057861
Test - acc:         0.919800 loss:        0.278197
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.977660 loss:        0.064751
Test - acc:         0.922500 loss:        0.266758
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.979000 loss:        0.062788
Test - acc:         0.922100 loss:        0.274359
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.978160 loss:        0.063652
Test - acc:         0.917300 loss:        0.292650
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.977420 loss:        0.065624
Test - acc:         0.923700 loss:        0.262600
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.980200 loss:        0.061142
Test - acc:         0.909900 loss:        0.325942
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.978340 loss:        0.064811
Test - acc:         0.914200 loss:        0.303607
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.060512
Test - acc:         0.917500 loss:        0.287587
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.979060 loss:        0.063399
Test - acc:         0.924000 loss:        0.279235
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.977480 loss:        0.065317
Test - acc:         0.918500 loss:        0.273568
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.980660 loss:        0.058403
Test - acc:         0.920800 loss:        0.280649
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.978480 loss:        0.063181
Test - acc:         0.918100 loss:        0.282137
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.978320 loss:        0.063201
Test - acc:         0.925300 loss:        0.258799
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.978940 loss:        0.062094
Test - acc:         0.920400 loss:        0.273228
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.978660 loss:        0.062836
Test - acc:         0.922600 loss:        0.274591
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.966220 loss:        0.097837
Test - acc:         0.918700 loss:        0.272188
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.971300 loss:        0.083173
Test - acc:         0.920900 loss:        0.260368
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.973120 loss:        0.078801
Test - acc:         0.919600 loss:        0.279791
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.975920 loss:        0.073275
Test - acc:         0.913900 loss:        0.294785
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.975020 loss:        0.074739
Test - acc:         0.914500 loss:        0.295581
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.974260 loss:        0.074487
Test - acc:         0.920000 loss:        0.269886
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.973980 loss:        0.075764
Test - acc:         0.917800 loss:        0.283512
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.974400 loss:        0.074695
Test - acc:         0.919700 loss:        0.271862
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.975380 loss:        0.072364
Test - acc:         0.914300 loss:        0.297412
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.975620 loss:        0.072567
Test - acc:         0.915200 loss:        0.314057
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.974600 loss:        0.073060
Test - acc:         0.920000 loss:        0.278277
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.975260 loss:        0.072750
Test - acc:         0.918900 loss:        0.278988
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.975460 loss:        0.070574
Test - acc:         0.920800 loss:        0.269735
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.975500 loss:        0.070347
Test - acc:         0.915400 loss:        0.296641
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.972700 loss:        0.079485
Test - acc:         0.919600 loss:        0.273295
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.975800 loss:        0.070215
Test - acc:         0.917000 loss:        0.295888
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.985700 loss:        0.045881
Test - acc:         0.932100 loss:        0.227941
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.989840 loss:        0.034595
Test - acc:         0.934100 loss:        0.223667
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.990960 loss:        0.032258
Test - acc:         0.933100 loss:        0.226800
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.991640 loss:        0.030570
Test - acc:         0.932800 loss:        0.227743
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.992380 loss:        0.028623
Test - acc:         0.933400 loss:        0.225818
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.993160 loss:        0.026453
Test - acc:         0.933600 loss:        0.227093
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.993200 loss:        0.025564
Test - acc:         0.933900 loss:        0.225824
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.993800 loss:        0.024570
Test - acc:         0.935600 loss:        0.227759
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.994580 loss:        0.022345
Test - acc:         0.934000 loss:        0.227613
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.993540 loss:        0.023516
Test - acc:         0.935200 loss:        0.228788
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.994840 loss:        0.021749
Test - acc:         0.935800 loss:        0.225717
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.995080 loss:        0.020769
Test - acc:         0.934700 loss:        0.228682
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.995080 loss:        0.020827
Test - acc:         0.933900 loss:        0.227877
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.994780 loss:        0.021270
Test - acc:         0.933700 loss:        0.227809
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.995540 loss:        0.019944
Test - acc:         0.934500 loss:        0.229329
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.995020 loss:        0.020027
Test - acc:         0.934600 loss:        0.229687
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.994940 loss:        0.019992
Test - acc:         0.934200 loss:        0.231234
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.995920 loss:        0.018645
Test - acc:         0.935100 loss:        0.229819
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.996420 loss:        0.017720
Test - acc:         0.935000 loss:        0.230002
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.995920 loss:        0.018138
Test - acc:         0.935400 loss:        0.229070
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.996320 loss:        0.017299
Test - acc:         0.935600 loss:        0.228978
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.996340 loss:        0.017096
Test - acc:         0.935500 loss:        0.230048
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.996320 loss:        0.016759
Test - acc:         0.936100 loss:        0.229315
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.951400 loss:        0.144153
Test - acc:         0.915600 loss:        0.271027
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.959620 loss:        0.113756
Test - acc:         0.916000 loss:        0.263064
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.966700 loss:        0.098831
Test - acc:         0.918000 loss:        0.257637
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.968760 loss:        0.092160
Test - acc:         0.920900 loss:        0.253832
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.971980 loss:        0.086250
Test - acc:         0.922700 loss:        0.253383
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.971540 loss:        0.084956
Test - acc:         0.921200 loss:        0.252988
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.973140 loss:        0.080604
Test - acc:         0.922600 loss:        0.250833
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.974980 loss:        0.075996
Test - acc:         0.923400 loss:        0.251742
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.975640 loss:        0.074476
Test - acc:         0.924400 loss:        0.250144
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.976400 loss:        0.072669
Test - acc:         0.924600 loss:        0.249492
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.976200 loss:        0.072301
Test - acc:         0.923900 loss:        0.250412
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.977220 loss:        0.069030
Test - acc:         0.923800 loss:        0.249037
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.978140 loss:        0.068188
Test - acc:         0.926000 loss:        0.250636
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.978140 loss:        0.067669
Test - acc:         0.924800 loss:        0.249922
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.978720 loss:        0.066404
Test - acc:         0.925000 loss:        0.249903
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.979460 loss:        0.064398
Test - acc:         0.925900 loss:        0.254434
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.980760 loss:        0.061827
Test - acc:         0.924800 loss:        0.252547
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.981880 loss:        0.059507
Test - acc:         0.925400 loss:        0.252878
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.981600 loss:        0.058417
Test - acc:         0.923800 loss:        0.251183
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.980700 loss:        0.059397
Test - acc:         0.925800 loss:        0.253564
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.981500 loss:        0.057446
Test - acc:         0.926600 loss:        0.252568
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.982140 loss:        0.057770
Test - acc:         0.925500 loss:        0.251477
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.981720 loss:        0.056669
Test - acc:         0.925900 loss:        0.256136
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.982520 loss:        0.054863
Test - acc:         0.927400 loss:        0.254278
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.981760 loss:        0.056898
Test - acc:         0.926700 loss:        0.255636
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.982340 loss:        0.055450
Test - acc:         0.924900 loss:        0.258663
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.982500 loss:        0.054849
Test - acc:         0.927700 loss:        0.254360
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.983480 loss:        0.052252
Test - acc:         0.927300 loss:        0.254414
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.983440 loss:        0.051830
Test - acc:         0.925500 loss:        0.256377
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.984400 loss:        0.050907
Test - acc:         0.926900 loss:        0.254413
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.984260 loss:        0.051231
Test - acc:         0.928700 loss:        0.255162
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.983620 loss:        0.052182
Test - acc:         0.927900 loss:        0.253741
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.983980 loss:        0.050417
Test - acc:         0.927400 loss:        0.254903
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.985140 loss:        0.048891
Test - acc:         0.925700 loss:        0.261808
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.984540 loss:        0.049365
Test - acc:         0.927400 loss:        0.255869
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.985580 loss:        0.048096
Test - acc:         0.926500 loss:        0.259680
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.985660 loss:        0.047883
Test - acc:         0.928200 loss:        0.255671
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.985500 loss:        0.047084
Test - acc:         0.927800 loss:        0.258358
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.985680 loss:        0.047069
Test - acc:         0.927100 loss:        0.257941
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.872940 loss:        0.370560
Test - acc:         0.880900 loss:        0.358184
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.904420 loss:        0.275654
Test - acc:         0.888200 loss:        0.331653
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.912840 loss:        0.249190
Test - acc:         0.890700 loss:        0.325973
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.918240 loss:        0.234381
Test - acc:         0.893600 loss:        0.314797
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.923940 loss:        0.220378
Test - acc:         0.897400 loss:        0.307540
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.925860 loss:        0.214303
Test - acc:         0.899100 loss:        0.301781
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.928140 loss:        0.208803
Test - acc:         0.900200 loss:        0.297417
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.930260 loss:        0.202301
Test - acc:         0.901800 loss:        0.292510
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.932220 loss:        0.198119
Test - acc:         0.902700 loss:        0.293671
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.931860 loss:        0.197965
Test - acc:         0.904200 loss:        0.293371
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.935120 loss:        0.191677
Test - acc:         0.901800 loss:        0.294602
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.933820 loss:        0.189539
Test - acc:         0.905200 loss:        0.290276
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.936900 loss:        0.184866
Test - acc:         0.903700 loss:        0.291557
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.936900 loss:        0.181640
Test - acc:         0.904400 loss:        0.288194
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.939420 loss:        0.180104
Test - acc:         0.906500 loss:        0.285175
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.939200 loss:        0.177023
Test - acc:         0.907500 loss:        0.285866
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.940440 loss:        0.174615
Test - acc:         0.906100 loss:        0.286309
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.939360 loss:        0.176173
Test - acc:         0.906000 loss:        0.287149
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.940920 loss:        0.171468
Test - acc:         0.906800 loss:        0.285019
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.941620 loss:        0.171162
Test - acc:         0.908900 loss:        0.286150
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.942120 loss:        0.169569
Test - acc:         0.908600 loss:        0.288173
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.941200 loss:        0.169773
Test - acc:         0.906500 loss:        0.290528
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.943440 loss:        0.164555
Test - acc:         0.910100 loss:        0.284570
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.943860 loss:        0.164909
Test - acc:         0.911200 loss:        0.280447
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.944920 loss:        0.161729
Test - acc:         0.907800 loss:        0.285199
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.943880 loss:        0.162886
Test - acc:         0.909500 loss:        0.284660
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.946360 loss:        0.158671
Test - acc:         0.907500 loss:        0.287893
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.943680 loss:        0.159896
Test - acc:         0.906700 loss:        0.281052
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.945520 loss:        0.160026
Test - acc:         0.909000 loss:        0.283151
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.944360 loss:        0.158248
Test - acc:         0.910200 loss:        0.281532
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.945360 loss:        0.158431
Test - acc:         0.908500 loss:        0.281479
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.945200 loss:        0.156384
Test - acc:         0.909600 loss:        0.283273
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.946940 loss:        0.155504
Test - acc:         0.908400 loss:        0.283250
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.947200 loss:        0.154771
Test - acc:         0.908100 loss:        0.283941
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.946800 loss:        0.155618
Test - acc:         0.909100 loss:        0.284664
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.945880 loss:        0.156848
Test - acc:         0.907900 loss:        0.286001
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.947280 loss:        0.151944
Test - acc:         0.911800 loss:        0.281611
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.946720 loss:        0.154325
Test - acc:         0.909600 loss:        0.282933
Sparsity :          0.9961
Wdecay :        0.000500
