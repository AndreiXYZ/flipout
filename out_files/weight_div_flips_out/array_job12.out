Running --model resnet18 --noise --prune_criterion weight_div_flips --seed 44 --prune_freq 70 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=70_seed=44 --save_model=pre-finetune/resnet18_weight_div_flips_pf70_s44 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 70,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf70_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.303340 loss:        2.045872
Test - acc:         0.449200 loss:        1.518474
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.486940 loss:        1.408749
Test - acc:         0.553600 loss:        1.216369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.595040 loss:        1.130263
Test - acc:         0.627400 loss:        1.061316
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.662820 loss:        0.954220
Test - acc:         0.666000 loss:        0.963036
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.710980 loss:        0.823103
Test - acc:         0.700400 loss:        0.900422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.754080 loss:        0.701555
Test - acc:         0.719300 loss:        0.843699
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.784640 loss:        0.626890
Test - acc:         0.762900 loss:        0.702303
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.800120 loss:        0.580976
Test - acc:         0.678300 loss:        1.056559
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.809040 loss:        0.553956
Test - acc:         0.750400 loss:        0.725537
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.817400 loss:        0.527065
Test - acc:         0.750100 loss:        0.776029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.828080 loss:        0.504549
Test - acc:         0.806100 loss:        0.560775
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.827040 loss:        0.498283
Test - acc:         0.787800 loss:        0.637411
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.835700 loss:        0.482435
Test - acc:         0.794000 loss:        0.615528
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838900 loss:        0.468737
Test - acc:         0.820400 loss:        0.537681
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844200 loss:        0.459359
Test - acc:         0.804800 loss:        0.583139
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845740 loss:        0.448696
Test - acc:         0.757400 loss:        0.755301
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.845840 loss:        0.444416
Test - acc:         0.784900 loss:        0.670693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.849560 loss:        0.437153
Test - acc:         0.836300 loss:        0.472858
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.852000 loss:        0.431503
Test - acc:         0.805300 loss:        0.564962
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.853780 loss:        0.428139
Test - acc:         0.822000 loss:        0.534250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.417603
Test - acc:         0.776100 loss:        0.709650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.857220 loss:        0.416609
Test - acc:         0.822800 loss:        0.534768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.418318
Test - acc:         0.835400 loss:        0.503948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.857700 loss:        0.416371
Test - acc:         0.797500 loss:        0.599861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.861780 loss:        0.406843
Test - acc:         0.789200 loss:        0.638827
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.858420 loss:        0.412091
Test - acc:         0.812200 loss:        0.574223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.861500 loss:        0.402603
Test - acc:         0.834200 loss:        0.487177
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.861100 loss:        0.403876
Test - acc:         0.788300 loss:        0.694953
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.862260 loss:        0.397948
Test - acc:         0.834000 loss:        0.496371
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.861600 loss:        0.404324
Test - acc:         0.767800 loss:        0.741355
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.863460 loss:        0.399084
Test - acc:         0.813800 loss:        0.548094
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.862380 loss:        0.399397
Test - acc:         0.821100 loss:        0.548047
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.866720 loss:        0.392388
Test - acc:         0.831100 loss:        0.522133
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.863960 loss:        0.395103
Test - acc:         0.791300 loss:        0.647549
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.865880 loss:        0.393367
Test - acc:         0.748300 loss:        0.861032
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.867420 loss:        0.392783
Test - acc:         0.793100 loss:        0.629516
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.867160 loss:        0.387712
Test - acc:         0.855000 loss:        0.438008
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.387555
Test - acc:         0.817900 loss:        0.557149
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.867240 loss:        0.389858
Test - acc:         0.841200 loss:        0.478896
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.869000 loss:        0.387413
Test - acc:         0.832700 loss:        0.490588
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.864980 loss:        0.394477
Test - acc:         0.841800 loss:        0.463607
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.869720 loss:        0.383677
Test - acc:         0.843400 loss:        0.466556
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.870200 loss:        0.381420
Test - acc:         0.781100 loss:        0.700399
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.869540 loss:        0.383390
Test - acc:         0.801500 loss:        0.623825
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.870180 loss:        0.382817
Test - acc:         0.783300 loss:        0.685600
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.870820 loss:        0.376388
Test - acc:         0.813000 loss:        0.573551
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.869700 loss:        0.380625
Test - acc:         0.832900 loss:        0.502137
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.869460 loss:        0.382523
Test - acc:         0.762700 loss:        0.762488
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.869520 loss:        0.381779
Test - acc:         0.829900 loss:        0.524733
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.867960 loss:        0.379790
Test - acc:         0.834400 loss:        0.510441
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.872280 loss:        0.373012
Test - acc:         0.811000 loss:        0.582797
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.873420 loss:        0.372036
Test - acc:         0.843100 loss:        0.475100
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.870400 loss:        0.378922
Test - acc:         0.856800 loss:        0.411110
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.874820 loss:        0.371978
Test - acc:         0.797100 loss:        0.629058
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.870640 loss:        0.378161
Test - acc:         0.767800 loss:        0.727634
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.872540 loss:        0.372711
Test - acc:         0.797500 loss:        0.611661
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.871460 loss:        0.373897
Test - acc:         0.854300 loss:        0.436976
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.872720 loss:        0.374363
Test - acc:         0.833100 loss:        0.507876
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.872780 loss:        0.371153
Test - acc:         0.867000 loss:        0.394288
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.874740 loss:        0.367565
Test - acc:         0.849200 loss:        0.475920
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.874780 loss:        0.369434
Test - acc:         0.845800 loss:        0.448344
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.871860 loss:        0.376186
Test - acc:         0.781900 loss:        0.695914
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.873440 loss:        0.371171
Test - acc:         0.834900 loss:        0.500511
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.871740 loss:        0.371933
Test - acc:         0.852100 loss:        0.444918
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.871780 loss:        0.371990
Test - acc:         0.841500 loss:        0.489206
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.873980 loss:        0.370594
Test - acc:         0.855200 loss:        0.438623
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.872280 loss:        0.372062
Test - acc:         0.828900 loss:        0.518650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.873000 loss:        0.370841
Test - acc:         0.805500 loss:        0.613012
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.874360 loss:        0.368795
Test - acc:         0.826500 loss:        0.525628
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.877540 loss:        0.359595
Test - acc:         0.854300 loss:        0.434711
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.884620 loss:        0.338187
Test - acc:         0.828000 loss:        0.558103
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.880520 loss:        0.349280
Test - acc:         0.817800 loss:        0.558045
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.880620 loss:        0.349689
Test - acc:         0.856000 loss:        0.424583
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.878880 loss:        0.350622
Test - acc:         0.828000 loss:        0.502709
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.879780 loss:        0.351275
Test - acc:         0.846200 loss:        0.471556
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.880640 loss:        0.347633
Test - acc:         0.851200 loss:        0.434744
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.881000 loss:        0.345814
Test - acc:         0.815000 loss:        0.556089
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.878360 loss:        0.354831
Test - acc:         0.809500 loss:        0.572888
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.880700 loss:        0.353540
Test - acc:         0.831200 loss:        0.513277
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.879500 loss:        0.350846
Test - acc:         0.849900 loss:        0.464846
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.879760 loss:        0.350797
Test - acc:         0.833100 loss:        0.489856
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.878620 loss:        0.351720
Test - acc:         0.835600 loss:        0.501954
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.879440 loss:        0.352814
Test - acc:         0.830600 loss:        0.525091
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.881380 loss:        0.350435
Test - acc:         0.827600 loss:        0.525445
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.880180 loss:        0.349545
Test - acc:         0.793500 loss:        0.635313
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.881640 loss:        0.350327
Test - acc:         0.858800 loss:        0.420131
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.881520 loss:        0.345320
Test - acc:         0.829600 loss:        0.506951
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.878640 loss:        0.351836
Test - acc:         0.829600 loss:        0.515687
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.877820 loss:        0.355224
Test - acc:         0.803100 loss:        0.632035
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.881580 loss:        0.349707
Test - acc:         0.852700 loss:        0.444070
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.879200 loss:        0.351497
Test - acc:         0.833300 loss:        0.480524
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.881640 loss:        0.349719
Test - acc:         0.840600 loss:        0.471887
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.880120 loss:        0.349654
Test - acc:         0.830900 loss:        0.519153
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.880040 loss:        0.350128
Test - acc:         0.858100 loss:        0.441203
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.880800 loss:        0.350042
Test - acc:         0.836000 loss:        0.484732
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.879840 loss:        0.352610
Test - acc:         0.820500 loss:        0.552534
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.878260 loss:        0.354580
Test - acc:         0.841800 loss:        0.497618
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.878900 loss:        0.350417
Test - acc:         0.818100 loss:        0.560194
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.880480 loss:        0.350196
Test - acc:         0.756400 loss:        0.830056
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.879220 loss:        0.354654
Test - acc:         0.825200 loss:        0.546415
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.878520 loss:        0.354309
Test - acc:         0.800500 loss:        0.611747
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.879220 loss:        0.355455
Test - acc:         0.816800 loss:        0.537338
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.882760 loss:        0.346539
Test - acc:         0.845900 loss:        0.464909
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.878880 loss:        0.351307
Test - acc:         0.841000 loss:        0.484783
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.880880 loss:        0.349199
Test - acc:         0.816800 loss:        0.579484
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.879920 loss:        0.352021
Test - acc:         0.836500 loss:        0.510408
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.878500 loss:        0.354864
Test - acc:         0.839100 loss:        0.472893
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.882040 loss:        0.348064
Test - acc:         0.796700 loss:        0.618370
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.877580 loss:        0.353194
Test - acc:         0.837400 loss:        0.488263
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.880640 loss:        0.348796
Test - acc:         0.851300 loss:        0.441078
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.881940 loss:        0.346752
Test - acc:         0.808300 loss:        0.572242
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.880560 loss:        0.352922
Test - acc:         0.820200 loss:        0.558789
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.879020 loss:        0.351462
Test - acc:         0.806400 loss:        0.613254
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.881400 loss:        0.351286
Test - acc:         0.827900 loss:        0.506270
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.879900 loss:        0.349055
Test - acc:         0.825800 loss:        0.526401
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.880040 loss:        0.350585
Test - acc:         0.819300 loss:        0.554282
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.880840 loss:        0.353372
Test - acc:         0.840000 loss:        0.499112
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.879940 loss:        0.349531
Test - acc:         0.856000 loss:        0.434397
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.882160 loss:        0.346510
Test - acc:         0.827700 loss:        0.538853
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.880580 loss:        0.347840
Test - acc:         0.810100 loss:        0.599571
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.879220 loss:        0.353279
Test - acc:         0.831000 loss:        0.524542
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.881640 loss:        0.349923
Test - acc:         0.830500 loss:        0.513698
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.880100 loss:        0.355393
Test - acc:         0.830400 loss:        0.500888
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.879700 loss:        0.352584
Test - acc:         0.820000 loss:        0.571878
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.882440 loss:        0.348138
Test - acc:         0.847400 loss:        0.462416
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.882020 loss:        0.345406
Test - acc:         0.827700 loss:        0.555027
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.881400 loss:        0.345148
Test - acc:         0.825900 loss:        0.524408
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.880380 loss:        0.352595
Test - acc:         0.825400 loss:        0.528101
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.880620 loss:        0.348247
Test - acc:         0.792400 loss:        0.646710
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.881740 loss:        0.350784
Test - acc:         0.824200 loss:        0.545941
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.882700 loss:        0.346605
Test - acc:         0.842700 loss:        0.478678
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.880560 loss:        0.349932
Test - acc:         0.839300 loss:        0.495574
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.876240 loss:        0.359632
Test - acc:         0.854700 loss:        0.446892
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.881180 loss:        0.349006
Test - acc:         0.814900 loss:        0.570681
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.881720 loss:        0.347379
Test - acc:         0.850800 loss:        0.457390
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.880420 loss:        0.347719
Test - acc:         0.842800 loss:        0.496442
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.878640 loss:        0.352996
Test - acc:         0.835400 loss:        0.492803
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.880880 loss:        0.349610
Test - acc:         0.766400 loss:        0.817988
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.880340 loss:        0.351392
Test - acc:         0.814500 loss:        0.599559
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.881520 loss:        0.348445
Test - acc:         0.816900 loss:        0.581120
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.889400 loss:        0.320221
Test - acc:         0.836000 loss:        0.505349
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.889500 loss:        0.322449
Test - acc:         0.848500 loss:        0.475518
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.888280 loss:        0.327804
Test - acc:         0.840300 loss:        0.489045
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.886120 loss:        0.331856
Test - acc:         0.852300 loss:        0.439881
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.886440 loss:        0.328500
Test - acc:         0.858900 loss:        0.427200
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.886160 loss:        0.330816
Test - acc:         0.808700 loss:        0.600840
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.885140 loss:        0.331760
Test - acc:         0.860000 loss:        0.427912
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.887420 loss:        0.326598
Test - acc:         0.843100 loss:        0.489790
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.887280 loss:        0.330899
Test - acc:         0.817300 loss:        0.567729
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.884840 loss:        0.338319
Test - acc:         0.851000 loss:        0.441749
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.936060 loss:        0.193045
Test - acc:         0.921300 loss:        0.230773
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.949260 loss:        0.151516
Test - acc:         0.926300 loss:        0.218956
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.956840 loss:        0.130384
Test - acc:         0.929700 loss:        0.211573
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.959760 loss:        0.118851
Test - acc:         0.930800 loss:        0.207299
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.962740 loss:        0.110641
Test - acc:         0.931300 loss:        0.211092
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.967140 loss:        0.098993
Test - acc:         0.931600 loss:        0.206817
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.969880 loss:        0.089392
Test - acc:         0.929900 loss:        0.219489
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.971860 loss:        0.086352
Test - acc:         0.932300 loss:        0.213125
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.973920 loss:        0.079537
Test - acc:         0.930300 loss:        0.219910
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.974580 loss:        0.075683
Test - acc:         0.930800 loss:        0.219460
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.975540 loss:        0.071914
Test - acc:         0.927400 loss:        0.232363
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.976820 loss:        0.067408
Test - acc:         0.932300 loss:        0.221745
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.978640 loss:        0.064384
Test - acc:         0.931800 loss:        0.223620
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.978880 loss:        0.063516
Test - acc:         0.930700 loss:        0.233168
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.979260 loss:        0.061967
Test - acc:         0.930400 loss:        0.239681
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.979920 loss:        0.059800
Test - acc:         0.930400 loss:        0.234301
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.981120 loss:        0.055879
Test - acc:         0.932400 loss:        0.233206
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.980460 loss:        0.057415
Test - acc:         0.930500 loss:        0.243294
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.983040 loss:        0.050750
Test - acc:         0.929300 loss:        0.249481
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.981860 loss:        0.054155
Test - acc:         0.929500 loss:        0.255920
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.981120 loss:        0.055729
Test - acc:         0.930800 loss:        0.239632
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.982220 loss:        0.053775
Test - acc:         0.928800 loss:        0.242699
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.984480 loss:        0.048340
Test - acc:         0.925300 loss:        0.253669
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.983880 loss:        0.050664
Test - acc:         0.930900 loss:        0.249106
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.983500 loss:        0.050911
Test - acc:         0.924800 loss:        0.256018
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.981540 loss:        0.054981
Test - acc:         0.925200 loss:        0.260857
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.980960 loss:        0.055030
Test - acc:         0.921500 loss:        0.270600
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.981200 loss:        0.055780
Test - acc:         0.922900 loss:        0.276015
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.982280 loss:        0.054531
Test - acc:         0.928700 loss:        0.246534
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.980140 loss:        0.057900
Test - acc:         0.924500 loss:        0.258589
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.980040 loss:        0.059500
Test - acc:         0.920100 loss:        0.273597
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.981180 loss:        0.057125
Test - acc:         0.926900 loss:        0.251560
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.981460 loss:        0.054553
Test - acc:         0.917100 loss:        0.288290
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.980280 loss:        0.056575
Test - acc:         0.923000 loss:        0.273187
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.058029
Test - acc:         0.923200 loss:        0.266288
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.979500 loss:        0.061106
Test - acc:         0.927700 loss:        0.253617
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.980080 loss:        0.058355
Test - acc:         0.923600 loss:        0.268771
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.979300 loss:        0.060321
Test - acc:         0.917400 loss:        0.293518
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.980020 loss:        0.059085
Test - acc:         0.920700 loss:        0.284940
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.978440 loss:        0.062887
Test - acc:         0.913400 loss:        0.299597
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.978680 loss:        0.062713
Test - acc:         0.921000 loss:        0.274207
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.980060 loss:        0.060758
Test - acc:         0.925900 loss:        0.256940
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.980520 loss:        0.058230
Test - acc:         0.923900 loss:        0.267555
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.977780 loss:        0.064822
Test - acc:         0.919000 loss:        0.282799
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.978640 loss:        0.063743
Test - acc:         0.909300 loss:        0.323454
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.979460 loss:        0.060669
Test - acc:         0.924700 loss:        0.260737
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.979000 loss:        0.063768
Test - acc:         0.921800 loss:        0.266147
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.978260 loss:        0.062893
Test - acc:         0.921300 loss:        0.274932
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.978560 loss:        0.063950
Test - acc:         0.925600 loss:        0.261011
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.977500 loss:        0.066005
Test - acc:         0.924200 loss:        0.260317
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.978640 loss:        0.062764
Test - acc:         0.924900 loss:        0.258998
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.979220 loss:        0.061172
Test - acc:         0.919400 loss:        0.286498
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.978780 loss:        0.063534
Test - acc:         0.913100 loss:        0.304504
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.978880 loss:        0.062100
Test - acc:         0.925900 loss:        0.252753
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.981320 loss:        0.056968
Test - acc:         0.919700 loss:        0.277124
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.977800 loss:        0.065096
Test - acc:         0.920000 loss:        0.292493
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.978120 loss:        0.064183
Test - acc:         0.916000 loss:        0.291319
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.977280 loss:        0.066356
Test - acc:         0.922000 loss:        0.275551
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.978600 loss:        0.063017
Test - acc:         0.919800 loss:        0.284874
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.978840 loss:        0.064057
Test - acc:         0.919200 loss:        0.271692
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.984800 loss:        0.045588
Test - acc:         0.922000 loss:        0.287955
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.987060 loss:        0.040449
Test - acc:         0.925300 loss:        0.263750
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.985540 loss:        0.044736
Test - acc:         0.924500 loss:        0.268485
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.984640 loss:        0.046548
Test - acc:         0.921100 loss:        0.279589
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.985340 loss:        0.045607
Test - acc:         0.925500 loss:        0.263975
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.984920 loss:        0.045036
Test - acc:         0.918500 loss:        0.288350
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.984600 loss:        0.047307
Test - acc:         0.924700 loss:        0.268868
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.982760 loss:        0.049797
Test - acc:         0.915700 loss:        0.301517
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.982860 loss:        0.051063
Test - acc:         0.928500 loss:        0.259198
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.985700 loss:        0.043487
Test - acc:         0.920300 loss:        0.303622
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.984120 loss:        0.048898
Test - acc:         0.922100 loss:        0.284010
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.981900 loss:        0.053335
Test - acc:         0.920800 loss:        0.277391
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.981720 loss:        0.053204
Test - acc:         0.928300 loss:        0.258397
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.985020 loss:        0.046474
Test - acc:         0.925400 loss:        0.257396
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.983520 loss:        0.049712
Test - acc:         0.921500 loss:        0.281257
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.983760 loss:        0.050248
Test - acc:         0.924800 loss:        0.278583
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.982240 loss:        0.053150
Test - acc:         0.917800 loss:        0.288282
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.983180 loss:        0.051225
Test - acc:         0.922500 loss:        0.279699
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.981720 loss:        0.055576
Test - acc:         0.914600 loss:        0.304014
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.980080 loss:        0.057093
Test - acc:         0.926700 loss:        0.259025
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.982460 loss:        0.051710
Test - acc:         0.926400 loss:        0.263620
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.981920 loss:        0.052924
Test - acc:         0.912600 loss:        0.331343
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.980820 loss:        0.056237
Test - acc:         0.917700 loss:        0.283071
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.981780 loss:        0.053959
Test - acc:         0.920100 loss:        0.291453
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.981440 loss:        0.055744
Test - acc:         0.927900 loss:        0.259689
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.981860 loss:        0.053687
Test - acc:         0.920100 loss:        0.281176
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.982740 loss:        0.051692
Test - acc:         0.922200 loss:        0.278884
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.982040 loss:        0.053550
Test - acc:         0.922400 loss:        0.282783
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.981180 loss:        0.054650
Test - acc:         0.917700 loss:        0.285348
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.981540 loss:        0.054668
Test - acc:         0.920800 loss:        0.287839
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.981700 loss:        0.054257
Test - acc:         0.921000 loss:        0.279895
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.982140 loss:        0.053767
Test - acc:         0.924700 loss:        0.258154
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.982580 loss:        0.051450
Test - acc:         0.925800 loss:        0.267623
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.982940 loss:        0.052256
Test - acc:         0.923400 loss:        0.277383
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.982040 loss:        0.053776
Test - acc:         0.922000 loss:        0.280489
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.981820 loss:        0.054398
Test - acc:         0.928200 loss:        0.257954
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.981400 loss:        0.054840
Test - acc:         0.918800 loss:        0.281773
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.982120 loss:        0.054970
Test - acc:         0.923900 loss:        0.266893
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.982360 loss:        0.053891
Test - acc:         0.921200 loss:        0.274236
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.981880 loss:        0.054172
Test - acc:         0.922100 loss:        0.269687
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.990880 loss:        0.029867
Test - acc:         0.938800 loss:        0.217756
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.994760 loss:        0.019951
Test - acc:         0.940500 loss:        0.215168
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.996020 loss:        0.016352
Test - acc:         0.941900 loss:        0.211780
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.996700 loss:        0.013718
Test - acc:         0.942100 loss:        0.210646
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.996960 loss:        0.012719
Test - acc:         0.941600 loss:        0.212995
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.997660 loss:        0.011475
Test - acc:         0.942000 loss:        0.210763
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.998000 loss:        0.010727
Test - acc:         0.942600 loss:        0.208057
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.997880 loss:        0.010378
Test - acc:         0.943300 loss:        0.208534
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.998200 loss:        0.009029
Test - acc:         0.941600 loss:        0.208976
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.998400 loss:        0.008461
Test - acc:         0.942700 loss:        0.209181
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.998480 loss:        0.008578
Test - acc:         0.942600 loss:        0.210606
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.998640 loss:        0.008265
Test - acc:         0.943200 loss:        0.209342
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.998740 loss:        0.007659
Test - acc:         0.943000 loss:        0.206703
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.998880 loss:        0.007272
Test - acc:         0.943100 loss:        0.207334
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.998900 loss:        0.006948
Test - acc:         0.943400 loss:        0.208987
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.998820 loss:        0.006898
Test - acc:         0.944300 loss:        0.206362
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999080 loss:        0.006423
Test - acc:         0.943600 loss:        0.206767
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.006303
Test - acc:         0.944300 loss:        0.205901
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.998960 loss:        0.006260
Test - acc:         0.944800 loss:        0.206081
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999220 loss:        0.006115
Test - acc:         0.943700 loss:        0.207204
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999140 loss:        0.005940
Test - acc:         0.943100 loss:        0.205499
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.005813
Test - acc:         0.943000 loss:        0.207201
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.005842
Test - acc:         0.943900 loss:        0.206475
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.005411
Test - acc:         0.944700 loss:        0.206715
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.005222
Test - acc:         0.944000 loss:        0.205344
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.005581
Test - acc:         0.943200 loss:        0.205313
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999280 loss:        0.005450
Test - acc:         0.943800 loss:        0.205809
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.005269
Test - acc:         0.944500 loss:        0.204823
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.005206
Test - acc:         0.944000 loss:        0.205688
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.004705
Test - acc:         0.945100 loss:        0.203976
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.998600 loss:        0.008744
Test - acc:         0.942500 loss:        0.205018
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.007868
Test - acc:         0.943300 loss:        0.205288
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.998740 loss:        0.008050
Test - acc:         0.942700 loss:        0.204211
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.998980 loss:        0.007303
Test - acc:         0.944700 loss:        0.204772
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.999040 loss:        0.007259
Test - acc:         0.943900 loss:        0.204913
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.998860 loss:        0.007006
Test - acc:         0.944200 loss:        0.204426
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.999000 loss:        0.006905
Test - acc:         0.944000 loss:        0.204843
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.006780
Test - acc:         0.942300 loss:        0.202502
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.006450
Test - acc:         0.944000 loss:        0.203562
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.999160 loss:        0.006354
Test - acc:         0.942400 loss:        0.204308
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.006124
Test - acc:         0.943400 loss:        0.202548
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.999280 loss:        0.006276
Test - acc:         0.944300 loss:        0.203817
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.005808
Test - acc:         0.944800 loss:        0.204444
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.999260 loss:        0.005789
Test - acc:         0.944500 loss:        0.205016
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.006126
Test - acc:         0.944200 loss:        0.206591
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.999280 loss:        0.005618
Test - acc:         0.944200 loss:        0.205988
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.006189
Test - acc:         0.944300 loss:        0.203592
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.005667
Test - acc:         0.943800 loss:        0.203184
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.999280 loss:        0.005600
Test - acc:         0.943800 loss:        0.204515
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999160 loss:        0.005871
Test - acc:         0.943700 loss:        0.202741
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.998960 loss:        0.005906
Test - acc:         0.944100 loss:        0.204270
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.999220 loss:        0.005412
Test - acc:         0.944900 loss:        0.203793
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.005431
Test - acc:         0.945300 loss:        0.201892
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.005308
Test - acc:         0.944300 loss:        0.204217
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.005247
Test - acc:         0.944300 loss:        0.204287
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.005191
Test - acc:         0.945300 loss:        0.204328
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.005241
Test - acc:         0.944700 loss:        0.204675
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.005343
Test - acc:         0.943800 loss:        0.205408
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.005136
Test - acc:         0.944700 loss:        0.202787
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.999260 loss:        0.005149
Test - acc:         0.943700 loss:        0.203349
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.004881
Test - acc:         0.944400 loss:        0.202905
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.004758
Test - acc:         0.943900 loss:        0.203241
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.004963
Test - acc:         0.942200 loss:        0.205957
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999380 loss:        0.004876
Test - acc:         0.942300 loss:        0.206327
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.004717
Test - acc:         0.943600 loss:        0.205088
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.004601
Test - acc:         0.943800 loss:        0.203725
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004477
Test - acc:         0.942200 loss:        0.207045
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.004055
Test - acc:         0.943600 loss:        0.205241
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.004451
Test - acc:         0.943300 loss:        0.205106
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.004640
Test - acc:         0.943200 loss:        0.204255
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.004385
Test - acc:         0.943700 loss:        0.203501
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004418
Test - acc:         0.944200 loss:        0.204894
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.004501
Test - acc:         0.943400 loss:        0.205546
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004352
Test - acc:         0.943400 loss:        0.205495
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004352
Test - acc:         0.943600 loss:        0.204554
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.004003
Test - acc:         0.943100 loss:        0.205552
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.004110
Test - acc:         0.943300 loss:        0.202957
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.004596
Test - acc:         0.943400 loss:        0.205311
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004222
Test - acc:         0.942600 loss:        0.205434
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.004419
Test - acc:         0.942600 loss:        0.204927
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.004441
Test - acc:         0.943700 loss:        0.205233
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.003941
Test - acc:         0.943600 loss:        0.203770
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004061
Test - acc:         0.942400 loss:        0.205035
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.004116
Test - acc:         0.943300 loss:        0.203142
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.004279
Test - acc:         0.943600 loss:        0.203423
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.003985
Test - acc:         0.942900 loss:        0.204008
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.003947
Test - acc:         0.943200 loss:        0.203442
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.003975
Test - acc:         0.944800 loss:        0.203351
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.004117
Test - acc:         0.943500 loss:        0.203718
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.003969
Test - acc:         0.942700 loss:        0.204071
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.003948
Test - acc:         0.944100 loss:        0.204091
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.003766
Test - acc:         0.942700 loss:        0.204830
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.004059
Test - acc:         0.943200 loss:        0.204311
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.004179
Test - acc:         0.942800 loss:        0.206389
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.004118
Test - acc:         0.942600 loss:        0.206507
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.003937
Test - acc:         0.942800 loss:        0.205370
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.003807
Test - acc:         0.942000 loss:        0.203794
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.004080
Test - acc:         0.944200 loss:        0.204046
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.003685
Test - acc:         0.944000 loss:        0.203500
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.003865
Test - acc:         0.944800 loss:        0.202897
Sparsity :          0.9375
Wdecay :        0.000500
