Running --model vgg19 --noise --prune_criterion weight_div_flips --seed 44 --prune_freq 70 --prune_rate 0.5 --comment=vgg19_crit=weight_div_flips_pf=70_seed=44 --save_model=pre-finetune/vgg19_weight_div_flips_pf70_s44 --logdir=criterion_experiment_no_bias/vgg19
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 70,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_weight_div_flips_pf70_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.142280 loss:        2.601396
Test - acc:         0.195100 loss:        2.206085
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.246620 loss:        1.929788
Test - acc:         0.236800 loss:        2.038829
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.317560 loss:        1.723914
Test - acc:         0.351700 loss:        1.641593
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.425080 loss:        1.509369
Test - acc:         0.464300 loss:        1.391149
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.541380 loss:        1.253156
Test - acc:         0.522800 loss:        1.493001
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.633180 loss:        1.041143
Test - acc:         0.596700 loss:        1.259192
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.678500 loss:        0.936769
Test - acc:         0.640400 loss:        1.105356
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.714680 loss:        0.849851
Test - acc:         0.644600 loss:        1.120985
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.737920 loss:        0.792265
Test - acc:         0.681900 loss:        0.964537
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.752860 loss:        0.748902
Test - acc:         0.563600 loss:        1.665525
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.767840 loss:        0.712795
Test - acc:         0.691600 loss:        0.983145
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.770600 loss:        0.698706
Test - acc:         0.638800 loss:        1.232823
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.781260 loss:        0.673360
Test - acc:         0.745400 loss:        0.827440
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.788280 loss:        0.654842
Test - acc:         0.754700 loss:        0.740684
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.795360 loss:        0.629470
Test - acc:         0.758100 loss:        0.771187
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.795060 loss:        0.629044
Test - acc:         0.725700 loss:        0.844025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.797820 loss:        0.619835
Test - acc:         0.711800 loss:        0.991450
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.802960 loss:        0.607546
Test - acc:         0.760700 loss:        0.740478
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.807800 loss:        0.589372
Test - acc:         0.766400 loss:        0.679525
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.808960 loss:        0.584498
Test - acc:         0.768200 loss:        0.704599
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.813640 loss:        0.575247
Test - acc:         0.781300 loss:        0.677502
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.815120 loss:        0.567870
Test - acc:         0.657500 loss:        1.206960
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.817880 loss:        0.565544
Test - acc:         0.748500 loss:        0.781382
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.816420 loss:        0.563773
Test - acc:         0.747700 loss:        0.820916
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.821880 loss:        0.550806
Test - acc:         0.784600 loss:        0.671629
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.822420 loss:        0.550200
Test - acc:         0.720700 loss:        0.864449
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.824940 loss:        0.540110
Test - acc:         0.777900 loss:        0.700156
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.825000 loss:        0.535996
Test - acc:         0.753200 loss:        0.851256
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.823200 loss:        0.541764
Test - acc:         0.794300 loss:        0.637654
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.825820 loss:        0.535093
Test - acc:         0.807500 loss:        0.606167
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.827760 loss:        0.530152
Test - acc:         0.766000 loss:        0.738118
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.828760 loss:        0.526739
Test - acc:         0.706400 loss:        0.947048
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.830080 loss:        0.521866
Test - acc:         0.806100 loss:        0.597559
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.828940 loss:        0.526150
Test - acc:         0.775400 loss:        0.670340
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.834500 loss:        0.507103
Test - acc:         0.767200 loss:        0.753308
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.832060 loss:        0.512132
Test - acc:         0.770600 loss:        0.721919
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.833820 loss:        0.511304
Test - acc:         0.778100 loss:        0.707310
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.833820 loss:        0.508718
Test - acc:         0.771000 loss:        0.770115
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.832700 loss:        0.512441
Test - acc:         0.787900 loss:        0.664319
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.834660 loss:        0.504084
Test - acc:         0.760100 loss:        0.795909
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.832100 loss:        0.510878
Test - acc:         0.798500 loss:        0.622620
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.835640 loss:        0.506822
Test - acc:         0.722400 loss:        0.936751
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.836760 loss:        0.500486
Test - acc:         0.794400 loss:        0.644790
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.836280 loss:        0.500193
Test - acc:         0.815700 loss:        0.576709
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.839780 loss:        0.494932
Test - acc:         0.824400 loss:        0.527701
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.839120 loss:        0.491536
Test - acc:         0.811300 loss:        0.575493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.839280 loss:        0.492528
Test - acc:         0.806200 loss:        0.582704
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.835840 loss:        0.499516
Test - acc:         0.826600 loss:        0.538733
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.838400 loss:        0.495237
Test - acc:         0.827300 loss:        0.543576
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.838480 loss:        0.497662
Test - acc:         0.715500 loss:        0.962318
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.836600 loss:        0.496041
Test - acc:         0.759900 loss:        0.766110
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.840260 loss:        0.489987
Test - acc:         0.775500 loss:        0.735669
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.838360 loss:        0.487913
Test - acc:         0.748200 loss:        0.860248
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.840340 loss:        0.487922
Test - acc:         0.720800 loss:        0.949842
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.840160 loss:        0.488129
Test - acc:         0.662700 loss:        1.263309
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.842740 loss:        0.481051
Test - acc:         0.746000 loss:        0.812895
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.838000 loss:        0.499553
Test - acc:         0.767900 loss:        0.734491
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.841780 loss:        0.486273
Test - acc:         0.719500 loss:        0.941604
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.840680 loss:        0.486788
Test - acc:         0.772700 loss:        0.717171
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.842160 loss:        0.480015
Test - acc:         0.762700 loss:        0.746719
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.841800 loss:        0.483841
Test - acc:         0.811000 loss:        0.566512
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.839920 loss:        0.483726
Test - acc:         0.761900 loss:        0.801496
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.843420 loss:        0.479714
Test - acc:         0.750700 loss:        0.818965
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.839340 loss:        0.485051
Test - acc:         0.796500 loss:        0.659835
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.843100 loss:        0.481682
Test - acc:         0.783900 loss:        0.666781
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.843140 loss:        0.480630
Test - acc:         0.731600 loss:        0.992442
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.840720 loss:        0.485064
Test - acc:         0.782000 loss:        0.676274
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.840960 loss:        0.479214
Test - acc:         0.834700 loss:        0.515106
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.841520 loss:        0.482550
Test - acc:         0.768000 loss:        0.739647
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.842760 loss:        0.473760
Test - acc:         0.769300 loss:        0.747690
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.847360 loss:        0.461237
Test - acc:         0.756900 loss:        0.762498
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.851500 loss:        0.454083
Test - acc:         0.818000 loss:        0.547466
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.848800 loss:        0.456818
Test - acc:         0.801900 loss:        0.639036
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.846020 loss:        0.467307
Test - acc:         0.746100 loss:        0.806611
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.848500 loss:        0.457045
Test - acc:         0.820400 loss:        0.561230
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.853340 loss:        0.450498
Test - acc:         0.797200 loss:        0.609644
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.844120 loss:        0.468909
Test - acc:         0.810400 loss:        0.605243
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.847900 loss:        0.462806
Test - acc:         0.775900 loss:        0.744548
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.848160 loss:        0.458476
Test - acc:         0.765800 loss:        0.742435
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.844180 loss:        0.467677
Test - acc:         0.777700 loss:        0.735399
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.848140 loss:        0.457692
Test - acc:         0.836900 loss:        0.508886
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.847960 loss:        0.459489
Test - acc:         0.812100 loss:        0.557579
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.849180 loss:        0.458539
Test - acc:         0.775000 loss:        0.728806
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.847700 loss:        0.460089
Test - acc:         0.759000 loss:        0.754435
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.844640 loss:        0.466698
Test - acc:         0.833000 loss:        0.517848
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.848920 loss:        0.456771
Test - acc:         0.828200 loss:        0.529859
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.848380 loss:        0.456513
Test - acc:         0.793200 loss:        0.609489
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.850860 loss:        0.451324
Test - acc:         0.820000 loss:        0.562534
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.851700 loss:        0.450805
Test - acc:         0.760100 loss:        0.745514
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.849280 loss:        0.457924
Test - acc:         0.812700 loss:        0.606051
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.849140 loss:        0.455876
Test - acc:         0.802700 loss:        0.629469
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.849540 loss:        0.452885
Test - acc:         0.804100 loss:        0.626663
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.851460 loss:        0.451828
Test - acc:         0.756800 loss:        0.774877
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.849280 loss:        0.459053
Test - acc:         0.832400 loss:        0.510942
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.848600 loss:        0.454211
Test - acc:         0.799300 loss:        0.621230
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.847720 loss:        0.463211
Test - acc:         0.772700 loss:        0.761609
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.854060 loss:        0.445912
Test - acc:         0.829800 loss:        0.521837
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.848900 loss:        0.454932
Test - acc:         0.799300 loss:        0.623814
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.849500 loss:        0.456720
Test - acc:         0.804800 loss:        0.599609
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.850460 loss:        0.448329
Test - acc:         0.812800 loss:        0.596402
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.851920 loss:        0.451294
Test - acc:         0.810900 loss:        0.578621
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.847860 loss:        0.460776
Test - acc:         0.829200 loss:        0.494211
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.849580 loss:        0.452041
Test - acc:         0.804900 loss:        0.602640
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.849700 loss:        0.456424
Test - acc:         0.789100 loss:        0.626166
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.850540 loss:        0.454081
Test - acc:         0.771400 loss:        0.741653
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.853180 loss:        0.448866
Test - acc:         0.816000 loss:        0.563313
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.847220 loss:        0.458037
Test - acc:         0.808700 loss:        0.589768
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.846980 loss:        0.462607
Test - acc:         0.778500 loss:        0.686798
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.852620 loss:        0.449462
Test - acc:         0.792300 loss:        0.647352
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.848460 loss:        0.461013
Test - acc:         0.788500 loss:        0.676915
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.850280 loss:        0.458835
Test - acc:         0.753800 loss:        0.770854
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.847480 loss:        0.458299
Test - acc:         0.777100 loss:        0.739462
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.848940 loss:        0.456744
Test - acc:         0.772700 loss:        0.767007
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.848860 loss:        0.457125
Test - acc:         0.791200 loss:        0.661913
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.850960 loss:        0.450489
Test - acc:         0.769400 loss:        0.734432
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.849840 loss:        0.457929
Test - acc:         0.823100 loss:        0.558741
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.851780 loss:        0.448699
Test - acc:         0.787900 loss:        0.706958
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.850260 loss:        0.453318
Test - acc:         0.808600 loss:        0.614858
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.852900 loss:        0.448642
Test - acc:         0.808700 loss:        0.598575
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.849900 loss:        0.452415
Test - acc:         0.796000 loss:        0.609795
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.852520 loss:        0.450556
Test - acc:         0.789000 loss:        0.655513
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.849520 loss:        0.456199
Test - acc:         0.827200 loss:        0.523138
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.852260 loss:        0.447167
Test - acc:         0.778400 loss:        0.708185
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.847340 loss:        0.456650
Test - acc:         0.807100 loss:        0.614372
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.851280 loss:        0.452675
Test - acc:         0.778300 loss:        0.724730
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.852720 loss:        0.444554
Test - acc:         0.746700 loss:        0.833294
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.853440 loss:        0.442492
Test - acc:         0.823800 loss:        0.566751
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.852300 loss:        0.450075
Test - acc:         0.801800 loss:        0.608340
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.849620 loss:        0.457106
Test - acc:         0.779300 loss:        0.692678
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.852520 loss:        0.447094
Test - acc:         0.799100 loss:        0.654193
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.849700 loss:        0.454371
Test - acc:         0.751400 loss:        0.772870
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.850000 loss:        0.453973
Test - acc:         0.799700 loss:        0.655573
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.852300 loss:        0.451357
Test - acc:         0.795200 loss:        0.641781
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.854000 loss:        0.445056
Test - acc:         0.804800 loss:        0.611769
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.851280 loss:        0.448265
Test - acc:         0.812200 loss:        0.570263
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.852100 loss:        0.453060
Test - acc:         0.789000 loss:        0.670543
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.850760 loss:        0.450620
Test - acc:         0.801800 loss:        0.621963
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.848140 loss:        0.457729
Test - acc:         0.778400 loss:        0.711099
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.851140 loss:        0.455812
Test - acc:         0.765700 loss:        0.714002
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.848420 loss:        0.455899
Test - acc:         0.766100 loss:        0.689260
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.856580 loss:        0.431065
Test - acc:         0.798400 loss:        0.621159
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.854060 loss:        0.434616
Test - acc:         0.797400 loss:        0.639098
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.855920 loss:        0.433354
Test - acc:         0.781200 loss:        0.712367
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.852320 loss:        0.443640
Test - acc:         0.750900 loss:        0.827194
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.853860 loss:        0.442612
Test - acc:         0.819200 loss:        0.569658
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.852440 loss:        0.441275
Test - acc:         0.788500 loss:        0.691454
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.853140 loss:        0.442012
Test - acc:         0.796500 loss:        0.632736
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.854720 loss:        0.432223
Test - acc:         0.808800 loss:        0.611756
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.852280 loss:        0.442191
Test - acc:         0.810300 loss:        0.589878
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.855660 loss:        0.429405
Test - acc:         0.813100 loss:        0.583905
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.912740 loss:        0.261134
Test - acc:         0.900300 loss:        0.301054
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.930020 loss:        0.207913
Test - acc:         0.906800 loss:        0.280418
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.935860 loss:        0.189516
Test - acc:         0.912300 loss:        0.263935
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.942600 loss:        0.170437
Test - acc:         0.909800 loss:        0.272782
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.945960 loss:        0.161264
Test - acc:         0.914800 loss:        0.268495
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.949340 loss:        0.147063
Test - acc:         0.915000 loss:        0.262418
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.950900 loss:        0.142679
Test - acc:         0.913400 loss:        0.270310
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.954740 loss:        0.131648
Test - acc:         0.912500 loss:        0.280163
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.956520 loss:        0.125915
Test - acc:         0.913300 loss:        0.273898
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.958880 loss:        0.120099
Test - acc:         0.910700 loss:        0.290425
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.960200 loss:        0.116071
Test - acc:         0.915100 loss:        0.283144
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.961320 loss:        0.113711
Test - acc:         0.912000 loss:        0.291542
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.963440 loss:        0.105780
Test - acc:         0.910200 loss:        0.300225
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.964120 loss:        0.105486
Test - acc:         0.910900 loss:        0.299815
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.966300 loss:        0.100168
Test - acc:         0.914500 loss:        0.285019
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.965100 loss:        0.100500
Test - acc:         0.914500 loss:        0.279811
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.966980 loss:        0.097214
Test - acc:         0.909400 loss:        0.294503
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.968420 loss:        0.094756
Test - acc:         0.908200 loss:        0.319061
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.966760 loss:        0.096646
Test - acc:         0.910900 loss:        0.298572
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.967860 loss:        0.093670
Test - acc:         0.911700 loss:        0.299528
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.968860 loss:        0.089447
Test - acc:         0.909500 loss:        0.309440
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.967460 loss:        0.094244
Test - acc:         0.901800 loss:        0.334688
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.969020 loss:        0.090171
Test - acc:         0.912800 loss:        0.300476
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.968860 loss:        0.087223
Test - acc:         0.901500 loss:        0.336210
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.968980 loss:        0.090455
Test - acc:         0.907800 loss:        0.318347
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.968340 loss:        0.089663
Test - acc:         0.905300 loss:        0.333176
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.967960 loss:        0.091616
Test - acc:         0.898800 loss:        0.355905
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.966400 loss:        0.097472
Test - acc:         0.900400 loss:        0.362144
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.967940 loss:        0.091540
Test - acc:         0.903800 loss:        0.333321
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.965980 loss:        0.094533
Test - acc:         0.905900 loss:        0.327655
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.967140 loss:        0.094814
Test - acc:         0.903900 loss:        0.343364
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.967660 loss:        0.092547
Test - acc:         0.906200 loss:        0.332538
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.967340 loss:        0.093993
Test - acc:         0.898100 loss:        0.366685
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.966800 loss:        0.094131
Test - acc:         0.902400 loss:        0.343490
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.967800 loss:        0.092534
Test - acc:         0.900600 loss:        0.357651
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.968760 loss:        0.091731
Test - acc:         0.907200 loss:        0.323069
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.966500 loss:        0.099688
Test - acc:         0.906000 loss:        0.318355
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.968560 loss:        0.091319
Test - acc:         0.898400 loss:        0.345557
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.968520 loss:        0.091085
Test - acc:         0.893300 loss:        0.392027
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.967160 loss:        0.094509
Test - acc:         0.897200 loss:        0.357360
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.968500 loss:        0.091483
Test - acc:         0.900900 loss:        0.339650
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.969180 loss:        0.091041
Test - acc:         0.896700 loss:        0.358874
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.967960 loss:        0.094698
Test - acc:         0.907000 loss:        0.316889
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.969240 loss:        0.090572
Test - acc:         0.901300 loss:        0.366173
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.968400 loss:        0.093441
Test - acc:         0.903700 loss:        0.344355
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.967360 loss:        0.096586
Test - acc:         0.890900 loss:        0.374450
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.965740 loss:        0.097333
Test - acc:         0.900300 loss:        0.353420
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.966480 loss:        0.097353
Test - acc:         0.898300 loss:        0.373829
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.967640 loss:        0.095968
Test - acc:         0.897600 loss:        0.358827
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.967780 loss:        0.091986
Test - acc:         0.901400 loss:        0.364000
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.966680 loss:        0.095975
Test - acc:         0.897300 loss:        0.374024
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.965220 loss:        0.100738
Test - acc:         0.902600 loss:        0.334253
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.968660 loss:        0.092175
Test - acc:         0.898000 loss:        0.362976
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.968140 loss:        0.093377
Test - acc:         0.893600 loss:        0.379454
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.967580 loss:        0.093841
Test - acc:         0.910100 loss:        0.316532
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.968560 loss:        0.090901
Test - acc:         0.902800 loss:        0.357545
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.965840 loss:        0.099185
Test - acc:         0.890600 loss:        0.394743
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.967360 loss:        0.096747
Test - acc:         0.905200 loss:        0.324227
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.969100 loss:        0.091896
Test - acc:         0.890200 loss:        0.409587
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.968340 loss:        0.092033
Test - acc:         0.902000 loss:        0.350612
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.971160 loss:        0.083233
Test - acc:         0.900400 loss:        0.355844
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.968740 loss:        0.090551
Test - acc:         0.896000 loss:        0.384169
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.972080 loss:        0.081522
Test - acc:         0.900900 loss:        0.361695
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.969500 loss:        0.088417
Test - acc:         0.903600 loss:        0.344704
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.970560 loss:        0.084714
Test - acc:         0.901000 loss:        0.354968
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.971540 loss:        0.083755
Test - acc:         0.899400 loss:        0.339538
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.970100 loss:        0.085520
Test - acc:         0.898300 loss:        0.359760
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.970000 loss:        0.085805
Test - acc:         0.901800 loss:        0.332238
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.971440 loss:        0.084640
Test - acc:         0.894100 loss:        0.366527
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.969520 loss:        0.088954
Test - acc:         0.901600 loss:        0.368669
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.971140 loss:        0.084257
Test - acc:         0.894000 loss:        0.382444
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.970060 loss:        0.085946
Test - acc:         0.900700 loss:        0.359743
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.971560 loss:        0.083685
Test - acc:         0.897400 loss:        0.372770
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.970920 loss:        0.083112
Test - acc:         0.904300 loss:        0.354909
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.971060 loss:        0.084632
Test - acc:         0.901200 loss:        0.357162
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.971780 loss:        0.083412
Test - acc:         0.902100 loss:        0.356060
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.970620 loss:        0.085551
Test - acc:         0.901100 loss:        0.347952
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.971280 loss:        0.083810
Test - acc:         0.904400 loss:        0.341955
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.969600 loss:        0.087096
Test - acc:         0.901500 loss:        0.340386
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.972360 loss:        0.080345
Test - acc:         0.899800 loss:        0.346596
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.970400 loss:        0.084486
Test - acc:         0.908300 loss:        0.321190
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.970680 loss:        0.085187
Test - acc:         0.898800 loss:        0.363061
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.970240 loss:        0.087802
Test - acc:         0.900300 loss:        0.358280
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.970160 loss:        0.085592
Test - acc:         0.904200 loss:        0.345862
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.972180 loss:        0.080860
Test - acc:         0.897000 loss:        0.368442
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.970480 loss:        0.087180
Test - acc:         0.904300 loss:        0.348461
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.971120 loss:        0.084993
Test - acc:         0.906000 loss:        0.349262
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.971860 loss:        0.082418
Test - acc:         0.897700 loss:        0.369138
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.972220 loss:        0.081470
Test - acc:         0.896800 loss:        0.378033
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.970520 loss:        0.084783
Test - acc:         0.896700 loss:        0.373900
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.971820 loss:        0.082474
Test - acc:         0.899800 loss:        0.361075
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.971420 loss:        0.084030
Test - acc:         0.897500 loss:        0.392565
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.970360 loss:        0.085133
Test - acc:         0.895400 loss:        0.364970
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.970260 loss:        0.085738
Test - acc:         0.897100 loss:        0.374513
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.971080 loss:        0.082625
Test - acc:         0.903000 loss:        0.363634
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.972260 loss:        0.083704
Test - acc:         0.892000 loss:        0.402849
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.970440 loss:        0.086417
Test - acc:         0.893900 loss:        0.371090
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.972060 loss:        0.080314
Test - acc:         0.898100 loss:        0.366989
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.971600 loss:        0.084166
Test - acc:         0.890800 loss:        0.397974
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.972220 loss:        0.081525
Test - acc:         0.894000 loss:        0.382837
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.985480 loss:        0.043258
Test - acc:         0.919200 loss:        0.289599
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.991520 loss:        0.027794
Test - acc:         0.920700 loss:        0.292058
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.993280 loss:        0.022779
Test - acc:         0.922500 loss:        0.294465
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.994380 loss:        0.019001
Test - acc:         0.923100 loss:        0.295410
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.994820 loss:        0.017019
Test - acc:         0.921300 loss:        0.300267
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.995480 loss:        0.015359
Test - acc:         0.923600 loss:        0.298383
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.996040 loss:        0.013651
Test - acc:         0.923000 loss:        0.304609
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.995920 loss:        0.013660
Test - acc:         0.923200 loss:        0.306490
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.996680 loss:        0.012334
Test - acc:         0.924600 loss:        0.307333
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.996660 loss:        0.012267
Test - acc:         0.924900 loss:        0.313616
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.997000 loss:        0.010499
Test - acc:         0.924700 loss:        0.312968
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.997140 loss:        0.010162
Test - acc:         0.924700 loss:        0.316338
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.997500 loss:        0.009181
Test - acc:         0.923900 loss:        0.320540
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.997220 loss:        0.010488
Test - acc:         0.922600 loss:        0.324444
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.997640 loss:        0.008352
Test - acc:         0.924100 loss:        0.323584
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.997800 loss:        0.007976
Test - acc:         0.923600 loss:        0.324105
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.997800 loss:        0.007984
Test - acc:         0.925300 loss:        0.325392
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.998060 loss:        0.007609
Test - acc:         0.925700 loss:        0.323216
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.998300 loss:        0.006952
Test - acc:         0.925300 loss:        0.330243
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.998300 loss:        0.006665
Test - acc:         0.925500 loss:        0.330814
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.998160 loss:        0.007167
Test - acc:         0.925200 loss:        0.327658
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.998360 loss:        0.006612
Test - acc:         0.924400 loss:        0.332983
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.998440 loss:        0.006120
Test - acc:         0.923800 loss:        0.332823
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.998360 loss:        0.005768
Test - acc:         0.924800 loss:        0.334392
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.998220 loss:        0.006350
Test - acc:         0.923700 loss:        0.339987
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.998180 loss:        0.006396
Test - acc:         0.923900 loss:        0.337934
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.998460 loss:        0.005625
Test - acc:         0.925400 loss:        0.336159
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.998640 loss:        0.005387
Test - acc:         0.926100 loss:        0.333949
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.998480 loss:        0.006178
Test - acc:         0.925600 loss:        0.339486
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.998480 loss:        0.005740
Test - acc:         0.924300 loss:        0.341742
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.998400 loss:        0.006815
Test - acc:         0.925200 loss:        0.326155
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.998340 loss:        0.006887
Test - acc:         0.924000 loss:        0.328512
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.998440 loss:        0.006256
Test - acc:         0.924700 loss:        0.326679
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.998540 loss:        0.005994
Test - acc:         0.924500 loss:        0.326235
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.998520 loss:        0.005825
Test - acc:         0.924800 loss:        0.328780
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.998760 loss:        0.005210
Test - acc:         0.925600 loss:        0.327978
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.998880 loss:        0.005035
Test - acc:         0.926200 loss:        0.329814
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.998520 loss:        0.005422
Test - acc:         0.925300 loss:        0.334521
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.998760 loss:        0.004750
Test - acc:         0.926900 loss:        0.333587
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.998940 loss:        0.004770
Test - acc:         0.926500 loss:        0.329157
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.998280 loss:        0.005804
Test - acc:         0.926400 loss:        0.331455
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.998740 loss:        0.004860
Test - acc:         0.925600 loss:        0.334442
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.999100 loss:        0.004258
Test - acc:         0.925800 loss:        0.335712
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.998860 loss:        0.004599
Test - acc:         0.927100 loss:        0.338723
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.999040 loss:        0.004392
Test - acc:         0.926100 loss:        0.341416
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.004163
Test - acc:         0.925900 loss:        0.340324
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.003958
Test - acc:         0.927400 loss:        0.335279
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.998920 loss:        0.004416
Test - acc:         0.927100 loss:        0.338524
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.998900 loss:        0.004094
Test - acc:         0.925400 loss:        0.339192
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.003734
Test - acc:         0.926800 loss:        0.339457
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.004246
Test - acc:         0.926600 loss:        0.340982
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.998860 loss:        0.004229
Test - acc:         0.926200 loss:        0.344190
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.998920 loss:        0.004298
Test - acc:         0.926600 loss:        0.344716
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.003593
Test - acc:         0.927000 loss:        0.341399
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.003775
Test - acc:         0.926600 loss:        0.343200
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.003669
Test - acc:         0.927200 loss:        0.341468
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999040 loss:        0.003844
Test - acc:         0.927500 loss:        0.342449
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999080 loss:        0.003784
Test - acc:         0.928600 loss:        0.344393
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.004011
Test - acc:         0.929700 loss:        0.341483
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.003491
Test - acc:         0.928200 loss:        0.345347
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999180 loss:        0.003702
Test - acc:         0.928400 loss:        0.353952
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.003335
Test - acc:         0.928300 loss:        0.347607
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.003249
Test - acc:         0.926800 loss:        0.354323
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999080 loss:        0.003536
Test - acc:         0.927700 loss:        0.351368
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999120 loss:        0.003574
Test - acc:         0.926400 loss:        0.356044
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999180 loss:        0.003335
Test - acc:         0.928400 loss:        0.353550
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.003263
Test - acc:         0.927300 loss:        0.353755
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.003150
Test - acc:         0.926000 loss:        0.354041
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999180 loss:        0.003546
Test - acc:         0.926900 loss:        0.354921
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.003859
Test - acc:         0.928500 loss:        0.352952
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999100 loss:        0.003532
Test - acc:         0.927200 loss:        0.351551
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.998980 loss:        0.003752
Test - acc:         0.926100 loss:        0.352405
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.999140 loss:        0.003836
Test - acc:         0.926400 loss:        0.353063
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999220 loss:        0.003297
Test - acc:         0.926500 loss:        0.355077
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.003695
Test - acc:         0.927100 loss:        0.348597
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.003160
Test - acc:         0.928700 loss:        0.350488
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.003315
Test - acc:         0.926700 loss:        0.355852
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.003044
Test - acc:         0.927200 loss:        0.354255
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999220 loss:        0.003409
Test - acc:         0.926000 loss:        0.355250
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.999280 loss:        0.003203
Test - acc:         0.926200 loss:        0.355244
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.002783
Test - acc:         0.927200 loss:        0.354427
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.002793
Test - acc:         0.926700 loss:        0.358671
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.002605
Test - acc:         0.928300 loss:        0.353655
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.003273
Test - acc:         0.927300 loss:        0.353461
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.002706
Test - acc:         0.926100 loss:        0.359205
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.003036
Test - acc:         0.926800 loss:        0.361383
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.002915
Test - acc:         0.926300 loss:        0.363216
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999380 loss:        0.002766
Test - acc:         0.926900 loss:        0.358052
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.002507
Test - acc:         0.925700 loss:        0.364632
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.002638
Test - acc:         0.926300 loss:        0.363042
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.002546
Test - acc:         0.926400 loss:        0.365926
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999260 loss:        0.003160
Test - acc:         0.928100 loss:        0.363387
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.002944
Test - acc:         0.927000 loss:        0.362872
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.002574
Test - acc:         0.926600 loss:        0.360446
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.999280 loss:        0.003251
Test - acc:         0.925000 loss:        0.371983
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.002916
Test - acc:         0.925000 loss:        0.366431
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.002461
Test - acc:         0.927500 loss:        0.360249
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.002563
Test - acc:         0.927900 loss:        0.362522
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.002997
Test - acc:         0.927100 loss:        0.360010
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.002548
Test - acc:         0.927600 loss:        0.363279
Sparsity :          0.9375
Wdecay :        0.000500
