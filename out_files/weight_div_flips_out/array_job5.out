Running --model resnet18 --noise --prune_criterion weight_div_flips --seed 42 --prune_freq 32 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=32_seed=42 --save_model=pre-finetune/resnet18_weight_div_flips_pf32_s42 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf32_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.319760 loss:        2.027701
Test - acc:         0.373600 loss:        1.709656
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.483520 loss:        1.421054
Test - acc:         0.526700 loss:        1.263736
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.596320 loss:        1.129661
Test - acc:         0.595600 loss:        1.171379
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.673280 loss:        0.927933
Test - acc:         0.610100 loss:        1.134739
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.723460 loss:        0.790148
Test - acc:         0.705200 loss:        0.872062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.766400 loss:        0.675023
Test - acc:         0.746900 loss:        0.720768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.790000 loss:        0.608257
Test - acc:         0.766300 loss:        0.692052
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.805140 loss:        0.568890
Test - acc:         0.777000 loss:        0.661023
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.812440 loss:        0.540974
Test - acc:         0.748500 loss:        0.765813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.823580 loss:        0.513555
Test - acc:         0.742700 loss:        0.756514
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.831620 loss:        0.490783
Test - acc:         0.778800 loss:        0.671311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.834560 loss:        0.478143
Test - acc:         0.793000 loss:        0.626211
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.841480 loss:        0.462775
Test - acc:         0.792800 loss:        0.604948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838760 loss:        0.468699
Test - acc:         0.789100 loss:        0.624176
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844780 loss:        0.450396
Test - acc:         0.816100 loss:        0.559841
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.846280 loss:        0.448807
Test - acc:         0.764600 loss:        0.719807
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.850540 loss:        0.438978
Test - acc:         0.818300 loss:        0.530640
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.850840 loss:        0.433470
Test - acc:         0.796600 loss:        0.604748
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.853620 loss:        0.425959
Test - acc:         0.841700 loss:        0.488544
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.854620 loss:        0.421329
Test - acc:         0.795500 loss:        0.623324
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.859220 loss:        0.417579
Test - acc:         0.823000 loss:        0.516899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.856440 loss:        0.420470
Test - acc:         0.758600 loss:        0.712926
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.858640 loss:        0.411732
Test - acc:         0.793800 loss:        0.625225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.858100 loss:        0.413444
Test - acc:         0.793100 loss:        0.628817
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.862000 loss:        0.407803
Test - acc:         0.819100 loss:        0.573117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.861660 loss:        0.407108
Test - acc:         0.805500 loss:        0.599522
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.864180 loss:        0.397832
Test - acc:         0.836000 loss:        0.468893
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.863120 loss:        0.402166
Test - acc:         0.833700 loss:        0.486745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.863140 loss:        0.396906
Test - acc:         0.855500 loss:        0.423829
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.867600 loss:        0.394886
Test - acc:         0.810100 loss:        0.579276
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.865620 loss:        0.394207
Test - acc:         0.853400 loss:        0.434255
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.866820 loss:        0.393427
Test - acc:         0.698200 loss:        1.116456
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.878000 loss:        0.360237
Test - acc:         0.813700 loss:        0.560753
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.874260 loss:        0.366944
Test - acc:         0.838000 loss:        0.489026
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.875880 loss:        0.362550
Test - acc:         0.810600 loss:        0.588037
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.872780 loss:        0.369041
Test - acc:         0.835600 loss:        0.486819
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.873740 loss:        0.369941
Test - acc:         0.797000 loss:        0.622816
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.874800 loss:        0.368508
Test - acc:         0.821400 loss:        0.579785
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.871600 loss:        0.374821
Test - acc:         0.839700 loss:        0.477365
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.875440 loss:        0.361872
Test - acc:         0.823700 loss:        0.529171
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.875500 loss:        0.369461
Test - acc:         0.811300 loss:        0.571206
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.876100 loss:        0.365152
Test - acc:         0.840800 loss:        0.472521
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.875760 loss:        0.359769
Test - acc:         0.797100 loss:        0.645004
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.874740 loss:        0.364072
Test - acc:         0.809400 loss:        0.619120
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.875860 loss:        0.365261
Test - acc:         0.856300 loss:        0.425624
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.875060 loss:        0.365868
Test - acc:         0.806100 loss:        0.603037
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.873760 loss:        0.368282
Test - acc:         0.805500 loss:        0.579913
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.872980 loss:        0.369905
Test - acc:         0.832300 loss:        0.498549
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.876180 loss:        0.361437
Test - acc:         0.821200 loss:        0.572497
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.875400 loss:        0.364160
Test - acc:         0.833600 loss:        0.489842
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.873840 loss:        0.368753
Test - acc:         0.849500 loss:        0.469002
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.876900 loss:        0.359047
Test - acc:         0.852100 loss:        0.446216
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.874380 loss:        0.365478
Test - acc:         0.831200 loss:        0.509087
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.877880 loss:        0.357110
Test - acc:         0.805000 loss:        0.598567
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.876520 loss:        0.361536
Test - acc:         0.790100 loss:        0.666540
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.874200 loss:        0.362808
Test - acc:         0.812900 loss:        0.593489
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.875260 loss:        0.364682
Test - acc:         0.852600 loss:        0.429973
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.877700 loss:        0.360325
Test - acc:         0.837200 loss:        0.472199
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.876100 loss:        0.362141
Test - acc:         0.859700 loss:        0.425374
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.873000 loss:        0.371131
Test - acc:         0.811700 loss:        0.554421
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.875800 loss:        0.363452
Test - acc:         0.835700 loss:        0.508672
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.874060 loss:        0.367142
Test - acc:         0.782800 loss:        0.670002
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.877660 loss:        0.360201
Test - acc:         0.818100 loss:        0.544095
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.878920 loss:        0.361377
Test - acc:         0.826500 loss:        0.528879
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.885640 loss:        0.332417
Test - acc:         0.834600 loss:        0.517274
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.887900 loss:        0.329762
Test - acc:         0.855400 loss:        0.434660
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.885140 loss:        0.335162
Test - acc:         0.808900 loss:        0.589146
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.882820 loss:        0.343821
Test - acc:         0.840800 loss:        0.480343
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.883440 loss:        0.341590
Test - acc:         0.848700 loss:        0.450118
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.882320 loss:        0.343845
Test - acc:         0.805900 loss:        0.577030
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.883560 loss:        0.337346
Test - acc:         0.839500 loss:        0.494568
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.881540 loss:        0.345285
Test - acc:         0.823200 loss:        0.535081
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.882560 loss:        0.343300
Test - acc:         0.857700 loss:        0.417345
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.883260 loss:        0.342857
Test - acc:         0.828400 loss:        0.515374
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.882440 loss:        0.340403
Test - acc:         0.821800 loss:        0.538708
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.882660 loss:        0.341112
Test - acc:         0.846200 loss:        0.455937
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.883760 loss:        0.338664
Test - acc:         0.840200 loss:        0.483055
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.884540 loss:        0.338423
Test - acc:         0.846500 loss:        0.442759
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.882640 loss:        0.341870
Test - acc:         0.858800 loss:        0.420840
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.884560 loss:        0.336407
Test - acc:         0.836200 loss:        0.497149
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.881060 loss:        0.342844
Test - acc:         0.821500 loss:        0.559854
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.883080 loss:        0.340194
Test - acc:         0.857600 loss:        0.419192
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.885840 loss:        0.333285
Test - acc:         0.785400 loss:        0.721249
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.885180 loss:        0.335692
Test - acc:         0.808800 loss:        0.642569
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.881860 loss:        0.346604
Test - acc:         0.834900 loss:        0.474687
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.882260 loss:        0.342610
Test - acc:         0.852400 loss:        0.453811
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.884600 loss:        0.337310
Test - acc:         0.813000 loss:        0.569547
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.884700 loss:        0.337931
Test - acc:         0.822600 loss:        0.525316
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.883060 loss:        0.341040
Test - acc:         0.840100 loss:        0.469956
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.885120 loss:        0.335938
Test - acc:         0.846700 loss:        0.463022
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.881760 loss:        0.343943
Test - acc:         0.847400 loss:        0.442840
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.883200 loss:        0.340289
Test - acc:         0.863100 loss:        0.404432
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.885100 loss:        0.336520
Test - acc:         0.855800 loss:        0.431871
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.881980 loss:        0.344033
Test - acc:         0.809700 loss:        0.602713
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.883480 loss:        0.341161
Test - acc:         0.829100 loss:        0.516936
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.883800 loss:        0.342014
Test - acc:         0.852800 loss:        0.445543
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.897420 loss:        0.298310
Test - acc:         0.854200 loss:        0.435857
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.893220 loss:        0.311172
Test - acc:         0.852600 loss:        0.440279
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.891380 loss:        0.313874
Test - acc:         0.851300 loss:        0.479010
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.889920 loss:        0.316942
Test - acc:         0.866700 loss:        0.399895
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.889440 loss:        0.317638
Test - acc:         0.867600 loss:        0.391076
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.892520 loss:        0.312337
Test - acc:         0.860100 loss:        0.412248
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.892200 loss:        0.317805
Test - acc:         0.831200 loss:        0.511170
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.891560 loss:        0.317270
Test - acc:         0.832100 loss:        0.523780
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.890900 loss:        0.318509
Test - acc:         0.862600 loss:        0.418508
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.889920 loss:        0.322377
Test - acc:         0.856100 loss:        0.435795
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.891040 loss:        0.315811
Test - acc:         0.836900 loss:        0.502892
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.890300 loss:        0.321754
Test - acc:         0.844300 loss:        0.480540
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.889200 loss:        0.318580
Test - acc:         0.855000 loss:        0.442310
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.891140 loss:        0.318150
Test - acc:         0.853800 loss:        0.440571
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.887760 loss:        0.320823
Test - acc:         0.848100 loss:        0.464104
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.890700 loss:        0.317865
Test - acc:         0.822500 loss:        0.525052
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.888680 loss:        0.323570
Test - acc:         0.828700 loss:        0.561415
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.888920 loss:        0.320258
Test - acc:         0.854600 loss:        0.449747
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.892400 loss:        0.315825
Test - acc:         0.853400 loss:        0.431317
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.889560 loss:        0.323531
Test - acc:         0.844400 loss:        0.484790
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.893060 loss:        0.310566
Test - acc:         0.831600 loss:        0.536971
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.889160 loss:        0.320018
Test - acc:         0.829400 loss:        0.510934
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.893080 loss:        0.314194
Test - acc:         0.841800 loss:        0.481329
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.891120 loss:        0.321149
Test - acc:         0.855600 loss:        0.452571
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.892260 loss:        0.313722
Test - acc:         0.842700 loss:        0.480721
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.892340 loss:        0.316255
Test - acc:         0.828600 loss:        0.537252
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.889700 loss:        0.319781
Test - acc:         0.845800 loss:        0.465594
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.892560 loss:        0.316316
Test - acc:         0.850600 loss:        0.450373
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.890980 loss:        0.318001
Test - acc:         0.835700 loss:        0.497023
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.891780 loss:        0.316749
Test - acc:         0.853300 loss:        0.443363
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.890700 loss:        0.316548
Test - acc:         0.821000 loss:        0.545246
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.891180 loss:        0.319930
Test - acc:         0.787700 loss:        0.673683
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.901060 loss:        0.285946
Test - acc:         0.852800 loss:        0.442007
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.901340 loss:        0.289668
Test - acc:         0.858000 loss:        0.425168
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.900340 loss:        0.288971
Test - acc:         0.859700 loss:        0.403280
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.896860 loss:        0.299938
Test - acc:         0.857400 loss:        0.411467
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.899360 loss:        0.292071
Test - acc:         0.858400 loss:        0.431890
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.895260 loss:        0.301167
Test - acc:         0.849200 loss:        0.474570
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.897440 loss:        0.296884
Test - acc:         0.863700 loss:        0.417424
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.899540 loss:        0.291411
Test - acc:         0.861700 loss:        0.432144
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.897620 loss:        0.297561
Test - acc:         0.854900 loss:        0.441673
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.898140 loss:        0.296463
Test - acc:         0.864300 loss:        0.411612
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.895560 loss:        0.298930
Test - acc:         0.839700 loss:        0.509508
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.897040 loss:        0.297221
Test - acc:         0.798800 loss:        0.698379
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.896200 loss:        0.298407
Test - acc:         0.843600 loss:        0.486462
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.899720 loss:        0.292892
Test - acc:         0.859400 loss:        0.417027
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.898240 loss:        0.294938
Test - acc:         0.866800 loss:        0.406857
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.897820 loss:        0.295654
Test - acc:         0.848300 loss:        0.463590
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.896940 loss:        0.298285
Test - acc:         0.865100 loss:        0.423733
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.898240 loss:        0.296761
Test - acc:         0.863400 loss:        0.427487
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.899880 loss:        0.297354
Test - acc:         0.863900 loss:        0.398758
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.896680 loss:        0.301295
Test - acc:         0.835200 loss:        0.484311
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.897380 loss:        0.297604
Test - acc:         0.849200 loss:        0.462964
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.898640 loss:        0.295223
Test - acc:         0.839200 loss:        0.487059
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.940080 loss:        0.177331
Test - acc:         0.925600 loss:        0.220208
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.954660 loss:        0.137355
Test - acc:         0.927900 loss:        0.210354
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.959620 loss:        0.122015
Test - acc:         0.929200 loss:        0.212340
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.964620 loss:        0.107135
Test - acc:         0.931400 loss:        0.206942
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.965780 loss:        0.104191
Test - acc:         0.931000 loss:        0.207699
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.967960 loss:        0.094261
Test - acc:         0.929500 loss:        0.217750
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.970440 loss:        0.089501
Test - acc:         0.931500 loss:        0.209312
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.971520 loss:        0.085309
Test - acc:         0.933000 loss:        0.208570
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.972900 loss:        0.080057
Test - acc:         0.931400 loss:        0.218251
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.975300 loss:        0.075499
Test - acc:         0.932100 loss:        0.209810
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.974320 loss:        0.076992
Test - acc:         0.933300 loss:        0.212506
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.976640 loss:        0.072243
Test - acc:         0.930700 loss:        0.217401
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.977040 loss:        0.068676
Test - acc:         0.933100 loss:        0.214272
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.977240 loss:        0.067280
Test - acc:         0.932200 loss:        0.221648
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.978580 loss:        0.063078
Test - acc:         0.933000 loss:        0.214285
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.978060 loss:        0.064214
Test - acc:         0.930600 loss:        0.219815
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.979100 loss:        0.063091
Test - acc:         0.928600 loss:        0.226578
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.980000 loss:        0.060993
Test - acc:         0.930800 loss:        0.228253
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.979240 loss:        0.060986
Test - acc:         0.928000 loss:        0.237497
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.979720 loss:        0.060059
Test - acc:         0.926700 loss:        0.239917
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.980400 loss:        0.057563
Test - acc:         0.928200 loss:        0.236981
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.980820 loss:        0.056724
Test - acc:         0.934500 loss:        0.220446
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.980460 loss:        0.058521
Test - acc:         0.926800 loss:        0.243004
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.980740 loss:        0.056504
Test - acc:         0.930200 loss:        0.241115
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.981220 loss:        0.056362
Test - acc:         0.924200 loss:        0.248420
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.980880 loss:        0.056648
Test - acc:         0.930200 loss:        0.241667
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.981400 loss:        0.056851
Test - acc:         0.926200 loss:        0.262058
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.980600 loss:        0.057149
Test - acc:         0.930400 loss:        0.236116
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.980860 loss:        0.056514
Test - acc:         0.926400 loss:        0.247967
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.979920 loss:        0.058363
Test - acc:         0.921200 loss:        0.268351
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.981300 loss:        0.056763
Test - acc:         0.925900 loss:        0.260078
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.979960 loss:        0.059893
Test - acc:         0.924200 loss:        0.263143
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.978440 loss:        0.063158
Test - acc:         0.924400 loss:        0.259823
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.979620 loss:        0.059555
Test - acc:         0.926000 loss:        0.254835
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.979140 loss:        0.061530
Test - acc:         0.923900 loss:        0.261663
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.979460 loss:        0.060706
Test - acc:         0.926700 loss:        0.256497
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.979060 loss:        0.061820
Test - acc:         0.924500 loss:        0.255301
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.978820 loss:        0.062198
Test - acc:         0.927900 loss:        0.244442
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.978220 loss:        0.064622
Test - acc:         0.922800 loss:        0.263942
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.977100 loss:        0.067471
Test - acc:         0.914100 loss:        0.299023
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.977780 loss:        0.065303
Test - acc:         0.927300 loss:        0.250348
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.977280 loss:        0.065158
Test - acc:         0.922400 loss:        0.257980
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.963940 loss:        0.101935
Test - acc:         0.914700 loss:        0.280864
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.971200 loss:        0.084343
Test - acc:         0.920900 loss:        0.260860
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.971120 loss:        0.085282
Test - acc:         0.919900 loss:        0.265827
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.972200 loss:        0.080940
Test - acc:         0.918800 loss:        0.266562
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.973140 loss:        0.079267
Test - acc:         0.916300 loss:        0.267846
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.975060 loss:        0.075127
Test - acc:         0.919300 loss:        0.287848
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.972920 loss:        0.078375
Test - acc:         0.917800 loss:        0.279227
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.973100 loss:        0.077758
Test - acc:         0.916500 loss:        0.264397
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.973760 loss:        0.076655
Test - acc:         0.921500 loss:        0.272663
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.974300 loss:        0.076181
Test - acc:         0.918100 loss:        0.284641
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.974360 loss:        0.075313
Test - acc:         0.921800 loss:        0.270370
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.974840 loss:        0.075037
Test - acc:         0.921400 loss:        0.265072
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.974860 loss:        0.073737
Test - acc:         0.923100 loss:        0.263232
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.973500 loss:        0.077476
Test - acc:         0.919900 loss:        0.264716
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.972720 loss:        0.079379
Test - acc:         0.918200 loss:        0.273259
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.974280 loss:        0.075438
Test - acc:         0.911000 loss:        0.300446
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.973100 loss:        0.077318
Test - acc:         0.916200 loss:        0.288207
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.974340 loss:        0.075046
Test - acc:         0.918700 loss:        0.281126
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.972880 loss:        0.081080
Test - acc:         0.923000 loss:        0.254918
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.973740 loss:        0.075907
Test - acc:         0.919800 loss:        0.275963
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.974180 loss:        0.075988
Test - acc:         0.915800 loss:        0.279301
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.973520 loss:        0.078294
Test - acc:         0.920300 loss:        0.269361
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.973460 loss:        0.078209
Test - acc:         0.920500 loss:        0.279272
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.974060 loss:        0.076047
Test - acc:         0.919300 loss:        0.280855
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.972460 loss:        0.078662
Test - acc:         0.921500 loss:        0.262908
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.974600 loss:        0.073071
Test - acc:         0.920400 loss:        0.266311
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.974180 loss:        0.076667
Test - acc:         0.925500 loss:        0.269656
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.975240 loss:        0.074809
Test - acc:         0.922800 loss:        0.272948
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.973780 loss:        0.076175
Test - acc:         0.925400 loss:        0.263441
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.974920 loss:        0.072341
Test - acc:         0.923000 loss:        0.266257
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.973020 loss:        0.078039
Test - acc:         0.915600 loss:        0.301288
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.971800 loss:        0.080048
Test - acc:         0.914400 loss:        0.293413
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.939460 loss:        0.173511
Test - acc:         0.905600 loss:        0.302056
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.950100 loss:        0.144865
Test - acc:         0.905300 loss:        0.308641
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.952720 loss:        0.137677
Test - acc:         0.904200 loss:        0.305013
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.952560 loss:        0.135147
Test - acc:         0.909500 loss:        0.298960
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.954800 loss:        0.129695
Test - acc:         0.906200 loss:        0.297000
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.954580 loss:        0.131256
Test - acc:         0.905500 loss:        0.300710
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.957620 loss:        0.123166
Test - acc:         0.910100 loss:        0.287501
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.956920 loss:        0.121715
Test - acc:         0.912600 loss:        0.283874
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.957040 loss:        0.122705
Test - acc:         0.907500 loss:        0.292226
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.957640 loss:        0.120228
Test - acc:         0.911700 loss:        0.282534
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.958760 loss:        0.119714
Test - acc:         0.911100 loss:        0.281563
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.957880 loss:        0.123167
Test - acc:         0.913000 loss:        0.284111
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.960960 loss:        0.114704
Test - acc:         0.915900 loss:        0.262875
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.958720 loss:        0.118177
Test - acc:         0.910000 loss:        0.286001
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.960300 loss:        0.114612
Test - acc:         0.912100 loss:        0.284749
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.959220 loss:        0.117718
Test - acc:         0.914800 loss:        0.274190
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.958380 loss:        0.118119
Test - acc:         0.913700 loss:        0.281441
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.958640 loss:        0.119851
Test - acc:         0.917200 loss:        0.277750
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.960840 loss:        0.114217
Test - acc:         0.910900 loss:        0.291830
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.960480 loss:        0.113643
Test - acc:         0.913200 loss:        0.281945
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.958560 loss:        0.115675
Test - acc:         0.915700 loss:        0.265618
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.962640 loss:        0.109498
Test - acc:         0.916600 loss:        0.287198
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.960180 loss:        0.113930
Test - acc:         0.908100 loss:        0.290389
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.960360 loss:        0.115436
Test - acc:         0.912400 loss:        0.283020
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.960400 loss:        0.112609
Test - acc:         0.910600 loss:        0.293380
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.959840 loss:        0.115305
Test - acc:         0.916400 loss:        0.267564
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.970460 loss:        0.087094
Test - acc:         0.926100 loss:        0.235386
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.977540 loss:        0.070526
Test - acc:         0.929000 loss:        0.231988
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.978800 loss:        0.067524
Test - acc:         0.930400 loss:        0.229068
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.980280 loss:        0.062561
Test - acc:         0.930000 loss:        0.230158
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.982080 loss:        0.059878
Test - acc:         0.930600 loss:        0.228238
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.982660 loss:        0.056870
Test - acc:         0.930500 loss:        0.230726
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.894880 loss:        0.303664
Test - acc:         0.889400 loss:        0.335427
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.918160 loss:        0.236844
Test - acc:         0.897200 loss:        0.312630
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.925260 loss:        0.216316
Test - acc:         0.902300 loss:        0.304552
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.929940 loss:        0.202294
Test - acc:         0.903600 loss:        0.299404
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.932300 loss:        0.193508
Test - acc:         0.903700 loss:        0.293380
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.936200 loss:        0.186189
Test - acc:         0.905800 loss:        0.288640
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.936820 loss:        0.183010
Test - acc:         0.906100 loss:        0.288615
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.939020 loss:        0.178932
Test - acc:         0.906700 loss:        0.288734
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.939680 loss:        0.175088
Test - acc:         0.907600 loss:        0.283297
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.939200 loss:        0.174990
Test - acc:         0.906000 loss:        0.283093
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.942100 loss:        0.167636
Test - acc:         0.909800 loss:        0.281731
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.944040 loss:        0.166819
Test - acc:         0.910100 loss:        0.274520
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.945000 loss:        0.160873
Test - acc:         0.907800 loss:        0.280284
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.944660 loss:        0.160316
Test - acc:         0.910000 loss:        0.276276
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.946080 loss:        0.156616
Test - acc:         0.911100 loss:        0.275674
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.947100 loss:        0.156122
Test - acc:         0.911800 loss:        0.275037
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.949060 loss:        0.148908
Test - acc:         0.911600 loss:        0.274713
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.948340 loss:        0.150563
Test - acc:         0.910600 loss:        0.273308
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.948360 loss:        0.151502
Test - acc:         0.913400 loss:        0.271464
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.949060 loss:        0.149783
Test - acc:         0.912800 loss:        0.268021
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.950180 loss:        0.146472
Test - acc:         0.912600 loss:        0.270312
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.949660 loss:        0.148173
Test - acc:         0.912300 loss:        0.269499
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.950740 loss:        0.143546
Test - acc:         0.914000 loss:        0.270723
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.950580 loss:        0.143011
Test - acc:         0.913000 loss:        0.268236
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.951200 loss:        0.140301
Test - acc:         0.912900 loss:        0.267878
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.952840 loss:        0.139742
Test - acc:         0.911800 loss:        0.271753
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.952020 loss:        0.137349
Test - acc:         0.913600 loss:        0.268889
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.953620 loss:        0.136516
Test - acc:         0.914300 loss:        0.268087
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.951460 loss:        0.139583
Test - acc:         0.914400 loss:        0.268200
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.953080 loss:        0.136765
Test - acc:         0.914000 loss:        0.272285
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.953760 loss:        0.135214
Test - acc:         0.914900 loss:        0.267595
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.953880 loss:        0.134186
Test - acc:         0.912500 loss:        0.270488
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.766740 loss:        0.678943
Test - acc:         0.814200 loss:        0.547890
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.826560 loss:        0.509749
Test - acc:         0.830600 loss:        0.493684
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.840540 loss:        0.461684
Test - acc:         0.843200 loss:        0.466555
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.847960 loss:        0.439866
Test - acc:         0.847500 loss:        0.449900
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.855340 loss:        0.426471
Test - acc:         0.850700 loss:        0.437802
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.857860 loss:        0.411569
Test - acc:         0.855000 loss:        0.425974
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.860340 loss:        0.403663
Test - acc:         0.856700 loss:        0.420659
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.863780 loss:        0.393386
Test - acc:         0.856900 loss:        0.413870
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.868740 loss:        0.383904
Test - acc:         0.859700 loss:        0.409319
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.870720 loss:        0.379313
Test - acc:         0.862000 loss:        0.409424
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.871260 loss:        0.375771
Test - acc:         0.861500 loss:        0.404481
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.871980 loss:        0.370505
Test - acc:         0.863100 loss:        0.400729
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.875100 loss:        0.364003
Test - acc:         0.864700 loss:        0.396263
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.877580 loss:        0.359508
Test - acc:         0.864800 loss:        0.396328
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.876900 loss:        0.354085
Test - acc:         0.866900 loss:        0.397245
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.879060 loss:        0.352646
Test - acc:         0.864800 loss:        0.399085
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.881060 loss:        0.349989
Test - acc:         0.866400 loss:        0.395522
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.880040 loss:        0.348692
Test - acc:         0.867700 loss:        0.392772
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.880660 loss:        0.346355
Test - acc:         0.870100 loss:        0.389879
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.881140 loss:        0.346364
Test - acc:         0.868000 loss:        0.385156
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.880340 loss:        0.346786
Test - acc:         0.868000 loss:        0.386160
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.883720 loss:        0.340837
Test - acc:         0.870300 loss:        0.384344
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.882660 loss:        0.337609
Test - acc:         0.869700 loss:        0.384287
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.884960 loss:        0.333638
Test - acc:         0.869800 loss:        0.385566
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.886060 loss:        0.333807
Test - acc:         0.873000 loss:        0.382847
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.884440 loss:        0.332975
Test - acc:         0.870800 loss:        0.387117
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.885400 loss:        0.333201
Test - acc:         0.869000 loss:        0.383398
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.887660 loss:        0.328580
Test - acc:         0.870800 loss:        0.384551
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.887320 loss:        0.327664
Test - acc:         0.871500 loss:        0.382834
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.888100 loss:        0.323748
Test - acc:         0.871400 loss:        0.382827
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.885840 loss:        0.329687
Test - acc:         0.875000 loss:        0.378328
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.887840 loss:        0.326004
Test - acc:         0.870800 loss:        0.386369
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.647020 loss:        1.012630
Test - acc:         0.705400 loss:        0.825910
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.720940 loss:        0.808218
Test - acc:         0.738800 loss:        0.750985
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.741160 loss:        0.756721
Test - acc:         0.746400 loss:        0.722087
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.753000 loss:        0.722664
Test - acc:         0.760300 loss:        0.688216
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.761480 loss:        0.694671
Test - acc:         0.769200 loss:        0.672764
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.764800 loss:        0.685225
Test - acc:         0.772300 loss:        0.652485
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.767700 loss:        0.671692
Test - acc:         0.777300 loss:        0.648026
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.775680 loss:        0.656845
Test - acc:         0.781000 loss:        0.634231
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.777300 loss:        0.648746
Test - acc:         0.782700 loss:        0.631695
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.778300 loss:        0.642787
Test - acc:         0.786200 loss:        0.621901
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.779440 loss:        0.635253
Test - acc:         0.788900 loss:        0.619165
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.783600 loss:        0.629916
Test - acc:         0.789300 loss:        0.614045
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.784920 loss:        0.627391
Test - acc:         0.790600 loss:        0.614463
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.789700 loss:        0.618728
Test - acc:         0.790400 loss:        0.606612
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.787440 loss:        0.616046
Test - acc:         0.790200 loss:        0.607596
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.789440 loss:        0.611685
Test - acc:         0.789300 loss:        0.611562
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.791060 loss:        0.608567
Test - acc:         0.793400 loss:        0.600742
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.791600 loss:        0.606877
Test - acc:         0.790400 loss:        0.607193
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.793080 loss:        0.604736
Test - acc:         0.794100 loss:        0.596717
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.795860 loss:        0.596805
Test - acc:         0.793900 loss:        0.592717
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.795720 loss:        0.593029
Test - acc:         0.793500 loss:        0.598330
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.796800 loss:        0.590224
Test - acc:         0.799400 loss:        0.588483
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.797020 loss:        0.590614
Test - acc:         0.791700 loss:        0.593591
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.798380 loss:        0.585542
Test - acc:         0.800100 loss:        0.586732
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.800740 loss:        0.581353
Test - acc:         0.800700 loss:        0.583666
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.797460 loss:        0.582672
Test - acc:         0.799900 loss:        0.587660
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.802800 loss:        0.576287
Test - acc:         0.801700 loss:        0.581498
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.803420 loss:        0.573980
Test - acc:         0.798700 loss:        0.579585
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.803000 loss:        0.578284
Test - acc:         0.801400 loss:        0.576707
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.802020 loss:        0.577253
Test - acc:         0.801200 loss:        0.578736
Sparsity :          0.9990
Wdecay :        0.000500
