Running --model resnet18 --noise --prune_criterion weight_div_flips --seed 44 --prune_freq 32 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=32_seed=44 --save_model=pre-finetune/resnet18_weight_div_flips_pf32_s44 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf32_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.303340 loss:        2.045872
Test - acc:         0.449200 loss:        1.518474
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.486940 loss:        1.408749
Test - acc:         0.553600 loss:        1.216369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.595040 loss:        1.130263
Test - acc:         0.627400 loss:        1.061316
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.662820 loss:        0.954220
Test - acc:         0.666000 loss:        0.963036
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.710980 loss:        0.823103
Test - acc:         0.700400 loss:        0.900422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.754080 loss:        0.701555
Test - acc:         0.719300 loss:        0.843699
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.784640 loss:        0.626890
Test - acc:         0.762900 loss:        0.702303
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.800120 loss:        0.580976
Test - acc:         0.678300 loss:        1.056559
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.809040 loss:        0.553956
Test - acc:         0.750400 loss:        0.725537
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.817400 loss:        0.527065
Test - acc:         0.750100 loss:        0.776029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.828080 loss:        0.504549
Test - acc:         0.806100 loss:        0.560775
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.827040 loss:        0.498283
Test - acc:         0.787800 loss:        0.637411
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.835700 loss:        0.482435
Test - acc:         0.794000 loss:        0.615528
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838900 loss:        0.468737
Test - acc:         0.820400 loss:        0.537681
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844200 loss:        0.459359
Test - acc:         0.804800 loss:        0.583139
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845740 loss:        0.448696
Test - acc:         0.757400 loss:        0.755301
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.845840 loss:        0.444416
Test - acc:         0.784900 loss:        0.670693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.849560 loss:        0.437153
Test - acc:         0.836300 loss:        0.472858
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.852000 loss:        0.431503
Test - acc:         0.805300 loss:        0.564962
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.853780 loss:        0.428139
Test - acc:         0.822000 loss:        0.534250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.417603
Test - acc:         0.776100 loss:        0.709650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.857220 loss:        0.416609
Test - acc:         0.822800 loss:        0.534768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.418318
Test - acc:         0.835400 loss:        0.503948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.857700 loss:        0.416371
Test - acc:         0.797500 loss:        0.599861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.861780 loss:        0.406843
Test - acc:         0.789200 loss:        0.638827
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.858420 loss:        0.412091
Test - acc:         0.812200 loss:        0.574223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.861500 loss:        0.402603
Test - acc:         0.834200 loss:        0.487177
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.861100 loss:        0.403876
Test - acc:         0.788300 loss:        0.694953
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.862260 loss:        0.397948
Test - acc:         0.834000 loss:        0.496371
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.861600 loss:        0.404324
Test - acc:         0.767800 loss:        0.741355
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.863460 loss:        0.399084
Test - acc:         0.813800 loss:        0.548094
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.862380 loss:        0.399397
Test - acc:         0.821100 loss:        0.548047
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.878200 loss:        0.354777
Test - acc:         0.819000 loss:        0.564875
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.873520 loss:        0.367706
Test - acc:         0.806200 loss:        0.636396
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.873660 loss:        0.368741
Test - acc:         0.806600 loss:        0.588061
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.873600 loss:        0.372170
Test - acc:         0.796600 loss:        0.625754
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.872040 loss:        0.368558
Test - acc:         0.848900 loss:        0.468316
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.872440 loss:        0.371684
Test - acc:         0.850400 loss:        0.447030
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.871760 loss:        0.373209
Test - acc:         0.829000 loss:        0.513964
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.872920 loss:        0.371518
Test - acc:         0.840100 loss:        0.469748
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.870460 loss:        0.375873
Test - acc:         0.823400 loss:        0.536643
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.872640 loss:        0.371793
Test - acc:         0.849000 loss:        0.452569
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.873660 loss:        0.367493
Test - acc:         0.838600 loss:        0.481126
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.873880 loss:        0.366203
Test - acc:         0.830900 loss:        0.530187
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.875440 loss:        0.366960
Test - acc:         0.813300 loss:        0.578886
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.872960 loss:        0.366939
Test - acc:         0.827100 loss:        0.559516
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.875120 loss:        0.364601
Test - acc:         0.832700 loss:        0.499013
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.873500 loss:        0.370725
Test - acc:         0.805800 loss:        0.599244
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.874460 loss:        0.364911
Test - acc:         0.787100 loss:        0.700527
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.874400 loss:        0.369176
Test - acc:         0.840400 loss:        0.488557
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.876560 loss:        0.359248
Test - acc:         0.830600 loss:        0.513897
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.876480 loss:        0.362339
Test - acc:         0.837000 loss:        0.491464
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.872620 loss:        0.368473
Test - acc:         0.816700 loss:        0.563013
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.875820 loss:        0.363967
Test - acc:         0.849500 loss:        0.434539
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.877180 loss:        0.361203
Test - acc:         0.773200 loss:        0.711935
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.875960 loss:        0.362541
Test - acc:         0.826200 loss:        0.525426
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.876980 loss:        0.362250
Test - acc:         0.792100 loss:        0.656029
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.875800 loss:        0.365248
Test - acc:         0.836700 loss:        0.504009
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.876540 loss:        0.361192
Test - acc:         0.847000 loss:        0.454914
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.877500 loss:        0.359342
Test - acc:         0.835500 loss:        0.498035
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.875240 loss:        0.361474
Test - acc:         0.811100 loss:        0.579442
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.874020 loss:        0.368182
Test - acc:         0.819200 loss:        0.542495
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.878080 loss:        0.358903
Test - acc:         0.851100 loss:        0.453766
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.875520 loss:        0.363930
Test - acc:         0.829000 loss:        0.522019
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.887200 loss:        0.326139
Test - acc:         0.845200 loss:        0.496655
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.885280 loss:        0.334425
Test - acc:         0.860300 loss:        0.424144
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.885360 loss:        0.333098
Test - acc:         0.850300 loss:        0.432986
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.883940 loss:        0.340181
Test - acc:         0.839900 loss:        0.492904
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.881400 loss:        0.346119
Test - acc:         0.857500 loss:        0.432245
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.883840 loss:        0.334058
Test - acc:         0.840600 loss:        0.473741
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.884200 loss:        0.335492
Test - acc:         0.822100 loss:        0.539963
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.881300 loss:        0.344134
Test - acc:         0.828300 loss:        0.513609
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.883620 loss:        0.339659
Test - acc:         0.838200 loss:        0.486562
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.881240 loss:        0.341432
Test - acc:         0.846800 loss:        0.464066
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.886580 loss:        0.333858
Test - acc:         0.835700 loss:        0.494008
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.885180 loss:        0.337088
Test - acc:         0.808900 loss:        0.562132
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.885680 loss:        0.335447
Test - acc:         0.809300 loss:        0.615999
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.882800 loss:        0.341520
Test - acc:         0.762400 loss:        0.757511
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.884500 loss:        0.341276
Test - acc:         0.765800 loss:        0.769205
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.883380 loss:        0.338323
Test - acc:         0.837200 loss:        0.503694
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.881560 loss:        0.342357
Test - acc:         0.858100 loss:        0.428641
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.884920 loss:        0.336471
Test - acc:         0.844400 loss:        0.489762
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.883260 loss:        0.342874
Test - acc:         0.860100 loss:        0.419785
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.883720 loss:        0.339998
Test - acc:         0.825500 loss:        0.539036
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.883260 loss:        0.339136
Test - acc:         0.835200 loss:        0.507525
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.884040 loss:        0.339186
Test - acc:         0.850400 loss:        0.449657
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.885960 loss:        0.336063
Test - acc:         0.832500 loss:        0.517910
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.884560 loss:        0.338732
Test - acc:         0.830300 loss:        0.535602
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.883780 loss:        0.337708
Test - acc:         0.782200 loss:        0.687971
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.883580 loss:        0.337763
Test - acc:         0.841500 loss:        0.494877
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.883220 loss:        0.338686
Test - acc:         0.836500 loss:        0.515046
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.882700 loss:        0.342621
Test - acc:         0.804200 loss:        0.613966
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.885380 loss:        0.334235
Test - acc:         0.856200 loss:        0.428119
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.884100 loss:        0.338573
Test - acc:         0.861100 loss:        0.403505
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.883680 loss:        0.339366
Test - acc:         0.825600 loss:        0.528169
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.884120 loss:        0.339093
Test - acc:         0.849700 loss:        0.464319
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.895800 loss:        0.303362
Test - acc:         0.861500 loss:        0.439664
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.895340 loss:        0.305996
Test - acc:         0.812500 loss:        0.588389
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.891540 loss:        0.314355
Test - acc:         0.850100 loss:        0.440707
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.892160 loss:        0.316084
Test - acc:         0.858500 loss:        0.424969
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.892160 loss:        0.315534
Test - acc:         0.840900 loss:        0.480419
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.890700 loss:        0.317268
Test - acc:         0.803800 loss:        0.609377
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.893200 loss:        0.312856
Test - acc:         0.838600 loss:        0.489591
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.889080 loss:        0.319552
Test - acc:         0.841600 loss:        0.493494
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.892060 loss:        0.313175
Test - acc:         0.814000 loss:        0.622689
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.890720 loss:        0.315958
Test - acc:         0.875700 loss:        0.378785
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.888180 loss:        0.322895
Test - acc:         0.850800 loss:        0.457493
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.890560 loss:        0.316329
Test - acc:         0.848900 loss:        0.439772
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.890200 loss:        0.313413
Test - acc:         0.859900 loss:        0.433150
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.891580 loss:        0.316753
Test - acc:         0.805100 loss:        0.659797
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.889520 loss:        0.319817
Test - acc:         0.825300 loss:        0.515788
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.891740 loss:        0.315025
Test - acc:         0.869600 loss:        0.390067
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.888280 loss:        0.319789
Test - acc:         0.824400 loss:        0.534218
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.890480 loss:        0.315284
Test - acc:         0.836600 loss:        0.509881
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.891520 loss:        0.313529
Test - acc:         0.848900 loss:        0.448435
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.891720 loss:        0.316486
Test - acc:         0.869300 loss:        0.392180
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.892600 loss:        0.316061
Test - acc:         0.846600 loss:        0.470893
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.890560 loss:        0.320105
Test - acc:         0.843400 loss:        0.471915
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.891880 loss:        0.318119
Test - acc:         0.858100 loss:        0.442936
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.892780 loss:        0.312959
Test - acc:         0.859200 loss:        0.422484
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.891900 loss:        0.314182
Test - acc:         0.877400 loss:        0.370003
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.892340 loss:        0.314812
Test - acc:         0.851700 loss:        0.460927
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.889560 loss:        0.321566
Test - acc:         0.835100 loss:        0.513273
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.889660 loss:        0.320348
Test - acc:         0.823500 loss:        0.575026
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.890820 loss:        0.317824
Test - acc:         0.848900 loss:        0.459558
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.889300 loss:        0.318681
Test - acc:         0.834700 loss:        0.495238
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.890040 loss:        0.316969
Test - acc:         0.864000 loss:        0.405818
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.892240 loss:        0.316258
Test - acc:         0.848700 loss:        0.470921
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.903200 loss:        0.281316
Test - acc:         0.866500 loss:        0.407054
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.901420 loss:        0.287487
Test - acc:         0.853800 loss:        0.448348
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.901440 loss:        0.287503
Test - acc:         0.864600 loss:        0.402809
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.898900 loss:        0.292064
Test - acc:         0.861200 loss:        0.402982
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.899380 loss:        0.292917
Test - acc:         0.866000 loss:        0.421991
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.898160 loss:        0.298297
Test - acc:         0.835800 loss:        0.488423
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.898600 loss:        0.295573
Test - acc:         0.840800 loss:        0.483138
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.898420 loss:        0.295366
Test - acc:         0.867900 loss:        0.392628
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.897080 loss:        0.296183
Test - acc:         0.835600 loss:        0.493108
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.898840 loss:        0.295527
Test - acc:         0.847000 loss:        0.472004
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.898800 loss:        0.293778
Test - acc:         0.877600 loss:        0.375578
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.898400 loss:        0.292877
Test - acc:         0.876000 loss:        0.382610
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.898220 loss:        0.295678
Test - acc:         0.834400 loss:        0.522563
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.897900 loss:        0.297271
Test - acc:         0.861100 loss:        0.431492
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.897080 loss:        0.294002
Test - acc:         0.872500 loss:        0.365608
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.899380 loss:        0.296162
Test - acc:         0.857900 loss:        0.413533
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.896100 loss:        0.301915
Test - acc:         0.851700 loss:        0.471277
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.899840 loss:        0.294155
Test - acc:         0.854900 loss:        0.435305
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.898600 loss:        0.297703
Test - acc:         0.863100 loss:        0.410418
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.899740 loss:        0.291363
Test - acc:         0.844400 loss:        0.487235
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.898380 loss:        0.296188
Test - acc:         0.842700 loss:        0.498165
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.896840 loss:        0.299289
Test - acc:         0.816600 loss:        0.570134
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.940420 loss:        0.180086
Test - acc:         0.922300 loss:        0.226362
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.953340 loss:        0.139701
Test - acc:         0.926900 loss:        0.215392
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.959480 loss:        0.122822
Test - acc:         0.930000 loss:        0.207683
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.961440 loss:        0.113588
Test - acc:         0.931400 loss:        0.210793
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.964820 loss:        0.105307
Test - acc:         0.928000 loss:        0.212737
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.967260 loss:        0.097430
Test - acc:         0.930100 loss:        0.203898
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.969880 loss:        0.091105
Test - acc:         0.931300 loss:        0.210695
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.971240 loss:        0.086271
Test - acc:         0.931700 loss:        0.213613
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.971500 loss:        0.083397
Test - acc:         0.929500 loss:        0.219293
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.973600 loss:        0.079312
Test - acc:         0.932800 loss:        0.213449
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.974320 loss:        0.077701
Test - acc:         0.931900 loss:        0.216524
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.975480 loss:        0.072826
Test - acc:         0.933500 loss:        0.212684
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.976460 loss:        0.070523
Test - acc:         0.933500 loss:        0.214423
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.977400 loss:        0.068080
Test - acc:         0.930200 loss:        0.223879
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.977900 loss:        0.065893
Test - acc:         0.932900 loss:        0.229509
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.978620 loss:        0.064503
Test - acc:         0.931900 loss:        0.224992
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.979420 loss:        0.061375
Test - acc:         0.931500 loss:        0.226264
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.980280 loss:        0.060181
Test - acc:         0.932300 loss:        0.230987
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.980940 loss:        0.057447
Test - acc:         0.927800 loss:        0.241344
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.059838
Test - acc:         0.927600 loss:        0.249974
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.059094
Test - acc:         0.928900 loss:        0.237113
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.056262
Test - acc:         0.927600 loss:        0.252421
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.978880 loss:        0.061167
Test - acc:         0.927100 loss:        0.255884
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.980960 loss:        0.056092
Test - acc:         0.929500 loss:        0.234059
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.980780 loss:        0.057088
Test - acc:         0.928900 loss:        0.243197
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.979540 loss:        0.060617
Test - acc:         0.925600 loss:        0.259773
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.980180 loss:        0.058631
Test - acc:         0.929400 loss:        0.244270
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.980100 loss:        0.060233
Test - acc:         0.931300 loss:        0.236605
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.980100 loss:        0.059722
Test - acc:         0.930500 loss:        0.243896
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.979220 loss:        0.060609
Test - acc:         0.926300 loss:        0.251410
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.979200 loss:        0.061005
Test - acc:         0.922400 loss:        0.272004
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.979960 loss:        0.059647
Test - acc:         0.927000 loss:        0.253677
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.980240 loss:        0.059406
Test - acc:         0.922900 loss:        0.266356
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.980740 loss:        0.057029
Test - acc:         0.927600 loss:        0.261322
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.978560 loss:        0.061854
Test - acc:         0.925600 loss:        0.264280
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.978180 loss:        0.064486
Test - acc:         0.926500 loss:        0.261848
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.978980 loss:        0.063195
Test - acc:         0.923200 loss:        0.277902
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.977300 loss:        0.065212
Test - acc:         0.926900 loss:        0.253317
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.978600 loss:        0.063741
Test - acc:         0.919600 loss:        0.269789
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.977420 loss:        0.067510
Test - acc:         0.922500 loss:        0.278744
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.978480 loss:        0.064493
Test - acc:         0.922200 loss:        0.275888
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.977340 loss:        0.064587
Test - acc:         0.925000 loss:        0.259291
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.965300 loss:        0.099726
Test - acc:         0.920500 loss:        0.257939
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.969680 loss:        0.088235
Test - acc:         0.921100 loss:        0.256547
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.972760 loss:        0.081213
Test - acc:         0.918400 loss:        0.267583
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.971740 loss:        0.082173
Test - acc:         0.928000 loss:        0.245788
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.972420 loss:        0.079634
Test - acc:         0.924900 loss:        0.255535
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.972260 loss:        0.079972
Test - acc:         0.918500 loss:        0.274906
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.972080 loss:        0.079928
Test - acc:         0.922000 loss:        0.270447
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.974340 loss:        0.076361
Test - acc:         0.916900 loss:        0.281592
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.973800 loss:        0.076569
Test - acc:         0.925200 loss:        0.254344
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.973300 loss:        0.077827
Test - acc:         0.925200 loss:        0.255656
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.972820 loss:        0.077367
Test - acc:         0.914700 loss:        0.289406
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.975240 loss:        0.073075
Test - acc:         0.923900 loss:        0.260838
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.974400 loss:        0.075044
Test - acc:         0.921700 loss:        0.275936
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.973960 loss:        0.075041
Test - acc:         0.919400 loss:        0.279982
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.972620 loss:        0.079520
Test - acc:         0.919200 loss:        0.284087
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.971620 loss:        0.080415
Test - acc:         0.923900 loss:        0.263518
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.973280 loss:        0.078204
Test - acc:         0.915000 loss:        0.290178
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.973140 loss:        0.078755
Test - acc:         0.919400 loss:        0.275922
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.973580 loss:        0.077417
Test - acc:         0.910800 loss:        0.319908
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.974260 loss:        0.075726
Test - acc:         0.921900 loss:        0.275344
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.973420 loss:        0.079301
Test - acc:         0.919900 loss:        0.279128
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.972440 loss:        0.080605
Test - acc:         0.916800 loss:        0.286295
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.973620 loss:        0.076787
Test - acc:         0.919700 loss:        0.285453
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.973420 loss:        0.078160
Test - acc:         0.922100 loss:        0.260933
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.972940 loss:        0.078342
Test - acc:         0.915900 loss:        0.286584
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.974660 loss:        0.075761
Test - acc:         0.920500 loss:        0.270199
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.972320 loss:        0.079157
Test - acc:         0.920600 loss:        0.271674
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.975680 loss:        0.073175
Test - acc:         0.920600 loss:        0.276751
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.974520 loss:        0.072926
Test - acc:         0.924400 loss:        0.260524
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.972000 loss:        0.080942
Test - acc:         0.922000 loss:        0.273255
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.972640 loss:        0.078838
Test - acc:         0.921500 loss:        0.268451
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.973000 loss:        0.079960
Test - acc:         0.916300 loss:        0.289343
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.939780 loss:        0.174756
Test - acc:         0.902500 loss:        0.301182
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.947180 loss:        0.152477
Test - acc:         0.908300 loss:        0.288266
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.951340 loss:        0.139865
Test - acc:         0.906400 loss:        0.288662
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.952120 loss:        0.138131
Test - acc:         0.906900 loss:        0.303053
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.953860 loss:        0.132458
Test - acc:         0.911500 loss:        0.284421
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.954340 loss:        0.132499
Test - acc:         0.907800 loss:        0.295198
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.954980 loss:        0.128243
Test - acc:         0.913800 loss:        0.278005
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.956760 loss:        0.125050
Test - acc:         0.910700 loss:        0.290076
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.955820 loss:        0.123586
Test - acc:         0.911800 loss:        0.277317
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.955300 loss:        0.125911
Test - acc:         0.910000 loss:        0.291416
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.956880 loss:        0.122549
Test - acc:         0.910300 loss:        0.293734
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.959440 loss:        0.116893
Test - acc:         0.912600 loss:        0.293726
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.956560 loss:        0.123830
Test - acc:         0.908100 loss:        0.288793
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.959440 loss:        0.119170
Test - acc:         0.917600 loss:        0.272526
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.959000 loss:        0.117149
Test - acc:         0.914200 loss:        0.277473
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.958980 loss:        0.116233
Test - acc:         0.908300 loss:        0.302044
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.960320 loss:        0.116678
Test - acc:         0.916200 loss:        0.271940
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.960660 loss:        0.115417
Test - acc:         0.916300 loss:        0.283117
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.958840 loss:        0.117641
Test - acc:         0.912500 loss:        0.282936
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.961400 loss:        0.113650
Test - acc:         0.916600 loss:        0.278065
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.958740 loss:        0.115360
Test - acc:         0.911700 loss:        0.286983
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.958680 loss:        0.115524
Test - acc:         0.916400 loss:        0.274355
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.958240 loss:        0.117668
Test - acc:         0.907300 loss:        0.303615
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.961040 loss:        0.113858
Test - acc:         0.911900 loss:        0.293844
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.959020 loss:        0.115453
Test - acc:         0.913600 loss:        0.285204
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.960740 loss:        0.112320
Test - acc:         0.917300 loss:        0.284718
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.971380 loss:        0.084432
Test - acc:         0.929000 loss:        0.231969
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.977580 loss:        0.071204
Test - acc:         0.930000 loss:        0.229185
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.977740 loss:        0.067710
Test - acc:         0.930900 loss:        0.228444
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.979840 loss:        0.064127
Test - acc:         0.930100 loss:        0.227413
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.980100 loss:        0.062429
Test - acc:         0.931300 loss:        0.226964
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.981600 loss:        0.058808
Test - acc:         0.930700 loss:        0.228050
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.897720 loss:        0.298782
Test - acc:         0.887900 loss:        0.334024
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.919560 loss:        0.233279
Test - acc:         0.897000 loss:        0.310136
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.925500 loss:        0.214096
Test - acc:         0.901700 loss:        0.298937
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.929780 loss:        0.202049
Test - acc:         0.904500 loss:        0.289634
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.933880 loss:        0.192975
Test - acc:         0.905500 loss:        0.287578
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.935120 loss:        0.187419
Test - acc:         0.906200 loss:        0.281800
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.937520 loss:        0.182972
Test - acc:         0.908500 loss:        0.277255
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.938240 loss:        0.177823
Test - acc:         0.906000 loss:        0.279144
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.940620 loss:        0.174001
Test - acc:         0.910100 loss:        0.274288
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.940320 loss:        0.172734
Test - acc:         0.909600 loss:        0.273112
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.943420 loss:        0.167578
Test - acc:         0.907500 loss:        0.274779
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.944600 loss:        0.163273
Test - acc:         0.910200 loss:        0.271630
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.942460 loss:        0.163419
Test - acc:         0.909700 loss:        0.272358
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.945460 loss:        0.158419
Test - acc:         0.912900 loss:        0.271645
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.944620 loss:        0.159714
Test - acc:         0.912500 loss:        0.269319
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.947060 loss:        0.154509
Test - acc:         0.913700 loss:        0.268923
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.947980 loss:        0.153831
Test - acc:         0.912000 loss:        0.268687
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.947020 loss:        0.153453
Test - acc:         0.913900 loss:        0.267860
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.948540 loss:        0.151120
Test - acc:         0.913300 loss:        0.269358
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.946300 loss:        0.153196
Test - acc:         0.913800 loss:        0.267601
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.949440 loss:        0.147745
Test - acc:         0.914800 loss:        0.265154
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.949960 loss:        0.148354
Test - acc:         0.913200 loss:        0.266843
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.949620 loss:        0.146443
Test - acc:         0.913800 loss:        0.266548
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.950540 loss:        0.144329
Test - acc:         0.911800 loss:        0.269235
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.951700 loss:        0.142495
Test - acc:         0.912300 loss:        0.269836
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.951920 loss:        0.141917
Test - acc:         0.912800 loss:        0.272021
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.952000 loss:        0.142641
Test - acc:         0.914500 loss:        0.274044
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.950620 loss:        0.142268
Test - acc:         0.913700 loss:        0.271416
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.951900 loss:        0.139714
Test - acc:         0.915800 loss:        0.267222
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.952980 loss:        0.137924
Test - acc:         0.915800 loss:        0.266224
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.953640 loss:        0.136246
Test - acc:         0.915800 loss:        0.265493
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.952980 loss:        0.137253
Test - acc:         0.915800 loss:        0.267049
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.760760 loss:        0.703196
Test - acc:         0.813700 loss:        0.556797
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.819040 loss:        0.525213
Test - acc:         0.830700 loss:        0.500369
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.835700 loss:        0.481285
Test - acc:         0.840000 loss:        0.469043
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.844880 loss:        0.455756
Test - acc:         0.844100 loss:        0.453010
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.850080 loss:        0.436398
Test - acc:         0.849200 loss:        0.439444
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.855060 loss:        0.422819
Test - acc:         0.852000 loss:        0.435411
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.856080 loss:        0.415262
Test - acc:         0.857500 loss:        0.425717
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.862100 loss:        0.402841
Test - acc:         0.857000 loss:        0.419095
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.863800 loss:        0.396096
Test - acc:         0.860000 loss:        0.412894
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.866240 loss:        0.385401
Test - acc:         0.860700 loss:        0.408543
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.865920 loss:        0.386000
Test - acc:         0.861800 loss:        0.410211
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.868860 loss:        0.380308
Test - acc:         0.862700 loss:        0.403291
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.871060 loss:        0.375465
Test - acc:         0.863000 loss:        0.402509
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.870980 loss:        0.374700
Test - acc:         0.867700 loss:        0.396397
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.872780 loss:        0.367652
Test - acc:         0.864800 loss:        0.398520
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.874560 loss:        0.364232
Test - acc:         0.865400 loss:        0.389771
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.876620 loss:        0.359307
Test - acc:         0.867500 loss:        0.396848
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.876660 loss:        0.359471
Test - acc:         0.869400 loss:        0.391138
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.876960 loss:        0.358478
Test - acc:         0.865800 loss:        0.393356
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.880040 loss:        0.351723
Test - acc:         0.866800 loss:        0.388277
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.878920 loss:        0.354082
Test - acc:         0.869200 loss:        0.382993
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.879440 loss:        0.351687
Test - acc:         0.869300 loss:        0.393114
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.880700 loss:        0.344828
Test - acc:         0.867400 loss:        0.387268
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.880960 loss:        0.346170
Test - acc:         0.868200 loss:        0.386877
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.880480 loss:        0.342531
Test - acc:         0.871700 loss:        0.383644
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.882800 loss:        0.341182
Test - acc:         0.872500 loss:        0.382058
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.881700 loss:        0.341598
Test - acc:         0.872400 loss:        0.384997
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.882440 loss:        0.338290
Test - acc:         0.870500 loss:        0.386735
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.884800 loss:        0.334718
Test - acc:         0.871900 loss:        0.382449
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.884260 loss:        0.335815
Test - acc:         0.872100 loss:        0.379138
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.884940 loss:        0.336561
Test - acc:         0.873800 loss:        0.377704
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.884200 loss:        0.335384
Test - acc:         0.874700 loss:        0.376363
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.648080 loss:        1.012489
Test - acc:         0.726400 loss:        0.795675
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.728160 loss:        0.796041
Test - acc:         0.745900 loss:        0.727317
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.738000 loss:        0.748174
Test - acc:         0.754400 loss:        0.701201
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.751060 loss:        0.718930
Test - acc:         0.766500 loss:        0.675373
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.759520 loss:        0.700018
Test - acc:         0.774900 loss:        0.656665
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.764460 loss:        0.683066
Test - acc:         0.775200 loss:        0.649603
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.768540 loss:        0.668261
Test - acc:         0.777100 loss:        0.638482
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.770900 loss:        0.659836
Test - acc:         0.783900 loss:        0.631604
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.772240 loss:        0.654829
Test - acc:         0.781700 loss:        0.633403
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.778440 loss:        0.644479
Test - acc:         0.785700 loss:        0.619101
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.778720 loss:        0.638800
Test - acc:         0.788500 loss:        0.609931
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.780780 loss:        0.632290
Test - acc:         0.787500 loss:        0.603709
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.782660 loss:        0.626503
Test - acc:         0.792600 loss:        0.598099
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.784680 loss:        0.622660
Test - acc:         0.792600 loss:        0.596939
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.788840 loss:        0.615316
Test - acc:         0.793900 loss:        0.595544
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.787340 loss:        0.613850
Test - acc:         0.795400 loss:        0.589613
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.790680 loss:        0.608149
Test - acc:         0.794600 loss:        0.592336
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.788760 loss:        0.607564
Test - acc:         0.790800 loss:        0.594200
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.791000 loss:        0.604429
Test - acc:         0.795900 loss:        0.586235
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.794480 loss:        0.601406
Test - acc:         0.797100 loss:        0.580724
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.795040 loss:        0.599525
Test - acc:         0.796900 loss:        0.580629
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.795700 loss:        0.594293
Test - acc:         0.799400 loss:        0.577837
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.795960 loss:        0.591723
Test - acc:         0.805400 loss:        0.572038
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.795160 loss:        0.591095
Test - acc:         0.798200 loss:        0.579592
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.797180 loss:        0.588656
Test - acc:         0.797300 loss:        0.573946
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.798700 loss:        0.584682
Test - acc:         0.798800 loss:        0.568183
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.797860 loss:        0.585755
Test - acc:         0.801800 loss:        0.564532
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.798840 loss:        0.584891
Test - acc:         0.800700 loss:        0.567497
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.799060 loss:        0.581838
Test - acc:         0.803400 loss:        0.569017
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.798720 loss:        0.581751
Test - acc:         0.805100 loss:        0.560873
Sparsity :          0.9990
Wdecay :        0.000500
