Running --model resnet18 --noise --prune_criterion weight_div_flips --seed 42 --prune_freq 50 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=50_seed=42 --save_model=pre-finetune/resnet18_weight_div_flips_pf50_s42 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf50_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.319760 loss:        2.027701
Test - acc:         0.373600 loss:        1.709656
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.483520 loss:        1.421054
Test - acc:         0.526700 loss:        1.263736
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.596320 loss:        1.129661
Test - acc:         0.595600 loss:        1.171379
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.673280 loss:        0.927933
Test - acc:         0.610100 loss:        1.134739
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.723460 loss:        0.790148
Test - acc:         0.705200 loss:        0.872062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.766400 loss:        0.675023
Test - acc:         0.746900 loss:        0.720768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.790000 loss:        0.608257
Test - acc:         0.766300 loss:        0.692052
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.805140 loss:        0.568890
Test - acc:         0.777000 loss:        0.661023
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.812440 loss:        0.540974
Test - acc:         0.748500 loss:        0.765813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.823580 loss:        0.513555
Test - acc:         0.742700 loss:        0.756514
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.831620 loss:        0.490783
Test - acc:         0.778800 loss:        0.671311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.834560 loss:        0.478143
Test - acc:         0.793000 loss:        0.626211
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.841480 loss:        0.462775
Test - acc:         0.792800 loss:        0.604948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838760 loss:        0.468699
Test - acc:         0.789100 loss:        0.624176
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844780 loss:        0.450396
Test - acc:         0.816100 loss:        0.559841
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.846280 loss:        0.448807
Test - acc:         0.764600 loss:        0.719807
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.850540 loss:        0.438978
Test - acc:         0.818300 loss:        0.530640
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.850840 loss:        0.433470
Test - acc:         0.796600 loss:        0.604748
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.853620 loss:        0.425959
Test - acc:         0.841700 loss:        0.488544
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.854620 loss:        0.421329
Test - acc:         0.795500 loss:        0.623324
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.859220 loss:        0.417579
Test - acc:         0.823000 loss:        0.516899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.856440 loss:        0.420470
Test - acc:         0.758600 loss:        0.712926
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.858640 loss:        0.411732
Test - acc:         0.793800 loss:        0.625225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.858100 loss:        0.413444
Test - acc:         0.793100 loss:        0.628817
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.862000 loss:        0.407803
Test - acc:         0.819100 loss:        0.573117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.861660 loss:        0.407108
Test - acc:         0.805500 loss:        0.599522
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.864180 loss:        0.397832
Test - acc:         0.836000 loss:        0.468893
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.863120 loss:        0.402166
Test - acc:         0.833700 loss:        0.486745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.863140 loss:        0.396906
Test - acc:         0.855500 loss:        0.423829
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.867600 loss:        0.394886
Test - acc:         0.810100 loss:        0.579276
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.865620 loss:        0.394207
Test - acc:         0.853400 loss:        0.434255
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.866820 loss:        0.393427
Test - acc:         0.698200 loss:        1.116456
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.866560 loss:        0.390132
Test - acc:         0.833300 loss:        0.506263
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.866820 loss:        0.389891
Test - acc:         0.838100 loss:        0.489114
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.868660 loss:        0.383678
Test - acc:         0.817400 loss:        0.559010
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.384254
Test - acc:         0.822300 loss:        0.562526
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.866740 loss:        0.387204
Test - acc:         0.820700 loss:        0.539781
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.868140 loss:        0.390223
Test - acc:         0.839300 loss:        0.510206
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.866480 loss:        0.388159
Test - acc:         0.846800 loss:        0.458624
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.870960 loss:        0.379702
Test - acc:         0.817800 loss:        0.564097
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.387867
Test - acc:         0.835000 loss:        0.494873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.872200 loss:        0.374784
Test - acc:         0.814500 loss:        0.557098
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.873960 loss:        0.370231
Test - acc:         0.826900 loss:        0.516385
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.869500 loss:        0.379596
Test - acc:         0.837700 loss:        0.498690
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.868400 loss:        0.381868
Test - acc:         0.857200 loss:        0.420949
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.871940 loss:        0.377637
Test - acc:         0.825900 loss:        0.547199
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.870680 loss:        0.378822
Test - acc:         0.831400 loss:        0.497769
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.872980 loss:        0.375904
Test - acc:         0.809000 loss:        0.558676
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.872880 loss:        0.375953
Test - acc:         0.809400 loss:        0.604101
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.870100 loss:        0.383866
Test - acc:         0.825900 loss:        0.523767
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.880760 loss:        0.345776
Test - acc:         0.845200 loss:        0.466696
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.877580 loss:        0.356896
Test - acc:         0.847100 loss:        0.462307
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.879240 loss:        0.352293
Test - acc:         0.828800 loss:        0.525920
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.879840 loss:        0.353587
Test - acc:         0.845200 loss:        0.463266
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.877580 loss:        0.360388
Test - acc:         0.844000 loss:        0.472663
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.879520 loss:        0.356736
Test - acc:         0.820700 loss:        0.537045
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.876540 loss:        0.357714
Test - acc:         0.817500 loss:        0.542244
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.879860 loss:        0.354678
Test - acc:         0.839200 loss:        0.479823
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.877420 loss:        0.357189
Test - acc:         0.849800 loss:        0.449078
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.874420 loss:        0.365734
Test - acc:         0.823800 loss:        0.527807
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.878860 loss:        0.355568
Test - acc:         0.847100 loss:        0.465630
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.878940 loss:        0.357380
Test - acc:         0.779000 loss:        0.755309
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.878060 loss:        0.358674
Test - acc:         0.825000 loss:        0.549558
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.877020 loss:        0.360072
Test - acc:         0.832600 loss:        0.506881
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.876900 loss:        0.364829
Test - acc:         0.840000 loss:        0.478669
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.879020 loss:        0.352411
Test - acc:         0.817600 loss:        0.562312
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.880260 loss:        0.352973
Test - acc:         0.842500 loss:        0.476287
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.876120 loss:        0.363309
Test - acc:         0.827600 loss:        0.512046
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.878900 loss:        0.353115
Test - acc:         0.859900 loss:        0.432598
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.878780 loss:        0.357823
Test - acc:         0.822400 loss:        0.537222
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.879000 loss:        0.355529
Test - acc:         0.811900 loss:        0.578088
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.876920 loss:        0.359044
Test - acc:         0.801800 loss:        0.638460
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.875400 loss:        0.365054
Test - acc:         0.838000 loss:        0.503777
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.878620 loss:        0.357846
Test - acc:         0.832300 loss:        0.504188
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.880120 loss:        0.351840
Test - acc:         0.821600 loss:        0.546569
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.876000 loss:        0.360635
Test - acc:         0.826200 loss:        0.538456
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.878840 loss:        0.355405
Test - acc:         0.825000 loss:        0.531160
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.878660 loss:        0.355573
Test - acc:         0.847600 loss:        0.452677
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.878560 loss:        0.353715
Test - acc:         0.859700 loss:        0.412775
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.880100 loss:        0.353245
Test - acc:         0.852900 loss:        0.453294
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.878420 loss:        0.358112
Test - acc:         0.820400 loss:        0.581650
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.880860 loss:        0.351268
Test - acc:         0.844500 loss:        0.476576
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.880860 loss:        0.349331
Test - acc:         0.846900 loss:        0.466606
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.879080 loss:        0.351758
Test - acc:         0.832900 loss:        0.508772
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.878060 loss:        0.355224
Test - acc:         0.816400 loss:        0.577929
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.876560 loss:        0.360490
Test - acc:         0.854500 loss:        0.428871
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.882200 loss:        0.347905
Test - acc:         0.795100 loss:        0.621914
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.878680 loss:        0.354221
Test - acc:         0.858800 loss:        0.420270
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.878800 loss:        0.354812
Test - acc:         0.813000 loss:        0.572786
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.879200 loss:        0.354090
Test - acc:         0.797600 loss:        0.620386
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.877200 loss:        0.357997
Test - acc:         0.859000 loss:        0.415621
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.882660 loss:        0.346218
Test - acc:         0.834600 loss:        0.492591
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.878740 loss:        0.354222
Test - acc:         0.840500 loss:        0.491634
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.876980 loss:        0.358314
Test - acc:         0.781600 loss:        0.666990
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.877360 loss:        0.356365
Test - acc:         0.836500 loss:        0.503460
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.879240 loss:        0.356188
Test - acc:         0.816700 loss:        0.589676
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.880280 loss:        0.353940
Test - acc:         0.825300 loss:        0.541309
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.877040 loss:        0.359462
Test - acc:         0.826000 loss:        0.512330
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.877800 loss:        0.357645
Test - acc:         0.849900 loss:        0.468125
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.880880 loss:        0.351080
Test - acc:         0.825000 loss:        0.543244
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.889940 loss:        0.319411
Test - acc:         0.872200 loss:        0.379275
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.889320 loss:        0.325979
Test - acc:         0.861500 loss:        0.421906
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.886480 loss:        0.334009
Test - acc:         0.839500 loss:        0.501858
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.884580 loss:        0.334714
Test - acc:         0.854600 loss:        0.447061
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.885420 loss:        0.332146
Test - acc:         0.832300 loss:        0.541985
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.885020 loss:        0.335726
Test - acc:         0.837400 loss:        0.520318
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.884420 loss:        0.336787
Test - acc:         0.824900 loss:        0.542552
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.885340 loss:        0.334566
Test - acc:         0.797800 loss:        0.684593
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.883280 loss:        0.339631
Test - acc:         0.824700 loss:        0.538822
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.884960 loss:        0.336908
Test - acc:         0.825300 loss:        0.540400
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.884160 loss:        0.334951
Test - acc:         0.837600 loss:        0.499288
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.885580 loss:        0.332204
Test - acc:         0.858700 loss:        0.428207
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.885060 loss:        0.340009
Test - acc:         0.820900 loss:        0.534463
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.885220 loss:        0.336058
Test - acc:         0.844300 loss:        0.466090
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.887640 loss:        0.330747
Test - acc:         0.843000 loss:        0.450840
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.884580 loss:        0.337931
Test - acc:         0.856400 loss:        0.437366
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.886080 loss:        0.332035
Test - acc:         0.694500 loss:        1.164989
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.886380 loss:        0.332276
Test - acc:         0.816400 loss:        0.562331
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.888280 loss:        0.330200
Test - acc:         0.800000 loss:        0.646310
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.883800 loss:        0.339860
Test - acc:         0.836200 loss:        0.514347
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.886680 loss:        0.330059
Test - acc:         0.818000 loss:        0.552405
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.883280 loss:        0.340964
Test - acc:         0.834000 loss:        0.497849
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.884880 loss:        0.334341
Test - acc:         0.855700 loss:        0.435677
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.884620 loss:        0.337061
Test - acc:         0.842100 loss:        0.489071
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.886020 loss:        0.336012
Test - acc:         0.866800 loss:        0.390479
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.885120 loss:        0.336589
Test - acc:         0.841200 loss:        0.480105
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.884220 loss:        0.339281
Test - acc:         0.811800 loss:        0.608718
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.882960 loss:        0.338948
Test - acc:         0.840000 loss:        0.488287
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.884120 loss:        0.334493
Test - acc:         0.823600 loss:        0.530313
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.884960 loss:        0.337480
Test - acc:         0.820200 loss:        0.545106
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.884240 loss:        0.335082
Test - acc:         0.842200 loss:        0.468286
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.882680 loss:        0.341640
Test - acc:         0.859300 loss:        0.404621
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.885540 loss:        0.333580
Test - acc:         0.858300 loss:        0.436387
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.883600 loss:        0.338303
Test - acc:         0.802700 loss:        0.627146
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.884520 loss:        0.336382
Test - acc:         0.849800 loss:        0.455005
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.884900 loss:        0.332634
Test - acc:         0.838000 loss:        0.500912
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.884600 loss:        0.337225
Test - acc:         0.838500 loss:        0.498823
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.883700 loss:        0.337039
Test - acc:         0.847300 loss:        0.443481
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.883980 loss:        0.337127
Test - acc:         0.842000 loss:        0.475394
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.884720 loss:        0.339698
Test - acc:         0.805400 loss:        0.615557
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.883740 loss:        0.341455
Test - acc:         0.854100 loss:        0.451966
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.885360 loss:        0.335877
Test - acc:         0.850900 loss:        0.461633
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.885860 loss:        0.331319
Test - acc:         0.829600 loss:        0.525041
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.886260 loss:        0.330054
Test - acc:         0.856000 loss:        0.445297
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.883600 loss:        0.338911
Test - acc:         0.833000 loss:        0.530931
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.883900 loss:        0.338486
Test - acc:         0.845900 loss:        0.474917
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.885440 loss:        0.338638
Test - acc:         0.862200 loss:        0.418731
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.887460 loss:        0.329240
Test - acc:         0.836300 loss:        0.501432
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.885160 loss:        0.334490
Test - acc:         0.849400 loss:        0.461254
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.886460 loss:        0.330311
Test - acc:         0.839400 loss:        0.491243
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.932680 loss:        0.200815
Test - acc:         0.920400 loss:        0.232916
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.948120 loss:        0.152408
Test - acc:         0.923800 loss:        0.223084
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.952480 loss:        0.138192
Test - acc:         0.925500 loss:        0.214345
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.959640 loss:        0.122452
Test - acc:         0.927400 loss:        0.212847
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.960800 loss:        0.115009
Test - acc:         0.929900 loss:        0.206307
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.963420 loss:        0.106814
Test - acc:         0.929100 loss:        0.209959
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.964900 loss:        0.101149
Test - acc:         0.931800 loss:        0.204853
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.967620 loss:        0.094808
Test - acc:         0.932500 loss:        0.206253
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.970020 loss:        0.089942
Test - acc:         0.930500 loss:        0.215483
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.971180 loss:        0.084910
Test - acc:         0.929900 loss:        0.211773
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.973500 loss:        0.079500
Test - acc:         0.932900 loss:        0.215486
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.974280 loss:        0.075717
Test - acc:         0.930200 loss:        0.219177
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.975980 loss:        0.071166
Test - acc:         0.933000 loss:        0.210704
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.976600 loss:        0.068989
Test - acc:         0.930000 loss:        0.230491
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.978380 loss:        0.064413
Test - acc:         0.933000 loss:        0.213288
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.978440 loss:        0.063229
Test - acc:         0.931900 loss:        0.227470
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.978660 loss:        0.061832
Test - acc:         0.930600 loss:        0.244159
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.980180 loss:        0.060462
Test - acc:         0.929500 loss:        0.234708
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.979860 loss:        0.061315
Test - acc:         0.929000 loss:        0.235508
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.980640 loss:        0.058337
Test - acc:         0.929200 loss:        0.243858
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.980620 loss:        0.057978
Test - acc:         0.927300 loss:        0.245354
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.981040 loss:        0.056990
Test - acc:         0.929400 loss:        0.240326
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.981080 loss:        0.056799
Test - acc:         0.926600 loss:        0.260028
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.981580 loss:        0.053985
Test - acc:         0.931500 loss:        0.235425
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.979620 loss:        0.059083
Test - acc:         0.928500 loss:        0.240647
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.982860 loss:        0.052769
Test - acc:         0.923100 loss:        0.268556
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.979880 loss:        0.058950
Test - acc:         0.928000 loss:        0.260810
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.982180 loss:        0.053964
Test - acc:         0.924600 loss:        0.272713
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.981540 loss:        0.054418
Test - acc:         0.928700 loss:        0.255817
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.980800 loss:        0.056186
Test - acc:         0.924000 loss:        0.261416
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.979220 loss:        0.060944
Test - acc:         0.924300 loss:        0.258019
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.979800 loss:        0.059075
Test - acc:         0.924700 loss:        0.254881
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.980660 loss:        0.056842
Test - acc:         0.926800 loss:        0.252097
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.980180 loss:        0.057817
Test - acc:         0.925100 loss:        0.262304
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.980220 loss:        0.058912
Test - acc:         0.926100 loss:        0.260824
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.980040 loss:        0.059494
Test - acc:         0.927100 loss:        0.251696
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.978740 loss:        0.062620
Test - acc:         0.913300 loss:        0.301748
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.978860 loss:        0.062452
Test - acc:         0.922700 loss:        0.265002
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.980640 loss:        0.058427
Test - acc:         0.926200 loss:        0.262934
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.979020 loss:        0.062154
Test - acc:         0.910100 loss:        0.323711
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.978640 loss:        0.063052
Test - acc:         0.919100 loss:        0.278111
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.979060 loss:        0.061462
Test - acc:         0.926900 loss:        0.253398
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.979740 loss:        0.060344
Test - acc:         0.923400 loss:        0.260446
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.978400 loss:        0.063451
Test - acc:         0.919700 loss:        0.290481
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.977580 loss:        0.065052
Test - acc:         0.927200 loss:        0.259458
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.977900 loss:        0.064983
Test - acc:         0.918500 loss:        0.283004
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.977320 loss:        0.065646
Test - acc:         0.918600 loss:        0.272513
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.978540 loss:        0.062575
Test - acc:         0.910500 loss:        0.315803
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.978840 loss:        0.064370
Test - acc:         0.921500 loss:        0.278861
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.977980 loss:        0.064771
Test - acc:         0.916700 loss:        0.282038
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.982620 loss:        0.053283
Test - acc:         0.929800 loss:        0.247444
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.982720 loss:        0.050903
Test - acc:         0.930500 loss:        0.241718
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.984840 loss:        0.046630
Test - acc:         0.923500 loss:        0.264493
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.984700 loss:        0.045528
Test - acc:         0.919900 loss:        0.280915
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.982800 loss:        0.052606
Test - acc:         0.919700 loss:        0.284609
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.983260 loss:        0.049515
Test - acc:         0.922500 loss:        0.271633
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.984300 loss:        0.049096
Test - acc:         0.920400 loss:        0.285040
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.982540 loss:        0.051397
Test - acc:         0.914300 loss:        0.326204
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.981800 loss:        0.054205
Test - acc:         0.915800 loss:        0.314636
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.981860 loss:        0.053981
Test - acc:         0.926900 loss:        0.265299
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.981420 loss:        0.055784
Test - acc:         0.924700 loss:        0.265029
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.981560 loss:        0.055088
Test - acc:         0.920900 loss:        0.289018
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.981140 loss:        0.056096
Test - acc:         0.923300 loss:        0.268367
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.980420 loss:        0.057442
Test - acc:         0.927900 loss:        0.259875
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.982360 loss:        0.052904
Test - acc:         0.923500 loss:        0.264250
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.054790
Test - acc:         0.921900 loss:        0.277645
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.981820 loss:        0.054680
Test - acc:         0.925700 loss:        0.251570
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.056414
Test - acc:         0.923500 loss:        0.265894
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.981840 loss:        0.053900
Test - acc:         0.922100 loss:        0.270943
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.981140 loss:        0.054553
Test - acc:         0.922100 loss:        0.272414
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.981920 loss:        0.054466
Test - acc:         0.924500 loss:        0.264009
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.980460 loss:        0.056165
Test - acc:         0.924200 loss:        0.274708
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.981700 loss:        0.055108
Test - acc:         0.924100 loss:        0.274088
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.979360 loss:        0.059709
Test - acc:         0.928600 loss:        0.260517
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.979060 loss:        0.062224
Test - acc:         0.922400 loss:        0.280544
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.981900 loss:        0.055050
Test - acc:         0.925700 loss:        0.259129
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.980120 loss:        0.058783
Test - acc:         0.922700 loss:        0.278769
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.979920 loss:        0.060185
Test - acc:         0.921700 loss:        0.293701
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.981340 loss:        0.056148
Test - acc:         0.926300 loss:        0.263399
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.981360 loss:        0.055724
Test - acc:         0.910600 loss:        0.332492
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.979640 loss:        0.060049
Test - acc:         0.926500 loss:        0.252731
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.983880 loss:        0.048461
Test - acc:         0.920600 loss:        0.295366
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.980680 loss:        0.056165
Test - acc:         0.918500 loss:        0.289974
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.981180 loss:        0.056473
Test - acc:         0.925300 loss:        0.270279
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.057563
Test - acc:         0.917100 loss:        0.297292
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.979720 loss:        0.060361
Test - acc:         0.925900 loss:        0.271125
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.978660 loss:        0.062767
Test - acc:         0.922700 loss:        0.269032
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.979780 loss:        0.058899
Test - acc:         0.922000 loss:        0.271771
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.980680 loss:        0.059036
Test - acc:         0.910900 loss:        0.311686
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.056906
Test - acc:         0.924300 loss:        0.266101
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.981880 loss:        0.053612
Test - acc:         0.918400 loss:        0.290868
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.978660 loss:        0.064095
Test - acc:         0.918500 loss:        0.283751
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.978700 loss:        0.061801
Test - acc:         0.922900 loss:        0.267605
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.980320 loss:        0.060070
Test - acc:         0.924100 loss:        0.275722
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.981460 loss:        0.055958
Test - acc:         0.922800 loss:        0.272297
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.981660 loss:        0.054610
Test - acc:         0.908000 loss:        0.328967
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.978380 loss:        0.063645
Test - acc:         0.924500 loss:        0.261308
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.981200 loss:        0.057773
Test - acc:         0.919100 loss:        0.286096
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.979620 loss:        0.059744
Test - acc:         0.918500 loss:        0.276325
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.980360 loss:        0.059954
Test - acc:         0.922200 loss:        0.263349
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.984660 loss:        0.049489
Test - acc:         0.937400 loss:        0.210073
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.990740 loss:        0.032570
Test - acc:         0.939300 loss:        0.206515
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.992360 loss:        0.027814
Test - acc:         0.938300 loss:        0.204726
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.993500 loss:        0.025288
Test - acc:         0.938900 loss:        0.204192
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.993560 loss:        0.023698
Test - acc:         0.940100 loss:        0.203315
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.994820 loss:        0.020966
Test - acc:         0.939600 loss:        0.205941
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.995180 loss:        0.019971
Test - acc:         0.940500 loss:        0.204583
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.994660 loss:        0.020342
Test - acc:         0.941100 loss:        0.204327
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.995560 loss:        0.018416
Test - acc:         0.941100 loss:        0.207733
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.996160 loss:        0.016843
Test - acc:         0.940700 loss:        0.207076
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.996240 loss:        0.016419
Test - acc:         0.941600 loss:        0.207633
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.995780 loss:        0.016746
Test - acc:         0.941500 loss:        0.207799
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.996520 loss:        0.015317
Test - acc:         0.942100 loss:        0.208648
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.996800 loss:        0.014768
Test - acc:         0.942200 loss:        0.208531
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.996940 loss:        0.014335
Test - acc:         0.942100 loss:        0.208718
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.996840 loss:        0.014617
Test - acc:         0.941800 loss:        0.209586
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.996900 loss:        0.013925
Test - acc:         0.942000 loss:        0.209987
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.997200 loss:        0.012950
Test - acc:         0.942500 loss:        0.211517
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.997400 loss:        0.012516
Test - acc:         0.941700 loss:        0.211470
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.997400 loss:        0.012424
Test - acc:         0.942900 loss:        0.210680
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.997980 loss:        0.011646
Test - acc:         0.942500 loss:        0.210426
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.997340 loss:        0.012283
Test - acc:         0.941800 loss:        0.210097
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.998100 loss:        0.010747
Test - acc:         0.942100 loss:        0.211255
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.997840 loss:        0.011410
Test - acc:         0.940800 loss:        0.211332
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.997900 loss:        0.010763
Test - acc:         0.941400 loss:        0.212357
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.997660 loss:        0.011169
Test - acc:         0.942100 loss:        0.212390
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.997920 loss:        0.010522
Test - acc:         0.942100 loss:        0.215298
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.998040 loss:        0.010558
Test - acc:         0.942500 loss:        0.210721
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.998200 loss:        0.009878
Test - acc:         0.941800 loss:        0.212288
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.998080 loss:        0.010132
Test - acc:         0.941700 loss:        0.215184
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.998260 loss:        0.009420
Test - acc:         0.942300 loss:        0.213208
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.997880 loss:        0.009897
Test - acc:         0.941700 loss:        0.214395
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.998320 loss:        0.009648
Test - acc:         0.942100 loss:        0.213785
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.998320 loss:        0.009291
Test - acc:         0.941400 loss:        0.212360
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.998360 loss:        0.009181
Test - acc:         0.940900 loss:        0.216337
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.998500 loss:        0.008914
Test - acc:         0.941700 loss:        0.212918
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.998640 loss:        0.008291
Test - acc:         0.941700 loss:        0.213896
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.998260 loss:        0.008699
Test - acc:         0.941900 loss:        0.213589
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.998420 loss:        0.009061
Test - acc:         0.942000 loss:        0.215890
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.998420 loss:        0.008855
Test - acc:         0.942000 loss:        0.213961
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.998640 loss:        0.008414
Test - acc:         0.943000 loss:        0.214976
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.998680 loss:        0.008372
Test - acc:         0.941900 loss:        0.214299
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.998700 loss:        0.007966
Test - acc:         0.942400 loss:        0.215978
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.998780 loss:        0.007949
Test - acc:         0.943000 loss:        0.216255
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.007819
Test - acc:         0.941300 loss:        0.218498
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.998880 loss:        0.007464
Test - acc:         0.941300 loss:        0.218992
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.998700 loss:        0.008063
Test - acc:         0.943400 loss:        0.215351
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.998880 loss:        0.007650
Test - acc:         0.942600 loss:        0.213986
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.998560 loss:        0.007683
Test - acc:         0.943500 loss:        0.215012
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.998740 loss:        0.007519
Test - acc:         0.942000 loss:        0.216303
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.976460 loss:        0.074036
Test - acc:         0.926600 loss:        0.246153
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.983200 loss:        0.054256
Test - acc:         0.930400 loss:        0.238380
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.985200 loss:        0.048395
Test - acc:         0.930000 loss:        0.235930
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.987640 loss:        0.042675
Test - acc:         0.929600 loss:        0.239039
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.987340 loss:        0.041387
Test - acc:         0.931600 loss:        0.234466
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.988640 loss:        0.037541
Test - acc:         0.932000 loss:        0.237241
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.989880 loss:        0.036753
Test - acc:         0.931500 loss:        0.232282
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.990600 loss:        0.033956
Test - acc:         0.933400 loss:        0.234852
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.990640 loss:        0.033193
Test - acc:         0.933300 loss:        0.231560
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.990560 loss:        0.033212
Test - acc:         0.934700 loss:        0.231441
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.991620 loss:        0.030427
Test - acc:         0.934200 loss:        0.230159
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.991840 loss:        0.029248
Test - acc:         0.934300 loss:        0.230786
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.992180 loss:        0.029247
Test - acc:         0.933500 loss:        0.234311
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.992260 loss:        0.027765
Test - acc:         0.934500 loss:        0.235674
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.992660 loss:        0.026240
Test - acc:         0.934500 loss:        0.237878
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.993060 loss:        0.026062
Test - acc:         0.934400 loss:        0.236264
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.992980 loss:        0.025919
Test - acc:         0.933000 loss:        0.236360
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.993200 loss:        0.024888
Test - acc:         0.933800 loss:        0.238341
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.993300 loss:        0.024712
Test - acc:         0.934100 loss:        0.237816
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.993720 loss:        0.023911
Test - acc:         0.934200 loss:        0.238911
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.994220 loss:        0.022681
Test - acc:         0.934800 loss:        0.237183
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.993820 loss:        0.023530
Test - acc:         0.935200 loss:        0.239535
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.994340 loss:        0.023072
Test - acc:         0.935200 loss:        0.236995
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.994520 loss:        0.020741
Test - acc:         0.937100 loss:        0.238182
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.994760 loss:        0.020505
Test - acc:         0.935800 loss:        0.239633
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.994360 loss:        0.021937
Test - acc:         0.934600 loss:        0.241423
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.994640 loss:        0.020975
Test - acc:         0.935100 loss:        0.239169
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.994900 loss:        0.020133
Test - acc:         0.936000 loss:        0.238623
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.995500 loss:        0.018899
Test - acc:         0.933500 loss:        0.246850
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.994620 loss:        0.020724
Test - acc:         0.934600 loss:        0.243989
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.995840 loss:        0.018562
Test - acc:         0.935700 loss:        0.241089
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.995040 loss:        0.019303
Test - acc:         0.935700 loss:        0.242587
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.995140 loss:        0.019682
Test - acc:         0.936400 loss:        0.241394
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.995280 loss:        0.017903
Test - acc:         0.934700 loss:        0.243727
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.995580 loss:        0.017536
Test - acc:         0.935700 loss:        0.244909
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.995320 loss:        0.017949
Test - acc:         0.937300 loss:        0.241764
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.995540 loss:        0.018023
Test - acc:         0.936600 loss:        0.241499
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.995900 loss:        0.017484
Test - acc:         0.935700 loss:        0.246511
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.995860 loss:        0.017342
Test - acc:         0.935700 loss:        0.244128
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.996320 loss:        0.016409
Test - acc:         0.936000 loss:        0.244599
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.996460 loss:        0.015909
Test - acc:         0.935800 loss:        0.242150
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.996040 loss:        0.015972
Test - acc:         0.935600 loss:        0.240682
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.995780 loss:        0.017175
Test - acc:         0.933800 loss:        0.242630
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.996120 loss:        0.016692
Test - acc:         0.935700 loss:        0.240415
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.996160 loss:        0.015739
Test - acc:         0.936000 loss:        0.243775
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.995720 loss:        0.016188
Test - acc:         0.936100 loss:        0.242197
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.996760 loss:        0.014822
Test - acc:         0.935000 loss:        0.245396
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.996160 loss:        0.016107
Test - acc:         0.935500 loss:        0.240598
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.996740 loss:        0.014891
Test - acc:         0.935900 loss:        0.242022
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.996640 loss:        0.015083
Test - acc:         0.937900 loss:        0.238647
Sparsity :          0.9844
Wdecay :        0.000500
