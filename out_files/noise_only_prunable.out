******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "topflip",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "logdir": "topflip_ablations",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": true,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_topflip_pf50_s42_noise_only_prunable",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.318220 loss:        1.985412
Test - acc:         0.429200 loss:        1.525093
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.499620 loss:        1.373114
Test - acc:         0.534800 loss:        1.265797
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.612200 loss:        1.089528
Test - acc:         0.653400 loss:        0.964139
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.677780 loss:        0.906348
Test - acc:         0.666400 loss:        0.965327
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.729820 loss:        0.776061
Test - acc:         0.652200 loss:        1.036147
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.764740 loss:        0.675035
Test - acc:         0.761600 loss:        0.693125
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.789760 loss:        0.608187
Test - acc:         0.779900 loss:        0.629873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.806000 loss:        0.564707
Test - acc:         0.744900 loss:        0.750463
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.812960 loss:        0.540295
Test - acc:         0.752900 loss:        0.729536
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.823800 loss:        0.513245
Test - acc:         0.751700 loss:        0.748913
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.830140 loss:        0.492257
Test - acc:         0.804100 loss:        0.595031
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.837740 loss:        0.471075
Test - acc:         0.741400 loss:        0.860206
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.842220 loss:        0.461984
Test - acc:         0.786800 loss:        0.653712
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.842520 loss:        0.461559
Test - acc:         0.795700 loss:        0.594977
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.845480 loss:        0.448581
Test - acc:         0.815600 loss:        0.571001
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.850840 loss:        0.433909
Test - acc:         0.796800 loss:        0.609109
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.853580 loss:        0.426681
Test - acc:         0.809100 loss:        0.550305
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.858000 loss:        0.419201
Test - acc:         0.767200 loss:        0.747714
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.859720 loss:        0.411343
Test - acc:         0.788600 loss:        0.619552
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.857900 loss:        0.408749
Test - acc:         0.795800 loss:        0.706319
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.862800 loss:        0.401139
Test - acc:         0.807700 loss:        0.576402
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.862940 loss:        0.397437
Test - acc:         0.788600 loss:        0.643756
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.866600 loss:        0.394531
Test - acc:         0.793400 loss:        0.640682
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.866780 loss:        0.387493
Test - acc:         0.816300 loss:        0.570344
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.869900 loss:        0.382501
Test - acc:         0.846400 loss:        0.465978
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.867140 loss:        0.386222
Test - acc:         0.772700 loss:        0.679029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.869620 loss:        0.382511
Test - acc:         0.828200 loss:        0.528247
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.868500 loss:        0.385333
Test - acc:         0.852900 loss:        0.431600
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.872400 loss:        0.372402
Test - acc:         0.816700 loss:        0.584232
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.870760 loss:        0.378113
Test - acc:         0.839300 loss:        0.486770
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.871560 loss:        0.374331
Test - acc:         0.849400 loss:        0.454670
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.871420 loss:        0.375747
Test - acc:         0.818200 loss:        0.551412
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.876200 loss:        0.366422
Test - acc:         0.840600 loss:        0.476191
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.873680 loss:        0.369777
Test - acc:         0.844100 loss:        0.462965
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.874180 loss:        0.365100
Test - acc:         0.827700 loss:        0.540672
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.877380 loss:        0.364332
Test - acc:         0.830500 loss:        0.533784
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.873280 loss:        0.369054
Test - acc:         0.838600 loss:        0.502545
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.874080 loss:        0.365253
Test - acc:         0.828800 loss:        0.542968
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.877160 loss:        0.363019
Test - acc:         0.832200 loss:        0.506629
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.877080 loss:        0.362398
Test - acc:         0.827000 loss:        0.529568
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.876080 loss:        0.363974
Test - acc:         0.851300 loss:        0.451986
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.879560 loss:        0.352398
Test - acc:         0.814000 loss:        0.556395
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.879660 loss:        0.353418
Test - acc:         0.853100 loss:        0.445217
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.878160 loss:        0.356876
Test - acc:         0.819400 loss:        0.569926
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.878980 loss:        0.357387
Test - acc:         0.824300 loss:        0.534669
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.878960 loss:        0.358085
Test - acc:         0.839600 loss:        0.467365
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.878900 loss:        0.356167
Test - acc:         0.813100 loss:        0.581091
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.874480 loss:        0.362986
Test - acc:         0.820300 loss:        0.553920
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.880600 loss:        0.352904
Test - acc:         0.754000 loss:        0.831924
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.878560 loss:        0.357645
Test - acc:         0.847300 loss:        0.459975
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.883400 loss:        0.341756
Test - acc:         0.783600 loss:        0.719168
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.888420 loss:        0.329962
Test - acc:         0.852700 loss:        0.432542
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.891120 loss:        0.319368
Test - acc:         0.824400 loss:        0.545667
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.891500 loss:        0.320674
Test - acc:         0.831200 loss:        0.527989
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.891660 loss:        0.315010
Test - acc:         0.854900 loss:        0.454266
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.892400 loss:        0.312947
Test - acc:         0.815500 loss:        0.574957
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.892580 loss:        0.315634
Test - acc:         0.847000 loss:        0.471733
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.892500 loss:        0.312857
Test - acc:         0.856400 loss:        0.462671
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.893780 loss:        0.310270
Test - acc:         0.858800 loss:        0.432768
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.893680 loss:        0.313483
Test - acc:         0.835000 loss:        0.517230
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.895160 loss:        0.308119
Test - acc:         0.845300 loss:        0.467068
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.893180 loss:        0.313216
Test - acc:         0.788400 loss:        0.702558
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.894980 loss:        0.307947
Test - acc:         0.813200 loss:        0.563137
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.893700 loss:        0.313082
Test - acc:         0.806200 loss:        0.626865
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.892820 loss:        0.312576
Test - acc:         0.849900 loss:        0.449962
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.894220 loss:        0.306994
Test - acc:         0.819100 loss:        0.556622
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.894840 loss:        0.307258
Test - acc:         0.843600 loss:        0.482088
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.893120 loss:        0.309945
Test - acc:         0.848800 loss:        0.457258
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.894960 loss:        0.307485
Test - acc:         0.832600 loss:        0.538056
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.894620 loss:        0.310330
Test - acc:         0.818600 loss:        0.546940
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.895520 loss:        0.306440
Test - acc:         0.830800 loss:        0.525297
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.895880 loss:        0.308988
Test - acc:         0.856400 loss:        0.423100
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.895500 loss:        0.305037
Test - acc:         0.845800 loss:        0.477611
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.895840 loss:        0.308040
Test - acc:         0.835400 loss:        0.501767
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.895400 loss:        0.304209
Test - acc:         0.806900 loss:        0.607684
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.897400 loss:        0.303682
Test - acc:         0.818800 loss:        0.569815
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.894220 loss:        0.310280
Test - acc:         0.848700 loss:        0.452488
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.895200 loss:        0.306259
Test - acc:         0.828700 loss:        0.496551
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.897180 loss:        0.305029
Test - acc:         0.827100 loss:        0.539707
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.895480 loss:        0.304453
Test - acc:         0.877300 loss:        0.373067
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.894160 loss:        0.310606
Test - acc:         0.819900 loss:        0.581521
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.898500 loss:        0.301535
Test - acc:         0.861400 loss:        0.423958
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.896760 loss:        0.304687
Test - acc:         0.855400 loss:        0.443645
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.895900 loss:        0.301971
Test - acc:         0.848300 loss:        0.450166
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.893900 loss:        0.309002
Test - acc:         0.868100 loss:        0.395543
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.894300 loss:        0.305467
Test - acc:         0.839500 loss:        0.500409
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.896240 loss:        0.305607
Test - acc:         0.843300 loss:        0.483354
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.898660 loss:        0.299383
Test - acc:         0.855900 loss:        0.428527
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.891700 loss:        0.310941
Test - acc:         0.814900 loss:        0.557836
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.894820 loss:        0.307269
Test - acc:         0.797800 loss:        0.656578
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.893940 loss:        0.309938
Test - acc:         0.825800 loss:        0.533317
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.895440 loss:        0.307234
Test - acc:         0.867200 loss:        0.404496
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.896600 loss:        0.303308
Test - acc:         0.862900 loss:        0.407549
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.891540 loss:        0.315598
Test - acc:         0.824800 loss:        0.551887
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.896280 loss:        0.305625
Test - acc:         0.864800 loss:        0.398445
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.894580 loss:        0.308448
Test - acc:         0.857800 loss:        0.429796
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.895280 loss:        0.308850
Test - acc:         0.818900 loss:        0.595043
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.896900 loss:        0.306471
Test - acc:         0.856300 loss:        0.422456
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.893500 loss:        0.310266
Test - acc:         0.847200 loss:        0.489275
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.892780 loss:        0.313613
Test - acc:         0.849900 loss:        0.462184
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.909880 loss:        0.262300
Test - acc:         0.845200 loss:        0.470050
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.905520 loss:        0.273508
Test - acc:         0.856000 loss:        0.454822
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.904820 loss:        0.277054
Test - acc:         0.877600 loss:        0.361255
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.905440 loss:        0.274717
Test - acc:         0.879400 loss:        0.371137
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.907580 loss:        0.266536
Test - acc:         0.855700 loss:        0.441609
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.907280 loss:        0.270371
Test - acc:         0.849200 loss:        0.459523
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.909780 loss:        0.264454
Test - acc:         0.849500 loss:        0.463436
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.907900 loss:        0.268064
Test - acc:         0.870900 loss:        0.400003
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.907760 loss:        0.268102
Test - acc:         0.840800 loss:        0.509957
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.906640 loss:        0.268455
Test - acc:         0.872300 loss:        0.384433
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.909760 loss:        0.260954
Test - acc:         0.832500 loss:        0.503374
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.907560 loss:        0.268204
Test - acc:         0.879300 loss:        0.353857
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.907020 loss:        0.269358
Test - acc:         0.868500 loss:        0.417245
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.908840 loss:        0.265793
Test - acc:         0.859100 loss:        0.426496
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.908920 loss:        0.265182
Test - acc:         0.860300 loss:        0.414973
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.908240 loss:        0.268524
Test - acc:         0.881000 loss:        0.350551
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.909500 loss:        0.261355
Test - acc:         0.850200 loss:        0.461708
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.906980 loss:        0.268015
Test - acc:         0.860700 loss:        0.427923
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.908500 loss:        0.263540
Test - acc:         0.845500 loss:        0.487374
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.907800 loss:        0.267950
Test - acc:         0.836400 loss:        0.528893
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.908720 loss:        0.264483
Test - acc:         0.871100 loss:        0.390988
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.909700 loss:        0.263494
Test - acc:         0.849600 loss:        0.474499
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.910820 loss:        0.261501
Test - acc:         0.881000 loss:        0.362323
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.908420 loss:        0.265853
Test - acc:         0.851400 loss:        0.472884
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.909580 loss:        0.263402
Test - acc:         0.848500 loss:        0.461318
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.908920 loss:        0.266969
Test - acc:         0.859100 loss:        0.424322
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.908360 loss:        0.263510
Test - acc:         0.843900 loss:        0.495934
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.907760 loss:        0.266591
Test - acc:         0.847400 loss:        0.473389
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.909380 loss:        0.263172
Test - acc:         0.843000 loss:        0.484509
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.908340 loss:        0.269243
Test - acc:         0.891900 loss:        0.327772
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.908720 loss:        0.264762
Test - acc:         0.862300 loss:        0.410642
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.906840 loss:        0.267480
Test - acc:         0.844100 loss:        0.481538
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.908220 loss:        0.264084
Test - acc:         0.781800 loss:        0.721330
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.909680 loss:        0.262909
Test - acc:         0.850600 loss:        0.488182
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.905800 loss:        0.271021
Test - acc:         0.874200 loss:        0.394138
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.909800 loss:        0.262459
Test - acc:         0.881000 loss:        0.358627
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.909060 loss:        0.264292
Test - acc:         0.851500 loss:        0.479272
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.907640 loss:        0.267207
Test - acc:         0.852400 loss:        0.443923
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.908920 loss:        0.263575
Test - acc:         0.773600 loss:        0.792962
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.907500 loss:        0.265933
Test - acc:         0.858700 loss:        0.427898
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.909100 loss:        0.264241
Test - acc:         0.871100 loss:        0.390327
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.910320 loss:        0.266209
Test - acc:         0.863500 loss:        0.413951
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.908260 loss:        0.265046
Test - acc:         0.866200 loss:        0.400255
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.909200 loss:        0.263255
Test - acc:         0.852200 loss:        0.448551
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.906520 loss:        0.270003
Test - acc:         0.863900 loss:        0.419452
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.908180 loss:        0.266914
Test - acc:         0.873200 loss:        0.399155
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.909840 loss:        0.263623
Test - acc:         0.878300 loss:        0.367469
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.908680 loss:        0.263907
Test - acc:         0.834500 loss:        0.526778
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.907640 loss:        0.267172
Test - acc:         0.846500 loss:        0.496903
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.910340 loss:        0.261860
Test - acc:         0.879600 loss:        0.356980
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.952660 loss:        0.144574
Test - acc:         0.930700 loss:        0.204276
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.963220 loss:        0.112459
Test - acc:         0.934500 loss:        0.195250
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.966720 loss:        0.100211
Test - acc:         0.936300 loss:        0.196024
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.970080 loss:        0.088824
Test - acc:         0.937000 loss:        0.195559
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.971240 loss:        0.084622
Test - acc:         0.937800 loss:        0.194792
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.973380 loss:        0.078375
Test - acc:         0.937900 loss:        0.197984
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.976440 loss:        0.072438
Test - acc:         0.937000 loss:        0.199064
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.976840 loss:        0.070476
Test - acc:         0.937800 loss:        0.198579
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.977320 loss:        0.066934
Test - acc:         0.937000 loss:        0.203090
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.978900 loss:        0.062028
Test - acc:         0.935100 loss:        0.208296
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.980300 loss:        0.060554
Test - acc:         0.936900 loss:        0.205118
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.980880 loss:        0.057326
Test - acc:         0.935200 loss:        0.213326
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.983340 loss:        0.052164
Test - acc:         0.937800 loss:        0.207457
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.982840 loss:        0.051508
Test - acc:         0.934900 loss:        0.217030
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.984200 loss:        0.047792
Test - acc:         0.938600 loss:        0.212554
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.984560 loss:        0.048675
Test - acc:         0.937200 loss:        0.217811
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.984840 loss:        0.045731
Test - acc:         0.936200 loss:        0.223199
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.985920 loss:        0.044533
Test - acc:         0.930500 loss:        0.231224
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.984360 loss:        0.047232
Test - acc:         0.933300 loss:        0.228034
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.986500 loss:        0.043099
Test - acc:         0.933200 loss:        0.225907
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.986260 loss:        0.043034
Test - acc:         0.932800 loss:        0.231017
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.985860 loss:        0.043412
Test - acc:         0.934700 loss:        0.223826
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.985640 loss:        0.042923
Test - acc:         0.932800 loss:        0.237558
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.985840 loss:        0.043876
Test - acc:         0.933900 loss:        0.232200
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.985560 loss:        0.045255
Test - acc:         0.926300 loss:        0.249154
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.985440 loss:        0.043279
Test - acc:         0.932300 loss:        0.239745
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.985320 loss:        0.044172
Test - acc:         0.931200 loss:        0.241853
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.985480 loss:        0.043451
Test - acc:         0.929700 loss:        0.256093
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.983740 loss:        0.047035
Test - acc:         0.927300 loss:        0.245486
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.984740 loss:        0.045906
Test - acc:         0.931600 loss:        0.234546
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.984300 loss:        0.046801
Test - acc:         0.929200 loss:        0.241208
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.985920 loss:        0.044568
Test - acc:         0.935900 loss:        0.224948
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.984060 loss:        0.047199
Test - acc:         0.925700 loss:        0.258511
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.983260 loss:        0.049095
Test - acc:         0.934900 loss:        0.232831
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.983660 loss:        0.049584
Test - acc:         0.931100 loss:        0.236262
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.982680 loss:        0.051009
Test - acc:         0.923700 loss:        0.263973
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.984020 loss:        0.049600
Test - acc:         0.924400 loss:        0.260896
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.984360 loss:        0.047735
Test - acc:         0.930000 loss:        0.257830
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.983280 loss:        0.049545
Test - acc:         0.925100 loss:        0.249186
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.983200 loss:        0.052173
Test - acc:         0.927600 loss:        0.254324
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.984100 loss:        0.048820
Test - acc:         0.925300 loss:        0.255240
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.983000 loss:        0.052254
Test - acc:         0.928100 loss:        0.252472
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.982860 loss:        0.050349
Test - acc:         0.924600 loss:        0.261987
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.984000 loss:        0.048044
Test - acc:         0.926900 loss:        0.265125
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.982220 loss:        0.053435
Test - acc:         0.927400 loss:        0.262212
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.983040 loss:        0.051910
Test - acc:         0.927000 loss:        0.250369
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.984280 loss:        0.048733
Test - acc:         0.930900 loss:        0.245335
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.982460 loss:        0.052261
Test - acc:         0.921800 loss:        0.275825
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.981780 loss:        0.053436
Test - acc:         0.922000 loss:        0.262909
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.983440 loss:        0.050798
Test - acc:         0.927700 loss:        0.256554
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.972340 loss:        0.081482
Test - acc:         0.923500 loss:        0.251175
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.974080 loss:        0.073919
Test - acc:         0.925100 loss:        0.246110
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.976520 loss:        0.069677
Test - acc:         0.922300 loss:        0.262636
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.976620 loss:        0.069829
Test - acc:         0.923000 loss:        0.258646
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.977520 loss:        0.066886
Test - acc:         0.925700 loss:        0.255293
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.976880 loss:        0.066965
Test - acc:         0.924200 loss:        0.261531
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.976980 loss:        0.068095
Test - acc:         0.924200 loss:        0.265424
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.978400 loss:        0.063581
Test - acc:         0.913000 loss:        0.304147
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.977380 loss:        0.065940
Test - acc:         0.925700 loss:        0.249674
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.978820 loss:        0.062663
Test - acc:         0.922800 loss:        0.265378
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.977360 loss:        0.066354
Test - acc:         0.920700 loss:        0.274410
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.978440 loss:        0.062076
Test - acc:         0.922700 loss:        0.261238
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.978580 loss:        0.062819
Test - acc:         0.919400 loss:        0.288891
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.978980 loss:        0.063693
Test - acc:         0.922100 loss:        0.263789
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.978380 loss:        0.063832
Test - acc:         0.923500 loss:        0.263799
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.979760 loss:        0.059698
Test - acc:         0.923000 loss:        0.282686
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.979420 loss:        0.061242
Test - acc:         0.928900 loss:        0.253823
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.980200 loss:        0.058958
Test - acc:         0.927300 loss:        0.262721
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.980880 loss:        0.058011
Test - acc:         0.927700 loss:        0.262517
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.980600 loss:        0.060014
Test - acc:         0.927700 loss:        0.249714
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.981380 loss:        0.055089
Test - acc:         0.922500 loss:        0.276656
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.979520 loss:        0.059650
Test - acc:         0.928000 loss:        0.251079
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.979300 loss:        0.060201
Test - acc:         0.921300 loss:        0.284429
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.978260 loss:        0.063469
Test - acc:         0.924400 loss:        0.264196
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.979720 loss:        0.059221
Test - acc:         0.929000 loss:        0.253874
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.980440 loss:        0.057168
Test - acc:         0.923000 loss:        0.273013
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.978520 loss:        0.061958
Test - acc:         0.926200 loss:        0.264328
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.979440 loss:        0.060337
Test - acc:         0.924400 loss:        0.263338
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.981320 loss:        0.055656
Test - acc:         0.926900 loss:        0.249794
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.981280 loss:        0.055751
Test - acc:         0.920900 loss:        0.291122
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.981360 loss:        0.056216
Test - acc:         0.922800 loss:        0.272662
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.981500 loss:        0.055026
Test - acc:         0.914400 loss:        0.306710
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.980680 loss:        0.057453
Test - acc:         0.925900 loss:        0.258365
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.979740 loss:        0.060220
Test - acc:         0.917200 loss:        0.301281
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.981240 loss:        0.058003
Test - acc:         0.918700 loss:        0.274536
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.979880 loss:        0.058510
Test - acc:         0.920800 loss:        0.270286
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.979860 loss:        0.059976
Test - acc:         0.915300 loss:        0.303997
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.979640 loss:        0.060550
Test - acc:         0.921200 loss:        0.269021
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.981180 loss:        0.057345
Test - acc:         0.919400 loss:        0.280204
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.981280 loss:        0.056774
Test - acc:         0.925800 loss:        0.252620
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.981240 loss:        0.056028
Test - acc:         0.923500 loss:        0.264988
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.981720 loss:        0.055465
Test - acc:         0.923400 loss:        0.279971
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.981220 loss:        0.055632
Test - acc:         0.919600 loss:        0.280531
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.979980 loss:        0.058646
Test - acc:         0.918800 loss:        0.276222
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.981580 loss:        0.055740
Test - acc:         0.927800 loss:        0.254396
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.983000 loss:        0.052307
Test - acc:         0.924000 loss:        0.277206
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.058983
Test - acc:         0.920300 loss:        0.287975
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.981940 loss:        0.054182
Test - acc:         0.922200 loss:        0.285674
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.980300 loss:        0.057146
Test - acc:         0.921800 loss:        0.289310
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.979760 loss:        0.060673
Test - acc:         0.924000 loss:        0.267868
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.974100 loss:        0.080350
Test - acc:         0.931400 loss:        0.229563
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.981820 loss:        0.057361
Test - acc:         0.934100 loss:        0.221780
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.984400 loss:        0.051605
Test - acc:         0.934500 loss:        0.217645
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.985480 loss:        0.047153
Test - acc:         0.932900 loss:        0.218698
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.986800 loss:        0.043283
Test - acc:         0.935100 loss:        0.219999
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.988220 loss:        0.040374
Test - acc:         0.935900 loss:        0.219374
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.988400 loss:        0.038650
Test - acc:         0.935600 loss:        0.217804
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.989380 loss:        0.036652
Test - acc:         0.935700 loss:        0.217564
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.990060 loss:        0.034324
Test - acc:         0.936300 loss:        0.219597
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.990940 loss:        0.032009
Test - acc:         0.936400 loss:        0.219920
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.990300 loss:        0.033450
Test - acc:         0.936200 loss:        0.217169
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.991400 loss:        0.031292
Test - acc:         0.935800 loss:        0.218444
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.991680 loss:        0.030281
Test - acc:         0.936000 loss:        0.221764
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.992380 loss:        0.028905
Test - acc:         0.936400 loss:        0.223082
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.992420 loss:        0.027921
Test - acc:         0.936700 loss:        0.221677
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.992280 loss:        0.027804
Test - acc:         0.937000 loss:        0.224056
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.993060 loss:        0.026551
Test - acc:         0.935000 loss:        0.224242
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.992800 loss:        0.026173
Test - acc:         0.936800 loss:        0.222438
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.993580 loss:        0.025318
Test - acc:         0.936700 loss:        0.221177
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.993520 loss:        0.024937
Test - acc:         0.935700 loss:        0.221056
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.993760 loss:        0.023502
Test - acc:         0.937700 loss:        0.224612
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.993940 loss:        0.024468
Test - acc:         0.936300 loss:        0.226151
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.994540 loss:        0.021798
Test - acc:         0.935800 loss:        0.226244
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.994260 loss:        0.022571
Test - acc:         0.937500 loss:        0.225710
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.994840 loss:        0.022178
Test - acc:         0.937800 loss:        0.226087
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.994780 loss:        0.021194
Test - acc:         0.937500 loss:        0.224810
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.994880 loss:        0.020775
Test - acc:         0.936200 loss:        0.229058
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.994780 loss:        0.021550
Test - acc:         0.938300 loss:        0.228053
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.994940 loss:        0.020699
Test - acc:         0.936800 loss:        0.231074
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.995200 loss:        0.019778
Test - acc:         0.936100 loss:        0.229102
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.994840 loss:        0.019957
Test - acc:         0.937800 loss:        0.230845
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.995240 loss:        0.019422
Test - acc:         0.936300 loss:        0.232157
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.995320 loss:        0.018914
Test - acc:         0.938000 loss:        0.230178
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.995340 loss:        0.018929
Test - acc:         0.937900 loss:        0.228816
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.995580 loss:        0.018257
Test - acc:         0.937700 loss:        0.230383
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.995520 loss:        0.018510
Test - acc:         0.937200 loss:        0.231567
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.995880 loss:        0.017328
Test - acc:         0.937300 loss:        0.232689
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.996240 loss:        0.016934
Test - acc:         0.937700 loss:        0.231299
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.995820 loss:        0.017378
Test - acc:         0.937400 loss:        0.232232
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.995540 loss:        0.017658
Test - acc:         0.936400 loss:        0.231678
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.995800 loss:        0.017362
Test - acc:         0.938400 loss:        0.232761
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.995980 loss:        0.016376
Test - acc:         0.937000 loss:        0.231132
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.996900 loss:        0.015237
Test - acc:         0.936300 loss:        0.233081
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.995960 loss:        0.016720
Test - acc:         0.938200 loss:        0.233287
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.995680 loss:        0.017739
Test - acc:         0.936400 loss:        0.234909
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.996520 loss:        0.015945
Test - acc:         0.937600 loss:        0.234169
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.996060 loss:        0.016140
Test - acc:         0.936100 loss:        0.236177
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.996360 loss:        0.015399
Test - acc:         0.936800 loss:        0.235438
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.995960 loss:        0.016196
Test - acc:         0.937000 loss:        0.234576
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.997060 loss:        0.014587
Test - acc:         0.935400 loss:        0.236489
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.935880 loss:        0.188228
Test - acc:         0.913700 loss:        0.270453
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.952340 loss:        0.137552
Test - acc:         0.917500 loss:        0.260556
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.957380 loss:        0.123332
Test - acc:         0.920600 loss:        0.250330
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.960680 loss:        0.115536
Test - acc:         0.922500 loss:        0.246387
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.963400 loss:        0.108708
Test - acc:         0.925100 loss:        0.239021
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.965140 loss:        0.101419
Test - acc:         0.923900 loss:        0.244204
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.966020 loss:        0.099172
Test - acc:         0.924700 loss:        0.237369
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.968160 loss:        0.094893
Test - acc:         0.926000 loss:        0.233950
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.970140 loss:        0.089418
Test - acc:         0.927100 loss:        0.238324
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.970000 loss:        0.088912
Test - acc:         0.927100 loss:        0.237102
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.971380 loss:        0.085570
Test - acc:         0.924800 loss:        0.238256
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.972680 loss:        0.082845
Test - acc:         0.927600 loss:        0.237508
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.973660 loss:        0.080901
Test - acc:         0.926600 loss:        0.236419
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.973220 loss:        0.080495
Test - acc:         0.928000 loss:        0.232999
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.973460 loss:        0.079579
Test - acc:         0.925900 loss:        0.237220
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.975200 loss:        0.074951
Test - acc:         0.927400 loss:        0.234348
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.975400 loss:        0.075509
Test - acc:         0.927300 loss:        0.236481
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.975300 loss:        0.074130
Test - acc:         0.926900 loss:        0.237810
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.975400 loss:        0.073766
Test - acc:         0.926500 loss:        0.234848
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.976380 loss:        0.071982
Test - acc:         0.927600 loss:        0.234242
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.977400 loss:        0.069746
Test - acc:         0.926200 loss:        0.235245
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.978300 loss:        0.068404
Test - acc:         0.929700 loss:        0.239019
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.976820 loss:        0.068914
Test - acc:         0.928500 loss:        0.237267
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.978860 loss:        0.064970
Test - acc:         0.928700 loss:        0.236494
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.978300 loss:        0.065848
Test - acc:         0.925900 loss:        0.239150
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.978460 loss:        0.065588
Test - acc:         0.929600 loss:        0.237970
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.978900 loss:        0.063838
Test - acc:         0.928200 loss:        0.239832
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.978740 loss:        0.063118
Test - acc:         0.927500 loss:        0.243418
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.979680 loss:        0.062252
Test - acc:         0.930100 loss:        0.239439
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.979140 loss:        0.062047
Test - acc:         0.928700 loss:        0.244147
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.980300 loss:        0.061490
Test - acc:         0.927600 loss:        0.242304
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.979260 loss:        0.062267
Test - acc:         0.926700 loss:        0.243514
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.980700 loss:        0.060307
Test - acc:         0.929100 loss:        0.242977
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.981520 loss:        0.057962
Test - acc:         0.928500 loss:        0.245306
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.981960 loss:        0.056809
Test - acc:         0.928900 loss:        0.241259
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.981340 loss:        0.058136
Test - acc:         0.927700 loss:        0.242328
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.982100 loss:        0.056439
Test - acc:         0.927300 loss:        0.241645
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.981520 loss:        0.056682
Test - acc:         0.928600 loss:        0.243044
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.981600 loss:        0.057322
Test - acc:         0.927400 loss:        0.242073
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.981700 loss:        0.056400
Test - acc:         0.926000 loss:        0.245363
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.982400 loss:        0.054095
Test - acc:         0.928700 loss:        0.243910
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.983420 loss:        0.052740
Test - acc:         0.928800 loss:        0.239864
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.982160 loss:        0.055285
Test - acc:         0.927400 loss:        0.247637
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.982260 loss:        0.055835
Test - acc:         0.928100 loss:        0.246578
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.983000 loss:        0.052911
Test - acc:         0.928500 loss:        0.243710
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.982540 loss:        0.054427
Test - acc:         0.927800 loss:        0.247209
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.983180 loss:        0.052265
Test - acc:         0.926300 loss:        0.251109
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.983700 loss:        0.051388
Test - acc:         0.929900 loss:        0.251826
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.982540 loss:        0.053148
Test - acc:         0.926800 loss:        0.249790
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.984020 loss:        0.051191
Test - acc:         0.928400 loss:        0.249062
Sparsity :          0.9844
Wdecay :        0.000500
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "topflip",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "logdir": "topflip_ablations",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": true,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_topflip_pf39_s42_noise_only_prunable",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.318220 loss:        1.985412
Test - acc:         0.429200 loss:        1.525093
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.499620 loss:        1.373114
Test - acc:         0.534800 loss:        1.265797
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.612200 loss:        1.089528
Test - acc:         0.653400 loss:        0.964139
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.677780 loss:        0.906348
Test - acc:         0.666400 loss:        0.965327
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.729820 loss:        0.776061
Test - acc:         0.652200 loss:        1.036147
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.764740 loss:        0.675035
Test - acc:         0.761600 loss:        0.693125
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.789760 loss:        0.608187
Test - acc:         0.779900 loss:        0.629873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.806000 loss:        0.564707
Test - acc:         0.744900 loss:        0.750463
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.812960 loss:        0.540295
Test - acc:         0.752900 loss:        0.729536
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.823800 loss:        0.513245
Test - acc:         0.751700 loss:        0.748913
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.830140 loss:        0.492257
Test - acc:         0.804100 loss:        0.595031
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.837740 loss:        0.471075
Test - acc:         0.741400 loss:        0.860206
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.842220 loss:        0.461984
Test - acc:         0.786800 loss:        0.653712
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.842520 loss:        0.461559
Test - acc:         0.795700 loss:        0.594977
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.845480 loss:        0.448581
Test - acc:         0.815600 loss:        0.571001
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.850840 loss:        0.433909
Test - acc:         0.796800 loss:        0.609109
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.853580 loss:        0.426681
Test - acc:         0.809100 loss:        0.550305
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.858000 loss:        0.419201
Test - acc:         0.767200 loss:        0.747714
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.859720 loss:        0.411343
Test - acc:         0.788600 loss:        0.619552
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.857900 loss:        0.408749
Test - acc:         0.795800 loss:        0.706319
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.862800 loss:        0.401139
Test - acc:         0.807700 loss:        0.576402
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.862940 loss:        0.397437
Test - acc:         0.788600 loss:        0.643756
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.866600 loss:        0.394531
Test - acc:         0.793400 loss:        0.640682
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.866780 loss:        0.387493
Test - acc:         0.816300 loss:        0.570344
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.869900 loss:        0.382501
Test - acc:         0.846400 loss:        0.465978
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.867140 loss:        0.386222
Test - acc:         0.772700 loss:        0.679029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.869620 loss:        0.382511
Test - acc:         0.828200 loss:        0.528247
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.868500 loss:        0.385333
Test - acc:         0.852900 loss:        0.431600
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.872400 loss:        0.372402
Test - acc:         0.816700 loss:        0.584232
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.870760 loss:        0.378113
Test - acc:         0.839300 loss:        0.486770
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.871560 loss:        0.374331
Test - acc:         0.849400 loss:        0.454670
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.871420 loss:        0.375747
Test - acc:         0.818200 loss:        0.551412
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.876200 loss:        0.366422
Test - acc:         0.840600 loss:        0.476191
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.873680 loss:        0.369777
Test - acc:         0.844100 loss:        0.462965
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.874180 loss:        0.365100
Test - acc:         0.827700 loss:        0.540672
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.877380 loss:        0.364332
Test - acc:         0.830500 loss:        0.533784
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.873280 loss:        0.369054
Test - acc:         0.838600 loss:        0.502545
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.874080 loss:        0.365253
Test - acc:         0.828800 loss:        0.542968
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.877160 loss:        0.363019
Test - acc:         0.832200 loss:        0.506629
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.881620 loss:        0.342563
Test - acc:         0.832800 loss:        0.498064
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.883960 loss:        0.341744
Test - acc:         0.804900 loss:        0.625522
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.887460 loss:        0.330490
Test - acc:         0.847900 loss:        0.450716
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.888660 loss:        0.325977
Test - acc:         0.864100 loss:        0.391630
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.885920 loss:        0.327149
Test - acc:         0.851000 loss:        0.447668
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.888480 loss:        0.325988
Test - acc:         0.842100 loss:        0.473583
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.888140 loss:        0.326734
Test - acc:         0.864100 loss:        0.416097
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.887520 loss:        0.327103
Test - acc:         0.837300 loss:        0.499138
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.887460 loss:        0.328620
Test - acc:         0.807100 loss:        0.575885
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.889600 loss:        0.323916
Test - acc:         0.791000 loss:        0.649963
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.889360 loss:        0.322434
Test - acc:         0.858000 loss:        0.410643
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.891320 loss:        0.317454
Test - acc:         0.817100 loss:        0.591650
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.890280 loss:        0.320679
Test - acc:         0.844300 loss:        0.458790
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.893000 loss:        0.314860
Test - acc:         0.839400 loss:        0.510985
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.889740 loss:        0.318185
Test - acc:         0.836400 loss:        0.506206
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.890440 loss:        0.319381
Test - acc:         0.847300 loss:        0.464286
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.890680 loss:        0.319132
Test - acc:         0.847200 loss:        0.453904
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.890100 loss:        0.321288
Test - acc:         0.844200 loss:        0.471779
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.888860 loss:        0.321702
Test - acc:         0.848000 loss:        0.461497
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.891360 loss:        0.318994
Test - acc:         0.844200 loss:        0.499027
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.892580 loss:        0.315715
Test - acc:         0.825300 loss:        0.526108
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.891820 loss:        0.316826
Test - acc:         0.847900 loss:        0.468448
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.889780 loss:        0.321693
Test - acc:         0.830700 loss:        0.548508
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.891180 loss:        0.317589
Test - acc:         0.827600 loss:        0.512497
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.891820 loss:        0.315907
Test - acc:         0.826600 loss:        0.548024
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.891100 loss:        0.319366
Test - acc:         0.824000 loss:        0.541091
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.892320 loss:        0.318295
Test - acc:         0.854400 loss:        0.444075
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.890980 loss:        0.316893
Test - acc:         0.841300 loss:        0.476348
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.890660 loss:        0.319477
Test - acc:         0.824500 loss:        0.539095
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.892580 loss:        0.314296
Test - acc:         0.843200 loss:        0.484507
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.890620 loss:        0.318546
Test - acc:         0.821000 loss:        0.539235
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.891420 loss:        0.318448
Test - acc:         0.852800 loss:        0.448881
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.892540 loss:        0.316525
Test - acc:         0.853000 loss:        0.443012
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.891960 loss:        0.314492
Test - acc:         0.843700 loss:        0.490436
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.891660 loss:        0.320471
Test - acc:         0.847900 loss:        0.461645
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.893420 loss:        0.313174
Test - acc:         0.834000 loss:        0.518331
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.894320 loss:        0.315867
Test - acc:         0.854500 loss:        0.434282
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.890920 loss:        0.318534
Test - acc:         0.853900 loss:        0.442021
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.892720 loss:        0.313593
Test - acc:         0.834900 loss:        0.487146
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.904860 loss:        0.275622
Test - acc:         0.850800 loss:        0.455088
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.905360 loss:        0.276886
Test - acc:         0.859800 loss:        0.424061
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.902880 loss:        0.283101
Test - acc:         0.845100 loss:        0.477353
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.902740 loss:        0.282256
Test - acc:         0.867900 loss:        0.398175
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.904720 loss:        0.278914
Test - acc:         0.858000 loss:        0.438659
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.904700 loss:        0.277611
Test - acc:         0.846400 loss:        0.480723
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.904820 loss:        0.278428
Test - acc:         0.858700 loss:        0.429291
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.901540 loss:        0.282041
Test - acc:         0.872100 loss:        0.375147
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.906200 loss:        0.272295
Test - acc:         0.870300 loss:        0.392145
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.904520 loss:        0.277038
Test - acc:         0.860400 loss:        0.414878
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.904820 loss:        0.279845
Test - acc:         0.859700 loss:        0.433701
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.905760 loss:        0.274287
Test - acc:         0.855800 loss:        0.437048
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.903980 loss:        0.278818
Test - acc:         0.861200 loss:        0.424390
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.905200 loss:        0.274599
Test - acc:         0.871800 loss:        0.399518
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.906220 loss:        0.274759
Test - acc:         0.850700 loss:        0.458875
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.903840 loss:        0.280669
Test - acc:         0.838400 loss:        0.513515
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.905720 loss:        0.273610
Test - acc:         0.863000 loss:        0.421251
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.906020 loss:        0.275208
Test - acc:         0.848200 loss:        0.485993
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.904560 loss:        0.277061
Test - acc:         0.868400 loss:        0.398299
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.906120 loss:        0.272732
Test - acc:         0.866100 loss:        0.399821
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.904220 loss:        0.276766
Test - acc:         0.850700 loss:        0.465093
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.903560 loss:        0.278085
Test - acc:         0.867000 loss:        0.404591
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.904940 loss:        0.275081
Test - acc:         0.864800 loss:        0.413011
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.905360 loss:        0.274605
Test - acc:         0.862200 loss:        0.422024
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.906740 loss:        0.275895
Test - acc:         0.851600 loss:        0.452159
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.903620 loss:        0.277615
Test - acc:         0.870300 loss:        0.401890
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.905060 loss:        0.273472
Test - acc:         0.870000 loss:        0.402636
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.906500 loss:        0.277713
Test - acc:         0.869100 loss:        0.399107
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.906660 loss:        0.271536
Test - acc:         0.857900 loss:        0.443579
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.906020 loss:        0.273472
Test - acc:         0.852800 loss:        0.466668
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.905800 loss:        0.274280
Test - acc:         0.849000 loss:        0.458835
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.903560 loss:        0.278108
Test - acc:         0.884300 loss:        0.342579
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.906940 loss:        0.271727
Test - acc:         0.870100 loss:        0.390035
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.906660 loss:        0.272778
Test - acc:         0.864700 loss:        0.417057
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.904680 loss:        0.275765
Test - acc:         0.865200 loss:        0.413772
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.905460 loss:        0.275056
Test - acc:         0.859400 loss:        0.430187
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.906940 loss:        0.273362
Test - acc:         0.869100 loss:        0.394639
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.905020 loss:        0.278839
Test - acc:         0.852300 loss:        0.462068
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.905980 loss:        0.273505
Test - acc:         0.877700 loss:        0.369539
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.913140 loss:        0.252530
Test - acc:         0.830600 loss:        0.544098
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.913840 loss:        0.252049
Test - acc:         0.847900 loss:        0.470344
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.911920 loss:        0.255890
Test - acc:         0.841200 loss:        0.506651
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.914280 loss:        0.250127
Test - acc:         0.846500 loss:        0.496183
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.912300 loss:        0.256158
Test - acc:         0.872800 loss:        0.380626
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.914620 loss:        0.248628
Test - acc:         0.875000 loss:        0.374149
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.912900 loss:        0.251245
Test - acc:         0.854900 loss:        0.455899
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.911840 loss:        0.253474
Test - acc:         0.880600 loss:        0.352563
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.914520 loss:        0.249135
Test - acc:         0.878700 loss:        0.358883
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.912560 loss:        0.251363
Test - acc:         0.833900 loss:        0.517308
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.912860 loss:        0.248648
Test - acc:         0.857600 loss:        0.455402
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.912740 loss:        0.250369
Test - acc:         0.875400 loss:        0.393031
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.914300 loss:        0.250502
Test - acc:         0.872600 loss:        0.388866
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.914560 loss:        0.246817
Test - acc:         0.873300 loss:        0.396751
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.912300 loss:        0.252137
Test - acc:         0.876800 loss:        0.364367
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.913980 loss:        0.246397
Test - acc:         0.796900 loss:        0.699307
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.914240 loss:        0.249188
Test - acc:         0.838200 loss:        0.529101
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.914560 loss:        0.249685
Test - acc:         0.877600 loss:        0.371246
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.914820 loss:        0.247559
Test - acc:         0.878300 loss:        0.368160
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.914300 loss:        0.249354
Test - acc:         0.853600 loss:        0.462132
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.914520 loss:        0.249869
Test - acc:         0.810700 loss:        0.593153
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.913440 loss:        0.249856
Test - acc:         0.863200 loss:        0.424865
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.913380 loss:        0.252360
Test - acc:         0.840000 loss:        0.504239
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.912740 loss:        0.252568
Test - acc:         0.877400 loss:        0.387587
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.912260 loss:        0.252707
Test - acc:         0.882500 loss:        0.368619
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.914000 loss:        0.246939
Test - acc:         0.865600 loss:        0.409112
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.915180 loss:        0.246649
Test - acc:         0.843200 loss:        0.489389
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.913020 loss:        0.250282
Test - acc:         0.853100 loss:        0.465269
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.914800 loss:        0.247727
Test - acc:         0.869600 loss:        0.413530
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.913720 loss:        0.247554
Test - acc:         0.883500 loss:        0.359170
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.913560 loss:        0.250736
Test - acc:         0.865700 loss:        0.413774
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.914560 loss:        0.245645
Test - acc:         0.869700 loss:        0.405633
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.914000 loss:        0.247108
Test - acc:         0.871900 loss:        0.396397
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.950820 loss:        0.146597
Test - acc:         0.928800 loss:        0.213676
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.962780 loss:        0.112973
Test - acc:         0.929900 loss:        0.210483
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.966160 loss:        0.099842
Test - acc:         0.930700 loss:        0.205260
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.971100 loss:        0.088121
Test - acc:         0.932600 loss:        0.206112
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.972480 loss:        0.082868
Test - acc:         0.934000 loss:        0.206766
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.973900 loss:        0.075717
Test - acc:         0.934300 loss:        0.209870
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.969960 loss:        0.088501
Test - acc:         0.932600 loss:        0.205686
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.973660 loss:        0.079216
Test - acc:         0.932500 loss:        0.201410
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.974180 loss:        0.075871
Test - acc:         0.931200 loss:        0.211549
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.976680 loss:        0.069801
Test - acc:         0.934600 loss:        0.210987
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.976880 loss:        0.068646
Test - acc:         0.933500 loss:        0.209946
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.978440 loss:        0.065277
Test - acc:         0.932700 loss:        0.217022
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.979980 loss:        0.059978
Test - acc:         0.933200 loss:        0.212608
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.979940 loss:        0.059313
Test - acc:         0.931100 loss:        0.215157
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.981360 loss:        0.055995
Test - acc:         0.934400 loss:        0.205725
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.981840 loss:        0.054864
Test - acc:         0.932800 loss:        0.223029
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.982640 loss:        0.053884
Test - acc:         0.932500 loss:        0.224620
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.982320 loss:        0.052594
Test - acc:         0.932600 loss:        0.223637
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.983260 loss:        0.049856
Test - acc:         0.933600 loss:        0.229845
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.982100 loss:        0.052919
Test - acc:         0.928600 loss:        0.241227
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.984000 loss:        0.049521
Test - acc:         0.933000 loss:        0.232862
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.984200 loss:        0.048905
Test - acc:         0.932400 loss:        0.232918
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.983440 loss:        0.049654
Test - acc:         0.928800 loss:        0.244224
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.981720 loss:        0.052556
Test - acc:         0.931900 loss:        0.232587
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.984020 loss:        0.048617
Test - acc:         0.931400 loss:        0.242129
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.983420 loss:        0.048632
Test - acc:         0.931000 loss:        0.244183
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.984040 loss:        0.049605
Test - acc:         0.927000 loss:        0.243980
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.982680 loss:        0.053020
Test - acc:         0.928500 loss:        0.242216
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.982020 loss:        0.052493
Test - acc:         0.931100 loss:        0.243194
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.984260 loss:        0.048998
Test - acc:         0.929100 loss:        0.238224
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.982300 loss:        0.051117
Test - acc:         0.926500 loss:        0.252497
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.982840 loss:        0.052039
Test - acc:         0.924500 loss:        0.262798
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.981340 loss:        0.055883
Test - acc:         0.925500 loss:        0.259241
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.983080 loss:        0.051453
Test - acc:         0.924000 loss:        0.263218
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.982020 loss:        0.053792
Test - acc:         0.931300 loss:        0.238843
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.981600 loss:        0.053397
Test - acc:         0.924900 loss:        0.260181
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.981080 loss:        0.056556
Test - acc:         0.922700 loss:        0.260505
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.980600 loss:        0.056028
Test - acc:         0.925200 loss:        0.244660
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.981920 loss:        0.054475
Test - acc:         0.930400 loss:        0.233710
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.980560 loss:        0.057238
Test - acc:         0.926900 loss:        0.258453
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.981440 loss:        0.055012
Test - acc:         0.925000 loss:        0.264554
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.981920 loss:        0.053159
Test - acc:         0.926300 loss:        0.256931
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.980200 loss:        0.058092
Test - acc:         0.928200 loss:        0.252900
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.980800 loss:        0.056578
Test - acc:         0.929500 loss:        0.247676
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.979460 loss:        0.060681
Test - acc:         0.921100 loss:        0.271590
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.960760 loss:        0.113013
Test - acc:         0.916000 loss:        0.264132
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.966820 loss:        0.097929
Test - acc:         0.920100 loss:        0.256952
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.967560 loss:        0.093767
Test - acc:         0.916500 loss:        0.266352
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.967240 loss:        0.093741
Test - acc:         0.912400 loss:        0.284256
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.969440 loss:        0.089587
Test - acc:         0.924100 loss:        0.256112
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.970560 loss:        0.084871
Test - acc:         0.920100 loss:        0.258296
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.971620 loss:        0.083332
Test - acc:         0.920000 loss:        0.275613
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.971080 loss:        0.082259
Test - acc:         0.917100 loss:        0.278639
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.971420 loss:        0.082672
Test - acc:         0.916100 loss:        0.264530
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.971760 loss:        0.082559
Test - acc:         0.921200 loss:        0.251750
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.973100 loss:        0.079680
Test - acc:         0.917000 loss:        0.269981
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.971640 loss:        0.083415
Test - acc:         0.920000 loss:        0.266101
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.972420 loss:        0.080455
Test - acc:         0.915300 loss:        0.293163
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.971540 loss:        0.081014
Test - acc:         0.921600 loss:        0.260188
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.973580 loss:        0.077513
Test - acc:         0.918400 loss:        0.279532
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.972160 loss:        0.079921
Test - acc:         0.924400 loss:        0.256929
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.974840 loss:        0.076478
Test - acc:         0.926700 loss:        0.255510
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.971700 loss:        0.079547
Test - acc:         0.912300 loss:        0.293541
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.973520 loss:        0.075613
Test - acc:         0.916800 loss:        0.287215
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.973360 loss:        0.078344
Test - acc:         0.920600 loss:        0.264079
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.973060 loss:        0.077504
Test - acc:         0.917000 loss:        0.283197
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.974360 loss:        0.075620
Test - acc:         0.921600 loss:        0.263010
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.974300 loss:        0.075632
Test - acc:         0.912400 loss:        0.290153
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.974460 loss:        0.076014
Test - acc:         0.921100 loss:        0.266407
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.975120 loss:        0.072476
Test - acc:         0.922600 loss:        0.269098
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.975320 loss:        0.071425
Test - acc:         0.921100 loss:        0.268162
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.974140 loss:        0.074943
Test - acc:         0.923000 loss:        0.261802
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.973520 loss:        0.075892
Test - acc:         0.919700 loss:        0.272890
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.974440 loss:        0.073915
Test - acc:         0.922900 loss:        0.266346
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.976900 loss:        0.069531
Test - acc:         0.922700 loss:        0.266645
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.976080 loss:        0.070350
Test - acc:         0.921100 loss:        0.262909
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.973520 loss:        0.075227
Test - acc:         0.918300 loss:        0.280855
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.973140 loss:        0.076136
Test - acc:         0.918600 loss:        0.273520
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.976080 loss:        0.069798
Test - acc:         0.920100 loss:        0.268625
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.975100 loss:        0.073655
Test - acc:         0.912800 loss:        0.292001
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.975040 loss:        0.073118
Test - acc:         0.922900 loss:        0.277343
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.976360 loss:        0.069113
Test - acc:         0.915800 loss:        0.287630
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.975700 loss:        0.070589
Test - acc:         0.917100 loss:        0.281070
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.973940 loss:        0.076473
Test - acc:         0.923600 loss:        0.257460
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.934020 loss:        0.190034
Test - acc:         0.907400 loss:        0.294523
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.943760 loss:        0.161028
Test - acc:         0.900800 loss:        0.306033
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.949640 loss:        0.147435
Test - acc:         0.909300 loss:        0.294605
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.951220 loss:        0.141967
Test - acc:         0.913300 loss:        0.270681
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.952080 loss:        0.136229
Test - acc:         0.909500 loss:        0.287727
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.953040 loss:        0.134158
Test - acc:         0.912700 loss:        0.274653
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.953720 loss:        0.131419
Test - acc:         0.910500 loss:        0.280032
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.954900 loss:        0.129622
Test - acc:         0.919600 loss:        0.251796
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.955620 loss:        0.127007
Test - acc:         0.914000 loss:        0.261299
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.957340 loss:        0.123227
Test - acc:         0.917700 loss:        0.259522
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.957640 loss:        0.120192
Test - acc:         0.917100 loss:        0.253041
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.956940 loss:        0.120288
Test - acc:         0.918500 loss:        0.255857
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.956840 loss:        0.123240
Test - acc:         0.917300 loss:        0.262395
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.959740 loss:        0.117393
Test - acc:         0.915900 loss:        0.262615
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.959080 loss:        0.118647
Test - acc:         0.913200 loss:        0.271525
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.958240 loss:        0.119139
Test - acc:         0.915400 loss:        0.268388
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.969140 loss:        0.090376
Test - acc:         0.929900 loss:        0.219936
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.976320 loss:        0.073789
Test - acc:         0.930900 loss:        0.218064
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.978280 loss:        0.068875
Test - acc:         0.931100 loss:        0.217248
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.978880 loss:        0.066604
Test - acc:         0.931800 loss:        0.216554
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.978680 loss:        0.066653
Test - acc:         0.931400 loss:        0.216788
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.980300 loss:        0.063129
Test - acc:         0.931400 loss:        0.217237
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.981680 loss:        0.060085
Test - acc:         0.932100 loss:        0.216130
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.981440 loss:        0.060004
Test - acc:         0.931500 loss:        0.216195
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.982180 loss:        0.057780
Test - acc:         0.931200 loss:        0.219382
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.982420 loss:        0.056288
Test - acc:         0.932200 loss:        0.218022
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.981740 loss:        0.056835
Test - acc:         0.933000 loss:        0.216912
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.983480 loss:        0.054703
Test - acc:         0.931200 loss:        0.218636
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.983140 loss:        0.054932
Test - acc:         0.932200 loss:        0.219560
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.983360 loss:        0.053598
Test - acc:         0.931200 loss:        0.221415
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.984220 loss:        0.052004
Test - acc:         0.930900 loss:        0.220989
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.983840 loss:        0.053375
Test - acc:         0.932900 loss:        0.221460
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.983840 loss:        0.051280
Test - acc:         0.929100 loss:        0.222551
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.983920 loss:        0.050969
Test - acc:         0.931800 loss:        0.221643
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.983980 loss:        0.050135
Test - acc:         0.929800 loss:        0.223710
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.985540 loss:        0.047189
Test - acc:         0.931200 loss:        0.221035
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.985360 loss:        0.046819
Test - acc:         0.930400 loss:        0.224580
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.985600 loss:        0.047397
Test - acc:         0.931300 loss:        0.223709
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.986520 loss:        0.045649
Test - acc:         0.930800 loss:        0.226636
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.875440 loss:        0.357347
Test - acc:         0.880100 loss:        0.362339
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.906200 loss:        0.268129
Test - acc:         0.893400 loss:        0.332317
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.915020 loss:        0.244557
Test - acc:         0.898000 loss:        0.316468
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.920200 loss:        0.228748
Test - acc:         0.898600 loss:        0.312882
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.923020 loss:        0.220940
Test - acc:         0.901400 loss:        0.305729
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.926900 loss:        0.208892
Test - acc:         0.901400 loss:        0.300699
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.928120 loss:        0.206134
Test - acc:         0.905200 loss:        0.294568
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.929980 loss:        0.201255
Test - acc:         0.903800 loss:        0.295636
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.932320 loss:        0.193180
Test - acc:         0.904300 loss:        0.289552
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.933540 loss:        0.190378
Test - acc:         0.905200 loss:        0.286322
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.935580 loss:        0.185395
Test - acc:         0.903600 loss:        0.289518
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.936080 loss:        0.184553
Test - acc:         0.904600 loss:        0.282839
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.936040 loss:        0.182423
Test - acc:         0.903300 loss:        0.285311
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.939400 loss:        0.175491
Test - acc:         0.906100 loss:        0.282389
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.939180 loss:        0.175181
Test - acc:         0.906300 loss:        0.283153
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.940900 loss:        0.170291
Test - acc:         0.905600 loss:        0.286957
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.941260 loss:        0.170923
Test - acc:         0.906200 loss:        0.283350
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.942820 loss:        0.167730
Test - acc:         0.904900 loss:        0.286321
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.941180 loss:        0.168297
Test - acc:         0.908100 loss:        0.279712
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.943960 loss:        0.164158
Test - acc:         0.906500 loss:        0.281125
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.943780 loss:        0.161871
Test - acc:         0.907600 loss:        0.281045
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.944320 loss:        0.161666
Test - acc:         0.909200 loss:        0.279521
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.944540 loss:        0.160825
Test - acc:         0.907000 loss:        0.281594
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.944580 loss:        0.158231
Test - acc:         0.907500 loss:        0.282590
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.945280 loss:        0.158354
Test - acc:         0.910800 loss:        0.276913
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.945400 loss:        0.159056
Test - acc:         0.908200 loss:        0.282495
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.946240 loss:        0.156113
Test - acc:         0.909800 loss:        0.277971
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.947720 loss:        0.151501
Test - acc:         0.908000 loss:        0.283775
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.947280 loss:        0.152934
Test - acc:         0.911800 loss:        0.276790
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.948800 loss:        0.149727
Test - acc:         0.909800 loss:        0.279794
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.948280 loss:        0.150964
Test - acc:         0.910100 loss:        0.279991
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.947860 loss:        0.149516
Test - acc:         0.907100 loss:        0.283850
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.948560 loss:        0.148767
Test - acc:         0.906900 loss:        0.283603
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.948900 loss:        0.147728
Test - acc:         0.909600 loss:        0.276600
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.948260 loss:        0.149290
Test - acc:         0.909000 loss:        0.280928
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.950680 loss:        0.143411
Test - acc:         0.911100 loss:        0.279993
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.949760 loss:        0.144624
Test - acc:         0.909300 loss:        0.281118
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.949580 loss:        0.144757
Test - acc:         0.908700 loss:        0.280570
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.950580 loss:        0.142606
Test - acc:         0.909400 loss:        0.278894
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.737120 loss:        0.757604
Test - acc:         0.798500 loss:        0.594609
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.805100 loss:        0.560179
Test - acc:         0.823800 loss:        0.524506
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.822800 loss:        0.517407
Test - acc:         0.826700 loss:        0.506040
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.829160 loss:        0.487375
Test - acc:         0.834500 loss:        0.483879
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.838560 loss:        0.468681
Test - acc:         0.839100 loss:        0.474199
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.843420 loss:        0.453103
Test - acc:         0.839800 loss:        0.470517
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.847700 loss:        0.443025
Test - acc:         0.843000 loss:        0.460183
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.851360 loss:        0.434903
Test - acc:         0.846000 loss:        0.452506
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.853400 loss:        0.426271
Test - acc:         0.846400 loss:        0.448777
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.855760 loss:        0.419640
Test - acc:         0.849700 loss:        0.440086
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.857200 loss:        0.411436
Test - acc:         0.848600 loss:        0.440070
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.860680 loss:        0.403005
Test - acc:         0.850900 loss:        0.441696
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.861420 loss:        0.401936
Test - acc:         0.855100 loss:        0.433066
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.862660 loss:        0.399439
Test - acc:         0.854600 loss:        0.431380
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.865080 loss:        0.392739
Test - acc:         0.856300 loss:        0.423619
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.865080 loss:        0.388110
Test - acc:         0.853400 loss:        0.431947
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.867720 loss:        0.386662
Test - acc:         0.852400 loss:        0.422963
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.866360 loss:        0.385351
Test - acc:         0.856200 loss:        0.422394
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.867720 loss:        0.382417
Test - acc:         0.855300 loss:        0.416334
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.870460 loss:        0.377839
Test - acc:         0.855700 loss:        0.421407
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.870120 loss:        0.376127
Test - acc:         0.856300 loss:        0.422733
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.873260 loss:        0.371357
Test - acc:         0.859600 loss:        0.411566
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.872240 loss:        0.368500
Test - acc:         0.861700 loss:        0.411798
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.874480 loss:        0.364993
Test - acc:         0.861300 loss:        0.412078
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.872220 loss:        0.368492
Test - acc:         0.860800 loss:        0.408896
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.874440 loss:        0.365534
Test - acc:         0.862300 loss:        0.406269
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.873200 loss:        0.364067
Test - acc:         0.863100 loss:        0.405787
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.875420 loss:        0.362816
Test - acc:         0.861300 loss:        0.406160
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.876920 loss:        0.357561
Test - acc:         0.862600 loss:        0.407156
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.876160 loss:        0.356863
Test - acc:         0.860900 loss:        0.403518
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.875720 loss:        0.358325
Test - acc:         0.859500 loss:        0.412093
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.876760 loss:        0.355605
Test - acc:         0.864700 loss:        0.398695
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.877540 loss:        0.357356
Test - acc:         0.863100 loss:        0.401430
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.877720 loss:        0.352574
Test - acc:         0.863700 loss:        0.404973
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.878200 loss:        0.349883
Test - acc:         0.858500 loss:        0.402615
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.879740 loss:        0.350685
Test - acc:         0.863400 loss:        0.398263
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.879160 loss:        0.348693
Test - acc:         0.860700 loss:        0.403560
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.878640 loss:        0.348815
Test - acc:         0.862900 loss:        0.400819
Sparsity :          0.9961
Wdecay :        0.000500
