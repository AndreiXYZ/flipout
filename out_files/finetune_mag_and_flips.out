******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 100,
    "lr": 0.001,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "none",
    "prune_freq": 1,
    "prune_rate": 0.2,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "logdir": "test",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": false,
    "milestones": null,
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "criterion_experiment_no_bias/post-finetune/resnet18_topflip_pf50_s42_noise_only_prunable",
    "load_model": "/home/andreia/thesis/chkpts/criterion_experiment_no_bias/pre-finetune/resnet18_topflip_pf50_s42.pt",
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.0010000000000000002
Train - acc:        0.986860 loss:        0.043379
Test - acc:         0.924100 loss:        0.266258
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.0010000000000000002
Train - acc:        0.985620 loss:        0.044777
Test - acc:         0.925700 loss:        0.265612
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.0010000000000000002
Train - acc:        0.986980 loss:        0.043075
Test - acc:         0.925400 loss:        0.263353
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.0010000000000000002
Train - acc:        0.986440 loss:        0.042380
Test - acc:         0.926500 loss:        0.265588
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.0010000000000000002
Train - acc:        0.986820 loss:        0.041418
Test - acc:         0.927000 loss:        0.263112
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.0010000000000000002
Train - acc:        0.987080 loss:        0.041690
Test - acc:         0.926500 loss:        0.266872
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.0010000000000000002
Train - acc:        0.986960 loss:        0.042282
Test - acc:         0.925200 loss:        0.266677
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.0010000000000000002
Train - acc:        0.988200 loss:        0.039799
Test - acc:         0.925500 loss:        0.264851
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.0010000000000000002
Train - acc:        0.987360 loss:        0.040713
Test - acc:         0.924900 loss:        0.265280
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.0010000000000000002
Train - acc:        0.986640 loss:        0.041414
Test - acc:         0.923000 loss:        0.270243
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.0010000000000000002
Train - acc:        0.987740 loss:        0.039167
Test - acc:         0.923700 loss:        0.268529
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.0010000000000000002
Train - acc:        0.987160 loss:        0.039746
Test - acc:         0.924100 loss:        0.269976
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.0010000000000000002
Train - acc:        0.987360 loss:        0.040330
Test - acc:         0.922000 loss:        0.270130
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.0010000000000000002
Train - acc:        0.987660 loss:        0.040265
Test - acc:         0.922100 loss:        0.270473
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.0010000000000000002
Train - acc:        0.988360 loss:        0.039217
Test - acc:         0.921800 loss:        0.278225
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.0010000000000000002
Train - acc:        0.988300 loss:        0.039070
Test - acc:         0.925900 loss:        0.270438
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.0010000000000000002
Train - acc:        0.987700 loss:        0.039595
Test - acc:         0.924300 loss:        0.275083
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.0010000000000000002
Train - acc:        0.987720 loss:        0.039418
Test - acc:         0.924400 loss:        0.272665
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.0010000000000000002
Train - acc:        0.988420 loss:        0.038270
Test - acc:         0.923600 loss:        0.275028
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.0010000000000000002
Train - acc:        0.988100 loss:        0.037393
Test - acc:         0.925400 loss:        0.272596
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.0010000000000000002
Train - acc:        0.988900 loss:        0.037232
Test - acc:         0.923200 loss:        0.270918
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.0010000000000000002
Train - acc:        0.989160 loss:        0.037769
Test - acc:         0.924900 loss:        0.268133
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.0010000000000000002
Train - acc:        0.988540 loss:        0.037271
Test - acc:         0.926100 loss:        0.263960
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.0010000000000000002
Train - acc:        0.988760 loss:        0.037258
Test - acc:         0.923100 loss:        0.270537
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.0010000000000000002
Train - acc:        0.989140 loss:        0.036160
Test - acc:         0.924800 loss:        0.268461
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.0010000000000000002
Train - acc:        0.988920 loss:        0.036526
Test - acc:         0.924500 loss:        0.272940
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.0010000000000000002
Train - acc:        0.988740 loss:        0.036384
Test - acc:         0.925800 loss:        0.266545
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.0010000000000000002
Train - acc:        0.988840 loss:        0.036681
Test - acc:         0.924300 loss:        0.268961
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.0010000000000000002
Train - acc:        0.988860 loss:        0.036385
Test - acc:         0.923300 loss:        0.270081
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.0010000000000000002
Train - acc:        0.989460 loss:        0.035898
Test - acc:         0.925300 loss:        0.270784
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.0010000000000000002
Train - acc:        0.988580 loss:        0.037242
Test - acc:         0.924800 loss:        0.267302
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.0010000000000000002
Train - acc:        0.988380 loss:        0.037731
Test - acc:         0.925600 loss:        0.270515
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.0010000000000000002
Train - acc:        0.988400 loss:        0.036834
Test - acc:         0.923800 loss:        0.271110
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.0010000000000000002
Train - acc:        0.989560 loss:        0.035380
Test - acc:         0.926300 loss:        0.269314
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.0010000000000000002
Train - acc:        0.989700 loss:        0.034936
Test - acc:         0.925400 loss:        0.272727
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.0010000000000000002
Train - acc:        0.989480 loss:        0.034896
Test - acc:         0.924600 loss:        0.273672
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.0010000000000000002
Train - acc:        0.989060 loss:        0.035107
Test - acc:         0.926000 loss:        0.275775
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.0010000000000000002
Train - acc:        0.989140 loss:        0.034864
Test - acc:         0.925300 loss:        0.274326
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.0010000000000000002
Train - acc:        0.988160 loss:        0.036254
Test - acc:         0.923500 loss:        0.274945
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.0010000000000000002
Train - acc:        0.989620 loss:        0.034157
Test - acc:         0.924300 loss:        0.273781
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.0010000000000000002
Train - acc:        0.989540 loss:        0.035032
Test - acc:         0.925100 loss:        0.274088
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.0010000000000000002
Train - acc:        0.989760 loss:        0.033939
Test - acc:         0.925100 loss:        0.276020
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.0010000000000000002
Train - acc:        0.989340 loss:        0.034752
Test - acc:         0.926700 loss:        0.272549
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.0010000000000000002
Train - acc:        0.989300 loss:        0.033630
Test - acc:         0.923500 loss:        0.280747
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.0010000000000000002
Train - acc:        0.988880 loss:        0.036214
Test - acc:         0.924800 loss:        0.275834
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.0010000000000000002
Train - acc:        0.989440 loss:        0.034420
Test - acc:         0.925700 loss:        0.277530
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.0010000000000000002
Train - acc:        0.989760 loss:        0.033415
Test - acc:         0.926500 loss:        0.275686
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.0010000000000000002
Train - acc:        0.989260 loss:        0.034850
Test - acc:         0.922700 loss:        0.281175
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.0010000000000000002
Train - acc:        0.989600 loss:        0.034391
Test - acc:         0.924500 loss:        0.283733
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.0010000000000000002
Train - acc:        0.989700 loss:        0.033184
Test - acc:         0.923700 loss:        0.277696
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.0010000000000000002
Train - acc:        0.989400 loss:        0.034205
Test - acc:         0.925100 loss:        0.278284
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.0010000000000000002
Train - acc:        0.990400 loss:        0.032972
Test - acc:         0.927000 loss:        0.276950
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.0010000000000000002
Train - acc:        0.989420 loss:        0.033646
Test - acc:         0.922900 loss:        0.273770
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.0010000000000000002
Train - acc:        0.990600 loss:        0.032824
Test - acc:         0.926000 loss:        0.275868
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.0010000000000000002
Train - acc:        0.989540 loss:        0.034905
Test - acc:         0.924900 loss:        0.275320
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.0010000000000000002
Train - acc:        0.989700 loss:        0.033458
Test - acc:         0.925600 loss:        0.279936
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.0010000000000000002
Train - acc:        0.990300 loss:        0.032562
Test - acc:         0.923400 loss:        0.285188
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.0010000000000000002
Train - acc:        0.990420 loss:        0.032118
Test - acc:         0.925700 loss:        0.274272
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.0010000000000000002
Train - acc:        0.990700 loss:        0.031759
Test - acc:         0.926500 loss:        0.273278
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.0010000000000000002
Train - acc:        0.990580 loss:        0.032142
Test - acc:         0.926400 loss:        0.275638
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.0010000000000000002
Train - acc:        0.989860 loss:        0.032083
Test - acc:         0.925100 loss:        0.274957
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.0010000000000000002
Train - acc:        0.990220 loss:        0.032630
Test - acc:         0.924800 loss:        0.280930
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.0010000000000000002
Train - acc:        0.989580 loss:        0.033565
Test - acc:         0.924000 loss:        0.280441
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.0010000000000000002
Train - acc:        0.991040 loss:        0.031377
Test - acc:         0.925300 loss:        0.276102
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.0010000000000000002
Train - acc:        0.990440 loss:        0.032006
Test - acc:         0.924400 loss:        0.277620
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.0010000000000000002
Train - acc:        0.990180 loss:        0.032512
Test - acc:         0.923800 loss:        0.277787
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.0010000000000000002
Train - acc:        0.990320 loss:        0.032011
Test - acc:         0.926300 loss:        0.276021
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.0010000000000000002
Train - acc:        0.990620 loss:        0.032610
Test - acc:         0.924600 loss:        0.279073
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.0010000000000000002
Train - acc:        0.991180 loss:        0.030840
Test - acc:         0.925500 loss:        0.280248
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.0010000000000000002
Train - acc:        0.990260 loss:        0.031858
Test - acc:         0.921400 loss:        0.282085
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.0010000000000000002
Train - acc:        0.990540 loss:        0.031770
Test - acc:         0.926800 loss:        0.280685
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.0010000000000000002
Train - acc:        0.990300 loss:        0.032346
Test - acc:         0.923100 loss:        0.288612
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.0010000000000000002
Train - acc:        0.991020 loss:        0.030642
Test - acc:         0.924200 loss:        0.283163
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.0010000000000000002
Train - acc:        0.990060 loss:        0.032388
Test - acc:         0.922200 loss:        0.284724
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.0010000000000000002
Train - acc:        0.990820 loss:        0.030873
Test - acc:         0.923000 loss:        0.281320
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.0010000000000000002
Train - acc:        0.990880 loss:        0.030573
Test - acc:         0.925000 loss:        0.283444
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.0010000000000000002
Train - acc:        0.990580 loss:        0.031818
Test - acc:         0.923900 loss:        0.287213
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.0010000000000000002
Train - acc:        0.991100 loss:        0.030621
Test - acc:         0.923900 loss:        0.283999
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.0010000000000000002
Train - acc:        0.990840 loss:        0.031030
Test - acc:         0.922900 loss:        0.285346
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.0010000000000000002
Train - acc:        0.991640 loss:        0.028665
Test - acc:         0.925200 loss:        0.288485
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.0010000000000000002
Train - acc:        0.990820 loss:        0.030741
Test - acc:         0.923400 loss:        0.287028
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.0010000000000000002
Train - acc:        0.990260 loss:        0.031262
Test - acc:         0.926800 loss:        0.279426
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.0010000000000000002
Train - acc:        0.990820 loss:        0.030726
Test - acc:         0.925800 loss:        0.280992
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.0010000000000000002
Train - acc:        0.990580 loss:        0.030683
Test - acc:         0.925700 loss:        0.282291
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.0010000000000000002
Train - acc:        0.990880 loss:        0.030538
Test - acc:         0.925500 loss:        0.274945
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.0010000000000000002
Train - acc:        0.989900 loss:        0.032629
Test - acc:         0.922700 loss:        0.287033
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.0010000000000000002
Train - acc:        0.991000 loss:        0.030878
Test - acc:         0.924200 loss:        0.279845
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.0010000000000000002
Train - acc:        0.991500 loss:        0.029968
Test - acc:         0.926600 loss:        0.278205
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.0010000000000000002
Train - acc:        0.990940 loss:        0.031087
Test - acc:         0.924100 loss:        0.284601
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.0010000000000000002
Train - acc:        0.990800 loss:        0.031075
Test - acc:         0.925000 loss:        0.283682
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.0010000000000000002
Train - acc:        0.991380 loss:        0.029471
Test - acc:         0.922300 loss:        0.286510
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.0010000000000000002
Train - acc:        0.990680 loss:        0.031791
Test - acc:         0.923900 loss:        0.289013
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.0010000000000000002
Train - acc:        0.991040 loss:        0.029301
Test - acc:         0.924300 loss:        0.293430
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.0010000000000000002
Train - acc:        0.990640 loss:        0.031132
Test - acc:         0.923500 loss:        0.287434
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.0010000000000000002
Train - acc:        0.990940 loss:        0.031111
Test - acc:         0.924200 loss:        0.291726
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.0010000000000000002
Train - acc:        0.990380 loss:        0.031474
Test - acc:         0.925000 loss:        0.280956
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.0010000000000000002
Train - acc:        0.991100 loss:        0.030573
Test - acc:         0.923900 loss:        0.283355
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.0010000000000000002
Train - acc:        0.991460 loss:        0.029498
Test - acc:         0.924500 loss:        0.281744
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.0010000000000000002
Train - acc:        0.990540 loss:        0.030874
Test - acc:         0.924600 loss:        0.280360
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.0010000000000000002
Train - acc:        0.990380 loss:        0.031172
Test - acc:         0.923000 loss:        0.281630
Sparsity :          0.9844
Wdecay :        0.000500
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 100,
    "lr": 0.001,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "none",
    "prune_freq": 1,
    "prune_rate": 0.2,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "logdir": "test",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": false,
    "milestones": null,
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "criterion_experiment_no_bias/post-finetune/resnet18_topflip_pf50_s42_noise_only_prunable",
    "load_model": "/home/andreia/thesis/chkpts/criterion_experiment_no_bias/pre-finetune/resnet18_global_magnitude_pf50_s42.pt",
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.0010000000000000002
Train - acc:        0.998060 loss:        0.010011
Test - acc:         0.941000 loss:        0.225821
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.0010000000000000002
Train - acc:        0.998380 loss:        0.009983
Test - acc:         0.940300 loss:        0.227761
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.0010000000000000002
Train - acc:        0.998400 loss:        0.009591
Test - acc:         0.940600 loss:        0.225106
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.0010000000000000002
Train - acc:        0.998560 loss:        0.009315
Test - acc:         0.939800 loss:        0.224904
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.0010000000000000002
Train - acc:        0.998240 loss:        0.009877
Test - acc:         0.940100 loss:        0.226322
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.0010000000000000002
Train - acc:        0.998180 loss:        0.010037
Test - acc:         0.941400 loss:        0.225446
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.0010000000000000002
Train - acc:        0.998720 loss:        0.009253
Test - acc:         0.940500 loss:        0.223931
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.0010000000000000002
Train - acc:        0.998720 loss:        0.009525
Test - acc:         0.940500 loss:        0.224292
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.0010000000000000002
Train - acc:        0.998360 loss:        0.009144
Test - acc:         0.939400 loss:        0.227941
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.0010000000000000002
Train - acc:        0.998620 loss:        0.008974
Test - acc:         0.939600 loss:        0.224163
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.0010000000000000002
Train - acc:        0.998460 loss:        0.009064
Test - acc:         0.940600 loss:        0.229764
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.0010000000000000002
Train - acc:        0.998740 loss:        0.008670
Test - acc:         0.940700 loss:        0.224145
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.0010000000000000002
Train - acc:        0.998600 loss:        0.009026
Test - acc:         0.940800 loss:        0.227701
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.0010000000000000002
Train - acc:        0.998340 loss:        0.009498
Test - acc:         0.940200 loss:        0.223940
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.0010000000000000002
Train - acc:        0.998400 loss:        0.009327
Test - acc:         0.943100 loss:        0.224981
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.0010000000000000002
Train - acc:        0.998360 loss:        0.008740
Test - acc:         0.940600 loss:        0.232132
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.0010000000000000002
Train - acc:        0.998520 loss:        0.009037
Test - acc:         0.940500 loss:        0.227362
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.0010000000000000002
Train - acc:        0.998660 loss:        0.008796
Test - acc:         0.941000 loss:        0.227926
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.0010000000000000002
Train - acc:        0.998840 loss:        0.008274
Test - acc:         0.940900 loss:        0.224881
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.0010000000000000002
Train - acc:        0.998460 loss:        0.008804
Test - acc:         0.940200 loss:        0.230455
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.008382
Test - acc:         0.940900 loss:        0.226565
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.0010000000000000002
Train - acc:        0.998700 loss:        0.008380
Test - acc:         0.940500 loss:        0.225337
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.0010000000000000002
Train - acc:        0.998760 loss:        0.008058
Test - acc:         0.939900 loss:        0.225804
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.0010000000000000002
Train - acc:        0.998780 loss:        0.008714
Test - acc:         0.939000 loss:        0.230008
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.0010000000000000002
Train - acc:        0.998940 loss:        0.007785
Test - acc:         0.941100 loss:        0.229982
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.0010000000000000002
Train - acc:        0.998960 loss:        0.007916
Test - acc:         0.940900 loss:        0.228396
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.0010000000000000002
Train - acc:        0.998740 loss:        0.008278
Test - acc:         0.942100 loss:        0.224874
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.0010000000000000002
Train - acc:        0.998620 loss:        0.008493
Test - acc:         0.941500 loss:        0.227489
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.0010000000000000002
Train - acc:        0.998620 loss:        0.008180
Test - acc:         0.939200 loss:        0.233064
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.0010000000000000002
Train - acc:        0.998660 loss:        0.008394
Test - acc:         0.939700 loss:        0.232985
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.0010000000000000002
Train - acc:        0.998640 loss:        0.008098
Test - acc:         0.941200 loss:        0.228481
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.0010000000000000002
Train - acc:        0.998860 loss:        0.007832
Test - acc:         0.942000 loss:        0.227225
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.0010000000000000002
Train - acc:        0.998620 loss:        0.008138
Test - acc:         0.940700 loss:        0.227939
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.0010000000000000002
Train - acc:        0.998680 loss:        0.007957
Test - acc:         0.942900 loss:        0.228284
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.0010000000000000002
Train - acc:        0.998940 loss:        0.007598
Test - acc:         0.940900 loss:        0.226509
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.0010000000000000002
Train - acc:        0.999040 loss:        0.007635
Test - acc:         0.941700 loss:        0.226704
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.0010000000000000002
Train - acc:        0.998840 loss:        0.007929
Test - acc:         0.942900 loss:        0.226932
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.0010000000000000002
Train - acc:        0.998700 loss:        0.008021
Test - acc:         0.941600 loss:        0.230015
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.0010000000000000002
Train - acc:        0.998880 loss:        0.007626
Test - acc:         0.941500 loss:        0.228848
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.0010000000000000002
Train - acc:        0.998880 loss:        0.007596
Test - acc:         0.942000 loss:        0.226502
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.0010000000000000002
Train - acc:        0.998780 loss:        0.007818
Test - acc:         0.941500 loss:        0.229363
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.007380
Test - acc:         0.939200 loss:        0.230353
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.0010000000000000002
Train - acc:        0.998980 loss:        0.007353
Test - acc:         0.941100 loss:        0.224707
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.0010000000000000002
Train - acc:        0.999080 loss:        0.006971
Test - acc:         0.940500 loss:        0.227320
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.0010000000000000002
Train - acc:        0.998760 loss:        0.007512
Test - acc:         0.941000 loss:        0.231000
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.0010000000000000002
Train - acc:        0.999000 loss:        0.007237
Test - acc:         0.941000 loss:        0.225708
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.0010000000000000002
Train - acc:        0.999080 loss:        0.007148
Test - acc:         0.939000 loss:        0.235632
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.0010000000000000002
Train - acc:        0.998880 loss:        0.007247
Test - acc:         0.940000 loss:        0.237138
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.0010000000000000002
Train - acc:        0.998900 loss:        0.007111
Test - acc:         0.940400 loss:        0.232686
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.007552
Test - acc:         0.940100 loss:        0.233685
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.0010000000000000002
Train - acc:        0.998780 loss:        0.007438
Test - acc:         0.939200 loss:        0.237314
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.0010000000000000002
Train - acc:        0.998940 loss:        0.007440
Test - acc:         0.941400 loss:        0.232112
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.0010000000000000002
Train - acc:        0.998840 loss:        0.007155
Test - acc:         0.940900 loss:        0.229034
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.007008
Test - acc:         0.940800 loss:        0.229754
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.0010000000000000002
Train - acc:        0.999100 loss:        0.006978
Test - acc:         0.939300 loss:        0.232397
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.0010000000000000002
Train - acc:        0.998740 loss:        0.007924
Test - acc:         0.939400 loss:        0.233728
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.0010000000000000002
Train - acc:        0.998940 loss:        0.007174
Test - acc:         0.941100 loss:        0.234015
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.0010000000000000002
Train - acc:        0.998720 loss:        0.007115
Test - acc:         0.941400 loss:        0.231138
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.0010000000000000002
Train - acc:        0.999160 loss:        0.006642
Test - acc:         0.941300 loss:        0.233446
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.0010000000000000002
Train - acc:        0.999140 loss:        0.006977
Test - acc:         0.941100 loss:        0.231845
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.0010000000000000002
Train - acc:        0.998740 loss:        0.007231
Test - acc:         0.940900 loss:        0.232668
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.0010000000000000002
Train - acc:        0.998980 loss:        0.007144
Test - acc:         0.941100 loss:        0.230887
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.0010000000000000002
Train - acc:        0.999220 loss:        0.006684
Test - acc:         0.940200 loss:        0.228487
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.006905
Test - acc:         0.940200 loss:        0.227209
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.0010000000000000002
Train - acc:        0.998960 loss:        0.007198
Test - acc:         0.941300 loss:        0.233354
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.0010000000000000002
Train - acc:        0.999100 loss:        0.006776
Test - acc:         0.939900 loss:        0.233023
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.006789
Test - acc:         0.941500 loss:        0.227320
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.006836
Test - acc:         0.939100 loss:        0.238239
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.0010000000000000002
Train - acc:        0.999040 loss:        0.007110
Test - acc:         0.939600 loss:        0.237294
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.0010000000000000002
Train - acc:        0.999140 loss:        0.006651
Test - acc:         0.940500 loss:        0.232255
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.007027
Test - acc:         0.939700 loss:        0.234591
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.0010000000000000002
Train - acc:        0.998940 loss:        0.007124
Test - acc:         0.940400 loss:        0.230693
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.0010000000000000002
Train - acc:        0.998700 loss:        0.007250
Test - acc:         0.939400 loss:        0.234269
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.007754
Test - acc:         0.940400 loss:        0.233554
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.0010000000000000002
Train - acc:        0.999080 loss:        0.006874
Test - acc:         0.940900 loss:        0.231403
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.0010000000000000002
Train - acc:        0.998920 loss:        0.007175
Test - acc:         0.939100 loss:        0.230901
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.007062
Test - acc:         0.940600 loss:        0.232013
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.0010000000000000002
Train - acc:        0.999160 loss:        0.006476
Test - acc:         0.940900 loss:        0.232982
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.0010000000000000002
Train - acc:        0.999000 loss:        0.006730
Test - acc:         0.939900 loss:        0.231436
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.0010000000000000002
Train - acc:        0.999220 loss:        0.006551
Test - acc:         0.940300 loss:        0.232720
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.006425
Test - acc:         0.941500 loss:        0.234034
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.0010000000000000002
Train - acc:        0.999100 loss:        0.006476
Test - acc:         0.940300 loss:        0.232352
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.005940
Test - acc:         0.940200 loss:        0.231704
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.0010000000000000002
Train - acc:        0.999220 loss:        0.006488
Test - acc:         0.941300 loss:        0.237887
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.0010000000000000002
Train - acc:        0.999120 loss:        0.006295
Test - acc:         0.940500 loss:        0.233508
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.0010000000000000002
Train - acc:        0.999180 loss:        0.006915
Test - acc:         0.940500 loss:        0.231557
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.0010000000000000002
Train - acc:        0.999100 loss:        0.006300
Test - acc:         0.940100 loss:        0.234796
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.006070
Test - acc:         0.941500 loss:        0.230334
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.0010000000000000002
Train - acc:        0.998980 loss:        0.006448
Test - acc:         0.940200 loss:        0.237883
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.0010000000000000002
Train - acc:        0.998900 loss:        0.006917
Test - acc:         0.940900 loss:        0.234430
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.0010000000000000002
Train - acc:        0.998940 loss:        0.007181
Test - acc:         0.940500 loss:        0.234726
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.006332
Test - acc:         0.940100 loss:        0.238871
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.006398
Test - acc:         0.942000 loss:        0.234427
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.0010000000000000002
Train - acc:        0.998940 loss:        0.007135
Test - acc:         0.940100 loss:        0.233304
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.0010000000000000002
Train - acc:        0.998920 loss:        0.006714
Test - acc:         0.940200 loss:        0.232514
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.006602
Test - acc:         0.942200 loss:        0.229421
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.0010000000000000002
Train - acc:        0.999120 loss:        0.006720
Test - acc:         0.940500 loss:        0.231600
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.006546
Test - acc:         0.941400 loss:        0.234697
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.0010000000000000002
Train - acc:        0.999040 loss:        0.006352
Test - acc:         0.942900 loss:        0.227605
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.006349
Test - acc:         0.943300 loss:        0.227434
Sparsity :          0.9844
Wdecay :        0.000500
