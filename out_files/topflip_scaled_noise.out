Running 
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "topflip",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "logdir": "scaling_noise_experiment",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 0.5,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": null,
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.333960 loss:        1.862106
Test - acc:         0.432100 loss:        1.510146
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.501880 loss:        1.360095
Test - acc:         0.581500 loss:        1.161949
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.611680 loss:        1.085898
Test - acc:         0.632500 loss:        1.033598
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.675440 loss:        0.918068
Test - acc:         0.653800 loss:        0.976649
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.716420 loss:        0.803278
Test - acc:         0.695800 loss:        0.911705
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.758180 loss:        0.691189
Test - acc:         0.747000 loss:        0.737007
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.787000 loss:        0.616411
Test - acc:         0.755700 loss:        0.715024
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.803400 loss:        0.567391
Test - acc:         0.784900 loss:        0.626498
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.813300 loss:        0.544930
Test - acc:         0.756300 loss:        0.710817
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.823300 loss:        0.513112
Test - acc:         0.773800 loss:        0.677750
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.829420 loss:        0.496505
Test - acc:         0.760100 loss:        0.730440
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.834200 loss:        0.480800
Test - acc:         0.796400 loss:        0.631096
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.840580 loss:        0.462461
Test - acc:         0.782600 loss:        0.633116
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.843820 loss:        0.457057
Test - acc:         0.758700 loss:        0.733857
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.847020 loss:        0.445181
Test - acc:         0.772900 loss:        0.740408
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.851860 loss:        0.432313
Test - acc:         0.832100 loss:        0.495510
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.855680 loss:        0.424025
Test - acc:         0.827400 loss:        0.502972
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.858140 loss:        0.416248
Test - acc:         0.770800 loss:        0.755412
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.861200 loss:        0.408111
Test - acc:         0.817600 loss:        0.564910
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.862380 loss:        0.400156
Test - acc:         0.835300 loss:        0.509238
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.865620 loss:        0.396328
Test - acc:         0.841100 loss:        0.484278
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.865420 loss:        0.394472
Test - acc:         0.769300 loss:        0.709420
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.864760 loss:        0.393104
Test - acc:         0.790800 loss:        0.665163
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.870180 loss:        0.382956
Test - acc:         0.838300 loss:        0.473115
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.869380 loss:        0.382446
Test - acc:         0.794500 loss:        0.644293
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.866220 loss:        0.389617
Test - acc:         0.817300 loss:        0.557268
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.872060 loss:        0.376745
Test - acc:         0.834400 loss:        0.491060
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.869460 loss:        0.378626
Test - acc:         0.838400 loss:        0.474066
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.873580 loss:        0.367146
Test - acc:         0.841900 loss:        0.482488
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.875080 loss:        0.368154
Test - acc:         0.832500 loss:        0.484733
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.873820 loss:        0.371724
Test - acc:         0.830400 loss:        0.512692
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.876040 loss:        0.368693
Test - acc:         0.832200 loss:        0.513597
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.876120 loss:        0.364386
Test - acc:         0.791300 loss:        0.649783
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.876060 loss:        0.363972
Test - acc:         0.793800 loss:        0.644325
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.877340 loss:        0.360683
Test - acc:         0.835700 loss:        0.489128
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.877080 loss:        0.361789
Test - acc:         0.842700 loss:        0.480509
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.878040 loss:        0.358002
Test - acc:         0.860000 loss:        0.417958
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.878600 loss:        0.357547
Test - acc:         0.817400 loss:        0.613081
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.877200 loss:        0.357764
Test - acc:         0.851800 loss:        0.443996
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.878260 loss:        0.356707
Test - acc:         0.846800 loss:        0.464961
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.880020 loss:        0.354167
Test - acc:         0.849700 loss:        0.464173
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.880980 loss:        0.347883
Test - acc:         0.811600 loss:        0.567948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.879960 loss:        0.352392
Test - acc:         0.829600 loss:        0.508393
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.881480 loss:        0.348613
Test - acc:         0.835000 loss:        0.520499
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.880100 loss:        0.351200
Test - acc:         0.785300 loss:        0.698170
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.884660 loss:        0.342439
Test - acc:         0.806200 loss:        0.597158
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.881880 loss:        0.348372
Test - acc:         0.820900 loss:        0.530352
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.878860 loss:        0.354317
Test - acc:         0.784300 loss:        0.681925
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.883400 loss:        0.347450
Test - acc:         0.827500 loss:        0.531444
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.881760 loss:        0.348246
Test - acc:         0.822200 loss:        0.538077
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.895480 loss:        0.306103
Test - acc:         0.839500 loss:        0.514143
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.893980 loss:        0.310560
Test - acc:         0.876900 loss:        0.366737
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.896160 loss:        0.305126
Test - acc:         0.848700 loss:        0.467871
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.896720 loss:        0.304323
Test - acc:         0.837200 loss:        0.486489
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.896580 loss:        0.302574
Test - acc:         0.847600 loss:        0.461371
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.896100 loss:        0.298796
Test - acc:         0.848900 loss:        0.472557
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.897460 loss:        0.297593
Test - acc:         0.835800 loss:        0.518651
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.898960 loss:        0.294093
Test - acc:         0.845000 loss:        0.473870
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.899000 loss:        0.295099
Test - acc:         0.856500 loss:        0.438945
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.897800 loss:        0.298702
Test - acc:         0.830800 loss:        0.510392
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.898740 loss:        0.296041
Test - acc:         0.843200 loss:        0.476035
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.897900 loss:        0.299874
Test - acc:         0.807600 loss:        0.609364
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.898860 loss:        0.295831
Test - acc:         0.836800 loss:        0.513715
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.899500 loss:        0.293146
Test - acc:         0.855800 loss:        0.439662
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.898320 loss:        0.298222
Test - acc:         0.851900 loss:        0.452925
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.898320 loss:        0.294368
Test - acc:         0.863200 loss:        0.420811
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.898300 loss:        0.293591
Test - acc:         0.858800 loss:        0.430399
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.901140 loss:        0.291587
Test - acc:         0.828100 loss:        0.554042
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.900760 loss:        0.291203
Test - acc:         0.849200 loss:        0.472169
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.900440 loss:        0.293683
Test - acc:         0.851300 loss:        0.449356
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.899480 loss:        0.291900
Test - acc:         0.846400 loss:        0.453790
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.899040 loss:        0.293886
Test - acc:         0.874300 loss:        0.374482
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.900080 loss:        0.291649
Test - acc:         0.867000 loss:        0.395379
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.900400 loss:        0.291885
Test - acc:         0.839600 loss:        0.490536
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.900200 loss:        0.293302
Test - acc:         0.815000 loss:        0.604559
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.900160 loss:        0.292735
Test - acc:         0.826800 loss:        0.561458
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.899640 loss:        0.294653
Test - acc:         0.851100 loss:        0.448240
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.901460 loss:        0.289055
Test - acc:         0.844100 loss:        0.467411
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.900320 loss:        0.292424
Test - acc:         0.843500 loss:        0.477598
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.899120 loss:        0.294337
Test - acc:         0.842300 loss:        0.490865
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.899640 loss:        0.294556
Test - acc:         0.853800 loss:        0.458864
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.901980 loss:        0.289020
Test - acc:         0.870800 loss:        0.402215
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.901200 loss:        0.290490
Test - acc:         0.858200 loss:        0.420627
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.899220 loss:        0.290318
Test - acc:         0.863700 loss:        0.423797
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.898540 loss:        0.294878
Test - acc:         0.862300 loss:        0.413863
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.900120 loss:        0.292455
Test - acc:         0.873900 loss:        0.376157
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.900340 loss:        0.292749
Test - acc:         0.854400 loss:        0.434506
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.900800 loss:        0.291307
Test - acc:         0.847300 loss:        0.445497
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.898040 loss:        0.295660
Test - acc:         0.845700 loss:        0.458917
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.901240 loss:        0.288001
Test - acc:         0.850900 loss:        0.452721
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.897820 loss:        0.297960
Test - acc:         0.865300 loss:        0.417925
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.900040 loss:        0.293653
Test - acc:         0.862900 loss:        0.414617
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.901240 loss:        0.285790
Test - acc:         0.853600 loss:        0.464472
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.897620 loss:        0.297945
Test - acc:         0.853400 loss:        0.432598
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.899380 loss:        0.293703
Test - acc:         0.852300 loss:        0.439841
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.900880 loss:        0.289442
Test - acc:         0.874200 loss:        0.366097
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.901860 loss:        0.289569
Test - acc:         0.864000 loss:        0.403367
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.898300 loss:        0.296558
Test - acc:         0.843300 loss:        0.466720
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.901120 loss:        0.290774
Test - acc:         0.860900 loss:        0.427141
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.899320 loss:        0.292128
Test - acc:         0.845300 loss:        0.467197
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.915520 loss:        0.244538
Test - acc:         0.860700 loss:        0.422547
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.911640 loss:        0.257676
Test - acc:         0.853100 loss:        0.467570
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.913260 loss:        0.255414
Test - acc:         0.861400 loss:        0.434209
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.910160 loss:        0.259271
Test - acc:         0.870500 loss:        0.395827
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.913960 loss:        0.250970
Test - acc:         0.856000 loss:        0.453181
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.912300 loss:        0.257854
Test - acc:         0.890600 loss:        0.340776
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.913620 loss:        0.250713
Test - acc:         0.847300 loss:        0.492222
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.913980 loss:        0.249925
Test - acc:         0.856300 loss:        0.459156
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.911220 loss:        0.254477
Test - acc:         0.867000 loss:        0.418111
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.914600 loss:        0.249294
Test - acc:         0.878000 loss:        0.377416
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.913460 loss:        0.254063
Test - acc:         0.879500 loss:        0.369384
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.913940 loss:        0.251039
Test - acc:         0.871700 loss:        0.389560
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.912000 loss:        0.255805
Test - acc:         0.853200 loss:        0.468122
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.913580 loss:        0.252101
Test - acc:         0.874800 loss:        0.376317
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.913980 loss:        0.250015
Test - acc:         0.863700 loss:        0.415298
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.911920 loss:        0.255353
Test - acc:         0.865400 loss:        0.427168
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.913420 loss:        0.252879
Test - acc:         0.865300 loss:        0.413274
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.914480 loss:        0.250948
Test - acc:         0.843500 loss:        0.511407
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.914200 loss:        0.250265
Test - acc:         0.869400 loss:        0.389631
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.912920 loss:        0.253685
Test - acc:         0.853800 loss:        0.452849
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.915440 loss:        0.247043
Test - acc:         0.874600 loss:        0.368139
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.913460 loss:        0.254437
Test - acc:         0.846500 loss:        0.458986
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.914080 loss:        0.248127
Test - acc:         0.859200 loss:        0.428934
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.912880 loss:        0.255129
Test - acc:         0.869900 loss:        0.411080
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.914260 loss:        0.250364
Test - acc:         0.859300 loss:        0.433367
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.912900 loss:        0.249592
Test - acc:         0.864000 loss:        0.405595
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.912240 loss:        0.251049
Test - acc:         0.828800 loss:        0.557911
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.912160 loss:        0.252832
Test - acc:         0.820400 loss:        0.552804
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.914600 loss:        0.250101
Test - acc:         0.878400 loss:        0.368263
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.913840 loss:        0.253560
Test - acc:         0.855500 loss:        0.435916
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.914800 loss:        0.249253
Test - acc:         0.863400 loss:        0.420326
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.913820 loss:        0.251403
Test - acc:         0.874500 loss:        0.376949
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.912120 loss:        0.252265
Test - acc:         0.855200 loss:        0.434335
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.912480 loss:        0.253821
Test - acc:         0.869100 loss:        0.418312
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.913860 loss:        0.252155
Test - acc:         0.874400 loss:        0.393984
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.915780 loss:        0.246676
Test - acc:         0.850900 loss:        0.445281
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.913640 loss:        0.249755
Test - acc:         0.853800 loss:        0.465684
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.913780 loss:        0.251714
Test - acc:         0.846400 loss:        0.469389
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.913920 loss:        0.251634
Test - acc:         0.842200 loss:        0.483316
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.913800 loss:        0.252153
Test - acc:         0.825000 loss:        0.530170
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.912740 loss:        0.253186
Test - acc:         0.877800 loss:        0.372170
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.914620 loss:        0.249087
Test - acc:         0.878000 loss:        0.372405
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.914880 loss:        0.250599
Test - acc:         0.864600 loss:        0.410002
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.914040 loss:        0.246890
Test - acc:         0.854500 loss:        0.470385
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.914960 loss:        0.251245
Test - acc:         0.870100 loss:        0.404420
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.914380 loss:        0.251083
Test - acc:         0.855700 loss:        0.435501
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.914080 loss:        0.250193
Test - acc:         0.872900 loss:        0.380161
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.916140 loss:        0.247548
Test - acc:         0.842100 loss:        0.484563
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.915000 loss:        0.247164
Test - acc:         0.876000 loss:        0.371527
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.915960 loss:        0.248623
Test - acc:         0.876200 loss:        0.367983
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.956080 loss:        0.130477
Test - acc:         0.930900 loss:        0.203234
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.966760 loss:        0.100748
Test - acc:         0.933500 loss:        0.197878
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.968840 loss:        0.090700
Test - acc:         0.935700 loss:        0.195363
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.974100 loss:        0.077450
Test - acc:         0.936200 loss:        0.196803
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.975320 loss:        0.073867
Test - acc:         0.934100 loss:        0.199858
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.977300 loss:        0.067774
Test - acc:         0.937000 loss:        0.197755
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.979240 loss:        0.061777
Test - acc:         0.938900 loss:        0.195025
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.980000 loss:        0.059928
Test - acc:         0.935900 loss:        0.203673
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.981620 loss:        0.054631
Test - acc:         0.935300 loss:        0.211864
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.983300 loss:        0.050912
Test - acc:         0.936700 loss:        0.207469
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.984980 loss:        0.048931
Test - acc:         0.932500 loss:        0.211423
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.985620 loss:        0.044535
Test - acc:         0.933500 loss:        0.217513
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.986120 loss:        0.042820
Test - acc:         0.935500 loss:        0.210033
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.986480 loss:        0.041818
Test - acc:         0.931600 loss:        0.222740
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.987020 loss:        0.039692
Test - acc:         0.936800 loss:        0.214535
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.987060 loss:        0.039831
Test - acc:         0.934800 loss:        0.218345
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.987840 loss:        0.037856
Test - acc:         0.933800 loss:        0.222563
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.989340 loss:        0.034608
Test - acc:         0.937100 loss:        0.219711
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.987640 loss:        0.037764
Test - acc:         0.938300 loss:        0.219881
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.987520 loss:        0.037817
Test - acc:         0.932900 loss:        0.230678
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.987980 loss:        0.037542
Test - acc:         0.936300 loss:        0.231554
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.988120 loss:        0.036154
Test - acc:         0.931300 loss:        0.234925
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.987940 loss:        0.036365
Test - acc:         0.930700 loss:        0.245004
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.989380 loss:        0.034015
Test - acc:         0.934000 loss:        0.240917
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.988700 loss:        0.035566
Test - acc:         0.934600 loss:        0.234446
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.988060 loss:        0.035769
Test - acc:         0.930800 loss:        0.247711
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.988620 loss:        0.036360
Test - acc:         0.928600 loss:        0.252412
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.987260 loss:        0.038411
Test - acc:         0.928500 loss:        0.270733
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.987240 loss:        0.038806
Test - acc:         0.930400 loss:        0.255310
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.985500 loss:        0.041437
Test - acc:         0.929300 loss:        0.238539
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.987660 loss:        0.037914
Test - acc:         0.931100 loss:        0.240138
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.989240 loss:        0.034753
Test - acc:         0.928100 loss:        0.261588
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.986180 loss:        0.042302
Test - acc:         0.926100 loss:        0.252240
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.986280 loss:        0.042146
Test - acc:         0.930800 loss:        0.240660
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.985980 loss:        0.042818
Test - acc:         0.929500 loss:        0.247306
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.987240 loss:        0.040357
Test - acc:         0.931300 loss:        0.243561
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.984560 loss:        0.045527
Test - acc:         0.928600 loss:        0.261714
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.986060 loss:        0.042536
Test - acc:         0.930700 loss:        0.255869
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.985800 loss:        0.042690
Test - acc:         0.924600 loss:        0.259625
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.986040 loss:        0.042992
Test - acc:         0.923800 loss:        0.273815
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.985260 loss:        0.043819
Test - acc:         0.929200 loss:        0.247058
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.984760 loss:        0.045367
Test - acc:         0.929300 loss:        0.253379
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.986080 loss:        0.042664
Test - acc:         0.931800 loss:        0.239510
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.986220 loss:        0.043173
Test - acc:         0.926900 loss:        0.260793
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.985400 loss:        0.046010
Test - acc:         0.928800 loss:        0.259032
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.984380 loss:        0.045702
Test - acc:         0.925100 loss:        0.262228
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.985860 loss:        0.042554
Test - acc:         0.922700 loss:        0.265050
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.985300 loss:        0.044891
Test - acc:         0.926100 loss:        0.271001
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.983620 loss:        0.048645
Test - acc:         0.928100 loss:        0.257674
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.984200 loss:        0.047258
Test - acc:         0.926400 loss:        0.262557
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.971520 loss:        0.081562
Test - acc:         0.923400 loss:        0.262587
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.974540 loss:        0.074982
Test - acc:         0.925000 loss:        0.250106
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.976720 loss:        0.068988
Test - acc:         0.921200 loss:        0.276626
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.977140 loss:        0.067879
Test - acc:         0.926500 loss:        0.251952
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.977360 loss:        0.065994
Test - acc:         0.924300 loss:        0.254213
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.976480 loss:        0.068160
Test - acc:         0.923100 loss:        0.260151
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.978240 loss:        0.064997
Test - acc:         0.924200 loss:        0.256117
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.979960 loss:        0.060959
Test - acc:         0.923500 loss:        0.261055
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.978780 loss:        0.063059
Test - acc:         0.919300 loss:        0.263717
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.979800 loss:        0.060071
Test - acc:         0.922400 loss:        0.272111
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.979400 loss:        0.060505
Test - acc:         0.925100 loss:        0.268549
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.978700 loss:        0.060902
Test - acc:         0.924500 loss:        0.264488
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.978960 loss:        0.063259
Test - acc:         0.920200 loss:        0.284488
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.979800 loss:        0.060130
Test - acc:         0.927900 loss:        0.256927
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.980040 loss:        0.059035
Test - acc:         0.919400 loss:        0.285836
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.981420 loss:        0.055938
Test - acc:         0.923800 loss:        0.264591
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.980360 loss:        0.058698
Test - acc:         0.926400 loss:        0.250197
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.979500 loss:        0.060519
Test - acc:         0.926400 loss:        0.249658
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.982680 loss:        0.053577
Test - acc:         0.926000 loss:        0.254280
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.982040 loss:        0.053251
Test - acc:         0.924900 loss:        0.261015
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.981440 loss:        0.054982
Test - acc:         0.926800 loss:        0.257067
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.981560 loss:        0.056120
Test - acc:         0.923900 loss:        0.253629
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.980860 loss:        0.056684
Test - acc:         0.923000 loss:        0.269652
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.980720 loss:        0.056965
Test - acc:         0.927900 loss:        0.247042
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.983520 loss:        0.051489
Test - acc:         0.927000 loss:        0.260219
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.983540 loss:        0.051063
Test - acc:         0.923900 loss:        0.266845
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.981400 loss:        0.054700
Test - acc:         0.924200 loss:        0.276953
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.980820 loss:        0.056162
Test - acc:         0.921700 loss:        0.273073
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.981620 loss:        0.054621
Test - acc:         0.924700 loss:        0.268719
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.981760 loss:        0.055506
Test - acc:         0.922400 loss:        0.278256
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.982120 loss:        0.053026
Test - acc:         0.928800 loss:        0.256041
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.982240 loss:        0.053631
Test - acc:         0.923500 loss:        0.279214
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.981820 loss:        0.054504
Test - acc:         0.925800 loss:        0.254405
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.982080 loss:        0.052952
Test - acc:         0.923200 loss:        0.258695
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.982620 loss:        0.052752
Test - acc:         0.924300 loss:        0.264603
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.982400 loss:        0.052917
Test - acc:         0.923200 loss:        0.273155
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.981680 loss:        0.054697
Test - acc:         0.924600 loss:        0.260398
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.983460 loss:        0.049687
Test - acc:         0.924400 loss:        0.262892
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.982380 loss:        0.054705
Test - acc:         0.923200 loss:        0.266390
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.981840 loss:        0.054447
Test - acc:         0.923600 loss:        0.275784
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.981540 loss:        0.054283
Test - acc:         0.928200 loss:        0.253292
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.981580 loss:        0.054702
Test - acc:         0.928400 loss:        0.245969
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.983160 loss:        0.052143
Test - acc:         0.929100 loss:        0.250223
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.981780 loss:        0.052496
Test - acc:         0.927100 loss:        0.255179
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.983420 loss:        0.051356
Test - acc:         0.925800 loss:        0.252280
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.984440 loss:        0.046948
Test - acc:         0.925400 loss:        0.271176
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.982660 loss:        0.050692
Test - acc:         0.928600 loss:        0.245251
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.982240 loss:        0.052570
Test - acc:         0.921600 loss:        0.274063
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.982720 loss:        0.051786
Test - acc:         0.922500 loss:        0.275562
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.982200 loss:        0.053423
Test - acc:         0.927200 loss:        0.254337
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.964520 loss:        0.104361
Test - acc:         0.925100 loss:        0.241527
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.975540 loss:        0.074344
Test - acc:         0.927600 loss:        0.233152
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.979440 loss:        0.065751
Test - acc:         0.928700 loss:        0.231266
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.981480 loss:        0.058946
Test - acc:         0.930800 loss:        0.226852
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.981920 loss:        0.055999
Test - acc:         0.930600 loss:        0.226655
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.984160 loss:        0.051613
Test - acc:         0.930600 loss:        0.226886
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.985180 loss:        0.048156
Test - acc:         0.933000 loss:        0.222159
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.986080 loss:        0.046442
Test - acc:         0.932300 loss:        0.223434
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.987200 loss:        0.044333
Test - acc:         0.933800 loss:        0.225834
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.987520 loss:        0.042156
Test - acc:         0.934200 loss:        0.224970
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.988600 loss:        0.039019
Test - acc:         0.934700 loss:        0.225282
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.988520 loss:        0.039452
Test - acc:         0.933600 loss:        0.226759
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.989220 loss:        0.037435
Test - acc:         0.934200 loss:        0.224296
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.989180 loss:        0.036952
Test - acc:         0.934000 loss:        0.225869
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.989160 loss:        0.036203
Test - acc:         0.935000 loss:        0.222736
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.989680 loss:        0.034801
Test - acc:         0.933600 loss:        0.226687
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.991140 loss:        0.032596
Test - acc:         0.933500 loss:        0.230452
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.990160 loss:        0.033682
Test - acc:         0.934300 loss:        0.225444
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.991320 loss:        0.031195
Test - acc:         0.933200 loss:        0.229756
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.991620 loss:        0.030832
Test - acc:         0.935400 loss:        0.228777
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.992440 loss:        0.028265
Test - acc:         0.936000 loss:        0.227881
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.991400 loss:        0.030560
Test - acc:         0.936200 loss:        0.228976
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.992420 loss:        0.028144
Test - acc:         0.936000 loss:        0.228976
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.992120 loss:        0.029546
Test - acc:         0.935700 loss:        0.230810
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.991740 loss:        0.029399
Test - acc:         0.936500 loss:        0.230690
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.991700 loss:        0.029154
Test - acc:         0.935200 loss:        0.229777
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.992960 loss:        0.027017
Test - acc:         0.935800 loss:        0.228464
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.993280 loss:        0.025748
Test - acc:         0.937400 loss:        0.232573
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.992860 loss:        0.026861
Test - acc:         0.937100 loss:        0.232218
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.993320 loss:        0.025745
Test - acc:         0.936400 loss:        0.232079
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.993520 loss:        0.024435
Test - acc:         0.936100 loss:        0.231625
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.994000 loss:        0.024241
Test - acc:         0.937700 loss:        0.229153
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.994020 loss:        0.023504
Test - acc:         0.935500 loss:        0.235559
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.993680 loss:        0.023463
Test - acc:         0.937100 loss:        0.234734
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.994100 loss:        0.023121
Test - acc:         0.937200 loss:        0.235497
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.993380 loss:        0.023781
Test - acc:         0.936300 loss:        0.234793
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.994400 loss:        0.022093
Test - acc:         0.936700 loss:        0.235647
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.994180 loss:        0.022022
Test - acc:         0.936400 loss:        0.235238
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.994320 loss:        0.022403
Test - acc:         0.936500 loss:        0.237493
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.994100 loss:        0.023285
Test - acc:         0.936200 loss:        0.235997
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.994920 loss:        0.021596
Test - acc:         0.936600 loss:        0.238812
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.994480 loss:        0.021071
Test - acc:         0.936900 loss:        0.238693
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.994220 loss:        0.021929
Test - acc:         0.937800 loss:        0.234723
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.994520 loss:        0.021435
Test - acc:         0.937000 loss:        0.240386
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.995120 loss:        0.020143
Test - acc:         0.936700 loss:        0.234881
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.995380 loss:        0.019552
Test - acc:         0.936600 loss:        0.235200
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.995520 loss:        0.019108
Test - acc:         0.936800 loss:        0.235924
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.995800 loss:        0.018966
Test - acc:         0.937900 loss:        0.234314
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.995260 loss:        0.019472
Test - acc:         0.936300 loss:        0.234415
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.995800 loss:        0.018333
Test - acc:         0.937100 loss:        0.236529
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.894180 loss:        0.307845
Test - acc:         0.891800 loss:        0.325645
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.922220 loss:        0.225888
Test - acc:         0.901500 loss:        0.297009
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.931960 loss:        0.196183
Test - acc:         0.908000 loss:        0.284846
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.937060 loss:        0.181922
Test - acc:         0.910100 loss:        0.278264
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.939740 loss:        0.172507
Test - acc:         0.909500 loss:        0.279522
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.944120 loss:        0.163407
Test - acc:         0.913200 loss:        0.272164
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.946140 loss:        0.157293
Test - acc:         0.914100 loss:        0.265317
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.947960 loss:        0.152684
Test - acc:         0.913900 loss:        0.266876
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.948080 loss:        0.150509
Test - acc:         0.913700 loss:        0.265508
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.950220 loss:        0.143746
Test - acc:         0.913600 loss:        0.261239
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.952680 loss:        0.140654
Test - acc:         0.915900 loss:        0.266081
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.953880 loss:        0.135492
Test - acc:         0.917600 loss:        0.264804
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.954360 loss:        0.133913
Test - acc:         0.918900 loss:        0.258640
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.955080 loss:        0.130984
Test - acc:         0.917900 loss:        0.263281
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.955640 loss:        0.129443
Test - acc:         0.917200 loss:        0.261365
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.957820 loss:        0.124279
Test - acc:         0.915400 loss:        0.261315
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.957000 loss:        0.127964
Test - acc:         0.915900 loss:        0.258109
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.958180 loss:        0.121752
Test - acc:         0.915300 loss:        0.260426
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.958840 loss:        0.120120
Test - acc:         0.916400 loss:        0.254602
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.959820 loss:        0.118350
Test - acc:         0.917100 loss:        0.258578
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.960660 loss:        0.115497
Test - acc:         0.914900 loss:        0.264666
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.960600 loss:        0.114981
Test - acc:         0.913700 loss:        0.264856
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.960540 loss:        0.114190
Test - acc:         0.916200 loss:        0.261346
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.960840 loss:        0.112262
Test - acc:         0.918500 loss:        0.256330
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.961480 loss:        0.112889
Test - acc:         0.917900 loss:        0.258013
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.960760 loss:        0.113156
Test - acc:         0.918800 loss:        0.258064
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.961820 loss:        0.110268
Test - acc:         0.918200 loss:        0.258187
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.963240 loss:        0.107513
Test - acc:         0.918600 loss:        0.258727
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.963600 loss:        0.107110
Test - acc:         0.920500 loss:        0.253139
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.963660 loss:        0.107792
Test - acc:         0.918100 loss:        0.259770
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.963560 loss:        0.106524
Test - acc:         0.913500 loss:        0.266876
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.965460 loss:        0.102488
Test - acc:         0.915300 loss:        0.262000
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.965540 loss:        0.103833
Test - acc:         0.916300 loss:        0.258154
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.964960 loss:        0.101655
Test - acc:         0.917500 loss:        0.256140
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.965860 loss:        0.100103
Test - acc:         0.918600 loss:        0.256713
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.965840 loss:        0.100927
Test - acc:         0.919600 loss:        0.256907
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.966460 loss:        0.100246
Test - acc:         0.920700 loss:        0.255685
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.966420 loss:        0.097979
Test - acc:         0.916900 loss:        0.260043
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.966020 loss:        0.100276
Test - acc:         0.918100 loss:        0.259941
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.966460 loss:        0.097300
Test - acc:         0.916200 loss:        0.266088
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.967100 loss:        0.098299
Test - acc:         0.918700 loss:        0.263352
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.968740 loss:        0.094637
Test - acc:         0.918100 loss:        0.255984
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.966640 loss:        0.096089
Test - acc:         0.918900 loss:        0.260570
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.966960 loss:        0.095503
Test - acc:         0.917600 loss:        0.263385
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.967400 loss:        0.095557
Test - acc:         0.919200 loss:        0.263311
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.966820 loss:        0.096382
Test - acc:         0.916500 loss:        0.263455
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.969320 loss:        0.091332
Test - acc:         0.916500 loss:        0.264296
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.968580 loss:        0.093524
Test - acc:         0.917700 loss:        0.266155
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.968340 loss:        0.093574
Test - acc:         0.919100 loss:        0.262130
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.969580 loss:        0.091247
Test - acc:         0.918300 loss:        0.267147
Sparsity :          0.9844
Wdecay :        0.000500
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "topflip",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "logdir": "scaling_noise_experiment",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 0.25,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": null,
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.322340 loss:        1.886699
Test - acc:         0.412400 loss:        1.571300
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.495440 loss:        1.374135
Test - acc:         0.572500 loss:        1.184698
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.607540 loss:        1.091654
Test - acc:         0.621300 loss:        1.068805
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.687920 loss:        0.885695
Test - acc:         0.687300 loss:        0.890359
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.740660 loss:        0.739654
Test - acc:         0.720900 loss:        0.851775
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.779340 loss:        0.641494
Test - acc:         0.755800 loss:        0.704248
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.796220 loss:        0.587098
Test - acc:         0.781600 loss:        0.640724
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.811480 loss:        0.546275
Test - acc:         0.782300 loss:        0.630519
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.819840 loss:        0.522924
Test - acc:         0.765100 loss:        0.712430
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.829680 loss:        0.496940
Test - acc:         0.767200 loss:        0.697477
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.835840 loss:        0.478582
Test - acc:         0.712400 loss:        0.921297
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.841800 loss:        0.461143
Test - acc:         0.778700 loss:        0.671245
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.846760 loss:        0.448754
Test - acc:         0.758800 loss:        0.749820
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.848620 loss:        0.443076
Test - acc:         0.720000 loss:        0.927059
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.854120 loss:        0.428311
Test - acc:         0.812700 loss:        0.558105
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.856700 loss:        0.421219
Test - acc:         0.828200 loss:        0.504929
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.860420 loss:        0.408130
Test - acc:         0.822100 loss:        0.523616
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.862940 loss:        0.405288
Test - acc:         0.832300 loss:        0.493318
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.862000 loss:        0.405007
Test - acc:         0.836500 loss:        0.479825
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.865600 loss:        0.391311
Test - acc:         0.823000 loss:        0.535222
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.870000 loss:        0.380437
Test - acc:         0.835100 loss:        0.484840
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.867640 loss:        0.387460
Test - acc:         0.838300 loss:        0.484035
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.870820 loss:        0.376995
Test - acc:         0.812800 loss:        0.572729
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.871060 loss:        0.375839
Test - acc:         0.829500 loss:        0.505071
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.873980 loss:        0.368091
Test - acc:         0.827200 loss:        0.529063
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.871060 loss:        0.376472
Test - acc:         0.849400 loss:        0.440659
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.876220 loss:        0.364260
Test - acc:         0.843200 loss:        0.467006
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.875360 loss:        0.364017
Test - acc:         0.832600 loss:        0.503869
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.878660 loss:        0.357284
Test - acc:         0.847900 loss:        0.473449
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.876800 loss:        0.361031
Test - acc:         0.857500 loss:        0.425841
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.877560 loss:        0.356808
Test - acc:         0.848600 loss:        0.456704
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.879880 loss:        0.354396
Test - acc:         0.825300 loss:        0.558996
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.880140 loss:        0.352010
Test - acc:         0.845200 loss:        0.463289
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.880120 loss:        0.353469
Test - acc:         0.846700 loss:        0.461988
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.883240 loss:        0.347968
Test - acc:         0.824800 loss:        0.542132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.883140 loss:        0.343953
Test - acc:         0.849800 loss:        0.445216
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.882120 loss:        0.346582
Test - acc:         0.815900 loss:        0.561125
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.879460 loss:        0.351141
Test - acc:         0.851500 loss:        0.444231
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.881340 loss:        0.346336
Test - acc:         0.849600 loss:        0.444168
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.883480 loss:        0.341467
Test - acc:         0.792400 loss:        0.669404
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.882500 loss:        0.346159
Test - acc:         0.817600 loss:        0.576091
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.885120 loss:        0.334941
Test - acc:         0.821400 loss:        0.546956
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.887380 loss:        0.333498
Test - acc:         0.839600 loss:        0.478847
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.885240 loss:        0.335171
Test - acc:         0.835500 loss:        0.527048
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.884640 loss:        0.339486
Test - acc:         0.845700 loss:        0.458962
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.885660 loss:        0.335873
Test - acc:         0.820600 loss:        0.553372
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.884180 loss:        0.337440
Test - acc:         0.830600 loss:        0.534387
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.885460 loss:        0.337343
Test - acc:         0.804500 loss:        0.603033
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.885140 loss:        0.335577
Test - acc:         0.804700 loss:        0.644218
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.885620 loss:        0.338217
Test - acc:         0.854200 loss:        0.433284
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.901820 loss:        0.284767
Test - acc:         0.846300 loss:        0.499513
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.898040 loss:        0.297487
Test - acc:         0.865700 loss:        0.395689
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.902580 loss:        0.288851
Test - acc:         0.842100 loss:        0.476468
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.903260 loss:        0.281470
Test - acc:         0.857900 loss:        0.437261
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.903460 loss:        0.280841
Test - acc:         0.847500 loss:        0.459089
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.902960 loss:        0.281946
Test - acc:         0.850400 loss:        0.461135
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.902160 loss:        0.284847
Test - acc:         0.792800 loss:        0.647821
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.904700 loss:        0.280916
Test - acc:         0.857600 loss:        0.432929
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.903740 loss:        0.278502
Test - acc:         0.863700 loss:        0.426419
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.904940 loss:        0.283275
Test - acc:         0.841800 loss:        0.471267
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.906380 loss:        0.277187
Test - acc:         0.849600 loss:        0.476157
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.904440 loss:        0.279462
Test - acc:         0.801600 loss:        0.658246
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.907400 loss:        0.274454
Test - acc:         0.848300 loss:        0.467692
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.905460 loss:        0.275973
Test - acc:         0.842100 loss:        0.521922
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.901360 loss:        0.286016
Test - acc:         0.859400 loss:        0.431423
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.907020 loss:        0.273776
Test - acc:         0.842400 loss:        0.489373
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.903780 loss:        0.280559
Test - acc:         0.875400 loss:        0.367256
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.907600 loss:        0.273324
Test - acc:         0.852400 loss:        0.456968
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.904980 loss:        0.276573
Test - acc:         0.848700 loss:        0.477704
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.905720 loss:        0.277445
Test - acc:         0.845200 loss:        0.451738
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.905520 loss:        0.274649
Test - acc:         0.786300 loss:        0.698371
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.905240 loss:        0.279285
Test - acc:         0.840000 loss:        0.479111
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.905420 loss:        0.276532
Test - acc:         0.871400 loss:        0.395419
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.905540 loss:        0.278321
Test - acc:         0.865800 loss:        0.410768
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.906500 loss:        0.272852
Test - acc:         0.859000 loss:        0.443737
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.905740 loss:        0.277430
Test - acc:         0.856600 loss:        0.430718
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.905600 loss:        0.276657
Test - acc:         0.860500 loss:        0.431865
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.907760 loss:        0.270485
Test - acc:         0.855100 loss:        0.428410
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.905280 loss:        0.277871
Test - acc:         0.869000 loss:        0.388900
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.904500 loss:        0.277091
Test - acc:         0.867400 loss:        0.396915
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.904800 loss:        0.280696
Test - acc:         0.872500 loss:        0.380016
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.907500 loss:        0.272001
Test - acc:         0.874700 loss:        0.374863
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.905340 loss:        0.277066
Test - acc:         0.839700 loss:        0.502376
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.906060 loss:        0.275524
Test - acc:         0.851800 loss:        0.462693
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.905700 loss:        0.276927
Test - acc:         0.860500 loss:        0.421755
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.903360 loss:        0.280680
Test - acc:         0.875000 loss:        0.373091
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.905840 loss:        0.274657
Test - acc:         0.872800 loss:        0.388056
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.907720 loss:        0.271406
Test - acc:         0.862900 loss:        0.412740
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.904540 loss:        0.278480
Test - acc:         0.848200 loss:        0.468361
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.909780 loss:        0.265983
Test - acc:         0.848800 loss:        0.469985
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.900560 loss:        0.285565
Test - acc:         0.834500 loss:        0.514648
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.904620 loss:        0.277922
Test - acc:         0.872000 loss:        0.379100
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.905860 loss:        0.276666
Test - acc:         0.875900 loss:        0.371266
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.905480 loss:        0.277107
Test - acc:         0.833000 loss:        0.542346
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.904500 loss:        0.275367
Test - acc:         0.852400 loss:        0.445912
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.904380 loss:        0.279499
Test - acc:         0.835400 loss:        0.525861
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.907220 loss:        0.277760
Test - acc:         0.858400 loss:        0.446402
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.907260 loss:        0.274348
Test - acc:         0.859900 loss:        0.434173
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.903860 loss:        0.278447
Test - acc:         0.878400 loss:        0.364992
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.905620 loss:        0.274359
Test - acc:         0.836200 loss:        0.512515
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.920480 loss:        0.227973
Test - acc:         0.886100 loss:        0.347195
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.918460 loss:        0.236076
Test - acc:         0.857600 loss:        0.430130
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.918320 loss:        0.236295
Test - acc:         0.876700 loss:        0.374049
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.916820 loss:        0.242882
Test - acc:         0.879800 loss:        0.369353
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.920580 loss:        0.232469
Test - acc:         0.872400 loss:        0.393528
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.917680 loss:        0.238797
Test - acc:         0.846200 loss:        0.492659
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.919640 loss:        0.233101
Test - acc:         0.856400 loss:        0.440217
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.920360 loss:        0.233182
Test - acc:         0.862300 loss:        0.437038
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.917400 loss:        0.237201
Test - acc:         0.873800 loss:        0.398292
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.920560 loss:        0.229776
Test - acc:         0.873100 loss:        0.390399
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.918140 loss:        0.236601
Test - acc:         0.866800 loss:        0.412921
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.919840 loss:        0.233200
Test - acc:         0.889700 loss:        0.332856
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.919140 loss:        0.234108
Test - acc:         0.882800 loss:        0.367149
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.919940 loss:        0.233111
Test - acc:         0.875400 loss:        0.378239
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.922180 loss:        0.227796
Test - acc:         0.864100 loss:        0.415661
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.919140 loss:        0.235264
Test - acc:         0.870900 loss:        0.402413
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.921600 loss:        0.228370
Test - acc:         0.891600 loss:        0.325421
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.919120 loss:        0.235794
Test - acc:         0.852800 loss:        0.466334
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.922020 loss:        0.227656
Test - acc:         0.869900 loss:        0.393639
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.920340 loss:        0.230451
Test - acc:         0.876700 loss:        0.375699
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.921740 loss:        0.229700
Test - acc:         0.884600 loss:        0.353773
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.921200 loss:        0.228281
Test - acc:         0.881600 loss:        0.370412
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.919180 loss:        0.233299
Test - acc:         0.888300 loss:        0.328642
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.917940 loss:        0.237378
Test - acc:         0.853400 loss:        0.467596
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.920560 loss:        0.229335
Test - acc:         0.869600 loss:        0.414047
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.919660 loss:        0.233149
Test - acc:         0.874200 loss:        0.383205
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.923000 loss:        0.225220
Test - acc:         0.843900 loss:        0.501244
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.921220 loss:        0.230579
Test - acc:         0.874100 loss:        0.386366
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.921480 loss:        0.230359
Test - acc:         0.841400 loss:        0.518538
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.918960 loss:        0.234997
Test - acc:         0.886100 loss:        0.358813
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.920540 loss:        0.230856
Test - acc:         0.877400 loss:        0.381564
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.920440 loss:        0.232863
Test - acc:         0.870900 loss:        0.414256
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.918180 loss:        0.235584
Test - acc:         0.784400 loss:        0.747123
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.922060 loss:        0.228094
Test - acc:         0.872500 loss:        0.393875
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.918460 loss:        0.234283
Test - acc:         0.865800 loss:        0.406827
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.920660 loss:        0.231061
Test - acc:         0.835400 loss:        0.510721
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.920900 loss:        0.228355
Test - acc:         0.877500 loss:        0.376644
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.920660 loss:        0.231177
Test - acc:         0.874800 loss:        0.357552
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.919880 loss:        0.233294
Test - acc:         0.879600 loss:        0.376144
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.919860 loss:        0.231599
Test - acc:         0.859500 loss:        0.445578
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.921160 loss:        0.230963
Test - acc:         0.876700 loss:        0.385750
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.921720 loss:        0.229943
Test - acc:         0.890800 loss:        0.340155
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.918320 loss:        0.235976
Test - acc:         0.884800 loss:        0.342464
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.922660 loss:        0.225040
Test - acc:         0.872100 loss:        0.407752
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.919800 loss:        0.234955
Test - acc:         0.876500 loss:        0.392614
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.920300 loss:        0.232154
Test - acc:         0.858300 loss:        0.429289
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.920620 loss:        0.231704
Test - acc:         0.865300 loss:        0.411062
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.919260 loss:        0.233562
Test - acc:         0.877800 loss:        0.369520
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.920920 loss:        0.231631
Test - acc:         0.874200 loss:        0.388672
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.918720 loss:        0.231807
Test - acc:         0.874300 loss:        0.385685
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.960640 loss:        0.118885
Test - acc:         0.935100 loss:        0.196558
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.970040 loss:        0.091045
Test - acc:         0.939300 loss:        0.188999
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.974700 loss:        0.078528
Test - acc:         0.938700 loss:        0.185372
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.977640 loss:        0.068003
Test - acc:         0.939700 loss:        0.188089
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.979060 loss:        0.063898
Test - acc:         0.938400 loss:        0.189652
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.981520 loss:        0.057270
Test - acc:         0.941500 loss:        0.191377
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.983640 loss:        0.051203
Test - acc:         0.939000 loss:        0.188923
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.983520 loss:        0.050130
Test - acc:         0.938600 loss:        0.190389
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.985600 loss:        0.045231
Test - acc:         0.938700 loss:        0.198142
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.986300 loss:        0.043664
Test - acc:         0.938900 loss:        0.197247
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.986840 loss:        0.040160
Test - acc:         0.937600 loss:        0.200318
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.988240 loss:        0.038093
Test - acc:         0.938500 loss:        0.203573
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.987820 loss:        0.038087
Test - acc:         0.936600 loss:        0.211639
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.988200 loss:        0.036835
Test - acc:         0.938100 loss:        0.213917
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.989920 loss:        0.032567
Test - acc:         0.937300 loss:        0.207440
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.989260 loss:        0.032609
Test - acc:         0.936700 loss:        0.214310
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.989580 loss:        0.032071
Test - acc:         0.936900 loss:        0.222694
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.989380 loss:        0.032650
Test - acc:         0.938300 loss:        0.218974
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.989060 loss:        0.033772
Test - acc:         0.935400 loss:        0.225945
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.989820 loss:        0.032168
Test - acc:         0.936200 loss:        0.222839
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.989900 loss:        0.031901
Test - acc:         0.935800 loss:        0.231954
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.989700 loss:        0.031795
Test - acc:         0.937900 loss:        0.216705
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.989380 loss:        0.032850
Test - acc:         0.932900 loss:        0.238170
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.989380 loss:        0.032965
Test - acc:         0.935500 loss:        0.221804
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.989440 loss:        0.033186
Test - acc:         0.932500 loss:        0.235998
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.989180 loss:        0.032890
Test - acc:         0.934900 loss:        0.223775
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.988960 loss:        0.033928
Test - acc:         0.936600 loss:        0.224361
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.989740 loss:        0.032117
Test - acc:         0.937200 loss:        0.232373
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.989680 loss:        0.032322
Test - acc:         0.934700 loss:        0.238933
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.988280 loss:        0.036133
Test - acc:         0.934300 loss:        0.230233
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.987820 loss:        0.036518
Test - acc:         0.931900 loss:        0.240624
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.988200 loss:        0.036001
Test - acc:         0.929900 loss:        0.257372
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.987340 loss:        0.037843
Test - acc:         0.930600 loss:        0.240433
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.987780 loss:        0.037059
Test - acc:         0.935500 loss:        0.236315
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.988900 loss:        0.036710
Test - acc:         0.929000 loss:        0.250570
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.987720 loss:        0.036873
Test - acc:         0.934900 loss:        0.241670
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.988180 loss:        0.038359
Test - acc:         0.927100 loss:        0.258584
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.986500 loss:        0.040938
Test - acc:         0.934900 loss:        0.232233
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.988220 loss:        0.037367
Test - acc:         0.935600 loss:        0.235000
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.987400 loss:        0.039437
Test - acc:         0.934500 loss:        0.239802
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.987360 loss:        0.037899
Test - acc:         0.934400 loss:        0.236319
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.986800 loss:        0.040713
Test - acc:         0.924300 loss:        0.271995
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.986640 loss:        0.041201
Test - acc:         0.927300 loss:        0.256073
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.987760 loss:        0.038596
Test - acc:         0.933500 loss:        0.240149
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.985820 loss:        0.043338
Test - acc:         0.924000 loss:        0.263385
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.985840 loss:        0.042374
Test - acc:         0.922200 loss:        0.286094
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.986100 loss:        0.042749
Test - acc:         0.929300 loss:        0.249337
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.985660 loss:        0.043940
Test - acc:         0.929800 loss:        0.254784
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.987040 loss:        0.040726
Test - acc:         0.932100 loss:        0.253009
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.986140 loss:        0.042577
Test - acc:         0.933500 loss:        0.239687
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.948860 loss:        0.145592
Test - acc:         0.919700 loss:        0.255017
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.961500 loss:        0.112526
Test - acc:         0.919100 loss:        0.264021
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.962880 loss:        0.108239
Test - acc:         0.918100 loss:        0.258661
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.965500 loss:        0.099626
Test - acc:         0.913200 loss:        0.273413
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.966720 loss:        0.095599
Test - acc:         0.920500 loss:        0.248042
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.969740 loss:        0.088239
Test - acc:         0.919700 loss:        0.252231
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.969420 loss:        0.087865
Test - acc:         0.924700 loss:        0.239090
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.969880 loss:        0.086606
Test - acc:         0.926800 loss:        0.246858
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.970560 loss:        0.085315
Test - acc:         0.918400 loss:        0.276455
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.973140 loss:        0.079135
Test - acc:         0.926700 loss:        0.244067
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.970500 loss:        0.083365
Test - acc:         0.923900 loss:        0.253452
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.972480 loss:        0.077573
Test - acc:         0.924800 loss:        0.243566
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.973620 loss:        0.077414
Test - acc:         0.919900 loss:        0.260017
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.973860 loss:        0.076007
Test - acc:         0.924200 loss:        0.252547
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.972660 loss:        0.078027
Test - acc:         0.922900 loss:        0.254411
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.974780 loss:        0.071056
Test - acc:         0.920500 loss:        0.273241
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.974320 loss:        0.074387
Test - acc:         0.925700 loss:        0.242291
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.975000 loss:        0.072992
Test - acc:         0.922300 loss:        0.254939
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.975160 loss:        0.073280
Test - acc:         0.924400 loss:        0.249883
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.975640 loss:        0.070150
Test - acc:         0.922400 loss:        0.259818
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.975860 loss:        0.069923
Test - acc:         0.920700 loss:        0.273083
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.977640 loss:        0.066653
Test - acc:         0.922100 loss:        0.252206
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.975400 loss:        0.072560
Test - acc:         0.925000 loss:        0.251304
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.977500 loss:        0.067340
Test - acc:         0.928500 loss:        0.239968
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.976500 loss:        0.067642
Test - acc:         0.923500 loss:        0.256477
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.976520 loss:        0.067439
Test - acc:         0.928600 loss:        0.247262
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.978200 loss:        0.066344
Test - acc:         0.921700 loss:        0.270557
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.976540 loss:        0.066438
Test - acc:         0.925400 loss:        0.253632
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.979620 loss:        0.061806
Test - acc:         0.924800 loss:        0.249043
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.976580 loss:        0.065552
Test - acc:         0.918000 loss:        0.278741
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.978300 loss:        0.063649
Test - acc:         0.921600 loss:        0.273391
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.978020 loss:        0.064190
Test - acc:         0.925200 loss:        0.242892
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.978700 loss:        0.062557
Test - acc:         0.923700 loss:        0.260652
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.977260 loss:        0.065794
Test - acc:         0.926600 loss:        0.249914
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.978040 loss:        0.065969
Test - acc:         0.929100 loss:        0.253876
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.978660 loss:        0.062569
Test - acc:         0.923500 loss:        0.267046
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.978660 loss:        0.063520
Test - acc:         0.929200 loss:        0.233292
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.978940 loss:        0.061299
Test - acc:         0.919400 loss:        0.277741
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.979180 loss:        0.062287
Test - acc:         0.919100 loss:        0.272522
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.977980 loss:        0.065967
Test - acc:         0.924400 loss:        0.256339
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.978700 loss:        0.063860
Test - acc:         0.928700 loss:        0.243192
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.977680 loss:        0.065181
Test - acc:         0.918800 loss:        0.276836
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.979340 loss:        0.060037
Test - acc:         0.924400 loss:        0.265511
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.978820 loss:        0.062115
Test - acc:         0.922700 loss:        0.265672
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.978920 loss:        0.061104
Test - acc:         0.919900 loss:        0.265622
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.979800 loss:        0.060182
Test - acc:         0.922000 loss:        0.271833
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.978420 loss:        0.063309
Test - acc:         0.925400 loss:        0.251748
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.978080 loss:        0.063584
Test - acc:         0.922900 loss:        0.263520
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.979460 loss:        0.062181
Test - acc:         0.922500 loss:        0.266173
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.980380 loss:        0.060008
Test - acc:         0.928500 loss:        0.242007
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.908440 loss:        0.263974
Test - acc:         0.901200 loss:        0.301185
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.936080 loss:        0.185293
Test - acc:         0.909600 loss:        0.275264
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.942260 loss:        0.161780
Test - acc:         0.911800 loss:        0.269121
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.947980 loss:        0.149716
Test - acc:         0.914700 loss:        0.262279
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.953140 loss:        0.138346
Test - acc:         0.917700 loss:        0.258068
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.954160 loss:        0.132188
Test - acc:         0.918100 loss:        0.253487
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.956860 loss:        0.126222
Test - acc:         0.920000 loss:        0.250667
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.958420 loss:        0.121182
Test - acc:         0.920300 loss:        0.248051
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.960980 loss:        0.113376
Test - acc:         0.918700 loss:        0.250321
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.962640 loss:        0.110318
Test - acc:         0.921600 loss:        0.245341
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.963140 loss:        0.108995
Test - acc:         0.922400 loss:        0.240279
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.964820 loss:        0.103672
Test - acc:         0.923300 loss:        0.241132
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.965380 loss:        0.102635
Test - acc:         0.922700 loss:        0.240971
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.967240 loss:        0.097492
Test - acc:         0.920600 loss:        0.243744
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.965940 loss:        0.098538
Test - acc:         0.921900 loss:        0.240447
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.967120 loss:        0.096360
Test - acc:         0.923600 loss:        0.241271
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.967920 loss:        0.093962
Test - acc:         0.921800 loss:        0.243757
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.969040 loss:        0.090797
Test - acc:         0.923700 loss:        0.241541
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.969760 loss:        0.089501
Test - acc:         0.922600 loss:        0.243271
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.971040 loss:        0.086843
Test - acc:         0.922500 loss:        0.241433
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.971500 loss:        0.084973
Test - acc:         0.924300 loss:        0.236569
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.970600 loss:        0.086790
Test - acc:         0.924500 loss:        0.241442
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.973100 loss:        0.081667
Test - acc:         0.923900 loss:        0.241900
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.971800 loss:        0.082875
Test - acc:         0.923000 loss:        0.238217
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.974460 loss:        0.078500
Test - acc:         0.923400 loss:        0.239301
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.973320 loss:        0.079405
Test - acc:         0.923800 loss:        0.243874
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.972860 loss:        0.079372
Test - acc:         0.924500 loss:        0.241796
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.974260 loss:        0.078620
Test - acc:         0.923800 loss:        0.241365
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.975260 loss:        0.075542
Test - acc:         0.923800 loss:        0.246021
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.975020 loss:        0.076778
Test - acc:         0.926100 loss:        0.239916
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.975120 loss:        0.074807
Test - acc:         0.922600 loss:        0.240864
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.975520 loss:        0.073543
Test - acc:         0.923800 loss:        0.241656
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.975580 loss:        0.072303
Test - acc:         0.924200 loss:        0.240765
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.976480 loss:        0.070980
Test - acc:         0.927400 loss:        0.242237
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.976420 loss:        0.070087
Test - acc:         0.926900 loss:        0.242142
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.976740 loss:        0.070650
Test - acc:         0.924800 loss:        0.241549
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.977500 loss:        0.068009
Test - acc:         0.926600 loss:        0.239622
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.977920 loss:        0.067653
Test - acc:         0.925100 loss:        0.244462
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.978100 loss:        0.066817
Test - acc:         0.927900 loss:        0.238832
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.978440 loss:        0.066315
Test - acc:         0.924500 loss:        0.244090
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.978020 loss:        0.066569
Test - acc:         0.925700 loss:        0.242857
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.979360 loss:        0.064398
Test - acc:         0.925200 loss:        0.242867
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.979200 loss:        0.064723
Test - acc:         0.924000 loss:        0.247026
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.979040 loss:        0.064678
Test - acc:         0.926600 loss:        0.245831
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.979200 loss:        0.063472
Test - acc:         0.925700 loss:        0.245129
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.979620 loss:        0.062735
Test - acc:         0.927200 loss:        0.242638
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.980540 loss:        0.061889
Test - acc:         0.925000 loss:        0.245286
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.980080 loss:        0.062185
Test - acc:         0.926900 loss:        0.243645
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.979120 loss:        0.063188
Test - acc:         0.924500 loss:        0.245861
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.980280 loss:        0.059842
Test - acc:         0.925000 loss:        0.245614
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.754260 loss:        0.713789
Test - acc:         0.804000 loss:        0.570428
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.821660 loss:        0.515903
Test - acc:         0.832600 loss:        0.495655
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.839500 loss:        0.461630
Test - acc:         0.840000 loss:        0.469165
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.849740 loss:        0.430492
Test - acc:         0.849600 loss:        0.447158
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.858740 loss:        0.411482
Test - acc:         0.851900 loss:        0.440842
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.864180 loss:        0.392449
Test - acc:         0.852600 loss:        0.434311
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.869120 loss:        0.380411
Test - acc:         0.858700 loss:        0.421021
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.872680 loss:        0.369641
Test - acc:         0.861300 loss:        0.413529
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.874660 loss:        0.360969
Test - acc:         0.860200 loss:        0.408485
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.877140 loss:        0.357100
Test - acc:         0.862700 loss:        0.405432
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.879380 loss:        0.348753
Test - acc:         0.864000 loss:        0.403229
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.881440 loss:        0.343389
Test - acc:         0.865500 loss:        0.399812
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.882900 loss:        0.337922
Test - acc:         0.867300 loss:        0.393372
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.885080 loss:        0.331229
Test - acc:         0.871800 loss:        0.384474
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.885980 loss:        0.327932
Test - acc:         0.872400 loss:        0.385483
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.890600 loss:        0.320525
Test - acc:         0.875400 loss:        0.378611
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.889640 loss:        0.319622
Test - acc:         0.874000 loss:        0.372324
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.890660 loss:        0.316141
Test - acc:         0.873400 loss:        0.378389
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.893320 loss:        0.310034
Test - acc:         0.875500 loss:        0.376757
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.893380 loss:        0.308265
Test - acc:         0.872900 loss:        0.375728
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.894440 loss:        0.305738
Test - acc:         0.873600 loss:        0.377837
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.898020 loss:        0.301891
Test - acc:         0.875000 loss:        0.371413
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.894580 loss:        0.302300
Test - acc:         0.874000 loss:        0.374465
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.897800 loss:        0.297671
Test - acc:         0.878200 loss:        0.371126
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.898320 loss:        0.293286
Test - acc:         0.879300 loss:        0.369162
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.897400 loss:        0.295664
Test - acc:         0.878900 loss:        0.361651
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.898580 loss:        0.290950
Test - acc:         0.881000 loss:        0.362287
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.900500 loss:        0.289090
Test - acc:         0.877800 loss:        0.374027
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.899260 loss:        0.287848
Test - acc:         0.880500 loss:        0.356846
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.899500 loss:        0.289790
Test - acc:         0.880300 loss:        0.359840
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.901520 loss:        0.285188
Test - acc:         0.877900 loss:        0.368262
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.902720 loss:        0.281223
Test - acc:         0.882700 loss:        0.351975
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.902260 loss:        0.280296
Test - acc:         0.880100 loss:        0.359082
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.902660 loss:        0.280867
Test - acc:         0.883600 loss:        0.354713
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.903480 loss:        0.276611
Test - acc:         0.881400 loss:        0.362691
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.903020 loss:        0.278611
Test - acc:         0.879800 loss:        0.357592
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.905520 loss:        0.273832
Test - acc:         0.880600 loss:        0.354120
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.904980 loss:        0.274588
Test - acc:         0.880300 loss:        0.356161
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.905600 loss:        0.276106
Test - acc:         0.881200 loss:        0.352011
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.905860 loss:        0.268605
Test - acc:         0.882700 loss:        0.353825
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.906900 loss:        0.268982
Test - acc:         0.882500 loss:        0.354609
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.907600 loss:        0.266375
Test - acc:         0.879700 loss:        0.355574
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.906240 loss:        0.266772
Test - acc:         0.881200 loss:        0.362812
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.907240 loss:        0.265500
Test - acc:         0.881600 loss:        0.354892
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.909580 loss:        0.262755
Test - acc:         0.881400 loss:        0.350967
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.906420 loss:        0.264937
Test - acc:         0.880500 loss:        0.356850
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.909700 loss:        0.261164
Test - acc:         0.882300 loss:        0.352600
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.908640 loss:        0.262967
Test - acc:         0.881600 loss:        0.352299
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.909820 loss:        0.260291
Test - acc:         0.880200 loss:        0.357932
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.909240 loss:        0.261017
Test - acc:         0.884600 loss:        0.355839
Sparsity :          0.9844
Wdecay :        0.000500
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "topflip",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "logdir": "scaling_noise_experiment",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 0.5,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": null,
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.333960 loss:        1.862106
Test - acc:         0.432100 loss:        1.510146
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.501880 loss:        1.360095
Test - acc:         0.581500 loss:        1.161949
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.611680 loss:        1.085898
Test - acc:         0.632500 loss:        1.033598
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.675440 loss:        0.918068
Test - acc:         0.653800 loss:        0.976649
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.716420 loss:        0.803278
Test - acc:         0.695800 loss:        0.911705
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.758180 loss:        0.691189
Test - acc:         0.747000 loss:        0.737007
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.787000 loss:        0.616411
Test - acc:         0.755700 loss:        0.715024
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.803400 loss:        0.567391
Test - acc:         0.784900 loss:        0.626498
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.813300 loss:        0.544930
Test - acc:         0.756300 loss:        0.710817
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.823300 loss:        0.513112
Test - acc:         0.773800 loss:        0.677750
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.829420 loss:        0.496505
Test - acc:         0.760100 loss:        0.730440
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.834200 loss:        0.480800
Test - acc:         0.796400 loss:        0.631096
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.840580 loss:        0.462461
Test - acc:         0.782600 loss:        0.633116
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.843820 loss:        0.457057
Test - acc:         0.758700 loss:        0.733857
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.847020 loss:        0.445181
Test - acc:         0.772900 loss:        0.740408
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.851860 loss:        0.432313
Test - acc:         0.832100 loss:        0.495510
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.855680 loss:        0.424025
Test - acc:         0.827400 loss:        0.502972
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.858140 loss:        0.416248
Test - acc:         0.770800 loss:        0.755412
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.861200 loss:        0.408111
Test - acc:         0.817600 loss:        0.564910
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.862380 loss:        0.400156
Test - acc:         0.835300 loss:        0.509238
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.865620 loss:        0.396328
Test - acc:         0.841100 loss:        0.484278
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.865420 loss:        0.394472
Test - acc:         0.769300 loss:        0.709420
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.864760 loss:        0.393104
Test - acc:         0.790800 loss:        0.665163
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.870180 loss:        0.382956
Test - acc:         0.838300 loss:        0.473115
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.869380 loss:        0.382446
Test - acc:         0.794500 loss:        0.644293
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.866220 loss:        0.389617
Test - acc:         0.817300 loss:        0.557268
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.872060 loss:        0.376745
Test - acc:         0.834400 loss:        0.491060
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.869460 loss:        0.378626
Test - acc:         0.838400 loss:        0.474066
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.873580 loss:        0.367146
Test - acc:         0.841900 loss:        0.482488
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.875080 loss:        0.368154
Test - acc:         0.832500 loss:        0.484733
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.873820 loss:        0.371724
Test - acc:         0.830400 loss:        0.512692
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.876040 loss:        0.368693
Test - acc:         0.832200 loss:        0.513597
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.876120 loss:        0.364386
Test - acc:         0.791300 loss:        0.649783
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.876060 loss:        0.363972
Test - acc:         0.793800 loss:        0.644325
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.877340 loss:        0.360683
Test - acc:         0.835700 loss:        0.489128
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.877080 loss:        0.361789
Test - acc:         0.842700 loss:        0.480509
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.878040 loss:        0.358002
Test - acc:         0.860000 loss:        0.417958
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.878600 loss:        0.357547
Test - acc:         0.817400 loss:        0.613081
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.877200 loss:        0.357764
Test - acc:         0.851800 loss:        0.443996
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.890260 loss:        0.321160
Test - acc:         0.862100 loss:        0.402685
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.888680 loss:        0.327940
Test - acc:         0.867100 loss:        0.379096
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.892720 loss:        0.313816
Test - acc:         0.837300 loss:        0.490964
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.892000 loss:        0.312455
Test - acc:         0.832900 loss:        0.501018
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.891640 loss:        0.315666
Test - acc:         0.868900 loss:        0.408379
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.895020 loss:        0.309337
Test - acc:         0.848600 loss:        0.462981
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.893840 loss:        0.310814
Test - acc:         0.842700 loss:        0.496698
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.893860 loss:        0.308302
Test - acc:         0.861000 loss:        0.407778
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.892340 loss:        0.313743
Test - acc:         0.835200 loss:        0.505858
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.896160 loss:        0.304872
Test - acc:         0.810700 loss:        0.604858
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.894840 loss:        0.309532
Test - acc:         0.843500 loss:        0.475449
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.894320 loss:        0.307023
Test - acc:         0.839500 loss:        0.503876
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.895860 loss:        0.305045
Test - acc:         0.843700 loss:        0.481144
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.896360 loss:        0.305904
Test - acc:         0.853000 loss:        0.454421
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.895960 loss:        0.300715
Test - acc:         0.865200 loss:        0.416108
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.896320 loss:        0.304942
Test - acc:         0.839800 loss:        0.496242
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.895160 loss:        0.305016
Test - acc:         0.833600 loss:        0.525422
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.895960 loss:        0.305114
Test - acc:         0.853500 loss:        0.438209
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.896360 loss:        0.301910
Test - acc:         0.843000 loss:        0.490739
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.895220 loss:        0.304899
Test - acc:         0.862600 loss:        0.412827
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.895500 loss:        0.305751
Test - acc:         0.835800 loss:        0.495455
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.896940 loss:        0.299082
Test - acc:         0.870900 loss:        0.390876
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.892320 loss:        0.312928
Test - acc:         0.847200 loss:        0.467251
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.896160 loss:        0.301858
Test - acc:         0.866300 loss:        0.403748
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.895960 loss:        0.303223
Test - acc:         0.798000 loss:        0.665210
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.895340 loss:        0.310188
Test - acc:         0.829200 loss:        0.522307
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.896620 loss:        0.300564
Test - acc:         0.836900 loss:        0.524131
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.897020 loss:        0.300379
Test - acc:         0.865200 loss:        0.409907
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.896400 loss:        0.304034
Test - acc:         0.848100 loss:        0.479729
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.895540 loss:        0.304240
Test - acc:         0.861500 loss:        0.419925
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.897780 loss:        0.300490
Test - acc:         0.757700 loss:        0.814118
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.896840 loss:        0.301691
Test - acc:         0.847600 loss:        0.459583
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.894040 loss:        0.307531
Test - acc:         0.856100 loss:        0.431020
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.896300 loss:        0.303018
Test - acc:         0.856100 loss:        0.439431
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.897440 loss:        0.300462
Test - acc:         0.846800 loss:        0.466390
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.894960 loss:        0.306837
Test - acc:         0.806800 loss:        0.618026
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.896900 loss:        0.300702
Test - acc:         0.800000 loss:        0.652606
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.897020 loss:        0.302224
Test - acc:         0.855900 loss:        0.434421
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.896240 loss:        0.299471
Test - acc:         0.852800 loss:        0.442621
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.911940 loss:        0.259695
Test - acc:         0.863700 loss:        0.386680
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.908720 loss:        0.265591
Test - acc:         0.835000 loss:        0.529192
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.909060 loss:        0.266899
Test - acc:         0.855100 loss:        0.445841
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.907960 loss:        0.266229
Test - acc:         0.877300 loss:        0.373992
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.909720 loss:        0.266038
Test - acc:         0.871500 loss:        0.396641
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.909620 loss:        0.260764
Test - acc:         0.859800 loss:        0.436120
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.906540 loss:        0.268817
Test - acc:         0.863100 loss:        0.438066
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.907920 loss:        0.263176
Test - acc:         0.865400 loss:        0.412460
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.910500 loss:        0.262942
Test - acc:         0.869700 loss:        0.390282
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.912160 loss:        0.257507
Test - acc:         0.868800 loss:        0.387021
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.908780 loss:        0.263587
Test - acc:         0.841300 loss:        0.485464
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.910620 loss:        0.259600
Test - acc:         0.868800 loss:        0.390216
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.910040 loss:        0.264760
Test - acc:         0.861800 loss:        0.408206
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.911240 loss:        0.257691
Test - acc:         0.875500 loss:        0.383144
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.912040 loss:        0.259026
Test - acc:         0.865100 loss:        0.433761
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.907020 loss:        0.269818
Test - acc:         0.849700 loss:        0.461640
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.912520 loss:        0.257281
Test - acc:         0.840500 loss:        0.497298
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.910540 loss:        0.259558
Test - acc:         0.875200 loss:        0.385735
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.909160 loss:        0.265426
Test - acc:         0.873900 loss:        0.397872
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.910400 loss:        0.260629
Test - acc:         0.855500 loss:        0.442358
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.910040 loss:        0.263206
Test - acc:         0.865500 loss:        0.427733
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.909800 loss:        0.261863
Test - acc:         0.864400 loss:        0.411881
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.909380 loss:        0.260977
Test - acc:         0.869800 loss:        0.388163
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.911140 loss:        0.257363
Test - acc:         0.825500 loss:        0.546951
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.908140 loss:        0.262948
Test - acc:         0.881600 loss:        0.370195
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.910080 loss:        0.261855
Test - acc:         0.868500 loss:        0.392458
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.909900 loss:        0.261941
Test - acc:         0.859000 loss:        0.435299
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.910280 loss:        0.264721
Test - acc:         0.850600 loss:        0.461697
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.910840 loss:        0.259575
Test - acc:         0.862500 loss:        0.434771
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.912180 loss:        0.256429
Test - acc:         0.865000 loss:        0.409186
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.911800 loss:        0.256942
Test - acc:         0.881500 loss:        0.358653
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.908120 loss:        0.260969
Test - acc:         0.857000 loss:        0.430765
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.911940 loss:        0.259266
Test - acc:         0.877000 loss:        0.373867
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.910300 loss:        0.263647
Test - acc:         0.870500 loss:        0.396767
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.910760 loss:        0.259715
Test - acc:         0.880400 loss:        0.359974
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.910120 loss:        0.260772
Test - acc:         0.880100 loss:        0.371294
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.909140 loss:        0.260796
Test - acc:         0.840900 loss:        0.503318
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.909500 loss:        0.262411
Test - acc:         0.855500 loss:        0.453599
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.913300 loss:        0.252093
Test - acc:         0.855200 loss:        0.443623
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.916220 loss:        0.240625
Test - acc:         0.819800 loss:        0.552570
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.915800 loss:        0.244508
Test - acc:         0.860700 loss:        0.425939
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.915620 loss:        0.246800
Test - acc:         0.841800 loss:        0.516184
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.918380 loss:        0.237792
Test - acc:         0.831700 loss:        0.544372
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.917000 loss:        0.241310
Test - acc:         0.876500 loss:        0.379521
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.917080 loss:        0.239332
Test - acc:         0.862700 loss:        0.418498
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.918400 loss:        0.237576
Test - acc:         0.843200 loss:        0.521408
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.917660 loss:        0.239371
Test - acc:         0.877300 loss:        0.382455
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.915940 loss:        0.240843
Test - acc:         0.884800 loss:        0.355288
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.916880 loss:        0.238669
Test - acc:         0.855000 loss:        0.457705
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.917940 loss:        0.236390
Test - acc:         0.865500 loss:        0.416854
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.916280 loss:        0.240794
Test - acc:         0.889400 loss:        0.345028
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.918140 loss:        0.241222
Test - acc:         0.854600 loss:        0.460726
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.918500 loss:        0.232895
Test - acc:         0.856800 loss:        0.424880
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.918180 loss:        0.237536
Test - acc:         0.877300 loss:        0.381759
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.918980 loss:        0.237846
Test - acc:         0.829100 loss:        0.538086
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.918520 loss:        0.239463
Test - acc:         0.882100 loss:        0.360883
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.915960 loss:        0.240520
Test - acc:         0.872000 loss:        0.393838
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.917900 loss:        0.235718
Test - acc:         0.862400 loss:        0.415052
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.917940 loss:        0.237733
Test - acc:         0.864900 loss:        0.428711
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.917520 loss:        0.238141
Test - acc:         0.841400 loss:        0.485884
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.916900 loss:        0.240393
Test - acc:         0.885600 loss:        0.348481
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.918260 loss:        0.238646
Test - acc:         0.807700 loss:        0.653354
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.916600 loss:        0.242625
Test - acc:         0.861700 loss:        0.435419
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.918660 loss:        0.235431
Test - acc:         0.868600 loss:        0.417276
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.917580 loss:        0.239651
Test - acc:         0.882100 loss:        0.362572
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.919580 loss:        0.233534
Test - acc:         0.873600 loss:        0.397683
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.917800 loss:        0.238256
Test - acc:         0.875600 loss:        0.383271
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.916520 loss:        0.240938
Test - acc:         0.864200 loss:        0.410106
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.918340 loss:        0.238262
Test - acc:         0.864700 loss:        0.429713
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.917440 loss:        0.238353
Test - acc:         0.880100 loss:        0.356391
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.918880 loss:        0.234881
Test - acc:         0.880800 loss:        0.370446
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.918620 loss:        0.236056
Test - acc:         0.866100 loss:        0.400424
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.954240 loss:        0.136034
Test - acc:         0.930800 loss:        0.208718
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.965280 loss:        0.103899
Test - acc:         0.934100 loss:        0.201084
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.969160 loss:        0.091190
Test - acc:         0.933900 loss:        0.203933
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.975320 loss:        0.077072
Test - acc:         0.935200 loss:        0.204491
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.975520 loss:        0.074338
Test - acc:         0.936400 loss:        0.208948
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.978020 loss:        0.067338
Test - acc:         0.935400 loss:        0.206926
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.972900 loss:        0.081833
Test - acc:         0.935000 loss:        0.205635
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.975040 loss:        0.074017
Test - acc:         0.935200 loss:        0.208277
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.977500 loss:        0.067906
Test - acc:         0.934700 loss:        0.216626
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.979140 loss:        0.064437
Test - acc:         0.933000 loss:        0.217439
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.980760 loss:        0.059598
Test - acc:         0.932400 loss:        0.222239
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.057728
Test - acc:         0.934300 loss:        0.220912
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.981480 loss:        0.055635
Test - acc:         0.934900 loss:        0.218493
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.983600 loss:        0.052077
Test - acc:         0.934000 loss:        0.219571
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.983920 loss:        0.049107
Test - acc:         0.933000 loss:        0.221748
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.984360 loss:        0.049190
Test - acc:         0.933800 loss:        0.220825
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.984120 loss:        0.048793
Test - acc:         0.932400 loss:        0.224459
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.985040 loss:        0.047085
Test - acc:         0.933100 loss:        0.225708
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.983760 loss:        0.048978
Test - acc:         0.930600 loss:        0.242734
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.985280 loss:        0.045985
Test - acc:         0.931400 loss:        0.244427
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.985320 loss:        0.044970
Test - acc:         0.929500 loss:        0.240507
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.985100 loss:        0.046158
Test - acc:         0.930200 loss:        0.231770
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.984820 loss:        0.046462
Test - acc:         0.930700 loss:        0.239339
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.985000 loss:        0.045790
Test - acc:         0.930900 loss:        0.238571
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.985980 loss:        0.044277
Test - acc:         0.928500 loss:        0.254954
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.985740 loss:        0.044270
Test - acc:         0.922400 loss:        0.278000
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.984880 loss:        0.045727
Test - acc:         0.929500 loss:        0.263024
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.984340 loss:        0.048048
Test - acc:         0.929100 loss:        0.248433
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.985260 loss:        0.044647
Test - acc:         0.927200 loss:        0.251595
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.983520 loss:        0.049423
Test - acc:         0.929800 loss:        0.243415
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.984240 loss:        0.047047
Test - acc:         0.928100 loss:        0.250355
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.984540 loss:        0.048058
Test - acc:         0.927500 loss:        0.255962
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.983440 loss:        0.050124
Test - acc:         0.926900 loss:        0.248744
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.983000 loss:        0.050221
Test - acc:         0.928200 loss:        0.249250
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.983480 loss:        0.050886
Test - acc:         0.932000 loss:        0.241890
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.983560 loss:        0.049125
Test - acc:         0.926700 loss:        0.259115
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.983820 loss:        0.047967
Test - acc:         0.923100 loss:        0.267816
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.982820 loss:        0.052083
Test - acc:         0.927500 loss:        0.255374
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.983280 loss:        0.051704
Test - acc:         0.926500 loss:        0.256106
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.981840 loss:        0.054415
Test - acc:         0.925800 loss:        0.262406
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.982800 loss:        0.052676
Test - acc:         0.929900 loss:        0.240904
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.983060 loss:        0.051093
Test - acc:         0.923800 loss:        0.266978
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.983000 loss:        0.050919
Test - acc:         0.925800 loss:        0.265256
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.982440 loss:        0.052113
Test - acc:         0.922700 loss:        0.268288
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.982260 loss:        0.053769
Test - acc:         0.923000 loss:        0.261521
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.948980 loss:        0.148192
Test - acc:         0.913400 loss:        0.279798
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.957800 loss:        0.122589
Test - acc:         0.906600 loss:        0.297138
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.959300 loss:        0.115740
Test - acc:         0.909300 loss:        0.295240
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.962580 loss:        0.108767
Test - acc:         0.913300 loss:        0.271943
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.963900 loss:        0.104185
Test - acc:         0.922000 loss:        0.244103
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.965020 loss:        0.101083
Test - acc:         0.922200 loss:        0.254596
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.966120 loss:        0.097128
Test - acc:         0.923100 loss:        0.253971
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.968020 loss:        0.093293
Test - acc:         0.918200 loss:        0.279160
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.967940 loss:        0.093747
Test - acc:         0.916100 loss:        0.265949
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.969200 loss:        0.090619
Test - acc:         0.920900 loss:        0.259005
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.970320 loss:        0.087822
Test - acc:         0.919800 loss:        0.263318
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.967160 loss:        0.092940
Test - acc:         0.919000 loss:        0.262285
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.969240 loss:        0.089976
Test - acc:         0.916600 loss:        0.282610
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.969720 loss:        0.088319
Test - acc:         0.916200 loss:        0.277158
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.970220 loss:        0.087761
Test - acc:         0.921600 loss:        0.266621
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.969180 loss:        0.089247
Test - acc:         0.917600 loss:        0.281491
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.970480 loss:        0.086736
Test - acc:         0.921900 loss:        0.254945
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.969940 loss:        0.088393
Test - acc:         0.922000 loss:        0.262964
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.970100 loss:        0.086245
Test - acc:         0.919100 loss:        0.271317
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.971060 loss:        0.084203
Test - acc:         0.922300 loss:        0.256948
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.971060 loss:        0.083791
Test - acc:         0.918000 loss:        0.275670
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.971260 loss:        0.084277
Test - acc:         0.921000 loss:        0.261615
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.969940 loss:        0.086317
Test - acc:         0.922400 loss:        0.258783
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.971700 loss:        0.081782
Test - acc:         0.920500 loss:        0.274161
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.972620 loss:        0.080718
Test - acc:         0.921700 loss:        0.268990
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.972560 loss:        0.083179
Test - acc:         0.924400 loss:        0.258129
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.972020 loss:        0.080166
Test - acc:         0.916900 loss:        0.282040
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.971880 loss:        0.081972
Test - acc:         0.920400 loss:        0.261863
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.971520 loss:        0.082411
Test - acc:         0.919700 loss:        0.271845
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.972460 loss:        0.080489
Test - acc:         0.924500 loss:        0.260481
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.972040 loss:        0.080130
Test - acc:         0.922000 loss:        0.263896
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.972780 loss:        0.077802
Test - acc:         0.918800 loss:        0.281396
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.972740 loss:        0.079916
Test - acc:         0.917600 loss:        0.285178
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.972820 loss:        0.079214
Test - acc:         0.921100 loss:        0.269244
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.973420 loss:        0.077989
Test - acc:         0.918100 loss:        0.283983
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.973400 loss:        0.077524
Test - acc:         0.918300 loss:        0.278856
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.972620 loss:        0.079117
Test - acc:         0.920000 loss:        0.268306
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.971780 loss:        0.080874
Test - acc:         0.922200 loss:        0.257983
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.972460 loss:        0.080641
Test - acc:         0.919100 loss:        0.275098
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.900940 loss:        0.285625
Test - acc:         0.886100 loss:        0.360487
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.921320 loss:        0.226342
Test - acc:         0.887400 loss:        0.349724
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.928720 loss:        0.204859
Test - acc:         0.893300 loss:        0.321981
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.931120 loss:        0.195495
Test - acc:         0.892000 loss:        0.327029
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.934240 loss:        0.187584
Test - acc:         0.896100 loss:        0.311829
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.934420 loss:        0.185390
Test - acc:         0.900700 loss:        0.304949
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.938980 loss:        0.177450
Test - acc:         0.897800 loss:        0.327916
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.939620 loss:        0.174568
Test - acc:         0.901800 loss:        0.302272
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.938520 loss:        0.173191
Test - acc:         0.901500 loss:        0.301000
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.940260 loss:        0.171239
Test - acc:         0.906700 loss:        0.287869
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.942320 loss:        0.164478
Test - acc:         0.900100 loss:        0.307052
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.944580 loss:        0.161684
Test - acc:         0.902600 loss:        0.309126
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.941680 loss:        0.165740
Test - acc:         0.907500 loss:        0.291829
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.943980 loss:        0.159307
Test - acc:         0.896600 loss:        0.319964
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.945120 loss:        0.157617
Test - acc:         0.891300 loss:        0.331175
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.942460 loss:        0.161338
Test - acc:         0.903800 loss:        0.298593
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.956340 loss:        0.126672
Test - acc:         0.918600 loss:        0.249491
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.963480 loss:        0.109597
Test - acc:         0.919400 loss:        0.247421
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.965160 loss:        0.103042
Test - acc:         0.920800 loss:        0.244036
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.965720 loss:        0.101803
Test - acc:         0.922600 loss:        0.241043
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.967900 loss:        0.096879
Test - acc:         0.922700 loss:        0.241119
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.969040 loss:        0.095039
Test - acc:         0.922400 loss:        0.240755
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.969160 loss:        0.093230
Test - acc:         0.922800 loss:        0.239093
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.969120 loss:        0.092492
Test - acc:         0.922600 loss:        0.238919
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.969520 loss:        0.092071
Test - acc:         0.923300 loss:        0.240549
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.971400 loss:        0.087061
Test - acc:         0.922700 loss:        0.242104
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.971460 loss:        0.086916
Test - acc:         0.922900 loss:        0.240361
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.971880 loss:        0.085403
Test - acc:         0.924600 loss:        0.238667
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.972720 loss:        0.084230
Test - acc:         0.922800 loss:        0.241646
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.973160 loss:        0.083277
Test - acc:         0.922300 loss:        0.241144
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.972360 loss:        0.083737
Test - acc:         0.923700 loss:        0.240501
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.971900 loss:        0.083612
Test - acc:         0.922200 loss:        0.240269
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.972500 loss:        0.082591
Test - acc:         0.922800 loss:        0.242821
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.972340 loss:        0.082700
Test - acc:         0.921400 loss:        0.243753
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.973900 loss:        0.079962
Test - acc:         0.921900 loss:        0.243870
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.974360 loss:        0.078599
Test - acc:         0.923200 loss:        0.243626
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.974960 loss:        0.076277
Test - acc:         0.922000 loss:        0.244515
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.973860 loss:        0.079229
Test - acc:         0.923100 loss:        0.246108
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.975760 loss:        0.074884
Test - acc:         0.921200 loss:        0.246561
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.784140 loss:        0.621129
Test - acc:         0.823400 loss:        0.510198
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.842940 loss:        0.453414
Test - acc:         0.844800 loss:        0.454137
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.856920 loss:        0.410176
Test - acc:         0.853100 loss:        0.427607
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.864860 loss:        0.387202
Test - acc:         0.858000 loss:        0.412008
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.870400 loss:        0.369635
Test - acc:         0.860200 loss:        0.402554
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.874720 loss:        0.358941
Test - acc:         0.863800 loss:        0.399179
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.880120 loss:        0.344927
Test - acc:         0.866700 loss:        0.390750
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.883200 loss:        0.336263
Test - acc:         0.869500 loss:        0.383796
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.886500 loss:        0.330314
Test - acc:         0.870100 loss:        0.383237
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.888320 loss:        0.322613
Test - acc:         0.871600 loss:        0.375099
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.889920 loss:        0.315546
Test - acc:         0.872900 loss:        0.372137
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.891140 loss:        0.312816
Test - acc:         0.872300 loss:        0.368305
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.893140 loss:        0.310344
Test - acc:         0.874200 loss:        0.370317
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.894580 loss:        0.300380
Test - acc:         0.873900 loss:        0.367805
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.895180 loss:        0.300908
Test - acc:         0.875100 loss:        0.363124
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.896560 loss:        0.296623
Test - acc:         0.877500 loss:        0.362600
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.897580 loss:        0.296694
Test - acc:         0.880000 loss:        0.360165
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.898320 loss:        0.290908
Test - acc:         0.877900 loss:        0.360543
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.899300 loss:        0.289702
Test - acc:         0.878600 loss:        0.357372
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.900660 loss:        0.287962
Test - acc:         0.877800 loss:        0.358022
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.901540 loss:        0.284729
Test - acc:         0.883300 loss:        0.352638
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.902100 loss:        0.285662
Test - acc:         0.881100 loss:        0.354524
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.902420 loss:        0.279897
Test - acc:         0.880500 loss:        0.348546
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.903720 loss:        0.278168
Test - acc:         0.881000 loss:        0.349362
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.904060 loss:        0.275895
Test - acc:         0.880600 loss:        0.350899
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.905740 loss:        0.273619
Test - acc:         0.883100 loss:        0.347464
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.905380 loss:        0.272057
Test - acc:         0.883500 loss:        0.348900
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.906080 loss:        0.270345
Test - acc:         0.881200 loss:        0.349095
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.905940 loss:        0.267811
Test - acc:         0.882300 loss:        0.351357
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.908380 loss:        0.264327
Test - acc:         0.884800 loss:        0.348336
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.906680 loss:        0.267914
Test - acc:         0.884100 loss:        0.347571
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.908280 loss:        0.264646
Test - acc:         0.882700 loss:        0.349777
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.909260 loss:        0.264060
Test - acc:         0.882200 loss:        0.345445
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.908520 loss:        0.263505
Test - acc:         0.883200 loss:        0.347919
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.908460 loss:        0.264272
Test - acc:         0.886900 loss:        0.342247
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.908860 loss:        0.261561
Test - acc:         0.885400 loss:        0.342107
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.911220 loss:        0.257488
Test - acc:         0.887300 loss:        0.340712
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.911260 loss:        0.255032
Test - acc:         0.885900 loss:        0.345387
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.911460 loss:        0.252532
Test - acc:         0.886100 loss:        0.342814
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.585560 loss:        1.166656
Test - acc:         0.677400 loss:        0.916422
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.680020 loss:        0.905651
Test - acc:         0.714800 loss:        0.816053
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.707980 loss:        0.833702
Test - acc:         0.721300 loss:        0.785423
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.722640 loss:        0.793234
Test - acc:         0.741100 loss:        0.743754
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.733760 loss:        0.758615
Test - acc:         0.751800 loss:        0.713262
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.741140 loss:        0.739494
Test - acc:         0.752200 loss:        0.701974
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.750460 loss:        0.718520
Test - acc:         0.760500 loss:        0.684905
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.753720 loss:        0.707997
Test - acc:         0.763800 loss:        0.671475
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.760360 loss:        0.691030
Test - acc:         0.767700 loss:        0.657646
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.762180 loss:        0.681928
Test - acc:         0.773800 loss:        0.651360
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.766720 loss:        0.672252
Test - acc:         0.774200 loss:        0.646281
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.769440 loss:        0.663924
Test - acc:         0.780200 loss:        0.639215
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.774980 loss:        0.648506
Test - acc:         0.780200 loss:        0.642799
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.773920 loss:        0.649598
Test - acc:         0.778000 loss:        0.634057
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.777780 loss:        0.637164
Test - acc:         0.782400 loss:        0.625750
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.780340 loss:        0.630118
Test - acc:         0.784600 loss:        0.617949
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.780920 loss:        0.628193
Test - acc:         0.793000 loss:        0.604801
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.784020 loss:        0.623328
Test - acc:         0.795400 loss:        0.604363
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.786940 loss:        0.617418
Test - acc:         0.790200 loss:        0.607900
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.788100 loss:        0.615482
Test - acc:         0.793000 loss:        0.600713
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.785700 loss:        0.612805
Test - acc:         0.791100 loss:        0.600250
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.788820 loss:        0.606693
Test - acc:         0.790700 loss:        0.596930
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.791340 loss:        0.600908
Test - acc:         0.798500 loss:        0.593275
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.790360 loss:        0.601522
Test - acc:         0.795500 loss:        0.589014
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.793620 loss:        0.593502
Test - acc:         0.797900 loss:        0.584860
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.794800 loss:        0.592768
Test - acc:         0.795500 loss:        0.592637
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.793260 loss:        0.591664
Test - acc:         0.802500 loss:        0.579165
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.796040 loss:        0.584359
Test - acc:         0.802600 loss:        0.575281
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.798360 loss:        0.579984
Test - acc:         0.800200 loss:        0.581699
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.801220 loss:        0.573509
Test - acc:         0.801600 loss:        0.574495
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.799740 loss:        0.577828
Test - acc:         0.800900 loss:        0.581186
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.800220 loss:        0.572829
Test - acc:         0.806900 loss:        0.573147
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.801120 loss:        0.571634
Test - acc:         0.804600 loss:        0.569207
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.801720 loss:        0.566653
Test - acc:         0.805600 loss:        0.569483
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.804560 loss:        0.563707
Test - acc:         0.808000 loss:        0.565330
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.803100 loss:        0.564663
Test - acc:         0.801700 loss:        0.573864
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.805460 loss:        0.561301
Test - acc:         0.806100 loss:        0.565775
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.803880 loss:        0.560291
Test - acc:         0.807900 loss:        0.564490
Sparsity :          0.9961
Wdecay :        0.000500
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "topflip",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "logdir": "scaling_noise_experiment",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 0.25,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": null,
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.322340 loss:        1.886699
Test - acc:         0.412400 loss:        1.571300
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.495440 loss:        1.374135
Test - acc:         0.572500 loss:        1.184698
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.607540 loss:        1.091654
Test - acc:         0.621300 loss:        1.068805
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.687920 loss:        0.885695
Test - acc:         0.687300 loss:        0.890359
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.740660 loss:        0.739654
Test - acc:         0.720900 loss:        0.851775
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.779340 loss:        0.641494
Test - acc:         0.755800 loss:        0.704248
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.796220 loss:        0.587098
Test - acc:         0.781600 loss:        0.640724
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.811480 loss:        0.546275
Test - acc:         0.782300 loss:        0.630519
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.819840 loss:        0.522924
Test - acc:         0.765100 loss:        0.712430
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.829680 loss:        0.496940
Test - acc:         0.767200 loss:        0.697477
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.835840 loss:        0.478582
Test - acc:         0.712400 loss:        0.921297
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.841800 loss:        0.461143
Test - acc:         0.778700 loss:        0.671245
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.846760 loss:        0.448754
Test - acc:         0.758800 loss:        0.749820
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.848620 loss:        0.443076
Test - acc:         0.720000 loss:        0.927059
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.854120 loss:        0.428311
Test - acc:         0.812700 loss:        0.558105
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.856700 loss:        0.421219
Test - acc:         0.828200 loss:        0.504929
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.860420 loss:        0.408130
Test - acc:         0.822100 loss:        0.523616
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.862940 loss:        0.405288
Test - acc:         0.832300 loss:        0.493318
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.862000 loss:        0.405007
Test - acc:         0.836500 loss:        0.479825
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.865600 loss:        0.391311
Test - acc:         0.823000 loss:        0.535222
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.870000 loss:        0.380437
Test - acc:         0.835100 loss:        0.484840
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.867640 loss:        0.387460
Test - acc:         0.838300 loss:        0.484035
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.870820 loss:        0.376995
Test - acc:         0.812800 loss:        0.572729
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.871060 loss:        0.375839
Test - acc:         0.829500 loss:        0.505071
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.873980 loss:        0.368091
Test - acc:         0.827200 loss:        0.529063
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.871060 loss:        0.376472
Test - acc:         0.849400 loss:        0.440659
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.876220 loss:        0.364260
Test - acc:         0.843200 loss:        0.467006
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.875360 loss:        0.364017
Test - acc:         0.832600 loss:        0.503869
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.878660 loss:        0.357284
Test - acc:         0.847900 loss:        0.473449
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.876800 loss:        0.361031
Test - acc:         0.857500 loss:        0.425841
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.877560 loss:        0.356808
Test - acc:         0.848600 loss:        0.456704
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.879880 loss:        0.354396
Test - acc:         0.825300 loss:        0.558996
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.880140 loss:        0.352010
Test - acc:         0.845200 loss:        0.463289
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.880120 loss:        0.353469
Test - acc:         0.846700 loss:        0.461988
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.883240 loss:        0.347968
Test - acc:         0.824800 loss:        0.542132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.883140 loss:        0.343953
Test - acc:         0.849800 loss:        0.445216
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.882120 loss:        0.346582
Test - acc:         0.815900 loss:        0.561125
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.879460 loss:        0.351141
Test - acc:         0.851500 loss:        0.444231
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.881340 loss:        0.346336
Test - acc:         0.849600 loss:        0.444168
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.898580 loss:        0.297269
Test - acc:         0.872600 loss:        0.375523
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.896400 loss:        0.304959
Test - acc:         0.833100 loss:        0.518740
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.897060 loss:        0.299957
Test - acc:         0.823000 loss:        0.547111
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.898340 loss:        0.296718
Test - acc:         0.860600 loss:        0.404433
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.898760 loss:        0.296431
Test - acc:         0.853300 loss:        0.452555
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.898000 loss:        0.299419
Test - acc:         0.868800 loss:        0.381744
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.900780 loss:        0.290579
Test - acc:         0.842100 loss:        0.473541
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.898860 loss:        0.293719
Test - acc:         0.840800 loss:        0.476680
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.898800 loss:        0.296528
Test - acc:         0.834100 loss:        0.522198
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.900940 loss:        0.289443
Test - acc:         0.861100 loss:        0.414208
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.901500 loss:        0.289680
Test - acc:         0.861100 loss:        0.430740
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.898100 loss:        0.294402
Test - acc:         0.857900 loss:        0.446945
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.899900 loss:        0.290722
Test - acc:         0.873500 loss:        0.387044
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.901280 loss:        0.289912
Test - acc:         0.856000 loss:        0.422157
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.901180 loss:        0.286688
Test - acc:         0.848500 loss:        0.479423
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.901340 loss:        0.287673
Test - acc:         0.828600 loss:        0.527338
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.901920 loss:        0.290899
Test - acc:         0.823300 loss:        0.549903
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.900160 loss:        0.289355
Test - acc:         0.819800 loss:        0.551948
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.902640 loss:        0.285943
Test - acc:         0.867600 loss:        0.395521
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.901880 loss:        0.287193
Test - acc:         0.862200 loss:        0.412985
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.900620 loss:        0.288568
Test - acc:         0.863000 loss:        0.429545
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.901760 loss:        0.284090
Test - acc:         0.870200 loss:        0.394528
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.899900 loss:        0.292536
Test - acc:         0.843800 loss:        0.480617
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.903260 loss:        0.284980
Test - acc:         0.853200 loss:        0.458772
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.900400 loss:        0.288153
Test - acc:         0.858700 loss:        0.432119
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.901560 loss:        0.289174
Test - acc:         0.847600 loss:        0.477556
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.901640 loss:        0.285906
Test - acc:         0.862100 loss:        0.419550
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.901000 loss:        0.286423
Test - acc:         0.862300 loss:        0.419336
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.900480 loss:        0.288954
Test - acc:         0.850000 loss:        0.463576
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.902840 loss:        0.285751
Test - acc:         0.847000 loss:        0.471309
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.902460 loss:        0.286022
Test - acc:         0.852600 loss:        0.439334
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.903040 loss:        0.284255
Test - acc:         0.872700 loss:        0.389303
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.902980 loss:        0.286894
Test - acc:         0.869100 loss:        0.397429
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.901120 loss:        0.286898
Test - acc:         0.852000 loss:        0.458914
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.904040 loss:        0.283644
Test - acc:         0.800300 loss:        0.635698
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.902140 loss:        0.285581
Test - acc:         0.827600 loss:        0.559620
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.900160 loss:        0.288136
Test - acc:         0.860200 loss:        0.419350
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.900440 loss:        0.288901
Test - acc:         0.855700 loss:        0.449174
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.903580 loss:        0.283007
Test - acc:         0.840400 loss:        0.466675
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.920000 loss:        0.236667
Test - acc:         0.859200 loss:        0.451703
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.916100 loss:        0.244630
Test - acc:         0.877100 loss:        0.366526
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.913440 loss:        0.250432
Test - acc:         0.883900 loss:        0.364837
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.917260 loss:        0.245322
Test - acc:         0.871100 loss:        0.389698
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.914540 loss:        0.247563
Test - acc:         0.862800 loss:        0.437515
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.916840 loss:        0.239775
Test - acc:         0.856000 loss:        0.469340
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.914500 loss:        0.248675
Test - acc:         0.861000 loss:        0.422422
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.914700 loss:        0.245888
Test - acc:         0.867000 loss:        0.404655
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.914780 loss:        0.245396
Test - acc:         0.870900 loss:        0.383843
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.918400 loss:        0.241586
Test - acc:         0.879800 loss:        0.357537
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.916300 loss:        0.242209
Test - acc:         0.849400 loss:        0.464324
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.917920 loss:        0.238950
Test - acc:         0.875600 loss:        0.390176
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.914260 loss:        0.250432
Test - acc:         0.883300 loss:        0.358172
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.917920 loss:        0.238566
Test - acc:         0.882300 loss:        0.354714
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.918320 loss:        0.236702
Test - acc:         0.865200 loss:        0.414518
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.913780 loss:        0.249150
Test - acc:         0.845400 loss:        0.494025
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.918580 loss:        0.238559
Test - acc:         0.870900 loss:        0.375449
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.916280 loss:        0.243822
Test - acc:         0.864000 loss:        0.417508
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.917620 loss:        0.240845
Test - acc:         0.848100 loss:        0.541226
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.915160 loss:        0.245719
Test - acc:         0.873000 loss:        0.385476
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.915980 loss:        0.241402
Test - acc:         0.876200 loss:        0.383401
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.915560 loss:        0.241370
Test - acc:         0.813500 loss:        0.589092
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.916080 loss:        0.244685
Test - acc:         0.870600 loss:        0.394536
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.918240 loss:        0.240040
Test - acc:         0.865000 loss:        0.433662
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.916380 loss:        0.239476
Test - acc:         0.865800 loss:        0.403225
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.915340 loss:        0.244008
Test - acc:         0.865100 loss:        0.417533
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.914640 loss:        0.243943
Test - acc:         0.882500 loss:        0.366148
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.917240 loss:        0.240925
Test - acc:         0.821400 loss:        0.570377
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.916320 loss:        0.242441
Test - acc:         0.892900 loss:        0.321058
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.917320 loss:        0.236418
Test - acc:         0.863700 loss:        0.433367
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.917380 loss:        0.241841
Test - acc:         0.876500 loss:        0.384628
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.916160 loss:        0.239816
Test - acc:         0.879900 loss:        0.363445
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.917480 loss:        0.240871
Test - acc:         0.858300 loss:        0.434224
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.917300 loss:        0.242257
Test - acc:         0.865400 loss:        0.412512
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.915880 loss:        0.244696
Test - acc:         0.869700 loss:        0.410378
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.916300 loss:        0.241691
Test - acc:         0.862900 loss:        0.416944
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.917640 loss:        0.239537
Test - acc:         0.869400 loss:        0.391958
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.917260 loss:        0.241488
Test - acc:         0.871300 loss:        0.401366
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.918800 loss:        0.237744
Test - acc:         0.881400 loss:        0.372674
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.918460 loss:        0.233601
Test - acc:         0.875200 loss:        0.394002
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.922220 loss:        0.227655
Test - acc:         0.862500 loss:        0.431990
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.920060 loss:        0.231834
Test - acc:         0.831500 loss:        0.556278
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.921140 loss:        0.226479
Test - acc:         0.887000 loss:        0.343232
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.922780 loss:        0.224722
Test - acc:         0.884300 loss:        0.361985
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.923520 loss:        0.221565
Test - acc:         0.880200 loss:        0.351509
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.923360 loss:        0.222701
Test - acc:         0.878300 loss:        0.372275
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.922960 loss:        0.221959
Test - acc:         0.866900 loss:        0.415013
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.922960 loss:        0.221090
Test - acc:         0.891700 loss:        0.333247
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.922780 loss:        0.221746
Test - acc:         0.888300 loss:        0.350568
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.922260 loss:        0.223874
Test - acc:         0.801400 loss:        0.709758
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.924060 loss:        0.222623
Test - acc:         0.890300 loss:        0.331720
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.922960 loss:        0.221593
Test - acc:         0.867700 loss:        0.418506
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.923000 loss:        0.220755
Test - acc:         0.857300 loss:        0.431875
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.922100 loss:        0.225067
Test - acc:         0.883500 loss:        0.352516
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.920960 loss:        0.223356
Test - acc:         0.872500 loss:        0.387497
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.925000 loss:        0.216299
Test - acc:         0.866000 loss:        0.415727
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.926000 loss:        0.218770
Test - acc:         0.881900 loss:        0.361967
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.924300 loss:        0.221045
Test - acc:         0.842500 loss:        0.505172
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.923920 loss:        0.221952
Test - acc:         0.886300 loss:        0.333959
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.926560 loss:        0.216712
Test - acc:         0.888000 loss:        0.343656
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.922480 loss:        0.220785
Test - acc:         0.864500 loss:        0.417112
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.922000 loss:        0.224392
Test - acc:         0.871500 loss:        0.412891
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.924760 loss:        0.219329
Test - acc:         0.875700 loss:        0.391773
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.924840 loss:        0.215763
Test - acc:         0.887900 loss:        0.341705
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.924320 loss:        0.221327
Test - acc:         0.885600 loss:        0.345066
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.924960 loss:        0.217039
Test - acc:         0.884300 loss:        0.348355
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.924240 loss:        0.221176
Test - acc:         0.876600 loss:        0.381921
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.924500 loss:        0.219094
Test - acc:         0.875200 loss:        0.367156
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.922640 loss:        0.222176
Test - acc:         0.871800 loss:        0.401505
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.923860 loss:        0.216949
Test - acc:         0.854300 loss:        0.459825
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.923500 loss:        0.222433
Test - acc:         0.876000 loss:        0.377723
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.923460 loss:        0.219290
Test - acc:         0.886600 loss:        0.341705
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.959860 loss:        0.121850
Test - acc:         0.933500 loss:        0.197787
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.970380 loss:        0.090467
Test - acc:         0.937900 loss:        0.192097
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.973860 loss:        0.079928
Test - acc:         0.937900 loss:        0.188693
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.977580 loss:        0.067920
Test - acc:         0.937000 loss:        0.192302
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.978420 loss:        0.065738
Test - acc:         0.936000 loss:        0.193887
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.981460 loss:        0.056580
Test - acc:         0.936400 loss:        0.197112
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.964940 loss:        0.101208
Test - acc:         0.929900 loss:        0.214932
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.971620 loss:        0.083863
Test - acc:         0.930400 loss:        0.216195
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.973480 loss:        0.077624
Test - acc:         0.931800 loss:        0.214376
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.975980 loss:        0.070586
Test - acc:         0.931200 loss:        0.221508
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.977020 loss:        0.068004
Test - acc:         0.929100 loss:        0.217760
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.978840 loss:        0.063433
Test - acc:         0.932900 loss:        0.221808
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.980140 loss:        0.059462
Test - acc:         0.933200 loss:        0.213336
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.980220 loss:        0.059803
Test - acc:         0.930800 loss:        0.223229
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.981220 loss:        0.056116
Test - acc:         0.932500 loss:        0.220725
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.982180 loss:        0.054564
Test - acc:         0.933100 loss:        0.219022
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.981500 loss:        0.056061
Test - acc:         0.932600 loss:        0.224358
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.982400 loss:        0.053300
Test - acc:         0.929100 loss:        0.233186
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.983040 loss:        0.052130
Test - acc:         0.930600 loss:        0.232763
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.982220 loss:        0.053819
Test - acc:         0.927900 loss:        0.238434
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.980780 loss:        0.056259
Test - acc:         0.929800 loss:        0.242577
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.983160 loss:        0.052430
Test - acc:         0.930400 loss:        0.232603
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.982560 loss:        0.053293
Test - acc:         0.931100 loss:        0.234444
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.982980 loss:        0.052145
Test - acc:         0.929500 loss:        0.244961
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.981640 loss:        0.054649
Test - acc:         0.929200 loss:        0.246447
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.054415
Test - acc:         0.922700 loss:        0.263824
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.981640 loss:        0.053455
Test - acc:         0.934400 loss:        0.223550
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.981980 loss:        0.053594
Test - acc:         0.932000 loss:        0.244277
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.981800 loss:        0.054997
Test - acc:         0.927500 loss:        0.238801
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.981360 loss:        0.055552
Test - acc:         0.928000 loss:        0.240556
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.981300 loss:        0.055171
Test - acc:         0.929400 loss:        0.238925
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.980720 loss:        0.056332
Test - acc:         0.928900 loss:        0.242586
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.980240 loss:        0.056748
Test - acc:         0.925300 loss:        0.252410
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.980100 loss:        0.058401
Test - acc:         0.928300 loss:        0.241907
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.981040 loss:        0.056989
Test - acc:         0.927200 loss:        0.239766
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.980580 loss:        0.056954
Test - acc:         0.926100 loss:        0.255664
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.980740 loss:        0.057110
Test - acc:         0.927800 loss:        0.248988
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.979860 loss:        0.059208
Test - acc:         0.931300 loss:        0.235289
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.981320 loss:        0.057400
Test - acc:         0.928200 loss:        0.246152
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.980760 loss:        0.058186
Test - acc:         0.924000 loss:        0.253594
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.981240 loss:        0.057012
Test - acc:         0.926400 loss:        0.243111
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.980620 loss:        0.058088
Test - acc:         0.924500 loss:        0.265801
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.980540 loss:        0.058973
Test - acc:         0.924200 loss:        0.266121
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.980180 loss:        0.058441
Test - acc:         0.922200 loss:        0.269818
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.980000 loss:        0.060390
Test - acc:         0.925000 loss:        0.256902
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.893920 loss:        0.307151
Test - acc:         0.886400 loss:        0.337930
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.921280 loss:        0.226641
Test - acc:         0.883400 loss:        0.359010
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.925100 loss:        0.212275
Test - acc:         0.890700 loss:        0.337173
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.929640 loss:        0.200409
Test - acc:         0.896500 loss:        0.312255
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.932800 loss:        0.192176
Test - acc:         0.902300 loss:        0.296029
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.938080 loss:        0.179934
Test - acc:         0.903100 loss:        0.288020
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.938940 loss:        0.178015
Test - acc:         0.899100 loss:        0.309136
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.940600 loss:        0.171345
Test - acc:         0.901200 loss:        0.293670
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.941700 loss:        0.168407
Test - acc:         0.907800 loss:        0.279720
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.943280 loss:        0.163458
Test - acc:         0.909500 loss:        0.279845
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.944980 loss:        0.161053
Test - acc:         0.897500 loss:        0.309325
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.944200 loss:        0.161235
Test - acc:         0.906100 loss:        0.277934
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.945400 loss:        0.158951
Test - acc:         0.902900 loss:        0.297904
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.944920 loss:        0.157058
Test - acc:         0.905200 loss:        0.287122
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.944960 loss:        0.153859
Test - acc:         0.909700 loss:        0.276557
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.945720 loss:        0.153059
Test - acc:         0.905100 loss:        0.303046
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.948280 loss:        0.149478
Test - acc:         0.910800 loss:        0.273888
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.947580 loss:        0.149817
Test - acc:         0.911100 loss:        0.274286
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.947220 loss:        0.150490
Test - acc:         0.904700 loss:        0.290549
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.949880 loss:        0.146742
Test - acc:         0.917000 loss:        0.270380
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.948980 loss:        0.144990
Test - acc:         0.912400 loss:        0.275646
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.948960 loss:        0.144293
Test - acc:         0.911400 loss:        0.276716
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.949740 loss:        0.143829
Test - acc:         0.905700 loss:        0.281456
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.951080 loss:        0.142956
Test - acc:         0.909300 loss:        0.284316
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.952020 loss:        0.139277
Test - acc:         0.908400 loss:        0.289088
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.951020 loss:        0.139837
Test - acc:         0.910300 loss:        0.287328
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.952320 loss:        0.136911
Test - acc:         0.912700 loss:        0.277723
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.950020 loss:        0.142521
Test - acc:         0.910300 loss:        0.277373
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.950080 loss:        0.140920
Test - acc:         0.909000 loss:        0.286861
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.951760 loss:        0.138431
Test - acc:         0.911600 loss:        0.276199
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.952740 loss:        0.135855
Test - acc:         0.910600 loss:        0.275974
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.951740 loss:        0.138346
Test - acc:         0.913400 loss:        0.280153
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.950540 loss:        0.139028
Test - acc:         0.910300 loss:        0.286347
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.953560 loss:        0.134784
Test - acc:         0.911500 loss:        0.275937
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.951060 loss:        0.138887
Test - acc:         0.905300 loss:        0.303567
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.951020 loss:        0.138420
Test - acc:         0.912200 loss:        0.283066
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.954780 loss:        0.130341
Test - acc:         0.914000 loss:        0.276808
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.953140 loss:        0.133321
Test - acc:         0.911100 loss:        0.288294
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.952760 loss:        0.136970
Test - acc:         0.914800 loss:        0.278163
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.800220 loss:        0.570406
Test - acc:         0.828800 loss:        0.502256
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.843440 loss:        0.447053
Test - acc:         0.844100 loss:        0.458942
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.855700 loss:        0.413810
Test - acc:         0.835800 loss:        0.469765
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.862520 loss:        0.392276
Test - acc:         0.841900 loss:        0.459600
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.868320 loss:        0.376528
Test - acc:         0.852100 loss:        0.440863
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.870040 loss:        0.370941
Test - acc:         0.862000 loss:        0.411509
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.874940 loss:        0.359987
Test - acc:         0.854800 loss:        0.434294
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.874080 loss:        0.359419
Test - acc:         0.860400 loss:        0.409291
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.877560 loss:        0.350016
Test - acc:         0.863300 loss:        0.406554
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.880440 loss:        0.343628
Test - acc:         0.861900 loss:        0.408373
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.880140 loss:        0.344031
Test - acc:         0.866900 loss:        0.393083
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.883000 loss:        0.334327
Test - acc:         0.860200 loss:        0.423308
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.882860 loss:        0.335020
Test - acc:         0.856500 loss:        0.415303
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.883580 loss:        0.330936
Test - acc:         0.863000 loss:        0.409294
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.886160 loss:        0.327769
Test - acc:         0.858100 loss:        0.429927
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.886140 loss:        0.325112
Test - acc:         0.867100 loss:        0.408270
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.903160 loss:        0.282054
Test - acc:         0.887000 loss:        0.336549
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.908520 loss:        0.263274
Test - acc:         0.890900 loss:        0.329608
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.913020 loss:        0.253169
Test - acc:         0.891500 loss:        0.326535
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.912000 loss:        0.253987
Test - acc:         0.891200 loss:        0.328027
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.914300 loss:        0.248661
Test - acc:         0.891700 loss:        0.325683
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.914860 loss:        0.246963
Test - acc:         0.890700 loss:        0.326310
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.915620 loss:        0.242836
Test - acc:         0.891500 loss:        0.323980
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.915440 loss:        0.240878
Test - acc:         0.892300 loss:        0.322222
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.917060 loss:        0.238606
Test - acc:         0.891400 loss:        0.326370
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.917000 loss:        0.238543
Test - acc:         0.891300 loss:        0.326844
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.917460 loss:        0.236547
Test - acc:         0.893100 loss:        0.322133
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.919860 loss:        0.235276
Test - acc:         0.894000 loss:        0.321767
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.920220 loss:        0.231480
Test - acc:         0.894900 loss:        0.321119
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.919260 loss:        0.230706
Test - acc:         0.892600 loss:        0.322599
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.918980 loss:        0.233020
Test - acc:         0.894200 loss:        0.322923
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.920420 loss:        0.230369
Test - acc:         0.894200 loss:        0.322947
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.920240 loss:        0.229871
Test - acc:         0.892000 loss:        0.324134
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.922220 loss:        0.225026
Test - acc:         0.892700 loss:        0.321331
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.922080 loss:        0.227001
Test - acc:         0.891600 loss:        0.326300
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.921000 loss:        0.224585
Test - acc:         0.895300 loss:        0.319401
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.923500 loss:        0.220141
Test - acc:         0.894400 loss:        0.319788
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.922300 loss:        0.225499
Test - acc:         0.893300 loss:        0.322184
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.921440 loss:        0.222985
Test - acc:         0.894100 loss:        0.319234
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.574740 loss:        1.192439
Test - acc:         0.672400 loss:        0.936739
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.684960 loss:        0.898930
Test - acc:         0.717800 loss:        0.819717
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.713920 loss:        0.815205
Test - acc:         0.735400 loss:        0.761895
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.732080 loss:        0.769957
Test - acc:         0.748500 loss:        0.722796
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.741400 loss:        0.742278
Test - acc:         0.755600 loss:        0.698346
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.747340 loss:        0.723486
Test - acc:         0.764200 loss:        0.678738
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.756220 loss:        0.699562
Test - acc:         0.769900 loss:        0.662504
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.761280 loss:        0.689054
Test - acc:         0.773600 loss:        0.657855
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.766980 loss:        0.674064
Test - acc:         0.779400 loss:        0.643755
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.768340 loss:        0.666842
Test - acc:         0.781400 loss:        0.635911
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.773640 loss:        0.649413
Test - acc:         0.786700 loss:        0.624944
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.774620 loss:        0.645722
Test - acc:         0.784800 loss:        0.621970
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.777420 loss:        0.638232
Test - acc:         0.788400 loss:        0.611282
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.783660 loss:        0.626793
Test - acc:         0.788600 loss:        0.606741
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.784760 loss:        0.620875
Test - acc:         0.789500 loss:        0.607283
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.783620 loss:        0.617035
Test - acc:         0.795300 loss:        0.597411
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.787140 loss:        0.609456
Test - acc:         0.793300 loss:        0.596137
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.789520 loss:        0.607942
Test - acc:         0.797400 loss:        0.589948
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.790660 loss:        0.602392
Test - acc:         0.794500 loss:        0.587296
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.792940 loss:        0.597174
Test - acc:         0.800900 loss:        0.579885
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.792420 loss:        0.596357
Test - acc:         0.795500 loss:        0.583626
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.794080 loss:        0.592521
Test - acc:         0.801100 loss:        0.575167
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.797040 loss:        0.583263
Test - acc:         0.800600 loss:        0.575724
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.799040 loss:        0.583269
Test - acc:         0.802700 loss:        0.573454
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.800320 loss:        0.578514
Test - acc:         0.802300 loss:        0.570524
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.798620 loss:        0.576338
Test - acc:         0.805300 loss:        0.571439
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.801920 loss:        0.571477
Test - acc:         0.807300 loss:        0.564293
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.801920 loss:        0.569902
Test - acc:         0.804100 loss:        0.562884
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.802920 loss:        0.567571
Test - acc:         0.805400 loss:        0.559921
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.803600 loss:        0.562827
Test - acc:         0.804500 loss:        0.561143
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.805480 loss:        0.559846
Test - acc:         0.805300 loss:        0.562252
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.805140 loss:        0.560567
Test - acc:         0.806100 loss:        0.554938
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.805820 loss:        0.556981
Test - acc:         0.803100 loss:        0.563351
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.807660 loss:        0.554364
Test - acc:         0.807900 loss:        0.557168
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.808820 loss:        0.552001
Test - acc:         0.808600 loss:        0.552895
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.810760 loss:        0.548827
Test - acc:         0.809300 loss:        0.552046
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.809480 loss:        0.545760
Test - acc:         0.808600 loss:        0.554186
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.810600 loss:        0.546309
Test - acc:         0.808900 loss:        0.553785
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.812680 loss:        0.542506
Test - acc:         0.813200 loss:        0.551113
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.368960 loss:        1.701902
Test - acc:         0.459200 loss:        1.460475
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.473520 loss:        1.447941
Test - acc:         0.507700 loss:        1.351822
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.502380 loss:        1.369418
Test - acc:         0.534500 loss:        1.281841
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.527000 loss:        1.313998
Test - acc:         0.546900 loss:        1.248168
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.543840 loss:        1.269629
Test - acc:         0.555400 loss:        1.222643
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.550840 loss:        1.243674
Test - acc:         0.557200 loss:        1.220584
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.562560 loss:        1.221167
Test - acc:         0.584400 loss:        1.159362
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.571640 loss:        1.198275
Test - acc:         0.594900 loss:        1.132283
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.576220 loss:        1.182685
Test - acc:         0.602500 loss:        1.109245
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.582980 loss:        1.165619
Test - acc:         0.604300 loss:        1.106981
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.587300 loss:        1.157177
Test - acc:         0.604100 loss:        1.099394
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.591880 loss:        1.144052
Test - acc:         0.617700 loss:        1.076202
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.595480 loss:        1.132993
Test - acc:         0.618400 loss:        1.081817
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.600840 loss:        1.128927
Test - acc:         0.617200 loss:        1.077801
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.603620 loss:        1.115178
Test - acc:         0.617400 loss:        1.077956
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.606780 loss:        1.104115
Test - acc:         0.624400 loss:        1.047810
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.607080 loss:        1.101813
Test - acc:         0.631700 loss:        1.037586
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.608600 loss:        1.094107
Test - acc:         0.629400 loss:        1.035190
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.613920 loss:        1.086663
Test - acc:         0.627000 loss:        1.035482
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.616340 loss:        1.081869
Test - acc:         0.628000 loss:        1.031798
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.616340 loss:        1.080021
Test - acc:         0.634400 loss:        1.016908
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.618960 loss:        1.070161
Test - acc:         0.634200 loss:        1.017417
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.620080 loss:        1.068414
Test - acc:         0.638200 loss:        1.004858
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.623400 loss:        1.060384
Test - acc:         0.640500 loss:        1.004292
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.626380 loss:        1.056207
Test - acc:         0.642400 loss:        0.997721
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.626640 loss:        1.052135
Test - acc:         0.646700 loss:        0.991331
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.629700 loss:        1.046975
Test - acc:         0.641200 loss:        0.993589
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.629840 loss:        1.042466
Test - acc:         0.648000 loss:        0.997057
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.631460 loss:        1.041383
Test - acc:         0.646800 loss:        0.990018
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.631400 loss:        1.036306
Test - acc:         0.650300 loss:        0.978523
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.634960 loss:        1.031412
Test - acc:         0.649600 loss:        0.979203
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.635840 loss:        1.029450
Test - acc:         0.649100 loss:        0.983026
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.636340 loss:        1.024036
Test - acc:         0.650400 loss:        0.969001
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.640140 loss:        1.019817
Test - acc:         0.649200 loss:        0.982578
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.640020 loss:        1.019432
Test - acc:         0.655600 loss:        0.970706
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.638580 loss:        1.016364
Test - acc:         0.657100 loss:        0.959936
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.642740 loss:        1.011455
Test - acc:         0.659700 loss:        0.963710
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.641480 loss:        1.011583
Test - acc:         0.659700 loss:        0.952495
Sparsity :          0.9961
Wdecay :        0.000500
