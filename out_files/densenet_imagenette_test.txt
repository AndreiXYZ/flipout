******************************
Running
{
    "model": "densenet121",
    "dataset": "imagenette",
    "batch_size": 128,
    "test_batch_size": 512,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "none",
    "prune_freq": 1,
    "prune_rate": 0.2,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "test",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": null,
    "load_model": null,
    "parallel": false
}
******************************
Total prunable params of model: 6880448
Model has 6964106 total params.
num_weights=6922272
num_biases=41834
num.prunable=6880448
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.340057 loss:        2.001963
Test - acc:         0.414777 loss:        1.836501
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.518534 loss:        1.500024
Test - acc:         0.441274 loss:        1.983213
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.593938 loss:        1.228374
Test - acc:         0.537580 loss:        1.468120
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.665540 loss:        1.040086
Test - acc:         0.591338 loss:        1.324981
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.696061 loss:        0.940658
Test - acc:         0.617580 loss:        1.197785
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.720245 loss:        0.862992
Test - acc:         0.628535 loss:        1.235604
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.738515 loss:        0.804962
Test - acc:         0.720255 loss:        0.928469
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.764811 loss:        0.726608
Test - acc:         0.679490 loss:        1.055416
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.779491 loss:        0.671521
Test - acc:         0.697070 loss:        0.962817
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.791108 loss:        0.638072
Test - acc:         0.709045 loss:        1.012006
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.806104 loss:        0.602419
Test - acc:         0.653248 loss:        1.135798
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.817193 loss:        0.565918
Test - acc:         0.598217 loss:        1.573596
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.827965 loss:        0.531169
Test - acc:         0.751083 loss:        0.852642
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.833457 loss:        0.512793
Test - acc:         0.731720 loss:        0.877695
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.842433 loss:        0.474905
Test - acc:         0.728662 loss:        0.938761
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.851515 loss:        0.451664
Test - acc:         0.776306 loss:        0.759228
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.862816 loss:        0.425838
Test - acc:         0.750064 loss:        0.873627
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.865350 loss:        0.408757
Test - acc:         0.771210 loss:        0.808952
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.867673 loss:        0.407212
Test - acc:         0.698854 loss:        1.038439
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.866829 loss:        0.399424
Test - acc:         0.717707 loss:        0.942593
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.874749 loss:        0.386363
Test - acc:         0.736051 loss:        0.888345
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.866089 loss:        0.390241
Test - acc:         0.786497 loss:        0.747300
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.883726 loss:        0.348051
Test - acc:         0.769172 loss:        0.838432
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.885521 loss:        0.345130
Test - acc:         0.790318 loss:        0.776264
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.899989 loss:        0.305254
Test - acc:         0.756433 loss:        0.815513
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.901151 loss:        0.312781
Test - acc:         0.779108 loss:        0.779068
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.900517 loss:        0.308190
Test - acc:         0.762293 loss:        0.913827
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.900834 loss:        0.295818
Test - acc:         0.782166 loss:        0.814247
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.904847 loss:        0.288045
Test - acc:         0.735032 loss:        1.046404
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.903052 loss:        0.291860
Test - acc:         0.780382 loss:        0.787897
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.911395 loss:        0.270014
Test - acc:         0.776815 loss:        0.779545
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.911923 loss:        0.267168
Test - acc:         0.774522 loss:        0.806992
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.911184 loss:        0.267823
Test - acc:         0.812994 loss:        0.667515
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.909705 loss:        0.270279
Test - acc:         0.767134 loss:        0.859079
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.917626 loss:        0.250240
Test - acc:         0.799490 loss:        0.708106
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.907276 loss:        0.275796
Test - acc:         0.690955 loss:        1.151165
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.915936 loss:        0.255758
Test - acc:         0.771720 loss:        0.841428
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.924596 loss:        0.233704
Test - acc:         0.820127 loss:        0.648994
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.917520 loss:        0.242337
Test - acc:         0.752102 loss:        0.989524
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.925863 loss:        0.221360
Test - acc:         0.761019 loss:        0.956786
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.921533 loss:        0.243242
Test - acc:         0.741146 loss:        0.975619
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.923329 loss:        0.225338
Test - acc:         0.730955 loss:        1.188872
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.926286 loss:        0.219964
Test - acc:         0.798217 loss:        0.757615
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.933784 loss:        0.203685
Test - acc:         0.780892 loss:        0.855772
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.919632 loss:        0.238111
Test - acc:         0.776561 loss:        0.861043
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.933573 loss:        0.195768
Test - acc:         0.800000 loss:        0.674481
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.925969 loss:        0.214845
Test - acc:         0.795159 loss:        0.733799
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.926919 loss:        0.221871
Test - acc:         0.770955 loss:        0.807037
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.936319 loss:        0.200543
Test - acc:         0.781146 loss:        0.793202
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.933150 loss:        0.193285
Test - acc:         0.842548 loss:        0.607947
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.941493 loss:        0.176041
Test - acc:         0.753376 loss:        1.021221
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.934206 loss:        0.191928
Test - acc:         0.788535 loss:        0.780363
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.937269 loss:        0.186981
Test - acc:         0.801274 loss:        0.738559
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.937269 loss:        0.194155
Test - acc:         0.822420 loss:        0.642905
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.936319 loss:        0.196674
Test - acc:         0.825732 loss:        0.637651
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.930721 loss:        0.191994
Test - acc:         0.769936 loss:        1.015143
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.937163 loss:        0.189972
Test - acc:         0.826752 loss:        0.614223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.934840 loss:        0.196477
Test - acc:         0.795159 loss:        0.753612
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.945506 loss:        0.166031
Test - acc:         0.819363 loss:        0.706292
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.948358 loss:        0.155421
Test - acc:         0.828280 loss:        0.644238
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.937480 loss:        0.186353
Test - acc:         0.742420 loss:        1.136144
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.937269 loss:        0.189118
Test - acc:         0.791083 loss:        0.806515
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.939170 loss:        0.179596
Test - acc:         0.811210 loss:        0.736307
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.947196 loss:        0.161535
Test - acc:         0.714904 loss:        1.363919
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.944978 loss:        0.159493
Test - acc:         0.699363 loss:        1.389730
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.939381 loss:        0.182286
Test - acc:         0.810955 loss:        0.707531
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.950470 loss:        0.147159
Test - acc:         0.833631 loss:        0.647365
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.947091 loss:        0.159015
Test - acc:         0.736561 loss:        1.235673
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.941388 loss:        0.180014
Test - acc:         0.813503 loss:        0.673589
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.931038 loss:        0.201504
Test - acc:         0.744204 loss:        1.001299
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.946457 loss:        0.154154
Test - acc:         0.752357 loss:        0.986062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.940543 loss:        0.187228
Test - acc:         0.750318 loss:        1.036180
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.947830 loss:        0.161310
Test - acc:         0.816051 loss:        0.694884
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.950681 loss:        0.156690
Test - acc:         0.781656 loss:        0.860799
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.948252 loss:        0.154295
Test - acc:         0.736815 loss:        1.040905
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.941388 loss:        0.181511
Test - acc:         0.811720 loss:        0.764105
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.937903 loss:        0.183025
Test - acc:         0.799490 loss:        0.787530
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.948041 loss:        0.159421
Test - acc:         0.790064 loss:        0.829633
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.938536 loss:        0.184650
Test - acc:         0.790573 loss:        0.768897
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.954905 loss:        0.140039
Test - acc:         0.782420 loss:        0.903086
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.955434 loss:        0.136210
Test - acc:         0.621911 loss:        1.764963
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.943394 loss:        0.170629
Test - acc:         0.807389 loss:        0.742895
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.947196 loss:        0.155764
Test - acc:         0.835414 loss:        0.615213
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.950470 loss:        0.149832
Test - acc:         0.821146 loss:        0.781798
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.949414 loss:        0.147967
Test - acc:         0.651975 loss:        1.630188
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.944556 loss:        0.164436
Test - acc:         0.828790 loss:        0.636385
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.952371 loss:        0.146753
Test - acc:         0.767898 loss:        1.060088
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.944345 loss:        0.174590
Test - acc:         0.766879 loss:        0.974971
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.951104 loss:        0.147054
Test - acc:         0.864459 loss:        0.490092
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.956806 loss:        0.130111
Test - acc:         0.812739 loss:        0.826050
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.946774 loss:        0.159324
Test - acc:         0.861911 loss:        0.545981
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.950998 loss:        0.138537
Test - acc:         0.762548 loss:        1.051266
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.950787 loss:        0.143773
Test - acc:         0.807134 loss:        0.742854
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.956278 loss:        0.133272
Test - acc:         0.792611 loss:        0.937619
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.941388 loss:        0.181050
Test - acc:         0.776815 loss:        0.949166
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.945295 loss:        0.165607
Test - acc:         0.869299 loss:        0.490567
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.965677 loss:        0.104792
Test - acc:         0.835159 loss:        0.659423
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.963565 loss:        0.113264
Test - acc:         0.809682 loss:        0.679101
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.956278 loss:        0.134466
Test - acc:         0.768153 loss:        1.022368
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.945718 loss:        0.166588
Test - acc:         0.743694 loss:        1.043938
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.955645 loss:        0.134238
Test - acc:         0.733503 loss:        1.167250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.959552 loss:        0.125436
Test - acc:         0.835924 loss:        0.667447
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.950364 loss:        0.149560
Test - acc:         0.753376 loss:        1.053848
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.952371 loss:        0.142931
Test - acc:         0.847134 loss:        0.542817
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.955328 loss:        0.126592
Test - acc:         0.832866 loss:        0.636451
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.944450 loss:        0.167427
Test - acc:         0.773503 loss:        0.879894
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.946034 loss:        0.157337
Test - acc:         0.804841 loss:        0.766493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.959552 loss:        0.119162
Test - acc:         0.812994 loss:        0.709845
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.955117 loss:        0.139211
Test - acc:         0.801274 loss:        0.774810
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.946562 loss:        0.158765
Test - acc:         0.824459 loss:        0.703830
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.965572 loss:        0.106228
Test - acc:         0.826497 loss:        0.649692
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.964516 loss:        0.108326
Test - acc:         0.818344 loss:        0.725691
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.962509 loss:        0.117932
Test - acc:         0.744968 loss:        1.143719
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.945929 loss:        0.156043
Test - acc:         0.789809 loss:        0.856944
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.957546 loss:        0.126846
Test - acc:         0.805605 loss:        0.765829
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.953744 loss:        0.139983
Test - acc:         0.724331 loss:        1.284592
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.960608 loss:        0.124564
Test - acc:         0.852739 loss:        0.579567
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.967050 loss:        0.100324
Test - acc:         0.819108 loss:        0.734218
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.963143 loss:        0.109088
Test - acc:         0.800510 loss:        0.834551
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.945718 loss:        0.159993
Test - acc:         0.775541 loss:        0.873501
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.956173 loss:        0.132877
Test - acc:         0.774013 loss:        0.967610
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.960186 loss:        0.122954
Test - acc:         0.835924 loss:        0.630589
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.965149 loss:        0.105138
Test - acc:         0.810701 loss:        0.705637
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.955856 loss:        0.138363
Test - acc:         0.766115 loss:        1.041046
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.946668 loss:        0.162588
Test - acc:         0.816815 loss:        0.765297
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.959975 loss:        0.122375
Test - acc:         0.820637 loss:        0.706244
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.966100 loss:        0.099696
Test - acc:         0.841529 loss:        0.645152
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.959024 loss:        0.125889
Test - acc:         0.816815 loss:        0.672990
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.958707 loss:        0.133343
Test - acc:         0.816051 loss:        0.699649
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.954483 loss:        0.133942
Test - acc:         0.778599 loss:        0.849952
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.947724 loss:        0.156616
Test - acc:         0.813248 loss:        0.765125
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.967050 loss:        0.105859
Test - acc:         0.827771 loss:        0.666713
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.970852 loss:        0.090365
Test - acc:         0.834395 loss:        0.654752
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.959235 loss:        0.118027
Test - acc:         0.758981 loss:        1.150354
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.942549 loss:        0.174393
Test - acc:         0.760000 loss:        0.999844
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.957334 loss:        0.125777
Test - acc:         0.843057 loss:        0.564371
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.962826 loss:        0.113012
Test - acc:         0.816306 loss:        0.711916
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.965255 loss:        0.109598
Test - acc:         0.797197 loss:        0.789360
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.961136 loss:        0.121210
Test - acc:         0.824204 loss:        0.703220
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.963565 loss:        0.111540
Test - acc:         0.819618 loss:        0.712117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.964305 loss:        0.108189
Test - acc:         0.806369 loss:        0.821241
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.965889 loss:        0.107315
Test - acc:         0.816306 loss:        0.746277
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.961348 loss:        0.119095
Test - acc:         0.793885 loss:        0.784799
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.956173 loss:        0.133006
Test - acc:         0.842293 loss:        0.657656
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.955539 loss:        0.137530
Test - acc:         0.796433 loss:        0.833545
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.965044 loss:        0.114563
Test - acc:         0.812229 loss:        0.696388
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.964727 loss:        0.111459
Test - acc:         0.836688 loss:        0.609553
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.962932 loss:        0.112693
Test - acc:         0.792866 loss:        0.957318
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.959341 loss:        0.126826
Test - acc:         0.803057 loss:        0.822902
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.960714 loss:        0.122555
Test - acc:         0.801274 loss:        0.893153
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.983420 loss:        0.053665
Test - acc:         0.901146 loss:        0.346131
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.996093 loss:        0.021574
Test - acc:         0.904968 loss:        0.334551
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.998416 loss:        0.014520
Test - acc:         0.908280 loss:        0.326649
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.999155 loss:        0.010560
Test - acc:         0.907771 loss:        0.330447
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.009147
Test - acc:         0.906242 loss:        0.333696
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.007880
Test - acc:         0.908790 loss:        0.331754
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.006818
Test - acc:         0.907261 loss:        0.334673
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.006806
Test - acc:         0.908790 loss:        0.327726
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005720
Test - acc:         0.909299 loss:        0.323804
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.005643
Test - acc:         0.912102 loss:        0.323479
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.005743
Test - acc:         0.909045 loss:        0.328494
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.005155
Test - acc:         0.908025 loss:        0.326688
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004628
Test - acc:         0.909045 loss:        0.324631
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004094
Test - acc:         0.906242 loss:        0.326081
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003838
Test - acc:         0.907771 loss:        0.325395
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004012
Test - acc:         0.909299 loss:        0.318135
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003452
Test - acc:         0.908280 loss:        0.324404
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004139
Test - acc:         0.908790 loss:        0.324970
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003259
Test - acc:         0.907006 loss:        0.322711
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003332
Test - acc:         0.908535 loss:        0.320016
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003368
Test - acc:         0.907516 loss:        0.321009
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003522
Test - acc:         0.908025 loss:        0.319881
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003149
Test - acc:         0.909809 loss:        0.316240
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003040
Test - acc:         0.909809 loss:        0.312539
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002962
Test - acc:         0.907771 loss:        0.319580
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002973
Test - acc:         0.908790 loss:        0.320419
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003395
Test - acc:         0.907261 loss:        0.319078
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002890
Test - acc:         0.908025 loss:        0.315278
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003294
Test - acc:         0.907261 loss:        0.318631
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002989
Test - acc:         0.909809 loss:        0.316585
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002535
Test - acc:         0.907771 loss:        0.319047
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003164
Test - acc:         0.906242 loss:        0.317459
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002895
Test - acc:         0.909809 loss:        0.312974
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002798
Test - acc:         0.910318 loss:        0.310280
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002495
Test - acc:         0.910828 loss:        0.311938
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002925
Test - acc:         0.908790 loss:        0.311459
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002597
Test - acc:         0.907771 loss:        0.313707
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002810
Test - acc:         0.910573 loss:        0.312102
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003016
Test - acc:         0.909045 loss:        0.316123
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002511
Test - acc:         0.908280 loss:        0.309568
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002598
Test - acc:         0.910064 loss:        0.313535
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002739
Test - acc:         0.912102 loss:        0.312355
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002639
Test - acc:         0.911847 loss:        0.313926
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002673
Test - acc:         0.910064 loss:        0.306793
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002651
Test - acc:         0.907006 loss:        0.316507
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002790
Test - acc:         0.910318 loss:        0.310881
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002516
Test - acc:         0.909045 loss:        0.308253
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002791
Test - acc:         0.909809 loss:        0.310047
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002483
Test - acc:         0.910318 loss:        0.305468
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002398
Test - acc:         0.909809 loss:        0.311846
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002414
Test - acc:         0.911083 loss:        0.304250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002254
Test - acc:         0.911338 loss:        0.307766
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002405
Test - acc:         0.911338 loss:        0.309815
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002444
Test - acc:         0.907771 loss:        0.310217
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002436
Test - acc:         0.908790 loss:        0.307664
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002300
Test - acc:         0.911083 loss:        0.310358
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002335
Test - acc:         0.910573 loss:        0.302678
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002620
Test - acc:         0.908535 loss:        0.308760
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002411
Test - acc:         0.912866 loss:        0.302889
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002476
Test - acc:         0.909554 loss:        0.306537
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002724
Test - acc:         0.909809 loss:        0.306892
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002353
Test - acc:         0.910573 loss:        0.305903
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002192
Test - acc:         0.909045 loss:        0.302624
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002202
Test - acc:         0.907516 loss:        0.308203
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002404
Test - acc:         0.912866 loss:        0.304079
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002254
Test - acc:         0.911083 loss:        0.301195
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002272
Test - acc:         0.911083 loss:        0.303713
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002305
Test - acc:         0.909809 loss:        0.304828
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002202
Test - acc:         0.911083 loss:        0.302357
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002375
Test - acc:         0.910573 loss:        0.309307
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002333
Test - acc:         0.909045 loss:        0.308060
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002313
Test - acc:         0.911847 loss:        0.298508
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002400
Test - acc:         0.912866 loss:        0.301659
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002315
Test - acc:         0.910573 loss:        0.303674
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002377
Test - acc:         0.912102 loss:        0.302899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002181
Test - acc:         0.912357 loss:        0.301731
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002430
Test - acc:         0.914395 loss:        0.303563
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002284
Test - acc:         0.914140 loss:        0.303235
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002351
Test - acc:         0.912357 loss:        0.306686
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002076
Test - acc:         0.911847 loss:        0.306022
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002337
Test - acc:         0.913121 loss:        0.299651
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002412
Test - acc:         0.912357 loss:        0.300591
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002508
Test - acc:         0.909554 loss:        0.308823
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002314
Test - acc:         0.913631 loss:        0.304250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002386
Test - acc:         0.911847 loss:        0.304155
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002220
Test - acc:         0.915669 loss:        0.302271
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002305
Test - acc:         0.912866 loss:        0.303614
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002231
Test - acc:         0.911847 loss:        0.304676
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002149
Test - acc:         0.911847 loss:        0.299615
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002262
Test - acc:         0.913376 loss:        0.307539
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002199
Test - acc:         0.915159 loss:        0.304884
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002100
Test - acc:         0.911592 loss:        0.308564
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.001946
Test - acc:         0.912357 loss:        0.302047
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002158
Test - acc:         0.912357 loss:        0.309881
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002113
Test - acc:         0.911592 loss:        0.305123
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002179
Test - acc:         0.913121 loss:        0.301386
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002515
Test - acc:         0.913121 loss:        0.300555
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002151
Test - acc:         0.912866 loss:        0.311539
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002428
Test - acc:         0.914650 loss:        0.303102
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002236
Test - acc:         0.913631 loss:        0.301921
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002293
Test - acc:         0.916178 loss:        0.300813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002000
Test - acc:         0.914395 loss:        0.298737
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002085
Test - acc:         0.913121 loss:        0.302318
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002070
Test - acc:         0.915924 loss:        0.299737
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001963
Test - acc:         0.915669 loss:        0.299635
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002220
Test - acc:         0.912866 loss:        0.302366
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002078
Test - acc:         0.911847 loss:        0.304552
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002025
Test - acc:         0.910573 loss:        0.302066
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002052
Test - acc:         0.913376 loss:        0.303634
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002187
Test - acc:         0.913885 loss:        0.302075
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002063
Test - acc:         0.912866 loss:        0.299200
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001983
Test - acc:         0.910828 loss:        0.301370
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002061
Test - acc:         0.913885 loss:        0.298384
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002081
Test - acc:         0.913885 loss:        0.305847
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001955
Test - acc:         0.912102 loss:        0.302156
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002227
Test - acc:         0.912102 loss:        0.301749
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002073
Test - acc:         0.911847 loss:        0.305173
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001959
Test - acc:         0.912102 loss:        0.303104
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001991
Test - acc:         0.915159 loss:        0.297774
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002001
Test - acc:         0.911338 loss:        0.303600
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001947
Test - acc:         0.913376 loss:        0.304372
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002032
Test - acc:         0.911083 loss:        0.303439
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002064
Test - acc:         0.913885 loss:        0.300949
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002089
Test - acc:         0.912866 loss:        0.302093
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002069
Test - acc:         0.913376 loss:        0.305167
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001993
Test - acc:         0.911847 loss:        0.306588
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002091
Test - acc:         0.913885 loss:        0.303650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001970
Test - acc:         0.912866 loss:        0.301744
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001969
Test - acc:         0.912357 loss:        0.303570
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002058
Test - acc:         0.913121 loss:        0.303234
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001988
Test - acc:         0.911847 loss:        0.302992
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002220
Test - acc:         0.911338 loss:        0.299573
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002011
Test - acc:         0.912611 loss:        0.303584
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002231
Test - acc:         0.913631 loss:        0.298279
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001957
Test - acc:         0.912866 loss:        0.302390
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002042
Test - acc:         0.912611 loss:        0.304085
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002063
Test - acc:         0.911847 loss:        0.301331
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002037
Test - acc:         0.913376 loss:        0.298716
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001975
Test - acc:         0.909299 loss:        0.305992
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002056
Test - acc:         0.912866 loss:        0.304013
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002053
Test - acc:         0.912866 loss:        0.302353
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002163
Test - acc:         0.913631 loss:        0.302227
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001904
Test - acc:         0.914140 loss:        0.298996
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002020
Test - acc:         0.911338 loss:        0.307070
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001926
Test - acc:         0.912611 loss:        0.302962
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001928
Test - acc:         0.912611 loss:        0.301084
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002071
Test - acc:         0.912866 loss:        0.303018
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002008
Test - acc:         0.913631 loss:        0.300385
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002186
Test - acc:         0.913121 loss:        0.299576
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002064
Test - acc:         0.910828 loss:        0.302643
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001877
Test - acc:         0.912611 loss:        0.303118
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002005
Test - acc:         0.910828 loss:        0.301360
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001969
Test - acc:         0.913631 loss:        0.299148
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002212
Test - acc:         0.914395 loss:        0.299898
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001927
Test - acc:         0.913631 loss:        0.299702
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001927
Test - acc:         0.913376 loss:        0.299928
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002011
Test - acc:         0.911338 loss:        0.304389
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002137
Test - acc:         0.912611 loss:        0.301812
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002171
Test - acc:         0.911847 loss:        0.298181
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002033
Test - acc:         0.910318 loss:        0.305096
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002034
Test - acc:         0.914140 loss:        0.301250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001919
Test - acc:         0.911847 loss:        0.303589
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001964
Test - acc:         0.912611 loss:        0.300126
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002225
Test - acc:         0.911592 loss:        0.301111
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002092
Test - acc:         0.912102 loss:        0.303296
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001957
Test - acc:         0.912866 loss:        0.303425
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001852
Test - acc:         0.911083 loss:        0.304223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001944
Test - acc:         0.913885 loss:        0.304355
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002087
Test - acc:         0.911847 loss:        0.301064
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001852
Test - acc:         0.912102 loss:        0.302513
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001929
Test - acc:         0.910828 loss:        0.301891
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002220
Test - acc:         0.911083 loss:        0.304936
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001931
Test - acc:         0.911592 loss:        0.303742
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001849
Test - acc:         0.911592 loss:        0.303212
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002275
Test - acc:         0.912866 loss:        0.301234
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002179
Test - acc:         0.914140 loss:        0.302738
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002008
Test - acc:         0.914140 loss:        0.298948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001934
Test - acc:         0.912102 loss:        0.297935
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002079
Test - acc:         0.913885 loss:        0.298718
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002001
Test - acc:         0.913121 loss:        0.300908
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002093
Test - acc:         0.913631 loss:        0.300981
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002029
Test - acc:         0.911592 loss:        0.297825
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001984
Test - acc:         0.914140 loss:        0.304091
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002080
Test - acc:         0.912357 loss:        0.299630
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001882
Test - acc:         0.911592 loss:        0.299585
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001924
Test - acc:         0.912357 loss:        0.308374
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001918
Test - acc:         0.913885 loss:        0.297224
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001886
Test - acc:         0.912866 loss:        0.301761
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002011
Test - acc:         0.914395 loss:        0.299073
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001963
Test - acc:         0.910828 loss:        0.303650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002009
Test - acc:         0.913121 loss:        0.301723
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001942
Test - acc:         0.914395 loss:        0.300994
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002249
Test - acc:         0.914395 loss:        0.300017
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001891
Test - acc:         0.913631 loss:        0.300676
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002024
Test - acc:         0.912357 loss:        0.304139
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001964
Test - acc:         0.912357 loss:        0.302166
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001939
Test - acc:         0.911847 loss:        0.303262
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001994
Test - acc:         0.913885 loss:        0.300521
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002128
Test - acc:         0.913376 loss:        0.302445
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002029
Test - acc:         0.913376 loss:        0.299681
Sparsity :          0.0000
Wdecay :        0.000500
