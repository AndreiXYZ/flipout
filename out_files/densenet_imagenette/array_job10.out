Running --model densenet121 --dataset imagenette --seed 43 --logdir=criterion_experiment_no_bias/densenet121 --prune_criterion weight_squared_div_flips --prune_freq 32 --prune_rate 0.5 --noise --comment=densenet121_crit=weight_squared_div_flips_pf=32_seed=43 --save_model=pre-finetune/densenet121_weight_squared_div_flips_pf32_s43
******************************
Running
{
    "model": "densenet121",
    "dataset": "imagenette",
    "batch_size": 128,
    "test_batch_size": 500,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/densenet121",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/densenet121_weight_squared_div_flips_pf32_s43",
    "load_model": null,
    "parallel": false
}
******************************
Total prunable params of model: 6880448
Model has 6964106 total params.
num_weights=6922272
num_biases=41834
num.prunable=6880448
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.349773 loss:        2.060545
Test - acc:         0.464204 loss:        2.213250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.485585 loss:        1.615811
Test - acc:         0.464968 loss:        1.583937
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.557820 loss:        1.377145
Test - acc:         0.379873 loss:        5.230523
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.610730 loss:        1.211797
Test - acc:         0.612229 loss:        1.312108
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.673250 loss:        1.018220
Test - acc:         0.558981 loss:        3.248274
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.690569 loss:        0.968898
Test - acc:         0.641274 loss:        1.140370
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.724469 loss:        0.876802
Test - acc:         0.650955 loss:        1.158730
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.733552 loss:        0.826554
Test - acc:         0.597707 loss:        1.548129
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.762488 loss:        0.736475
Test - acc:         0.662930 loss:        1.179407
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.766396 loss:        0.710909
Test - acc:         0.670828 loss:        1.127672
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.789312 loss:        0.645313
Test - acc:         0.650446 loss:        1.270193
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.786778 loss:        0.655107
Test - acc:         0.701911 loss:        1.013121
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.809484 loss:        0.574430
Test - acc:         0.724841 loss:        0.893706
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.806843 loss:        0.586806
Test - acc:         0.715159 loss:        0.970227
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.826803 loss:        0.524665
Test - acc:         0.761019 loss:        0.762350
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.832612 loss:        0.507240
Test - acc:         0.746242 loss:        0.865811
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.837892 loss:        0.492861
Test - acc:         0.761019 loss:        0.791856
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.842222 loss:        0.474012
Test - acc:         0.767643 loss:        0.727084
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.854578 loss:        0.427188
Test - acc:         0.778599 loss:        0.729459
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.863977 loss:        0.413003
Test - acc:         0.802293 loss:        0.683786
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.869891 loss:        0.390734
Test - acc:         0.741146 loss:        0.951417
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.862182 loss:        0.419845
Test - acc:         0.756688 loss:        0.830134
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.872003 loss:        0.386279
Test - acc:         0.738854 loss:        0.971353
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.888373 loss:        0.338286
Test - acc:         0.753121 loss:        0.890835
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.881191 loss:        0.359579
Test - acc:         0.761783 loss:        0.870714
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.889006 loss:        0.331484
Test - acc:         0.766624 loss:        0.816003
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.891013 loss:        0.327791
Test - acc:         0.711338 loss:        1.099947
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.885416 loss:        0.344792
Test - acc:         0.723822 loss:        0.942273
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.886049 loss:        0.344339
Test - acc:         0.731975 loss:        0.986230
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.896293 loss:        0.309933
Test - acc:         0.806879 loss:        0.669766
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.902735 loss:        0.291633
Test - acc:         0.799745 loss:        0.692862
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.891330 loss:        0.325026
Test - acc:         0.783185 loss:        0.776448
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.926603 loss:        0.222185
Test - acc:         0.807134 loss:        0.703915
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.931249 loss:        0.209354
Test - acc:         0.760255 loss:        1.008253
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.928504 loss:        0.216180
Test - acc:         0.818854 loss:        0.649240
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.935368 loss:        0.204168
Test - acc:         0.787516 loss:        0.792519
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.920900 loss:        0.230984
Test - acc:         0.739873 loss:        0.978703
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.925758 loss:        0.217688
Test - acc:         0.746242 loss:        1.074337
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.914141 loss:        0.251515
Test - acc:         0.585732 loss:        1.970380
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.907382 loss:        0.279499
Test - acc:         0.766115 loss:        0.847784
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.913296 loss:        0.252401
Test - acc:         0.803312 loss:        0.741162
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.918365 loss:        0.238642
Test - acc:         0.793885 loss:        0.718118
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.921322 loss:        0.235733
Test - acc:         0.760764 loss:        0.867886
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.927025 loss:        0.215297
Test - acc:         0.788025 loss:        0.841103
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.927025 loss:        0.223481
Test - acc:         0.777834 loss:        0.857374
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.920161 loss:        0.240780
Test - acc:         0.767134 loss:        0.953318
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.933995 loss:        0.204343
Test - acc:         0.765350 loss:        0.901294
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.921956 loss:        0.231653
Test - acc:         0.745732 loss:        0.923323
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.922167 loss:        0.232883
Test - acc:         0.734777 loss:        1.158481
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.924702 loss:        0.227457
Test - acc:         0.785732 loss:        0.807358
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.926391 loss:        0.219398
Test - acc:         0.792866 loss:        0.775483
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.925018 loss:        0.220869
Test - acc:         0.787516 loss:        0.799996
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.930616 loss:        0.203542
Test - acc:         0.789554 loss:        0.733287
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.924596 loss:        0.228023
Test - acc:         0.766879 loss:        0.914341
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.925441 loss:        0.221183
Test - acc:         0.714650 loss:        1.186949
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.929876 loss:        0.210099
Test - acc:         0.823694 loss:        0.618406
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.930299 loss:        0.208696
Test - acc:         0.754650 loss:        0.932629
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.929348 loss:        0.208741
Test - acc:         0.732994 loss:        1.097910
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.928187 loss:        0.214976
Test - acc:         0.766369 loss:        0.952993
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.932411 loss:        0.201131
Test - acc:         0.675924 loss:        1.434241
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.917837 loss:        0.245649
Test - acc:         0.728408 loss:        1.265371
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.928820 loss:        0.217182
Test - acc:         0.792357 loss:        0.789037
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.937269 loss:        0.189720
Test - acc:         0.784968 loss:        0.870989
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.922906 loss:        0.229979
Test - acc:         0.376815 loss:        4.340586
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.925863 loss:        0.223526
Test - acc:         0.823185 loss:        0.671242
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.952054 loss:        0.148371
Test - acc:         0.818599 loss:        0.708291
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.958707 loss:        0.126213
Test - acc:         0.827516 loss:        0.694869
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.941705 loss:        0.176040
Test - acc:         0.813503 loss:        0.674708
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.946246 loss:        0.166712
Test - acc:         0.772229 loss:        0.891901
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.940015 loss:        0.167946
Test - acc:         0.747261 loss:        1.065472
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.935685 loss:        0.187210
Test - acc:         0.780127 loss:        0.825309
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.940226 loss:        0.175795
Test - acc:         0.766115 loss:        1.075235
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.947407 loss:        0.156556
Test - acc:         0.794395 loss:        0.826944
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.947196 loss:        0.158572
Test - acc:         0.754650 loss:        1.016626
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.945823 loss:        0.172086
Test - acc:         0.791083 loss:        0.918662
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.936952 loss:        0.181291
Test - acc:         0.806879 loss:        0.751485
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.939909 loss:        0.174136
Test - acc:         0.809936 loss:        0.723606
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.939909 loss:        0.180006
Test - acc:         0.779618 loss:        0.949168
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.945295 loss:        0.166488
Test - acc:         0.627006 loss:        1.772900
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.936952 loss:        0.182044
Test - acc:         0.756433 loss:        1.019930
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.936107 loss:        0.189803
Test - acc:         0.810701 loss:        0.765544
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.936952 loss:        0.189605
Test - acc:         0.775287 loss:        0.915133
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.941916 loss:        0.172306
Test - acc:         0.778089 loss:        0.866897
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.940437 loss:        0.176077
Test - acc:         0.783949 loss:        0.850558
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.942127 loss:        0.172408
Test - acc:         0.775796 loss:        0.938720
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.942655 loss:        0.177754
Test - acc:         0.795924 loss:        0.824652
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.941388 loss:        0.175702
Test - acc:         0.808408 loss:        0.766055
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.939592 loss:        0.179673
Test - acc:         0.822930 loss:        0.680806
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.943394 loss:        0.170068
Test - acc:         0.828280 loss:        0.686854
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.947935 loss:        0.153297
Test - acc:         0.821146 loss:        0.682822
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.937903 loss:        0.181151
Test - acc:         0.758726 loss:        0.956301
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.941599 loss:        0.178731
Test - acc:         0.824713 loss:        0.635901
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.944556 loss:        0.171177
Test - acc:         0.695287 loss:        1.281651
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.940965 loss:        0.177920
Test - acc:         0.830573 loss:        0.692837
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.939064 loss:        0.171896
Test - acc:         0.815032 loss:        0.693473
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.935474 loss:        0.192962
Test - acc:         0.803057 loss:        0.729553
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.966945 loss:        0.101963
Test - acc:         0.815032 loss:        0.790707
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.970958 loss:        0.091605
Test - acc:         0.803312 loss:        0.851957
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.954377 loss:        0.138828
Test - acc:         0.803057 loss:        0.757055
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.956067 loss:        0.132534
Test - acc:         0.816306 loss:        0.679172
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.959024 loss:        0.130288
Test - acc:         0.757707 loss:        1.004804
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.963882 loss:        0.110175
Test - acc:         0.807134 loss:        0.790045
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.953638 loss:        0.140076
Test - acc:         0.762293 loss:        1.036196
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.944556 loss:        0.167309
Test - acc:         0.788535 loss:        0.914662
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.956384 loss:        0.129585
Test - acc:         0.814522 loss:        0.789046
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.958391 loss:        0.127763
Test - acc:         0.808662 loss:        0.779242
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.950048 loss:        0.146302
Test - acc:         0.792357 loss:        0.791641
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.955328 loss:        0.132838
Test - acc:         0.788535 loss:        0.887196
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.954800 loss:        0.140997
Test - acc:         0.819618 loss:        0.705140
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.943817 loss:        0.170740
Test - acc:         0.672357 loss:        1.676162
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.943183 loss:        0.172073
Test - acc:         0.810191 loss:        0.749515
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.946879 loss:        0.165821
Test - acc:         0.792866 loss:        0.776231
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.957229 loss:        0.132357
Test - acc:         0.781146 loss:        0.975979
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.945190 loss:        0.168720
Test - acc:         0.789045 loss:        0.825625
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.949519 loss:        0.150522
Test - acc:         0.769936 loss:        0.919859
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.946351 loss:        0.152128
Test - acc:         0.821146 loss:        0.722431
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.952265 loss:        0.144022
Test - acc:         0.773503 loss:        0.999230
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.943500 loss:        0.168875
Test - acc:         0.823694 loss:        0.636910
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.956490 loss:        0.136364
Test - acc:         0.808408 loss:        0.857633
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.948675 loss:        0.152908
Test - acc:         0.750318 loss:        1.090419
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.956595 loss:        0.136987
Test - acc:         0.793376 loss:        0.883598
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.956701 loss:        0.135732
Test - acc:         0.816051 loss:        0.733640
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.948463 loss:        0.150504
Test - acc:         0.771210 loss:        0.945790
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.945612 loss:        0.161225
Test - acc:         0.788025 loss:        0.851758
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.940543 loss:        0.171820
Test - acc:         0.772229 loss:        0.911985
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.959658 loss:        0.123299
Test - acc:         0.808408 loss:        0.820099
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.945929 loss:        0.153498
Test - acc:         0.744968 loss:        1.155441
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.948886 loss:        0.152333
Test - acc:         0.796943 loss:        0.840302
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.965783 loss:        0.106552
Test - acc:         0.842038 loss:        0.615191
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.975710 loss:        0.076111
Test - acc:         0.847389 loss:        0.613832
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.967262 loss:        0.099653
Test - acc:         0.838981 loss:        0.675177
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.959869 loss:        0.122799
Test - acc:         0.765096 loss:        1.063539
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.955328 loss:        0.129626
Test - acc:         0.757962 loss:        1.091039
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.961136 loss:        0.124258
Test - acc:         0.805605 loss:        0.748403
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.958285 loss:        0.129306
Test - acc:         0.769172 loss:        0.943426
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.958179 loss:        0.127899
Test - acc:         0.687389 loss:        1.518652
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.949414 loss:        0.154866
Test - acc:         0.700637 loss:        1.563236
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.961559 loss:        0.118695
Test - acc:         0.832357 loss:        0.620829
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.965889 loss:        0.106867
Test - acc:         0.774777 loss:        0.999704
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.960925 loss:        0.118301
Test - acc:         0.814522 loss:        0.752734
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.953216 loss:        0.135768
Test - acc:         0.823439 loss:        0.706324
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.948358 loss:        0.146131
Test - acc:         0.787261 loss:        0.820618
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.950048 loss:        0.150797
Test - acc:         0.797962 loss:        0.792007
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.961453 loss:        0.125123
Test - acc:         0.774013 loss:        0.930662
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.960397 loss:        0.122233
Test - acc:         0.786497 loss:        0.938277
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.956278 loss:        0.138216
Test - acc:         0.773758 loss:        0.971557
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.954589 loss:        0.136065
Test - acc:         0.807643 loss:        0.717736
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.956067 loss:        0.133049
Test - acc:         0.786242 loss:        0.977647
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.953427 loss:        0.137388
Test - acc:         0.757707 loss:        0.943301
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.955434 loss:        0.131979
Test - acc:         0.799236 loss:        0.989780
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.983103 loss:        0.059201
Test - acc:         0.888408 loss:        0.401671
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.994825 loss:        0.027434
Test - acc:         0.892484 loss:        0.390639
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.996093 loss:        0.021073
Test - acc:         0.894013 loss:        0.385981
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.996621 loss:        0.018073
Test - acc:         0.895541 loss:        0.383286
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.998733 loss:        0.014423
Test - acc:         0.895541 loss:        0.387229
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.998733 loss:        0.012228
Test - acc:         0.892994 loss:        0.392979
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.011039
Test - acc:         0.896306 loss:        0.389641
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.999050 loss:        0.010584
Test - acc:         0.895032 loss:        0.392249
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.009005
Test - acc:         0.897834 loss:        0.389029
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.009010
Test - acc:         0.894268 loss:        0.392985
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.998627 loss:        0.014628
Test - acc:         0.890955 loss:        0.392314
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.998944 loss:        0.013367
Test - acc:         0.892994 loss:        0.389988
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.998838 loss:        0.012329
Test - acc:         0.893248 loss:        0.391002
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.998838 loss:        0.011738
Test - acc:         0.895541 loss:        0.386625
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.010925
Test - acc:         0.894268 loss:        0.393590
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.999155 loss:        0.010402
Test - acc:         0.895287 loss:        0.387221
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.009469
Test - acc:         0.895032 loss:        0.387309
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.008282
Test - acc:         0.895032 loss:        0.390861
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.008981
Test - acc:         0.895287 loss:        0.387723
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.008690
Test - acc:         0.895287 loss:        0.388216
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.007861
Test - acc:         0.896051 loss:        0.388788
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.007649
Test - acc:         0.895032 loss:        0.390299
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.007208
Test - acc:         0.896306 loss:        0.392884
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.006590
Test - acc:         0.896306 loss:        0.390934
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.006495
Test - acc:         0.897325 loss:        0.392749
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.006959
Test - acc:         0.894013 loss:        0.391949
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.006741
Test - acc:         0.895541 loss:        0.390680
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.006157
Test - acc:         0.897070 loss:        0.387053
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.005557
Test - acc:         0.897834 loss:        0.390569
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005391
Test - acc:         0.898599 loss:        0.382928
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005830
Test - acc:         0.899873 loss:        0.378609
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.006020
Test - acc:         0.898599 loss:        0.384072
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.005763
Test - acc:         0.898854 loss:        0.384249
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.005142
Test - acc:         0.897834 loss:        0.388176
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.005059
Test - acc:         0.897325 loss:        0.386484
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005153
Test - acc:         0.897834 loss:        0.388361
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004954
Test - acc:         0.897070 loss:        0.386726
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005324
Test - acc:         0.894777 loss:        0.389444
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005318
Test - acc:         0.896051 loss:        0.390664
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.005408
Test - acc:         0.896051 loss:        0.389805
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.005674
Test - acc:         0.897834 loss:        0.389823
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005369
Test - acc:         0.898089 loss:        0.396163
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.982680 loss:        0.069052
Test - acc:         0.880510 loss:        0.413816
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.992924 loss:        0.038851
Test - acc:         0.885350 loss:        0.409223
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.996093 loss:        0.026895
Test - acc:         0.888153 loss:        0.407644
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.997993 loss:        0.022409
Test - acc:         0.884586 loss:        0.417709
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.997149 loss:        0.020823
Test - acc:         0.889427 loss:        0.412844
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.997993 loss:        0.017834
Test - acc:         0.883567 loss:        0.429812
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.998627 loss:        0.016945
Test - acc:         0.884841 loss:        0.417226
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.999155 loss:        0.013287
Test - acc:         0.886369 loss:        0.423860
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.998416 loss:        0.014099
Test - acc:         0.885860 loss:        0.414340
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.999050 loss:        0.013142
Test - acc:         0.886369 loss:        0.434536
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.999050 loss:        0.011346
Test - acc:         0.888917 loss:        0.422357
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.010502
Test - acc:         0.884841 loss:        0.431837
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.999155 loss:        0.010751
Test - acc:         0.889936 loss:        0.425121
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.009580
Test - acc:         0.886624 loss:        0.428697
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.009699
Test - acc:         0.884841 loss:        0.421092
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.998944 loss:        0.009795
Test - acc:         0.885860 loss:        0.433130
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.999155 loss:        0.009335
Test - acc:         0.888153 loss:        0.431304
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.008933
Test - acc:         0.887643 loss:        0.428340
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.009120
Test - acc:         0.891975 loss:        0.420039
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.998838 loss:        0.008645
Test - acc:         0.890446 loss:        0.415570
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.998733 loss:        0.009727
Test - acc:         0.891975 loss:        0.423581
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.007871
Test - acc:         0.889682 loss:        0.417145
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.999155 loss:        0.008373
Test - acc:         0.889427 loss:        0.428622
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.007819
Test - acc:         0.892484 loss:        0.423545
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.007010
Test - acc:         0.893248 loss:        0.420213
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.006481
Test - acc:         0.893758 loss:        0.416990
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.006677
Test - acc:         0.892484 loss:        0.423521
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.007492
Test - acc:         0.891720 loss:        0.417630
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.007191
Test - acc:         0.891975 loss:        0.424439
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.999050 loss:        0.007686
Test - acc:         0.890701 loss:        0.429283
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.006352
Test - acc:         0.894777 loss:        0.418469
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.006062
Test - acc:         0.891975 loss:        0.424554
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.889851 loss:        0.336818
Test - acc:         0.845350 loss:        0.508675
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.947935 loss:        0.178443
Test - acc:         0.851975 loss:        0.523261
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.960291 loss:        0.136451
Test - acc:         0.855796 loss:        0.488218
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.969057 loss:        0.114611
Test - acc:         0.829554 loss:        0.619367
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.971697 loss:        0.096772
Test - acc:         0.870064 loss:        0.463993
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.975499 loss:        0.090611
Test - acc:         0.864968 loss:        0.466243
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.977822 loss:        0.079826
Test - acc:         0.865223 loss:        0.457276
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.982680 loss:        0.071012
Test - acc:         0.865223 loss:        0.498512
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.982892 loss:        0.065532
Test - acc:         0.871083 loss:        0.484293
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.980463 loss:        0.065402
Test - acc:         0.862166 loss:        0.502771
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.985215 loss:        0.061392
Test - acc:         0.862930 loss:        0.498881
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.982786 loss:        0.062828
Test - acc:         0.872866 loss:        0.457933
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.985004 loss:        0.058873
Test - acc:         0.869809 loss:        0.476156
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.985743 loss:        0.054731
Test - acc:         0.865478 loss:        0.504775
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.984053 loss:        0.056815
Test - acc:         0.864968 loss:        0.543092
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.984581 loss:        0.058493
Test - acc:         0.855796 loss:        0.548480
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.985532 loss:        0.053365
Test - acc:         0.882803 loss:        0.447043
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.987010 loss:        0.048671
Test - acc:         0.872102 loss:        0.471378
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.986165 loss:        0.047936
Test - acc:         0.877452 loss:        0.506440
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.986060 loss:        0.052414
Test - acc:         0.865732 loss:        0.524811
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.988172 loss:        0.048069
Test - acc:         0.877197 loss:        0.472847
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.988383 loss:        0.042968
Test - acc:         0.872357 loss:        0.512690
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.989967 loss:        0.040166
Test - acc:         0.880764 loss:        0.470120
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.988278 loss:        0.042726
Test - acc:         0.865987 loss:        0.529808
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.990390 loss:        0.038243
Test - acc:         0.872102 loss:        0.508832
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.989122 loss:        0.039101
Test - acc:         0.876178 loss:        0.496229
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.992185 loss:        0.030889
Test - acc:         0.882038 loss:        0.455138
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.994614 loss:        0.024188
Test - acc:         0.884841 loss:        0.453761
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.996198 loss:        0.020279
Test - acc:         0.883057 loss:        0.453027
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.997149 loss:        0.019087
Test - acc:         0.885350 loss:        0.455144
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.996515 loss:        0.018624
Test - acc:         0.884331 loss:        0.445515
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.997465 loss:        0.016987
Test - acc:         0.885605 loss:        0.445831
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.795332 loss:        0.623064
Test - acc:         0.818344 loss:        0.594630
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.879079 loss:        0.377682
Test - acc:         0.833631 loss:        0.544487
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.896188 loss:        0.328124
Test - acc:         0.843312 loss:        0.516436
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.903052 loss:        0.297481
Test - acc:         0.845605 loss:        0.496908
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.918893 loss:        0.264119
Test - acc:         0.847134 loss:        0.491612
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.923751 loss:        0.244189
Test - acc:         0.851975 loss:        0.484536
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.932200 loss:        0.227375
Test - acc:         0.850955 loss:        0.473732
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.934312 loss:        0.214472
Test - acc:         0.852739 loss:        0.474917
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.936952 loss:        0.210302
Test - acc:         0.854522 loss:        0.470161
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.937480 loss:        0.204644
Test - acc:         0.855796 loss:        0.460807
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.940120 loss:        0.196649
Test - acc:         0.856306 loss:        0.463141
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.944978 loss:        0.189592
Test - acc:         0.853503 loss:        0.469253
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.945718 loss:        0.180364
Test - acc:         0.858089 loss:        0.462973
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.948780 loss:        0.172838
Test - acc:         0.860637 loss:        0.459956
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.946774 loss:        0.170599
Test - acc:         0.858854 loss:        0.464309
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.950787 loss:        0.165837
Test - acc:         0.862166 loss:        0.450499
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.953638 loss:        0.161433
Test - acc:         0.865732 loss:        0.448492
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.954800 loss:        0.156426
Test - acc:         0.858854 loss:        0.453847
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.957968 loss:        0.148846
Test - acc:         0.861656 loss:        0.453879
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.957229 loss:        0.144669
Test - acc:         0.861146 loss:        0.448648
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.961031 loss:        0.141536
Test - acc:         0.860382 loss:        0.452391
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.958285 loss:        0.144494
Test - acc:         0.864713 loss:        0.450074
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.964938 loss:        0.133887
Test - acc:         0.866242 loss:        0.446165
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.962087 loss:        0.133228
Test - acc:         0.860892 loss:        0.457326
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.963988 loss:        0.131176
Test - acc:         0.860892 loss:        0.453543
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.962615 loss:        0.128371
Test - acc:         0.867006 loss:        0.447562
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.962192 loss:        0.131210
Test - acc:         0.867006 loss:        0.447608
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.966311 loss:        0.124830
Test - acc:         0.863949 loss:        0.445162
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.964305 loss:        0.125418
Test - acc:         0.868790 loss:        0.438628
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.967050 loss:        0.115393
Test - acc:         0.867516 loss:        0.439089
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.964516 loss:        0.122127
Test - acc:         0.868280 loss:        0.444165
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.966628 loss:        0.118651
Test - acc:         0.866752 loss:        0.434871
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.559510 loss:        1.391993
Test - acc:         0.683822 loss:        1.042672
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.706305 loss:        0.975522
Test - acc:         0.719745 loss:        0.917540
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.739782 loss:        0.862523
Test - acc:         0.738599 loss:        0.848019
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.763861 loss:        0.799892
Test - acc:         0.749299 loss:        0.805560
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.772943 loss:        0.753121
Test - acc:         0.762293 loss:        0.761590
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.781709 loss:        0.722146
Test - acc:         0.766879 loss:        0.746684
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.790263 loss:        0.702862
Test - acc:         0.767389 loss:        0.745565
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.796177 loss:        0.677832
Test - acc:         0.773758 loss:        0.725941
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.798817 loss:        0.663515
Test - acc:         0.783694 loss:        0.702126
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.800507 loss:        0.648214
Test - acc:         0.787006 loss:        0.687690
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.809378 loss:        0.624792
Test - acc:         0.786242 loss:        0.680106
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.815081 loss:        0.619213
Test - acc:         0.788025 loss:        0.675224
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.814870 loss:        0.604304
Test - acc:         0.785223 loss:        0.672251
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.821100 loss:        0.593658
Test - acc:         0.792611 loss:        0.664541
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.825325 loss:        0.578407
Test - acc:         0.796943 loss:        0.652480
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.825747 loss:        0.577587
Test - acc:         0.801019 loss:        0.639456
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.825958 loss:        0.566801
Test - acc:         0.797452 loss:        0.645479
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.827437 loss:        0.569692
Test - acc:         0.802548 loss:        0.626011
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.830077 loss:        0.549082
Test - acc:         0.802803 loss:        0.624126
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.831028 loss:        0.550953
Test - acc:         0.802803 loss:        0.632735
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.838209 loss:        0.534066
Test - acc:         0.803312 loss:        0.621640
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.834724 loss:        0.534466
Test - acc:         0.806624 loss:        0.615348
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.836942 loss:        0.537254
Test - acc:         0.803057 loss:        0.624170
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.838103 loss:        0.527078
Test - acc:         0.808153 loss:        0.599472
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.838631 loss:        0.519367
Test - acc:         0.804331 loss:        0.611814
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.844757 loss:        0.512893
Test - acc:         0.805860 loss:        0.604988
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.844968 loss:        0.503104
Test - acc:         0.808662 loss:        0.598104
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.840743 loss:        0.508670
Test - acc:         0.811975 loss:        0.593843
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.847080 loss:        0.500410
Test - acc:         0.810446 loss:        0.594780
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.846341 loss:        0.496419
Test - acc:         0.813503 loss:        0.595803
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.847502 loss:        0.498445
Test - acc:         0.813503 loss:        0.585351
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.849615 loss:        0.487509
Test - acc:         0.815796 loss:        0.578350
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.362657 loss:        1.878370
Test - acc:         0.431083 loss:        1.673708
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.500158 loss:        1.557950
Test - acc:         0.499873 loss:        1.510413
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.545887 loss:        1.448490
Test - acc:         0.539873 loss:        1.406169
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.580315 loss:        1.370133
Test - acc:         0.607134 loss:        1.339799
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.601225 loss:        1.316830
Test - acc:         0.617580 loss:        1.279006
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.619706 loss:        1.270854
Test - acc:         0.613758 loss:        1.265957
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.633013 loss:        1.235751
Test - acc:         0.663439 loss:        1.183668
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.645263 loss:        1.198709
Test - acc:         0.649936 loss:        1.205596
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.649277 loss:        1.181215
Test - acc:         0.529682 loss:        1.388157
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.657197 loss:        1.155953
Test - acc:         0.684076 loss:        1.108466
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.666807 loss:        1.128360
Test - acc:         0.674140 loss:        1.101664
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.669870 loss:        1.111543
Test - acc:         0.681529 loss:        1.086487
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.674411 loss:        1.095250
Test - acc:         0.682803 loss:        1.065946
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.672616 loss:        1.091228
Test - acc:         0.666242 loss:        1.112998
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.681170 loss:        1.064612
Test - acc:         0.693248 loss:        1.039282
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.686873 loss:        1.054848
Test - acc:         0.682038 loss:        1.038497
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.687507 loss:        1.048349
Test - acc:         0.682803 loss:        1.020784
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.691731 loss:        1.033944
Test - acc:         0.700892 loss:        0.994841
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.701236 loss:        1.023665
Test - acc:         0.687389 loss:        1.016638
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.694371 loss:        1.016170
Test - acc:         0.701146 loss:        0.994947
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.700813 loss:        1.002985
Test - acc:         0.702420 loss:        0.980506
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.705460 loss:        0.991380
Test - acc:         0.687898 loss:        1.030518
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.702080 loss:        0.989383
Test - acc:         0.714140 loss:        0.973632
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.708417 loss:        0.981721
Test - acc:         0.707006 loss:        0.964730
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.710740 loss:        0.969507
Test - acc:         0.716688 loss:        0.949548
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.712219 loss:        0.964460
Test - acc:         0.708790 loss:        0.946902
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.716021 loss:        0.949549
Test - acc:         0.709809 loss:        0.946360
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.718767 loss:        0.950612
Test - acc:         0.707006 loss:        0.960037
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.717499 loss:        0.941039
Test - acc:         0.700637 loss:        0.972245
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.722991 loss:        0.927143
Test - acc:         0.732229 loss:        0.914689
Sparsity :          0.9990
Wdecay :        0.000500
