Running --model densenet121 --dataset imagenette --seed 42 --logdir=criterion_experiment_no_bias/densenet121 --prune_criterion weight_squared_div_flips --prune_freq 117 --prune_rate 0.5 --noise --comment=densenet121_crit=weight_squared_div_flips_pf=117_seed=42 --save_model=pre-finetune/densenet121_weight_squared_div_flips_pf117_s42
******************************
Running
{
    "model": "densenet121",
    "dataset": "imagenette",
    "batch_size": 128,
    "test_batch_size": 500,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 117,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/densenet121",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/densenet121_weight_squared_div_flips_pf117_s42",
    "load_model": null,
    "parallel": false
}
******************************
Total prunable params of model: 6880448
Model has 6964106 total params.
num_weights=6922272
num_biases=41834
num.prunable=6880448
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.304467 loss:        2.247276
Test - acc:         0.223439 loss:        2.564861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.392227 loss:        2.030704
Test - acc:         0.414268 loss:        2.056558
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.492660 loss:        1.565982
Test - acc:         0.390318 loss:        6.541212
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.548949 loss:        1.371716
Test - acc:         0.542166 loss:        1.369238
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.599852 loss:        1.205749
Test - acc:         0.533503 loss:        1.420694
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.634280 loss:        1.127794
Test - acc:         0.631083 loss:        1.145493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.667124 loss:        1.025752
Test - acc:         0.644586 loss:        1.133085
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.692259 loss:        0.941326
Test - acc:         0.645096 loss:        1.132447
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.713803 loss:        0.867857
Test - acc:         0.648917 loss:        1.115370
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.727954 loss:        0.838575
Test - acc:         0.662166 loss:        1.119164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.745274 loss:        0.782161
Test - acc:         0.654013 loss:        1.224131
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.764389 loss:        0.713538
Test - acc:         0.711592 loss:        0.916667
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.773683 loss:        0.698923
Test - acc:         0.716433 loss:        0.919082
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.790791 loss:        0.640160
Test - acc:         0.708535 loss:        0.936016
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.788784 loss:        0.636279
Test - acc:         0.730446 loss:        0.930228
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.803253 loss:        0.592168
Test - acc:         0.742166 loss:        0.781737
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.815081 loss:        0.557255
Test - acc:         0.735796 loss:        0.883404
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.817827 loss:        0.552658
Test - acc:         0.743439 loss:        0.873422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.821840 loss:        0.536533
Test - acc:         0.739873 loss:        0.890160
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.820889 loss:        0.525431
Test - acc:         0.739618 loss:        0.904321
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.839054 loss:        0.492398
Test - acc:         0.744713 loss:        0.826697
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.844017 loss:        0.477280
Test - acc:         0.738854 loss:        0.952022
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.852677 loss:        0.436604
Test - acc:         0.699873 loss:        1.164193
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.847714 loss:        0.451899
Test - acc:         0.696306 loss:        1.140190
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.860175 loss:        0.431885
Test - acc:         0.790318 loss:        0.684371
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.864716 loss:        0.396631
Test - acc:         0.724076 loss:        0.997666
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.864716 loss:        0.403929
Test - acc:         0.731975 loss:        1.006542
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.867145 loss:        0.395460
Test - acc:         0.772229 loss:        0.789450
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.872109 loss:        0.378557
Test - acc:         0.675669 loss:        1.341159
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.874960 loss:        0.375969
Test - acc:         0.687643 loss:        1.195287
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.880452 loss:        0.354471
Test - acc:         0.740892 loss:        0.997564
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.886472 loss:        0.347214
Test - acc:         0.740892 loss:        0.909023
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.881614 loss:        0.351005
Test - acc:         0.764586 loss:        0.856611
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.891330 loss:        0.322453
Test - acc:         0.743185 loss:        0.995336
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.897772 loss:        0.307338
Test - acc:         0.754140 loss:        0.912247
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.889851 loss:        0.337876
Test - acc:         0.721019 loss:        1.011775
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.897983 loss:        0.305074
Test - acc:         0.659108 loss:        1.572657
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.893759 loss:        0.314284
Test - acc:         0.738089 loss:        0.954976
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.896610 loss:        0.307999
Test - acc:         0.735032 loss:        0.995577
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.904636 loss:        0.284709
Test - acc:         0.730446 loss:        0.979973
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.899250 loss:        0.300870
Test - acc:         0.789299 loss:        0.762062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.909072 loss:        0.276104
Test - acc:         0.772739 loss:        0.838292
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.903263 loss:        0.286621
Test - acc:         0.769427 loss:        0.813246
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.904847 loss:        0.279276
Test - acc:         0.702930 loss:        1.111748
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.908966 loss:        0.275545
Test - acc:         0.781146 loss:        0.751864
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.898194 loss:        0.301317
Test - acc:         0.759745 loss:        0.939245
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.903897 loss:        0.289866
Test - acc:         0.764331 loss:        0.855191
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.912346 loss:        0.263084
Test - acc:         0.767389 loss:        0.865867
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.912768 loss:        0.264473
Test - acc:         0.777580 loss:        0.842370
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.921428 loss:        0.229516
Test - acc:         0.768408 loss:        0.813424
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.904108 loss:        0.279005
Test - acc:         0.763057 loss:        0.844168
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.921217 loss:        0.240037
Test - acc:         0.811975 loss:        0.683740
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.918999 loss:        0.247259
Test - acc:         0.807643 loss:        0.729984
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.920372 loss:        0.241421
Test - acc:         0.755159 loss:        0.937320
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.916781 loss:        0.249266
Test - acc:         0.815032 loss:        0.655841
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.930299 loss:        0.210907
Test - acc:         0.677197 loss:        1.426336
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.916887 loss:        0.245042
Test - acc:         0.727134 loss:        1.107167
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.931672 loss:        0.195710
Test - acc:         0.762548 loss:        0.994090
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.923012 loss:        0.226994
Test - acc:         0.792357 loss:        0.749360
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.922801 loss:        0.227862
Test - acc:         0.731465 loss:        1.131942
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.914458 loss:        0.255154
Test - acc:         0.767389 loss:        0.894864
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.913507 loss:        0.260792
Test - acc:         0.798471 loss:        0.708206
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.928715 loss:        0.211369
Test - acc:         0.722803 loss:        1.110113
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.928820 loss:        0.213945
Test - acc:         0.770701 loss:        0.902927
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.931566 loss:        0.200986
Test - acc:         0.691465 loss:        1.350642
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.918471 loss:        0.237279
Test - acc:         0.768153 loss:        0.896510
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.925969 loss:        0.218501
Test - acc:         0.776306 loss:        0.922147
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.924807 loss:        0.218472
Test - acc:         0.804331 loss:        0.692022
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.929454 loss:        0.209102
Test - acc:         0.786497 loss:        0.788930
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.933890 loss:        0.192959
Test - acc:         0.721529 loss:        1.087821
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.935790 loss:        0.193158
Test - acc:         0.790828 loss:        0.773965
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.920689 loss:        0.228797
Test - acc:         0.810191 loss:        0.684486
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.929982 loss:        0.210960
Test - acc:         0.715159 loss:        1.173540
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.924174 loss:        0.216157
Test - acc:         0.696051 loss:        1.283384
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.934101 loss:        0.200850
Test - acc:         0.697070 loss:        1.245805
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.926603 loss:        0.215647
Test - acc:         0.795159 loss:        0.789705
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.932411 loss:        0.206750
Test - acc:         0.758726 loss:        0.946321
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.936424 loss:        0.190468
Test - acc:         0.821146 loss:        0.665166
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.924913 loss:        0.224524
Test - acc:         0.773503 loss:        0.905270
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.939487 loss:        0.178514
Test - acc:         0.783949 loss:        0.830896
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.936107 loss:        0.196375
Test - acc:         0.791592 loss:        0.724209
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.933890 loss:        0.201076
Test - acc:         0.766369 loss:        0.886062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.933467 loss:        0.197659
Test - acc:         0.704713 loss:        1.235384
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.932833 loss:        0.202619
Test - acc:         0.815287 loss:        0.711514
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.930193 loss:        0.203181
Test - acc:         0.695032 loss:        1.375744
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.931883 loss:        0.207657
Test - acc:         0.808662 loss:        0.721104
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.932728 loss:        0.204024
Test - acc:         0.775287 loss:        0.896260
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.924068 loss:        0.226663
Test - acc:         0.749809 loss:        0.976452
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.930827 loss:        0.205302
Test - acc:         0.773503 loss:        0.843425
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.951632 loss:        0.149842
Test - acc:         0.762548 loss:        1.024508
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.937480 loss:        0.183135
Test - acc:         0.803312 loss:        0.796250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.935685 loss:        0.192399
Test - acc:         0.819363 loss:        0.644077
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.936741 loss:        0.187542
Test - acc:         0.836178 loss:        0.616324
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.944345 loss:        0.171163
Test - acc:         0.812229 loss:        0.736030
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.941282 loss:        0.174625
Test - acc:         0.805096 loss:        0.773863
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.941282 loss:        0.175413
Test - acc:         0.774013 loss:        0.949398
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.941493 loss:        0.174572
Test - acc:         0.753885 loss:        0.905791
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.936952 loss:        0.192417
Test - acc:         0.834904 loss:        0.600748
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.943605 loss:        0.164746
Test - acc:         0.812994 loss:        0.761982
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.939804 loss:        0.177076
Test - acc:         0.816561 loss:        0.682452
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.942338 loss:        0.179424
Test - acc:         0.716433 loss:        1.184299
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.945295 loss:        0.169221
Test - acc:         0.816561 loss:        0.718980
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.946668 loss:        0.156176
Test - acc:         0.788025 loss:        0.889367
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.941176 loss:        0.170386
Test - acc:         0.801529 loss:        0.736969
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.940226 loss:        0.180332
Test - acc:         0.757707 loss:        0.967769
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.946668 loss:        0.155490
Test - acc:         0.809172 loss:        0.779326
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.940332 loss:        0.175685
Test - acc:         0.797452 loss:        0.737818
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.943711 loss:        0.161579
Test - acc:         0.788025 loss:        0.890930
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.950787 loss:        0.146198
Test - acc:         0.774013 loss:        0.879401
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.942761 loss:        0.174207
Test - acc:         0.794904 loss:        0.815083
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.944345 loss:        0.163455
Test - acc:         0.786242 loss:        0.847395
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.938853 loss:        0.185155
Test - acc:         0.739873 loss:        1.076384
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.944662 loss:        0.167687
Test - acc:         0.773758 loss:        0.992194
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.940754 loss:        0.179966
Test - acc:         0.792866 loss:        0.731209
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.948358 loss:        0.152817
Test - acc:         0.819108 loss:        0.663036
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.947091 loss:        0.159350
Test - acc:         0.738089 loss:        1.112048
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.945612 loss:        0.163534
Test - acc:         0.812739 loss:        0.773844
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.963565 loss:        0.112526
Test - acc:         0.789554 loss:        0.853690
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.975077 loss:        0.075692
Test - acc:         0.820382 loss:        0.778502
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.974232 loss:        0.081318
Test - acc:         0.836943 loss:        0.691414
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.967895 loss:        0.098147
Test - acc:         0.795669 loss:        0.834742
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.960291 loss:        0.120162
Test - acc:         0.833121 loss:        0.661706
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.949731 loss:        0.151159
Test - acc:         0.743694 loss:        1.095749
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.955222 loss:        0.129894
Test - acc:         0.773758 loss:        0.965941
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.960397 loss:        0.120171
Test - acc:         0.789045 loss:        0.904488
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.958074 loss:        0.124960
Test - acc:         0.819618 loss:        0.736331
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.955434 loss:        0.130333
Test - acc:         0.787006 loss:        1.033399
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.948041 loss:        0.151122
Test - acc:         0.848917 loss:        0.577982
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.953955 loss:        0.131979
Test - acc:         0.840764 loss:        0.611420
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.955222 loss:        0.130804
Test - acc:         0.813248 loss:        0.703659
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.947619 loss:        0.153825
Test - acc:         0.804586 loss:        0.791495
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.958602 loss:        0.127855
Test - acc:         0.851465 loss:        0.580317
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.961981 loss:        0.113391
Test - acc:         0.842803 loss:        0.601510
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.954589 loss:        0.126373
Test - acc:         0.791847 loss:        0.805048
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.960186 loss:        0.127593
Test - acc:         0.783439 loss:        0.890536
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.954905 loss:        0.139557
Test - acc:         0.780127 loss:        0.915455
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.957229 loss:        0.130032
Test - acc:         0.721019 loss:        1.651545
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.964410 loss:        0.107839
Test - acc:         0.827516 loss:        0.682794
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.958602 loss:        0.127546
Test - acc:         0.796433 loss:        0.809966
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.958074 loss:        0.126095
Test - acc:         0.830318 loss:        0.686015
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.954589 loss:        0.136613
Test - acc:         0.796688 loss:        0.765886
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.964621 loss:        0.110705
Test - acc:         0.812484 loss:        0.781781
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.955011 loss:        0.133766
Test - acc:         0.817580 loss:        0.669123
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.949519 loss:        0.149871
Test - acc:         0.800000 loss:        0.735521
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.952688 loss:        0.143199
Test - acc:         0.800000 loss:        0.725662
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.960080 loss:        0.124626
Test - acc:         0.799236 loss:        0.819825
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.967262 loss:        0.102368
Test - acc:         0.832102 loss:        0.675494
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.957440 loss:        0.127402
Test - acc:         0.799236 loss:        0.792213
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.949519 loss:        0.148952
Test - acc:         0.825223 loss:        0.690012
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.953005 loss:        0.138849
Test - acc:         0.715669 loss:        1.328462
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.981835 loss:        0.065285
Test - acc:         0.898089 loss:        0.382290
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.995036 loss:        0.026150
Test - acc:         0.898344 loss:        0.368776
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.997677 loss:        0.018224
Test - acc:         0.900382 loss:        0.363968
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.998416 loss:        0.013689
Test - acc:         0.900637 loss:        0.370367
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.998944 loss:        0.011709
Test - acc:         0.899618 loss:        0.373912
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.998627 loss:        0.010888
Test - acc:         0.900892 loss:        0.372176
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.998838 loss:        0.010286
Test - acc:         0.901401 loss:        0.372796
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.009097
Test - acc:         0.902930 loss:        0.368204
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.008505
Test - acc:         0.901401 loss:        0.362044
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.006770
Test - acc:         0.902166 loss:        0.365973
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.007462
Test - acc:         0.901656 loss:        0.372177
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.006811
Test - acc:         0.901656 loss:        0.369526
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.007102
Test - acc:         0.903949 loss:        0.366872
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.005640
Test - acc:         0.903185 loss:        0.367920
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005430
Test - acc:         0.903949 loss:        0.364309
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005326
Test - acc:         0.905223 loss:        0.363117
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005045
Test - acc:         0.906242 loss:        0.367222
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005289
Test - acc:         0.905223 loss:        0.369143
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004636
Test - acc:         0.904204 loss:        0.369008
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004803
Test - acc:         0.903949 loss:        0.362556
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004346
Test - acc:         0.902166 loss:        0.369111
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005074
Test - acc:         0.902930 loss:        0.371966
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004146
Test - acc:         0.904204 loss:        0.370486
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004124
Test - acc:         0.902166 loss:        0.362107
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004114
Test - acc:         0.903949 loss:        0.370139
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004050
Test - acc:         0.904204 loss:        0.369766
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004142
Test - acc:         0.903185 loss:        0.369171
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003776
Test - acc:         0.903439 loss:        0.367311
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004339
Test - acc:         0.905223 loss:        0.367288
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003698
Test - acc:         0.904459 loss:        0.367101
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003665
Test - acc:         0.903949 loss:        0.369460
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003765
Test - acc:         0.904968 loss:        0.367169
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003440
Test - acc:         0.905223 loss:        0.362352
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003634
Test - acc:         0.903185 loss:        0.362454
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003859
Test - acc:         0.900637 loss:        0.370986
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003670
Test - acc:         0.903694 loss:        0.365653
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003353
Test - acc:         0.902420 loss:        0.370126
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003654
Test - acc:         0.905223 loss:        0.362255
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003688
Test - acc:         0.903694 loss:        0.366395
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003308
Test - acc:         0.905987 loss:        0.361741
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003237
Test - acc:         0.907516 loss:        0.358529
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003336
Test - acc:         0.905478 loss:        0.362266
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003120
Test - acc:         0.906242 loss:        0.361800
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003251
Test - acc:         0.904713 loss:        0.359327
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003156
Test - acc:         0.902675 loss:        0.372676
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003386
Test - acc:         0.904204 loss:        0.369065
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002883
Test - acc:         0.902930 loss:        0.360426
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003182
Test - acc:         0.904713 loss:        0.365656
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002904
Test - acc:         0.906497 loss:        0.355423
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003065
Test - acc:         0.903185 loss:        0.363830
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003150
Test - acc:         0.907006 loss:        0.358897
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002780
Test - acc:         0.903185 loss:        0.360625
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003052
Test - acc:         0.903694 loss:        0.363300
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002950
Test - acc:         0.903949 loss:        0.363706
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002905
Test - acc:         0.903949 loss:        0.360060
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002853
Test - acc:         0.904204 loss:        0.363449
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002984
Test - acc:         0.905223 loss:        0.356289
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003004
Test - acc:         0.902166 loss:        0.364329
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003081
Test - acc:         0.903439 loss:        0.359280
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003097
Test - acc:         0.902930 loss:        0.362938
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003472
Test - acc:         0.903439 loss:        0.361951
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002828
Test - acc:         0.901656 loss:        0.364526
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002761
Test - acc:         0.905223 loss:        0.359487
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002665
Test - acc:         0.903694 loss:        0.363097
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002838
Test - acc:         0.905732 loss:        0.363422
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002492
Test - acc:         0.903694 loss:        0.359080
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002717
Test - acc:         0.905987 loss:        0.359636
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002751
Test - acc:         0.905223 loss:        0.359936
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002905
Test - acc:         0.906497 loss:        0.358775
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003168
Test - acc:         0.904713 loss:        0.364057
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002631
Test - acc:         0.904968 loss:        0.360188
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002701
Test - acc:         0.907771 loss:        0.355957
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003005
Test - acc:         0.904968 loss:        0.358859
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002686
Test - acc:         0.905223 loss:        0.362662
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002584
Test - acc:         0.902675 loss:        0.364219
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002640
Test - acc:         0.903949 loss:        0.361229
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003015
Test - acc:         0.904968 loss:        0.362811
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002781
Test - acc:         0.904968 loss:        0.358824
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002698
Test - acc:         0.905478 loss:        0.362841
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002630
Test - acc:         0.906752 loss:        0.362450
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002746
Test - acc:         0.907516 loss:        0.360897
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002768
Test - acc:         0.905732 loss:        0.362418
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.002877
Test - acc:         0.907261 loss:        0.367856
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002691
Test - acc:         0.906752 loss:        0.360433
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003495
Test - acc:         0.905987 loss:        0.358743
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002946
Test - acc:         0.907006 loss:        0.358629
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002853
Test - acc:         0.905732 loss:        0.356158
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003109
Test - acc:         0.909299 loss:        0.359978
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002961
Test - acc:         0.908790 loss:        0.357933
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003151
Test - acc:         0.905732 loss:        0.363393
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003151
Test - acc:         0.908535 loss:        0.357498
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002915
Test - acc:         0.906497 loss:        0.362303
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002922
Test - acc:         0.906752 loss:        0.361937
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002762
Test - acc:         0.907006 loss:        0.359739
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002729
Test - acc:         0.905223 loss:        0.360364
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002806
Test - acc:         0.905987 loss:        0.359283
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003079
Test - acc:         0.904713 loss:        0.360578
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002784
Test - acc:         0.907771 loss:        0.367319
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003197
Test - acc:         0.907006 loss:        0.363934
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.003010
Test - acc:         0.908280 loss:        0.365473
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.003121
Test - acc:         0.908280 loss:        0.358690
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002705
Test - acc:         0.907516 loss:        0.361931
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002741
Test - acc:         0.905732 loss:        0.364368
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002786
Test - acc:         0.905987 loss:        0.359624
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002476
Test - acc:         0.907516 loss:        0.359622
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002990
Test - acc:         0.904968 loss:        0.363442
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002648
Test - acc:         0.908535 loss:        0.363933
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002777
Test - acc:         0.907006 loss:        0.364066
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002599
Test - acc:         0.908790 loss:        0.363088
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002726
Test - acc:         0.905987 loss:        0.359616
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002635
Test - acc:         0.906497 loss:        0.358633
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002737
Test - acc:         0.907516 loss:        0.360858
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002642
Test - acc:         0.907771 loss:        0.358538
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002712
Test - acc:         0.905987 loss:        0.363830
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002634
Test - acc:         0.904459 loss:        0.362795
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002911
Test - acc:         0.907006 loss:        0.361809
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002698
Test - acc:         0.907261 loss:        0.363676
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002739
Test - acc:         0.906497 loss:        0.360924
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002844
Test - acc:         0.907261 loss:        0.355390
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002875
Test - acc:         0.908025 loss:        0.363141
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002524
Test - acc:         0.907516 loss:        0.364144
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002590
Test - acc:         0.908025 loss:        0.361901
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002860
Test - acc:         0.908535 loss:        0.357030
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002692
Test - acc:         0.903949 loss:        0.361774
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002565
Test - acc:         0.909045 loss:        0.362452
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002815
Test - acc:         0.908025 loss:        0.364299
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002713
Test - acc:         0.907516 loss:        0.361985
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002525
Test - acc:         0.907261 loss:        0.358801
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002466
Test - acc:         0.905987 loss:        0.361097
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002582
Test - acc:         0.906497 loss:        0.360545
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002706
Test - acc:         0.907516 loss:        0.360118
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002660
Test - acc:         0.906242 loss:        0.360108
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002559
Test - acc:         0.905987 loss:        0.364993
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002801
Test - acc:         0.906752 loss:        0.358239
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002775
Test - acc:         0.907771 loss:        0.359245
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002583
Test - acc:         0.907261 loss:        0.362201
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002669
Test - acc:         0.906752 loss:        0.359857
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002661
Test - acc:         0.905987 loss:        0.356839
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002610
Test - acc:         0.907006 loss:        0.362783
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002771
Test - acc:         0.908025 loss:        0.361003
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002482
Test - acc:         0.908280 loss:        0.360816
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.003006
Test - acc:         0.904204 loss:        0.362461
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002471
Test - acc:         0.907261 loss:        0.356558
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002646
Test - acc:         0.906752 loss:        0.363779
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002597
Test - acc:         0.908025 loss:        0.359116
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002540
Test - acc:         0.906752 loss:        0.359581
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002707
Test - acc:         0.905223 loss:        0.362280
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002513
Test - acc:         0.905987 loss:        0.357597
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002800
Test - acc:         0.907261 loss:        0.357813
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002664
Test - acc:         0.909045 loss:        0.359973
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002377
Test - acc:         0.909299 loss:        0.360700
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002475
Test - acc:         0.907771 loss:        0.360190
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002600
Test - acc:         0.906497 loss:        0.361488
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002673
Test - acc:         0.907261 loss:        0.359622
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002579
Test - acc:         0.907516 loss:        0.358932
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002480
Test - acc:         0.907261 loss:        0.359130
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002669
Test - acc:         0.906497 loss:        0.360906
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002540
Test - acc:         0.908535 loss:        0.358242
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002812
Test - acc:         0.906752 loss:        0.359032
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002492
Test - acc:         0.908025 loss:        0.362946
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002758
Test - acc:         0.906752 loss:        0.359924
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002505
Test - acc:         0.907771 loss:        0.363375
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002595
Test - acc:         0.907516 loss:        0.360828
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002741
Test - acc:         0.907261 loss:        0.360922
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002562
Test - acc:         0.907261 loss:        0.363162
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002546
Test - acc:         0.905478 loss:        0.364084
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002475
Test - acc:         0.906497 loss:        0.362678
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002564
Test - acc:         0.906497 loss:        0.365099
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002636
Test - acc:         0.905732 loss:        0.363291
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002717
Test - acc:         0.908280 loss:        0.360112
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002501
Test - acc:         0.908280 loss:        0.361044
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002670
Test - acc:         0.907771 loss:        0.365654
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002480
Test - acc:         0.905732 loss:        0.362079
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002407
Test - acc:         0.907261 loss:        0.364805
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002914
Test - acc:         0.906497 loss:        0.361582
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002659
Test - acc:         0.905987 loss:        0.361130
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002585
Test - acc:         0.905478 loss:        0.356986
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002522
Test - acc:         0.906497 loss:        0.360656
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002526
Test - acc:         0.905987 loss:        0.359868
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002657
Test - acc:         0.906752 loss:        0.357963
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002745
Test - acc:         0.909809 loss:        0.360975
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002747
Test - acc:         0.907516 loss:        0.358042
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002552
Test - acc:         0.907261 loss:        0.362699
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002523
Test - acc:         0.908025 loss:        0.360393
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002567
Test - acc:         0.906752 loss:        0.360585
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002599
Test - acc:         0.906497 loss:        0.369535
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002511
Test - acc:         0.906242 loss:        0.357286
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002404
Test - acc:         0.907771 loss:        0.363719
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002841
Test - acc:         0.909045 loss:        0.359911
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002474
Test - acc:         0.907261 loss:        0.364044
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002421
Test - acc:         0.906242 loss:        0.358343
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002461
Test - acc:         0.907516 loss:        0.359190
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002706
Test - acc:         0.906752 loss:        0.358529
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002335
Test - acc:         0.908025 loss:        0.358041
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002606
Test - acc:         0.907006 loss:        0.365475
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002434
Test - acc:         0.907006 loss:        0.361805
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002550
Test - acc:         0.907516 loss:        0.361223
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002652
Test - acc:         0.905987 loss:        0.360667
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002706
Test - acc:         0.905732 loss:        0.362430
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002638
Test - acc:         0.906497 loss:        0.357191
Sparsity :          0.7500
Wdecay :        0.000500
