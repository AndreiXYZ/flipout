Running --model densenet121 --dataset imagenette --seed 42 --logdir=criterion_experiment_no_bias/densenet121 --prune_criterion weight_squared_div_flips --prune_freq 50 --prune_rate 0.5 --noise --comment=densenet121_crit=weight_squared_div_flips_pf=50_seed=42 --save_model=pre-finetune/densenet121_weight_squared_div_flips_pf50_s42
******************************
Running
{
    "model": "densenet121",
    "dataset": "imagenette",
    "batch_size": 128,
    "test_batch_size": 500,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/densenet121",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/densenet121_weight_squared_div_flips_pf50_s42",
    "load_model": null,
    "parallel": false
}
******************************
Total prunable params of model: 6880448
Model has 6964106 total params.
num_weights=6922272
num_biases=41834
num.prunable=6880448
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.304467 loss:        2.247276
Test - acc:         0.223439 loss:        2.564861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.392227 loss:        2.030704
Test - acc:         0.414268 loss:        2.056558
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.492660 loss:        1.565982
Test - acc:         0.390318 loss:        6.541212
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.548949 loss:        1.371716
Test - acc:         0.542166 loss:        1.369238
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.599852 loss:        1.205749
Test - acc:         0.533503 loss:        1.420694
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.634280 loss:        1.127794
Test - acc:         0.631083 loss:        1.145493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.667124 loss:        1.025752
Test - acc:         0.644586 loss:        1.133085
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.692259 loss:        0.941326
Test - acc:         0.645096 loss:        1.132447
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.713803 loss:        0.867857
Test - acc:         0.648917 loss:        1.115370
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.727954 loss:        0.838575
Test - acc:         0.662166 loss:        1.119164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.745274 loss:        0.782161
Test - acc:         0.654013 loss:        1.224131
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.764389 loss:        0.713538
Test - acc:         0.711592 loss:        0.916667
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.773683 loss:        0.698923
Test - acc:         0.716433 loss:        0.919082
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.790791 loss:        0.640160
Test - acc:         0.708535 loss:        0.936016
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.788784 loss:        0.636279
Test - acc:         0.730446 loss:        0.930228
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.803253 loss:        0.592168
Test - acc:         0.742166 loss:        0.781737
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.815081 loss:        0.557255
Test - acc:         0.735796 loss:        0.883404
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.817827 loss:        0.552658
Test - acc:         0.743439 loss:        0.873422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.821840 loss:        0.536533
Test - acc:         0.739873 loss:        0.890160
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.820889 loss:        0.525431
Test - acc:         0.739618 loss:        0.904321
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.839054 loss:        0.492398
Test - acc:         0.744713 loss:        0.826697
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.844017 loss:        0.477280
Test - acc:         0.738854 loss:        0.952022
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.852677 loss:        0.436604
Test - acc:         0.699873 loss:        1.164193
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.847714 loss:        0.451899
Test - acc:         0.696306 loss:        1.140190
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.860175 loss:        0.431885
Test - acc:         0.790318 loss:        0.684371
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.864716 loss:        0.396631
Test - acc:         0.724076 loss:        0.997666
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.864716 loss:        0.403929
Test - acc:         0.731975 loss:        1.006542
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.867145 loss:        0.395460
Test - acc:         0.772229 loss:        0.789450
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.872109 loss:        0.378557
Test - acc:         0.675669 loss:        1.341159
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.874960 loss:        0.375969
Test - acc:         0.687643 loss:        1.195287
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.880452 loss:        0.354471
Test - acc:         0.740892 loss:        0.997564
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.886472 loss:        0.347214
Test - acc:         0.740892 loss:        0.909023
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.881614 loss:        0.351005
Test - acc:         0.764586 loss:        0.856611
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.891330 loss:        0.322453
Test - acc:         0.743185 loss:        0.995336
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.897772 loss:        0.307338
Test - acc:         0.754140 loss:        0.912247
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.889851 loss:        0.337876
Test - acc:         0.721019 loss:        1.011775
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.897983 loss:        0.305074
Test - acc:         0.659108 loss:        1.572657
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.893759 loss:        0.314284
Test - acc:         0.738089 loss:        0.954976
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.896610 loss:        0.307999
Test - acc:         0.735032 loss:        0.995577
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.904636 loss:        0.284709
Test - acc:         0.730446 loss:        0.979973
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.899250 loss:        0.300870
Test - acc:         0.789299 loss:        0.762062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.909072 loss:        0.276104
Test - acc:         0.772739 loss:        0.838292
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.903263 loss:        0.286621
Test - acc:         0.769427 loss:        0.813246
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.904847 loss:        0.279276
Test - acc:         0.702930 loss:        1.111748
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.908966 loss:        0.275545
Test - acc:         0.781146 loss:        0.751864
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.898194 loss:        0.301317
Test - acc:         0.759745 loss:        0.939245
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.903897 loss:        0.289866
Test - acc:         0.764331 loss:        0.855191
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.912346 loss:        0.263084
Test - acc:         0.767389 loss:        0.865867
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.912768 loss:        0.264473
Test - acc:         0.777580 loss:        0.842370
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.921428 loss:        0.229516
Test - acc:         0.768408 loss:        0.813424
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.946351 loss:        0.163584
Test - acc:         0.841274 loss:        0.581964
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.946774 loss:        0.160977
Test - acc:         0.773248 loss:        0.850325
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.940543 loss:        0.186400
Test - acc:         0.821401 loss:        0.670563
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.935368 loss:        0.191625
Test - acc:         0.776561 loss:        0.849227
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.938747 loss:        0.177924
Test - acc:         0.779363 loss:        0.858863
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.930510 loss:        0.210809
Test - acc:         0.755669 loss:        0.970476
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.940860 loss:        0.171636
Test - acc:         0.785732 loss:        0.826488
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.934312 loss:        0.188364
Test - acc:         0.801783 loss:        0.784275
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.935685 loss:        0.190423
Test - acc:         0.815287 loss:        0.709157
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.934629 loss:        0.199913
Test - acc:         0.731720 loss:        1.148539
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.924279 loss:        0.208634
Test - acc:         0.807134 loss:        0.728004
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.933045 loss:        0.203053
Test - acc:         0.764331 loss:        0.920375
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.927025 loss:        0.210987
Test - acc:         0.767134 loss:        0.934740
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.929665 loss:        0.211721
Test - acc:         0.806369 loss:        0.803775
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.941071 loss:        0.174256
Test - acc:         0.775032 loss:        0.886285
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.936530 loss:        0.190202
Test - acc:         0.790318 loss:        0.870247
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.930193 loss:        0.207933
Test - acc:         0.771465 loss:        0.930776
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.940437 loss:        0.182536
Test - acc:         0.750828 loss:        0.956357
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.925969 loss:        0.216260
Test - acc:         0.762548 loss:        0.892804
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.938536 loss:        0.186438
Test - acc:         0.729936 loss:        1.165512
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.936530 loss:        0.190642
Test - acc:         0.756433 loss:        0.969269
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.930193 loss:        0.210080
Test - acc:         0.788535 loss:        0.804818
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.903686 loss:        0.273224
Test - acc:         0.698344 loss:        1.198000
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.915725 loss:        0.256451
Test - acc:         0.733503 loss:        0.923974
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.932200 loss:        0.200647
Test - acc:         0.819363 loss:        0.681641
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.916464 loss:        0.243131
Test - acc:         0.792102 loss:        0.783659
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.932094 loss:        0.200364
Test - acc:         0.803057 loss:        0.766390
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.930616 loss:        0.201288
Test - acc:         0.806624 loss:        0.730209
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.931672 loss:        0.209899
Test - acc:         0.770701 loss:        0.834585
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.931461 loss:        0.195130
Test - acc:         0.775287 loss:        0.858404
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.930616 loss:        0.207308
Test - acc:         0.754395 loss:        1.052118
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.937480 loss:        0.187996
Test - acc:         0.772994 loss:        0.902170
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.934312 loss:        0.190534
Test - acc:         0.809172 loss:        0.757316
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.940332 loss:        0.178438
Test - acc:         0.757962 loss:        1.036134
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.939381 loss:        0.175231
Test - acc:         0.793121 loss:        0.785022
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.936741 loss:        0.183015
Test - acc:         0.788535 loss:        0.877628
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.939909 loss:        0.183340
Test - acc:         0.788280 loss:        0.782048
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.928926 loss:        0.205559
Test - acc:         0.756688 loss:        0.963996
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.937797 loss:        0.181310
Test - acc:         0.802293 loss:        0.740590
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.943289 loss:        0.168324
Test - acc:         0.768408 loss:        0.922744
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.941705 loss:        0.172451
Test - acc:         0.784713 loss:        0.830348
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.937163 loss:        0.186876
Test - acc:         0.797452 loss:        0.786805
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.935474 loss:        0.190619
Test - acc:         0.805605 loss:        0.778749
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.938219 loss:        0.184244
Test - acc:         0.765605 loss:        0.966475
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.946668 loss:        0.158158
Test - acc:         0.803312 loss:        0.780581
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.937269 loss:        0.187098
Test - acc:         0.793121 loss:        0.828528
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.938219 loss:        0.188206
Test - acc:         0.784713 loss:        0.887519
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.941388 loss:        0.178147
Test - acc:         0.823694 loss:        0.719590
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.946774 loss:        0.159611
Test - acc:         0.803822 loss:        0.757256
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.933361 loss:        0.194351
Test - acc:         0.794650 loss:        0.845589
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.964727 loss:        0.113262
Test - acc:         0.813248 loss:        0.732834
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.976344 loss:        0.073955
Test - acc:         0.838981 loss:        0.640526
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.963988 loss:        0.109727
Test - acc:         0.794395 loss:        0.949112
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.946246 loss:        0.170480
Test - acc:         0.838471 loss:        0.614034
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.959552 loss:        0.121784
Test - acc:         0.814522 loss:        0.681971
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.966417 loss:        0.099953
Test - acc:         0.798471 loss:        0.881481
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.955117 loss:        0.131313
Test - acc:         0.768153 loss:        1.004621
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.955328 loss:        0.133043
Test - acc:         0.784713 loss:        0.838630
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.955434 loss:        0.133991
Test - acc:         0.781656 loss:        0.918161
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.946034 loss:        0.155086
Test - acc:         0.833885 loss:        0.623518
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.957546 loss:        0.134113
Test - acc:         0.787771 loss:        0.859498
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.952582 loss:        0.142227
Test - acc:         0.805096 loss:        0.751721
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.949942 loss:        0.150345
Test - acc:         0.755669 loss:        1.050946
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.956912 loss:        0.128432
Test - acc:         0.770191 loss:        0.956781
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.953744 loss:        0.140361
Test - acc:         0.808662 loss:        0.791427
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.951104 loss:        0.150995
Test - acc:         0.812994 loss:        0.707541
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.945295 loss:        0.156814
Test - acc:         0.803057 loss:        0.831324
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.955011 loss:        0.136834
Test - acc:         0.770955 loss:        0.903637
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.955117 loss:        0.139263
Test - acc:         0.804586 loss:        0.806377
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.954061 loss:        0.134954
Test - acc:         0.820382 loss:        0.692042
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.957440 loss:        0.128729
Test - acc:         0.807643 loss:        0.783035
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.951632 loss:        0.148527
Test - acc:         0.797452 loss:        0.755777
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.944873 loss:        0.163426
Test - acc:         0.806115 loss:        0.748282
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.943922 loss:        0.159949
Test - acc:         0.731210 loss:        1.170062
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.959975 loss:        0.119381
Test - acc:         0.811210 loss:        0.754105
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.958285 loss:        0.121208
Test - acc:         0.818344 loss:        0.712848
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.945084 loss:        0.165001
Test - acc:         0.798726 loss:        0.791972
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.946774 loss:        0.156470
Test - acc:         0.823949 loss:        0.686179
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.962192 loss:        0.113043
Test - acc:         0.845350 loss:        0.618498
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.956067 loss:        0.132046
Test - acc:         0.807898 loss:        0.758749
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.955434 loss:        0.133095
Test - acc:         0.779618 loss:        0.866004
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.946457 loss:        0.163190
Test - acc:         0.773758 loss:        0.846537
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.952688 loss:        0.141875
Test - acc:         0.824968 loss:        0.679155
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.946034 loss:        0.159493
Test - acc:         0.780382 loss:        0.885799
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.916992 loss:        0.254952
Test - acc:         0.367389 loss:        4.131440
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.911184 loss:        0.277228
Test - acc:         0.787516 loss:        0.785688
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.955328 loss:        0.136769
Test - acc:         0.809172 loss:        0.768143
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.953427 loss:        0.138393
Test - acc:         0.801529 loss:        0.805415
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.944978 loss:        0.161710
Test - acc:         0.807389 loss:        0.801168
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.953005 loss:        0.140512
Test - acc:         0.828280 loss:        0.672770
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.949414 loss:        0.155001
Test - acc:         0.827516 loss:        0.676045
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.947407 loss:        0.158452
Test - acc:         0.815287 loss:        0.704929
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.947407 loss:        0.159610
Test - acc:         0.792102 loss:        0.799043
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.961876 loss:        0.119591
Test - acc:         0.791338 loss:        0.802552
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.954800 loss:        0.137555
Test - acc:         0.816561 loss:        0.851447
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.950998 loss:        0.146303
Test - acc:         0.771210 loss:        1.092516
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.950048 loss:        0.142478
Test - acc:         0.779108 loss:        0.886156
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.951843 loss:        0.143815
Test - acc:         0.768153 loss:        0.992333
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.942761 loss:        0.173694
Test - acc:         0.767389 loss:        1.023934
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.957440 loss:        0.127481
Test - acc:         0.764331 loss:        1.035239
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.981519 loss:        0.064553
Test - acc:         0.886624 loss:        0.411757
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.993769 loss:        0.030094
Test - acc:         0.892739 loss:        0.397601
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.996832 loss:        0.021676
Test - acc:         0.893503 loss:        0.396493
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.997465 loss:        0.018078
Test - acc:         0.894777 loss:        0.397695
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.997993 loss:        0.015400
Test - acc:         0.892739 loss:        0.399373
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.998733 loss:        0.013818
Test - acc:         0.894268 loss:        0.401255
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.999155 loss:        0.012533
Test - acc:         0.895796 loss:        0.398360
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.999050 loss:        0.011210
Test - acc:         0.896815 loss:        0.393312
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.010529
Test - acc:         0.898854 loss:        0.388350
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.009936
Test - acc:         0.898089 loss:        0.392444
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.009628
Test - acc:         0.896561 loss:        0.395812
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.008223
Test - acc:         0.894777 loss:        0.398033
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.008075
Test - acc:         0.895796 loss:        0.398744
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.007654
Test - acc:         0.896051 loss:        0.399694
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.007525
Test - acc:         0.894522 loss:        0.400041
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.007387
Test - acc:         0.896561 loss:        0.396158
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.006877
Test - acc:         0.894522 loss:        0.399554
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.007227
Test - acc:         0.897325 loss:        0.396595
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.006102
Test - acc:         0.899363 loss:        0.396702
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.006600
Test - acc:         0.897580 loss:        0.395341
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005920
Test - acc:         0.895796 loss:        0.402503
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.006849
Test - acc:         0.897070 loss:        0.402281
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.005497
Test - acc:         0.898599 loss:        0.395552
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005320
Test - acc:         0.897070 loss:        0.389333
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005533
Test - acc:         0.897580 loss:        0.398512
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.005290
Test - acc:         0.896306 loss:        0.394586
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005590
Test - acc:         0.896051 loss:        0.397139
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005045
Test - acc:         0.895032 loss:        0.395691
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.005873
Test - acc:         0.895287 loss:        0.397882
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005033
Test - acc:         0.896051 loss:        0.396050
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004935
Test - acc:         0.894777 loss:        0.395193
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004908
Test - acc:         0.897325 loss:        0.400384
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004508
Test - acc:         0.896815 loss:        0.394647
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005193
Test - acc:         0.898854 loss:        0.391648
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.004973
Test - acc:         0.896561 loss:        0.397832
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004764
Test - acc:         0.896815 loss:        0.396590
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004508
Test - acc:         0.897070 loss:        0.396957
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004761
Test - acc:         0.897834 loss:        0.397104
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004886
Test - acc:         0.898344 loss:        0.399276
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004511
Test - acc:         0.896561 loss:        0.395478
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004396
Test - acc:         0.896815 loss:        0.398371
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004613
Test - acc:         0.899363 loss:        0.392599
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004233
Test - acc:         0.896561 loss:        0.399424
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004424
Test - acc:         0.900127 loss:        0.389363
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004319
Test - acc:         0.897070 loss:        0.405101
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004461
Test - acc:         0.897580 loss:        0.396349
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003679
Test - acc:         0.898344 loss:        0.393049
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003995
Test - acc:         0.897834 loss:        0.398151
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003736
Test - acc:         0.898089 loss:        0.390750
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003838
Test - acc:         0.898599 loss:        0.396747
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.998944 loss:        0.009826
Test - acc:         0.896561 loss:        0.405895
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.007312
Test - acc:         0.900637 loss:        0.398157
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.006546
Test - acc:         0.899108 loss:        0.400699
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.006101
Test - acc:         0.897325 loss:        0.400931
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.006056
Test - acc:         0.902166 loss:        0.396847
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005155
Test - acc:         0.899873 loss:        0.400146
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.005598
Test - acc:         0.899363 loss:        0.396044
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.005905
Test - acc:         0.898089 loss:        0.402526
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005218
Test - acc:         0.899108 loss:        0.395204
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005789
Test - acc:         0.899108 loss:        0.399573
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.005789
Test - acc:         0.899618 loss:        0.400446
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.005126
Test - acc:         0.900127 loss:        0.402922
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004570
Test - acc:         0.899363 loss:        0.400393
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004492
Test - acc:         0.899363 loss:        0.400445
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005044
Test - acc:         0.899363 loss:        0.403463
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004798
Test - acc:         0.900127 loss:        0.400962
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004441
Test - acc:         0.898854 loss:        0.402120
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004319
Test - acc:         0.897580 loss:        0.402520
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004358
Test - acc:         0.898599 loss:        0.400947
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004467
Test - acc:         0.897580 loss:        0.405779
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004152
Test - acc:         0.898599 loss:        0.404430
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004170
Test - acc:         0.898089 loss:        0.396722
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004665
Test - acc:         0.897325 loss:        0.399649
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004729
Test - acc:         0.896306 loss:        0.405290
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004051
Test - acc:         0.901401 loss:        0.397800
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003910
Test - acc:         0.900637 loss:        0.394513
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004440
Test - acc:         0.900892 loss:        0.398253
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.003913
Test - acc:         0.899873 loss:        0.397855
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003985
Test - acc:         0.898854 loss:        0.403742
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003745
Test - acc:         0.896306 loss:        0.403902
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003939
Test - acc:         0.898089 loss:        0.399131
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004250
Test - acc:         0.898854 loss:        0.400454
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004374
Test - acc:         0.894777 loss:        0.410925
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003689
Test - acc:         0.900892 loss:        0.401184
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003802
Test - acc:         0.901401 loss:        0.401423
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003757
Test - acc:         0.899873 loss:        0.397134
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003720
Test - acc:         0.901146 loss:        0.397950
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003656
Test - acc:         0.899618 loss:        0.404304
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003431
Test - acc:         0.899873 loss:        0.397159
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003585
Test - acc:         0.898599 loss:        0.405695
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003487
Test - acc:         0.900127 loss:        0.402585
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003476
Test - acc:         0.896815 loss:        0.409920
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003247
Test - acc:         0.898344 loss:        0.402807
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003667
Test - acc:         0.896815 loss:        0.404602
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003509
Test - acc:         0.897580 loss:        0.401835
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003554
Test - acc:         0.895796 loss:        0.404463
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003719
Test - acc:         0.895796 loss:        0.402850
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003267
Test - acc:         0.897580 loss:        0.410746
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004033
Test - acc:         0.899618 loss:        0.402873
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.003655
Test - acc:         0.896815 loss:        0.403239
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.985637 loss:        0.052574
Test - acc:         0.877197 loss:        0.452630
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.993980 loss:        0.034395
Test - acc:         0.882293 loss:        0.436136
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.995459 loss:        0.026447
Test - acc:         0.885860 loss:        0.433453
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.995776 loss:        0.025770
Test - acc:         0.885350 loss:        0.430174
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.996832 loss:        0.022962
Test - acc:         0.885860 loss:        0.432472
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.996304 loss:        0.023196
Test - acc:         0.885860 loss:        0.429552
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.997571 loss:        0.019327
Test - acc:         0.887134 loss:        0.430905
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.998099 loss:        0.017929
Test - acc:         0.887898 loss:        0.430109
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.998521 loss:        0.018208
Test - acc:         0.885096 loss:        0.428213
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.998310 loss:        0.017519
Test - acc:         0.888408 loss:        0.420446
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.997993 loss:        0.016514
Test - acc:         0.887389 loss:        0.421771
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.998416 loss:        0.015718
Test - acc:         0.888662 loss:        0.428638
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.998944 loss:        0.014250
Test - acc:         0.887898 loss:        0.423035
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.998310 loss:        0.015342
Test - acc:         0.887389 loss:        0.427496
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.998944 loss:        0.013547
Test - acc:         0.886369 loss:        0.425482
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.998521 loss:        0.014507
Test - acc:         0.890191 loss:        0.426159
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.998627 loss:        0.014082
Test - acc:         0.886624 loss:        0.427658
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.998838 loss:        0.013314
Test - acc:         0.888408 loss:        0.424042
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.998521 loss:        0.013353
Test - acc:         0.891465 loss:        0.420888
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999578 loss:        0.011514
Test - acc:         0.887898 loss:        0.433357
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.010945
Test - acc:         0.888662 loss:        0.432376
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999050 loss:        0.012010
Test - acc:         0.889936 loss:        0.426525
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.998944 loss:        0.012175
Test - acc:         0.888662 loss:        0.427835
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.011247
Test - acc:         0.889427 loss:        0.428897
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.011144
Test - acc:         0.886369 loss:        0.435809
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999261 loss:        0.010717
Test - acc:         0.886624 loss:        0.434164
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999578 loss:        0.010922
Test - acc:         0.889682 loss:        0.429139
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.010712
Test - acc:         0.887134 loss:        0.431633
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.999683 loss:        0.010128
Test - acc:         0.890701 loss:        0.427799
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999366 loss:        0.010397
Test - acc:         0.887898 loss:        0.426275
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.999683 loss:        0.009865
Test - acc:         0.889172 loss:        0.425948
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.999366 loss:        0.010249
Test - acc:         0.889172 loss:        0.426245
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.999578 loss:        0.009706
Test - acc:         0.887389 loss:        0.437050
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.999050 loss:        0.010535
Test - acc:         0.890446 loss:        0.428161
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.009854
Test - acc:         0.889936 loss:        0.428414
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.999366 loss:        0.009435
Test - acc:         0.888662 loss:        0.430357
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.009388
Test - acc:         0.887643 loss:        0.432873
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.009593
Test - acc:         0.889682 loss:        0.428382
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.999366 loss:        0.009578
Test - acc:         0.886115 loss:        0.434329
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.999261 loss:        0.009268
Test - acc:         0.887898 loss:        0.435328
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.999683 loss:        0.008531
Test - acc:         0.889682 loss:        0.431004
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.999683 loss:        0.008902
Test - acc:         0.890191 loss:        0.432910
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.007974
Test - acc:         0.889682 loss:        0.431480
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.008600
Test - acc:         0.887643 loss:        0.439095
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.008652
Test - acc:         0.891210 loss:        0.430932
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.008652
Test - acc:         0.891210 loss:        0.430243
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.999366 loss:        0.007985
Test - acc:         0.890446 loss:        0.434563
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.008342
Test - acc:         0.891210 loss:        0.428598
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.999683 loss:        0.008355
Test - acc:         0.891975 loss:        0.427897
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999155 loss:        0.008600
Test - acc:         0.889936 loss:        0.433576
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.928715 loss:        0.222023
Test - acc:         0.858344 loss:        0.486831
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.964938 loss:        0.130848
Test - acc:         0.865987 loss:        0.454935
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.973281 loss:        0.106088
Test - acc:         0.867771 loss:        0.443364
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.975605 loss:        0.094283
Test - acc:         0.869299 loss:        0.438173
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.980146 loss:        0.084497
Test - acc:         0.870828 loss:        0.444992
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.985954 loss:        0.071192
Test - acc:         0.872102 loss:        0.440344
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.982152 loss:        0.071300
Test - acc:         0.874904 loss:        0.435045
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.984898 loss:        0.064690
Test - acc:         0.877452 loss:        0.426750
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.987221 loss:        0.062007
Test - acc:         0.877962 loss:        0.429189
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.991235 loss:        0.052174
Test - acc:         0.875669 loss:        0.432989
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.988278 loss:        0.055517
Test - acc:         0.879745 loss:        0.431937
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.991340 loss:        0.050785
Test - acc:         0.879745 loss:        0.431463
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.992819 loss:        0.044820
Test - acc:         0.882293 loss:        0.428207
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.992713 loss:        0.046116
Test - acc:         0.881274 loss:        0.428106
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.992819 loss:        0.041332
Test - acc:         0.881529 loss:        0.428321
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.992713 loss:        0.038893
Test - acc:         0.883567 loss:        0.433862
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.993769 loss:        0.038109
Test - acc:         0.880255 loss:        0.436290
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.994614 loss:        0.037776
Test - acc:         0.882803 loss:        0.434440
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.993135 loss:        0.038122
Test - acc:         0.882293 loss:        0.433859
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.993664 loss:        0.036984
Test - acc:         0.881529 loss:        0.431708
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.995036 loss:        0.033184
Test - acc:         0.879236 loss:        0.440325
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.995670 loss:        0.034881
Test - acc:         0.882293 loss:        0.445664
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.996937 loss:        0.030589
Test - acc:         0.880255 loss:        0.443604
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.996621 loss:        0.029128
Test - acc:         0.883057 loss:        0.445542
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.995881 loss:        0.030447
Test - acc:         0.881529 loss:        0.439719
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.994931 loss:        0.030572
Test - acc:         0.883312 loss:        0.439476
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.996093 loss:        0.028528
Test - acc:         0.884331 loss:        0.439558
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.995881 loss:        0.027496
Test - acc:         0.884586 loss:        0.442436
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.996304 loss:        0.027698
Test - acc:         0.885096 loss:        0.440700
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.997571 loss:        0.025190
Test - acc:         0.886115 loss:        0.438454
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.995881 loss:        0.027576
Test - acc:         0.887643 loss:        0.438083
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.997254 loss:        0.025189
Test - acc:         0.885350 loss:        0.438268
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.997360 loss:        0.025268
Test - acc:         0.887134 loss:        0.447514
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.997888 loss:        0.022855
Test - acc:         0.886115 loss:        0.442471
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.998310 loss:        0.022472
Test - acc:         0.884076 loss:        0.443563
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.996832 loss:        0.023194
Test - acc:         0.883057 loss:        0.451953
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.996621 loss:        0.022637
Test - acc:         0.886624 loss:        0.436958
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.997465 loss:        0.022457
Test - acc:         0.885605 loss:        0.446320
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.996832 loss:        0.022318
Test - acc:         0.885096 loss:        0.445209
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.997149 loss:        0.021394
Test - acc:         0.881274 loss:        0.452249
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.997254 loss:        0.020991
Test - acc:         0.883567 loss:        0.447993
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.997149 loss:        0.020947
Test - acc:         0.885860 loss:        0.448564
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.997465 loss:        0.021489
Test - acc:         0.887643 loss:        0.447606
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.998627 loss:        0.018571
Test - acc:         0.883057 loss:        0.454446
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.997677 loss:        0.019035
Test - acc:         0.882548 loss:        0.454831
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.997149 loss:        0.019880
Test - acc:         0.884331 loss:        0.456892
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.998627 loss:        0.017622
Test - acc:         0.883567 loss:        0.453474
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.997888 loss:        0.018975
Test - acc:         0.884841 loss:        0.454775
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.997465 loss:        0.018703
Test - acc:         0.883822 loss:        0.459842
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.997571 loss:        0.018465
Test - acc:         0.884331 loss:        0.454250
Sparsity :          0.9844
Wdecay :        0.000500
