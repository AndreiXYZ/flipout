Running --model densenet121 --dataset imagenette --seed 42 --logdir=criterion_experiment_no_bias/densenet121 --prune_criterion weight_squared_div_flips --prune_freq 39 --prune_rate 0.5 --noise --comment=densenet121_crit=weight_squared_div_flips_pf=39_seed=42 --save_model=pre-finetune/densenet121_weight_squared_div_flips_pf39_s42
******************************
Running
{
    "model": "densenet121",
    "dataset": "imagenette",
    "batch_size": 128,
    "test_batch_size": 500,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/densenet121",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/densenet121_weight_squared_div_flips_pf39_s42",
    "load_model": null,
    "parallel": false
}
******************************
Total prunable params of model: 6880448
Model has 6964106 total params.
num_weights=6922272
num_biases=41834
num.prunable=6880448
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.304467 loss:        2.247276
Test - acc:         0.223439 loss:        2.564861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.392227 loss:        2.030704
Test - acc:         0.414268 loss:        2.056558
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.492660 loss:        1.565982
Test - acc:         0.390318 loss:        6.541212
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.548949 loss:        1.371716
Test - acc:         0.542166 loss:        1.369238
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.599852 loss:        1.205749
Test - acc:         0.533503 loss:        1.420694
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.634280 loss:        1.127794
Test - acc:         0.631083 loss:        1.145493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.667124 loss:        1.025752
Test - acc:         0.644586 loss:        1.133085
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.692259 loss:        0.941326
Test - acc:         0.645096 loss:        1.132447
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.713803 loss:        0.867857
Test - acc:         0.648917 loss:        1.115370
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.727954 loss:        0.838575
Test - acc:         0.662166 loss:        1.119164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.745274 loss:        0.782161
Test - acc:         0.654013 loss:        1.224131
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.764389 loss:        0.713538
Test - acc:         0.711592 loss:        0.916667
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.773683 loss:        0.698923
Test - acc:         0.716433 loss:        0.919082
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.790791 loss:        0.640160
Test - acc:         0.708535 loss:        0.936016
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.788784 loss:        0.636279
Test - acc:         0.730446 loss:        0.930228
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.803253 loss:        0.592168
Test - acc:         0.742166 loss:        0.781737
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.815081 loss:        0.557255
Test - acc:         0.735796 loss:        0.883404
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.817827 loss:        0.552658
Test - acc:         0.743439 loss:        0.873422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.821840 loss:        0.536533
Test - acc:         0.739873 loss:        0.890160
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.820889 loss:        0.525431
Test - acc:         0.739618 loss:        0.904321
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.839054 loss:        0.492398
Test - acc:         0.744713 loss:        0.826697
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.844017 loss:        0.477280
Test - acc:         0.738854 loss:        0.952022
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.852677 loss:        0.436604
Test - acc:         0.699873 loss:        1.164193
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.847714 loss:        0.451899
Test - acc:         0.696306 loss:        1.140190
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.860175 loss:        0.431885
Test - acc:         0.790318 loss:        0.684371
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.864716 loss:        0.396631
Test - acc:         0.724076 loss:        0.997666
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.864716 loss:        0.403929
Test - acc:         0.731975 loss:        1.006542
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.867145 loss:        0.395460
Test - acc:         0.772229 loss:        0.789450
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.872109 loss:        0.378557
Test - acc:         0.675669 loss:        1.341159
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.874960 loss:        0.375969
Test - acc:         0.687643 loss:        1.195287
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.880452 loss:        0.354471
Test - acc:         0.740892 loss:        0.997564
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.886472 loss:        0.347214
Test - acc:         0.740892 loss:        0.909023
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.881614 loss:        0.351005
Test - acc:         0.764586 loss:        0.856611
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.891330 loss:        0.322453
Test - acc:         0.743185 loss:        0.995336
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.897772 loss:        0.307338
Test - acc:         0.754140 loss:        0.912247
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.889851 loss:        0.337876
Test - acc:         0.721019 loss:        1.011775
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.897983 loss:        0.305074
Test - acc:         0.659108 loss:        1.572657
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.893759 loss:        0.314284
Test - acc:         0.738089 loss:        0.954976
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.896610 loss:        0.307999
Test - acc:         0.735032 loss:        0.995577
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.927975 loss:        0.218948
Test - acc:         0.752866 loss:        0.931504
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.927975 loss:        0.217705
Test - acc:         0.783949 loss:        0.827831
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.927447 loss:        0.218085
Test - acc:         0.765096 loss:        0.907926
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.925018 loss:        0.220538
Test - acc:         0.793631 loss:        0.762755
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.923751 loss:        0.229922
Test - acc:         0.698089 loss:        1.405231
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.924385 loss:        0.232178
Test - acc:         0.698599 loss:        1.138645
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.924913 loss:        0.225790
Test - acc:         0.785987 loss:        0.802736
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.921322 loss:        0.232687
Test - acc:         0.675414 loss:        1.265794
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.936002 loss:        0.199620
Test - acc:         0.750064 loss:        1.219891
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.929454 loss:        0.218910
Test - acc:         0.718981 loss:        1.084558
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.930510 loss:        0.200927
Test - acc:         0.684841 loss:        1.328442
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.890379 loss:        0.332109
Test - acc:         0.737580 loss:        0.973699
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.915197 loss:        0.262985
Test - acc:         0.814777 loss:        0.639450
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.923962 loss:        0.224012
Test - acc:         0.794650 loss:        0.790860
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.923857 loss:        0.226829
Test - acc:         0.777580 loss:        0.810012
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.919210 loss:        0.238832
Test - acc:         0.789809 loss:        0.853931
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.926075 loss:        0.221263
Test - acc:         0.721274 loss:        1.184343
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.916464 loss:        0.240799
Test - acc:         0.798726 loss:        0.724818
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.931461 loss:        0.198586
Test - acc:         0.697834 loss:        1.386601
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.929982 loss:        0.218041
Test - acc:         0.701656 loss:        1.159603
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.924490 loss:        0.223317
Test - acc:         0.781656 loss:        0.886723
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.917837 loss:        0.237039
Test - acc:         0.791847 loss:        0.780887
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.926603 loss:        0.215749
Test - acc:         0.790828 loss:        0.751893
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.930088 loss:        0.203232
Test - acc:         0.694777 loss:        1.433530
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.918893 loss:        0.245994
Test - acc:         0.791592 loss:        0.771312
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.920161 loss:        0.236633
Test - acc:         0.662420 loss:        1.595880
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.926286 loss:        0.225908
Test - acc:         0.759490 loss:        0.961043
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.927131 loss:        0.221636
Test - acc:         0.747516 loss:        1.062274
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.929032 loss:        0.209251
Test - acc:         0.693503 loss:        1.400377
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.926814 loss:        0.211349
Test - acc:         0.719745 loss:        1.157158
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.934101 loss:        0.202136
Test - acc:         0.821146 loss:        0.651231
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.928081 loss:        0.212148
Test - acc:         0.762803 loss:        0.923306
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.930721 loss:        0.200728
Test - acc:         0.785987 loss:        0.822235
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.931249 loss:        0.205386
Test - acc:         0.705223 loss:        1.291069
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.922167 loss:        0.229383
Test - acc:         0.756688 loss:        0.993927
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.929771 loss:        0.215635
Test - acc:         0.729936 loss:        1.076234
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.938747 loss:        0.192131
Test - acc:         0.790318 loss:        0.770495
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.930827 loss:        0.199389
Test - acc:         0.815796 loss:        0.731784
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.926603 loss:        0.218878
Test - acc:         0.799745 loss:        0.753432
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.952477 loss:        0.144852
Test - acc:         0.764841 loss:        1.038439
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.967684 loss:        0.104598
Test - acc:         0.841783 loss:        0.574010
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.957123 loss:        0.127382
Test - acc:         0.771975 loss:        1.023154
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.945929 loss:        0.161082
Test - acc:         0.782420 loss:        0.828936
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.953005 loss:        0.142957
Test - acc:         0.820637 loss:        0.750669
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.954377 loss:        0.137835
Test - acc:         0.783185 loss:        0.955199
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.944556 loss:        0.169519
Test - acc:         0.798981 loss:        0.828379
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.949942 loss:        0.151482
Test - acc:         0.801529 loss:        0.797712
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.942761 loss:        0.172089
Test - acc:         0.815796 loss:        0.734382
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.944133 loss:        0.156004
Test - acc:         0.810701 loss:        0.738689
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.948147 loss:        0.155003
Test - acc:         0.800764 loss:        0.772755
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.950153 loss:        0.144607
Test - acc:         0.714395 loss:        1.183129
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.936213 loss:        0.184758
Test - acc:         0.771720 loss:        0.914105
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.940437 loss:        0.177502
Test - acc:         0.841019 loss:        0.595493
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.957862 loss:        0.134313
Test - acc:         0.806115 loss:        0.784377
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.946246 loss:        0.157666
Test - acc:         0.779108 loss:        0.835782
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.945612 loss:        0.158846
Test - acc:         0.804586 loss:        0.741056
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.941810 loss:        0.169364
Test - acc:         0.819618 loss:        0.729795
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.941810 loss:        0.173723
Test - acc:         0.770191 loss:        0.940642
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.942338 loss:        0.170652
Test - acc:         0.729427 loss:        1.086005
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.945190 loss:        0.161287
Test - acc:         0.803567 loss:        0.796843
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.940120 loss:        0.177241
Test - acc:         0.769682 loss:        0.923253
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.951209 loss:        0.144986
Test - acc:         0.798981 loss:        0.781962
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.945823 loss:        0.157760
Test - acc:         0.760255 loss:        0.952386
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.946034 loss:        0.159294
Test - acc:         0.798471 loss:        0.845730
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.950681 loss:        0.146620
Test - acc:         0.816815 loss:        0.740714
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.949942 loss:        0.142120
Test - acc:         0.792611 loss:        0.899669
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.949836 loss:        0.149708
Test - acc:         0.784968 loss:        0.851246
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.937797 loss:        0.182114
Test - acc:         0.796178 loss:        0.821563
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.947091 loss:        0.164981
Test - acc:         0.828025 loss:        0.653449
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.953321 loss:        0.144459
Test - acc:         0.781401 loss:        0.862962
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.947196 loss:        0.162398
Test - acc:         0.802548 loss:        0.821681
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.948358 loss:        0.158048
Test - acc:         0.755414 loss:        0.991657
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.946140 loss:        0.164099
Test - acc:         0.768662 loss:        0.902050
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.951948 loss:        0.143205
Test - acc:         0.770191 loss:        0.992541
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.950681 loss:        0.147830
Test - acc:         0.725096 loss:        1.140190
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.951104 loss:        0.144930
Test - acc:         0.771720 loss:        1.026403
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.939170 loss:        0.181423
Test - acc:         0.781911 loss:        0.815447
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.944239 loss:        0.165495
Test - acc:         0.759490 loss:        1.086618
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.965361 loss:        0.112690
Test - acc:         0.838217 loss:        0.613645
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.968846 loss:        0.091493
Test - acc:         0.814777 loss:        0.751574
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.974865 loss:        0.079135
Test - acc:         0.827006 loss:        0.721255
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.954694 loss:        0.132522
Test - acc:         0.781146 loss:        0.920282
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.949625 loss:        0.147231
Test - acc:         0.837452 loss:        0.609180
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.954589 loss:        0.135996
Test - acc:         0.823439 loss:        0.687661
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.948252 loss:        0.150121
Test - acc:         0.811720 loss:        0.788009
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.953110 loss:        0.135570
Test - acc:         0.798726 loss:        0.786282
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.949519 loss:        0.147167
Test - acc:         0.794904 loss:        0.849315
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.953427 loss:        0.144271
Test - acc:         0.779363 loss:        0.983065
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.961136 loss:        0.121982
Test - acc:         0.822420 loss:        0.675167
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.956067 loss:        0.128500
Test - acc:         0.828280 loss:        0.741577
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.949519 loss:        0.151869
Test - acc:         0.819108 loss:        0.695075
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.951104 loss:        0.142848
Test - acc:         0.816815 loss:        0.745866
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.950787 loss:        0.146813
Test - acc:         0.759490 loss:        1.022195
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.943289 loss:        0.164311
Test - acc:         0.831083 loss:        0.665892
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.951843 loss:        0.145241
Test - acc:         0.736051 loss:        1.173801
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.951420 loss:        0.143007
Test - acc:         0.708280 loss:        1.365215
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.951843 loss:        0.147505
Test - acc:         0.795414 loss:        0.860010
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.957862 loss:        0.129167
Test - acc:         0.807134 loss:        0.837093
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.962826 loss:        0.110623
Test - acc:         0.807898 loss:        0.741728
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.956067 loss:        0.134253
Test - acc:         0.751847 loss:        0.989058
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.946879 loss:        0.154984
Test - acc:         0.819873 loss:        0.695153
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.965361 loss:        0.104680
Test - acc:         0.791338 loss:        0.909799
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.953005 loss:        0.140775
Test - acc:         0.826242 loss:        0.729046
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.944767 loss:        0.157838
Test - acc:         0.742420 loss:        1.084914
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.933784 loss:        0.197089
Test - acc:         0.830064 loss:        0.646860
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.952054 loss:        0.143371
Test - acc:         0.833376 loss:        0.648936
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.957862 loss:        0.127378
Test - acc:         0.692994 loss:        1.671128
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.956278 loss:        0.136654
Test - acc:         0.838471 loss:        0.629180
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.948358 loss:        0.151321
Test - acc:         0.780127 loss:        0.879585
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.959341 loss:        0.121927
Test - acc:         0.794904 loss:        0.828590
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.957440 loss:        0.129749
Test - acc:         0.799745 loss:        0.869182
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.982258 loss:        0.061962
Test - acc:         0.886624 loss:        0.421613
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.995353 loss:        0.027023
Test - acc:         0.890191 loss:        0.413399
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.996621 loss:        0.021597
Test - acc:         0.893248 loss:        0.409239
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.998416 loss:        0.016058
Test - acc:         0.888153 loss:        0.416702
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.998733 loss:        0.013380
Test - acc:         0.888917 loss:        0.422872
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.998099 loss:        0.013096
Test - acc:         0.892994 loss:        0.418472
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.999155 loss:        0.013737
Test - acc:         0.892994 loss:        0.417283
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.999155 loss:        0.011797
Test - acc:         0.897070 loss:        0.412544
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.998733 loss:        0.011425
Test - acc:         0.895287 loss:        0.409997
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.010086
Test - acc:         0.894013 loss:        0.415179
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.009718
Test - acc:         0.892484 loss:        0.416040
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.009379
Test - acc:         0.892739 loss:        0.418374
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.008724
Test - acc:         0.894268 loss:        0.416397
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.008287
Test - acc:         0.893758 loss:        0.420883
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.007179
Test - acc:         0.896051 loss:        0.414936
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.007570
Test - acc:         0.896051 loss:        0.413089
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.007033
Test - acc:         0.896051 loss:        0.417123
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.006918
Test - acc:         0.897325 loss:        0.417741
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.006685
Test - acc:         0.896051 loss:        0.419615
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.006638
Test - acc:         0.896306 loss:        0.415310
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.006497
Test - acc:         0.894268 loss:        0.426658
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.999155 loss:        0.007087
Test - acc:         0.895796 loss:        0.426521
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.005821
Test - acc:         0.895541 loss:        0.420506
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005806
Test - acc:         0.894777 loss:        0.414767
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005467
Test - acc:         0.894777 loss:        0.426541
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005518
Test - acc:         0.894013 loss:        0.427043
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005955
Test - acc:         0.895796 loss:        0.424360
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.005539
Test - acc:         0.896051 loss:        0.425022
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.006395
Test - acc:         0.894777 loss:        0.429562
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.005286
Test - acc:         0.897325 loss:        0.422510
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005363
Test - acc:         0.897070 loss:        0.426850
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005133
Test - acc:         0.894522 loss:        0.425901
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005221
Test - acc:         0.894777 loss:        0.425060
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.005478
Test - acc:         0.896051 loss:        0.422392
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004670
Test - acc:         0.896051 loss:        0.424432
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.005032
Test - acc:         0.897070 loss:        0.423778
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004957
Test - acc:         0.895541 loss:        0.426621
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004830
Test - acc:         0.896306 loss:        0.426115
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004949
Test - acc:         0.896561 loss:        0.427350
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004457
Test - acc:         0.898089 loss:        0.422539
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004585
Test - acc:         0.897580 loss:        0.420428
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.005018
Test - acc:         0.897325 loss:        0.416813
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004186
Test - acc:         0.898344 loss:        0.420252
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004496
Test - acc:         0.897834 loss:        0.414344
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004809
Test - acc:         0.892994 loss:        0.429402
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.996198 loss:        0.021889
Test - acc:         0.889936 loss:        0.451236
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.998733 loss:        0.013856
Test - acc:         0.892229 loss:        0.445923
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.011839
Test - acc:         0.890955 loss:        0.441566
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.009973
Test - acc:         0.893758 loss:        0.440037
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.010125
Test - acc:         0.896561 loss:        0.439932
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.999050 loss:        0.009695
Test - acc:         0.891975 loss:        0.445410
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.008205
Test - acc:         0.895032 loss:        0.446890
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.008467
Test - acc:         0.894522 loss:        0.447536
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.007253
Test - acc:         0.895796 loss:        0.442861
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.006484
Test - acc:         0.898599 loss:        0.439606
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.007043
Test - acc:         0.895541 loss:        0.448918
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.007488
Test - acc:         0.894268 loss:        0.441703
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.006717
Test - acc:         0.893503 loss:        0.452260
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.006780
Test - acc:         0.897070 loss:        0.444382
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.006504
Test - acc:         0.896306 loss:        0.449088
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.007132
Test - acc:         0.893503 loss:        0.449604
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005771
Test - acc:         0.893503 loss:        0.454293
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005456
Test - acc:         0.894777 loss:        0.450451
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004909
Test - acc:         0.892484 loss:        0.456791
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005629
Test - acc:         0.893758 loss:        0.452361
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.005042
Test - acc:         0.892739 loss:        0.450120
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.005485
Test - acc:         0.895287 loss:        0.454301
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005164
Test - acc:         0.894268 loss:        0.456023
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.005359
Test - acc:         0.894777 loss:        0.455907
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005069
Test - acc:         0.892484 loss:        0.462461
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004840
Test - acc:         0.892994 loss:        0.459398
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004431
Test - acc:         0.892994 loss:        0.455137
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005054
Test - acc:         0.896051 loss:        0.459556
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004732
Test - acc:         0.894777 loss:        0.459547
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004457
Test - acc:         0.897325 loss:        0.457210
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004491
Test - acc:         0.896306 loss:        0.453567
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004961
Test - acc:         0.894777 loss:        0.461814
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.005244
Test - acc:         0.893248 loss:        0.462490
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004236
Test - acc:         0.894268 loss:        0.461301
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004160
Test - acc:         0.895032 loss:        0.458806
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004305
Test - acc:         0.897834 loss:        0.456206
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004191
Test - acc:         0.896306 loss:        0.457866
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004465
Test - acc:         0.894268 loss:        0.471622
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004165
Test - acc:         0.898344 loss:        0.462000
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.963565 loss:        0.119997
Test - acc:         0.863694 loss:        0.503314
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.983842 loss:        0.060318
Test - acc:         0.870828 loss:        0.483762
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.989334 loss:        0.044122
Test - acc:         0.876178 loss:        0.463222
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.991446 loss:        0.037496
Test - acc:         0.871338 loss:        0.496794
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.994403 loss:        0.029887
Test - acc:         0.880000 loss:        0.474903
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.993347 loss:        0.027137
Test - acc:         0.882293 loss:        0.474594
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.995459 loss:        0.024083
Test - acc:         0.881783 loss:        0.493703
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.995881 loss:        0.021245
Test - acc:         0.885350 loss:        0.485832
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.997043 loss:        0.017885
Test - acc:         0.878217 loss:        0.495588
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.995881 loss:        0.018687
Test - acc:         0.876688 loss:        0.525049
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.997149 loss:        0.018098
Test - acc:         0.875669 loss:        0.514248
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.997043 loss:        0.018083
Test - acc:         0.879236 loss:        0.509951
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.997149 loss:        0.016156
Test - acc:         0.880255 loss:        0.495300
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.996515 loss:        0.017885
Test - acc:         0.877707 loss:        0.503151
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.996515 loss:        0.016966
Test - acc:         0.885350 loss:        0.494572
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.998310 loss:        0.013390
Test - acc:         0.881274 loss:        0.531096
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.997571 loss:        0.012844
Test - acc:         0.885350 loss:        0.496605
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.999050 loss:        0.009655
Test - acc:         0.888408 loss:        0.491721
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.998944 loss:        0.009487
Test - acc:         0.889172 loss:        0.490540
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.998944 loss:        0.009253
Test - acc:         0.887389 loss:        0.485747
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.008491
Test - acc:         0.887643 loss:        0.486799
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.999050 loss:        0.009158
Test - acc:         0.887134 loss:        0.485490
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.999578 loss:        0.007700
Test - acc:         0.890701 loss:        0.490945
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.999155 loss:        0.008127
Test - acc:         0.886115 loss:        0.486541
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.999155 loss:        0.007772
Test - acc:         0.888153 loss:        0.483922
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.999155 loss:        0.008393
Test - acc:         0.888662 loss:        0.481602
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.007549
Test - acc:         0.887134 loss:        0.480421
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.007431
Test - acc:         0.890955 loss:        0.483255
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.998838 loss:        0.008093
Test - acc:         0.889427 loss:        0.479732
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.999366 loss:        0.007792
Test - acc:         0.888153 loss:        0.482654
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.006567
Test - acc:         0.889172 loss:        0.482112
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.999578 loss:        0.007278
Test - acc:         0.891210 loss:        0.482002
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999261 loss:        0.007448
Test - acc:         0.888917 loss:        0.486170
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999683 loss:        0.006909
Test - acc:         0.891465 loss:        0.483708
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999155 loss:        0.007816
Test - acc:         0.888917 loss:        0.480548
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999683 loss:        0.006692
Test - acc:         0.889172 loss:        0.492703
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.006201
Test - acc:         0.887643 loss:        0.487993
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.006669
Test - acc:         0.888662 loss:        0.487618
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999683 loss:        0.006598
Test - acc:         0.889936 loss:        0.482593
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.860492 loss:        0.436440
Test - acc:         0.833885 loss:        0.561446
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.928398 loss:        0.239054
Test - acc:         0.844076 loss:        0.530587
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.943711 loss:        0.192686
Test - acc:         0.848917 loss:        0.517706
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.949942 loss:        0.170665
Test - acc:         0.855541 loss:        0.494362
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.956490 loss:        0.148437
Test - acc:         0.862420 loss:        0.489954
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.963248 loss:        0.133165
Test - acc:         0.855287 loss:        0.499214
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.968529 loss:        0.121247
Test - acc:         0.860637 loss:        0.498364
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.966417 loss:        0.119519
Test - acc:         0.860637 loss:        0.497390
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.972859 loss:        0.107819
Test - acc:         0.862675 loss:        0.491683
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.973387 loss:        0.102522
Test - acc:         0.858089 loss:        0.500090
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.975816 loss:        0.097070
Test - acc:         0.865223 loss:        0.489073
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.977506 loss:        0.088584
Test - acc:         0.863185 loss:        0.493984
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.976872 loss:        0.089045
Test - acc:         0.864204 loss:        0.484892
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.981730 loss:        0.083473
Test - acc:         0.865478 loss:        0.494302
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.979723 loss:        0.078926
Test - acc:         0.863439 loss:        0.492179
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.984053 loss:        0.072882
Test - acc:         0.865732 loss:        0.492628
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.982047 loss:        0.074055
Test - acc:         0.863439 loss:        0.500248
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.983314 loss:        0.069297
Test - acc:         0.864204 loss:        0.496661
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.984792 loss:        0.068170
Test - acc:         0.866242 loss:        0.501963
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.986693 loss:        0.064045
Test - acc:         0.869554 loss:        0.492453
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.987749 loss:        0.062550
Test - acc:         0.865478 loss:        0.503343
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.986905 loss:        0.059741
Test - acc:         0.867261 loss:        0.499274
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.986588 loss:        0.057800
Test - acc:         0.866497 loss:        0.497940
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.987010 loss:        0.056787
Test - acc:         0.863694 loss:        0.505074
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.988806 loss:        0.053080
Test - acc:         0.866242 loss:        0.500038
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.990495 loss:        0.053065
Test - acc:         0.867006 loss:        0.503517
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.990601 loss:        0.049996
Test - acc:         0.864204 loss:        0.513637
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.992607 loss:        0.047159
Test - acc:         0.867516 loss:        0.507153
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.990178 loss:        0.049604
Test - acc:         0.868025 loss:        0.510828
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.990495 loss:        0.048343
Test - acc:         0.868280 loss:        0.505365
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.989967 loss:        0.050319
Test - acc:         0.872102 loss:        0.500153
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.992185 loss:        0.043618
Test - acc:         0.868280 loss:        0.504554
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.991974 loss:        0.044280
Test - acc:         0.866242 loss:        0.504859
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.991763 loss:        0.044234
Test - acc:         0.868790 loss:        0.508413
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.991868 loss:        0.044180
Test - acc:         0.868280 loss:        0.505762
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.990812 loss:        0.044811
Test - acc:         0.867006 loss:        0.514233
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.993135 loss:        0.041102
Test - acc:         0.867006 loss:        0.514884
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.992502 loss:        0.040685
Test - acc:         0.871847 loss:        0.513962
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.993241 loss:        0.039879
Test - acc:         0.866497 loss:        0.515707
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.715704 loss:        0.876690
Test - acc:         0.777070 loss:        0.723365
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.826381 loss:        0.546548
Test - acc:         0.800764 loss:        0.637127
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.853205 loss:        0.467158
Test - acc:         0.806624 loss:        0.628347
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.868096 loss:        0.420321
Test - acc:         0.817580 loss:        0.585075
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.875911 loss:        0.386948
Test - acc:         0.822930 loss:        0.576807
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.888056 loss:        0.358444
Test - acc:         0.823185 loss:        0.564406
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.891752 loss:        0.349971
Test - acc:         0.832866 loss:        0.544831
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.897244 loss:        0.328689
Test - acc:         0.836943 loss:        0.521965
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.902313 loss:        0.317822
Test - acc:         0.833885 loss:        0.532371
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.904319 loss:        0.309314
Test - acc:         0.837452 loss:        0.529536
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.911923 loss:        0.293296
Test - acc:         0.844076 loss:        0.511571
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.912979 loss:        0.287378
Test - acc:         0.842038 loss:        0.512540
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.912346 loss:        0.281125
Test - acc:         0.839236 loss:        0.502146
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.918893 loss:        0.270247
Test - acc:         0.836943 loss:        0.510102
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.919632 loss:        0.258574
Test - acc:         0.842803 loss:        0.506663
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.920689 loss:        0.259222
Test - acc:         0.846115 loss:        0.496063
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.921005 loss:        0.255884
Test - acc:         0.845096 loss:        0.505171
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.927659 loss:        0.242394
Test - acc:         0.852229 loss:        0.486236
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.926603 loss:        0.241167
Test - acc:         0.850955 loss:        0.484893
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.927553 loss:        0.232754
Test - acc:         0.854522 loss:        0.480423
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.930721 loss:        0.227313
Test - acc:         0.854522 loss:        0.482865
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.932517 loss:        0.223441
Test - acc:         0.856815 loss:        0.483411
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.934101 loss:        0.217373
Test - acc:         0.847643 loss:        0.498550
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.935896 loss:        0.213923
Test - acc:         0.854522 loss:        0.486466
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.940648 loss:        0.204645
Test - acc:         0.857580 loss:        0.479300
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.937903 loss:        0.204927
Test - acc:         0.857070 loss:        0.478474
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.938536 loss:        0.208214
Test - acc:         0.852739 loss:        0.489164
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.941916 loss:        0.199113
Test - acc:         0.856815 loss:        0.480495
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.942972 loss:        0.194597
Test - acc:         0.858599 loss:        0.474440
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.941071 loss:        0.196402
Test - acc:         0.856306 loss:        0.483966
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.942761 loss:        0.194585
Test - acc:         0.858089 loss:        0.478324
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.945612 loss:        0.186095
Test - acc:         0.857070 loss:        0.474798
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.944873 loss:        0.184824
Test - acc:         0.853503 loss:        0.486003
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.944978 loss:        0.186628
Test - acc:         0.858854 loss:        0.480461
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.943922 loss:        0.181959
Test - acc:         0.859618 loss:        0.470236
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.946879 loss:        0.174549
Test - acc:         0.856815 loss:        0.480404
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.951526 loss:        0.168509
Test - acc:         0.855541 loss:        0.480269
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.950153 loss:        0.172992
Test - acc:         0.857325 loss:        0.482854
Sparsity :          0.9961
Wdecay :        0.000500
