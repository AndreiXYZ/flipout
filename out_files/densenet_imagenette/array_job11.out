Running --model densenet121 --dataset imagenette --seed 44 --logdir=criterion_experiment_no_bias/densenet121 --prune_criterion weight_squared_div_flips --prune_freq 117 --prune_rate 0.5 --noise --comment=densenet121_crit=weight_squared_div_flips_pf=117_seed=44 --save_model=pre-finetune/densenet121_weight_squared_div_flips_pf117_s44
******************************
Running
{
    "model": "densenet121",
    "dataset": "imagenette",
    "batch_size": 128,
    "test_batch_size": 500,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 117,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/densenet121",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/densenet121_weight_squared_div_flips_pf117_s44",
    "load_model": null,
    "parallel": false
}
******************************
Total prunable params of model: 6880448
Model has 6964106 total params.
num_weights=6922272
num_biases=41834
num.prunable=6880448
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.333721 loss:        2.092967
Test - acc:         0.307771 loss:        6.056442
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.480199 loss:        1.621340
Test - acc:         0.505987 loss:        1.908920
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.573767 loss:        1.333350
Test - acc:         0.537070 loss:        1.505730
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.623614 loss:        1.201456
Test - acc:         0.503694 loss:        1.606702
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.653818 loss:        1.074715
Test - acc:         0.642548 loss:        1.140564
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.697223 loss:        0.927492
Test - acc:         0.683057 loss:        1.024992
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.722674 loss:        0.861224
Test - acc:         0.612484 loss:        1.328435
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.734502 loss:        0.817018
Test - acc:         0.656815 loss:        1.200333
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.757630 loss:        0.751320
Test - acc:         0.680764 loss:        0.998965
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.771993 loss:        0.702173
Test - acc:         0.716943 loss:        0.997309
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.786039 loss:        0.662253
Test - acc:         0.653248 loss:        1.191444
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.795438 loss:        0.622515
Test - acc:         0.749554 loss:        0.882538
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.812335 loss:        0.580071
Test - acc:         0.742420 loss:        0.863264
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.825114 loss:        0.533465
Test - acc:         0.702675 loss:        1.015203
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.834407 loss:        0.504240
Test - acc:         0.750064 loss:        0.772536
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845918 loss:        0.469809
Test - acc:         0.696815 loss:        1.170160
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.850037 loss:        0.444308
Test - acc:         0.740637 loss:        0.906648
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.857430 loss:        0.430658
Test - acc:         0.779873 loss:        0.737842
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.870736 loss:        0.387454
Test - acc:         0.675414 loss:        1.428247
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.860281 loss:        0.420546
Test - acc:         0.656306 loss:        1.565538
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.881297 loss:        0.360454
Test - acc:         0.784968 loss:        0.739873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.878340 loss:        0.382929
Test - acc:         0.776051 loss:        0.803206
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.884359 loss:        0.344970
Test - acc:         0.727134 loss:        1.069458
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.888795 loss:        0.346668
Test - acc:         0.771465 loss:        0.793985
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.895871 loss:        0.316962
Test - acc:         0.769427 loss:        0.810582
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.891118 loss:        0.326808
Test - acc:         0.769682 loss:        0.770453
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.901996 loss:        0.307741
Test - acc:         0.736306 loss:        0.998031
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.898828 loss:        0.298970
Test - acc:         0.796433 loss:        0.809667
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.899673 loss:        0.305613
Test - acc:         0.765096 loss:        0.852713
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.905587 loss:        0.286490
Test - acc:         0.676178 loss:        1.335155
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.896082 loss:        0.312915
Test - acc:         0.730955 loss:        0.970557
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.904003 loss:        0.281854
Test - acc:         0.766879 loss:        0.839994
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.913613 loss:        0.262699
Test - acc:         0.801274 loss:        0.716547
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.911923 loss:        0.261046
Test - acc:         0.761529 loss:        0.948662
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.912662 loss:        0.265486
Test - acc:         0.780892 loss:        0.799466
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.908966 loss:        0.273010
Test - acc:         0.758981 loss:        0.971180
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.916464 loss:        0.252723
Test - acc:         0.743694 loss:        0.879393
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.916992 loss:        0.247641
Test - acc:         0.651975 loss:        1.671991
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.917943 loss:        0.238841
Test - acc:         0.743694 loss:        0.962231
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.920266 loss:        0.238883
Test - acc:         0.647389 loss:        1.702062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.923751 loss:        0.234690
Test - acc:         0.790828 loss:        0.822939
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.929137 loss:        0.212244
Test - acc:         0.701911 loss:        1.229443
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.907804 loss:        0.273883
Test - acc:         0.802803 loss:        0.698272
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.919632 loss:        0.241945
Test - acc:         0.798726 loss:        0.701875
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.918471 loss:        0.243604
Test - acc:         0.792866 loss:        0.816701
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.925863 loss:        0.221479
Test - acc:         0.711338 loss:        1.276467
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.921850 loss:        0.241697
Test - acc:         0.802548 loss:        0.746040
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.910656 loss:        0.273963
Test - acc:         0.426242 loss:        2.942865
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.888373 loss:        0.332485
Test - acc:         0.783185 loss:        0.749481
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.927870 loss:        0.221475
Test - acc:         0.771465 loss:        0.908757
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.911923 loss:        0.259537
Test - acc:         0.789554 loss:        0.747312
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.920055 loss:        0.241351
Test - acc:         0.798471 loss:        0.760223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.921111 loss:        0.237593
Test - acc:         0.770446 loss:        0.862252
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.925969 loss:        0.222965
Test - acc:         0.784713 loss:        0.795751
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.931883 loss:        0.211062
Test - acc:         0.806369 loss:        0.698372
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.920161 loss:        0.241977
Test - acc:         0.830828 loss:        0.611886
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.933150 loss:        0.200237
Test - acc:         0.832611 loss:        0.587423
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.925230 loss:        0.220845
Test - acc:         0.693758 loss:        1.176126
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.922801 loss:        0.229710
Test - acc:         0.803567 loss:        0.752749
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.935051 loss:        0.196382
Test - acc:         0.792611 loss:        0.772364
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.926391 loss:        0.222342
Test - acc:         0.819363 loss:        0.653327
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.925969 loss:        0.211836
Test - acc:         0.720000 loss:        1.101422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.931249 loss:        0.206102
Test - acc:         0.815287 loss:        0.705944
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.925758 loss:        0.215592
Test - acc:         0.816815 loss:        0.650783
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.938325 loss:        0.189618
Test - acc:         0.793376 loss:        0.804342
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.922061 loss:        0.232599
Test - acc:         0.730701 loss:        1.218470
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.936741 loss:        0.190754
Test - acc:         0.826497 loss:        0.643881
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.927236 loss:        0.218581
Test - acc:         0.734268 loss:        1.079342
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.926286 loss:        0.219830
Test - acc:         0.769682 loss:        0.926411
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.924807 loss:        0.226449
Test - acc:         0.801019 loss:        0.790222
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.931249 loss:        0.205732
Test - acc:         0.811210 loss:        0.724615
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.935368 loss:        0.191748
Test - acc:         0.778854 loss:        0.860094
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.935474 loss:        0.197207
Test - acc:         0.808153 loss:        0.730517
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.940437 loss:        0.186014
Test - acc:         0.812484 loss:        0.716805
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.935051 loss:        0.189641
Test - acc:         0.731975 loss:        0.985983
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.923751 loss:        0.228481
Test - acc:         0.804076 loss:        0.685051
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.944978 loss:        0.162618
Test - acc:         0.778089 loss:        0.937092
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.927870 loss:        0.219382
Test - acc:         0.726369 loss:        1.249147
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.944133 loss:        0.170984
Test - acc:         0.837962 loss:        0.606852
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.942549 loss:        0.172621
Test - acc:         0.752866 loss:        1.062058
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.931566 loss:        0.201676
Test - acc:         0.748280 loss:        1.020947
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.935262 loss:        0.198148
Test - acc:         0.798981 loss:        0.773121
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.941916 loss:        0.180019
Test - acc:         0.830573 loss:        0.613012
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.941810 loss:        0.168404
Test - acc:         0.810701 loss:        0.732481
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.936107 loss:        0.189315
Test - acc:         0.779618 loss:        0.899162
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.936213 loss:        0.188540
Test - acc:         0.809936 loss:        0.804049
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.937480 loss:        0.189265
Test - acc:         0.806879 loss:        0.726916
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.946246 loss:        0.165143
Test - acc:         0.777325 loss:        0.898547
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.943077 loss:        0.174603
Test - acc:         0.706752 loss:        1.669681
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.937903 loss:        0.187410
Test - acc:         0.784968 loss:        0.788530
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.944239 loss:        0.169601
Test - acc:         0.826242 loss:        0.642493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.942972 loss:        0.174312
Test - acc:         0.662166 loss:        1.767646
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.929137 loss:        0.211553
Test - acc:         0.757707 loss:        0.988738
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.948886 loss:        0.166678
Test - acc:         0.818854 loss:        0.670484
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.947935 loss:        0.157119
Test - acc:         0.804331 loss:        0.848989
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.947619 loss:        0.161323
Test - acc:         0.822166 loss:        0.726818
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.940754 loss:        0.180528
Test - acc:         0.820382 loss:        0.757905
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.944873 loss:        0.164944
Test - acc:         0.805096 loss:        0.802050
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.940543 loss:        0.174864
Test - acc:         0.816051 loss:        0.749401
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.943817 loss:        0.168993
Test - acc:         0.755414 loss:        1.066612
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.941071 loss:        0.177992
Test - acc:         0.779108 loss:        0.842359
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.944450 loss:        0.169879
Test - acc:         0.794650 loss:        0.786937
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.944133 loss:        0.163772
Test - acc:         0.842293 loss:        0.572976
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.952477 loss:        0.141206
Test - acc:         0.775032 loss:        0.886668
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.947302 loss:        0.160805
Test - acc:         0.789299 loss:        0.836654
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.945929 loss:        0.170047
Test - acc:         0.824968 loss:        0.710110
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.944767 loss:        0.163944
Test - acc:         0.812229 loss:        0.729285
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.948780 loss:        0.156172
Test - acc:         0.674650 loss:        1.445472
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.940648 loss:        0.172368
Test - acc:         0.797707 loss:        0.805419
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.944028 loss:        0.173577
Test - acc:         0.798217 loss:        0.781829
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.949203 loss:        0.151407
Test - acc:         0.829554 loss:        0.679913
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.949731 loss:        0.146752
Test - acc:         0.823185 loss:        0.644400
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.950681 loss:        0.144753
Test - acc:         0.672357 loss:        1.524366
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.932094 loss:        0.194246
Test - acc:         0.836688 loss:        0.625374
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.946351 loss:        0.163284
Test - acc:         0.832102 loss:        0.621684
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.961242 loss:        0.117306
Test - acc:         0.787516 loss:        0.785044
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.950787 loss:        0.143373
Test - acc:         0.827261 loss:        0.691585
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.970430 loss:        0.092152
Test - acc:         0.869045 loss:        0.510130
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.976238 loss:        0.070625
Test - acc:         0.837707 loss:        0.652098
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.957862 loss:        0.124347
Test - acc:         0.800764 loss:        0.826031
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.958707 loss:        0.127404
Test - acc:         0.806115 loss:        0.743616
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.954377 loss:        0.131489
Test - acc:         0.774777 loss:        0.917266
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.956173 loss:        0.129017
Test - acc:         0.779873 loss:        0.966309
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.957018 loss:        0.127926
Test - acc:         0.859873 loss:        0.525926
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.963882 loss:        0.118171
Test - acc:         0.830828 loss:        0.620092
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.957018 loss:        0.132476
Test - acc:         0.830573 loss:        0.673366
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.965149 loss:        0.110220
Test - acc:         0.833376 loss:        0.627861
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.956384 loss:        0.125315
Test - acc:         0.784459 loss:        0.932482
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.960291 loss:        0.119591
Test - acc:         0.820637 loss:        0.704263
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.962192 loss:        0.120204
Test - acc:         0.807134 loss:        0.886381
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.958919 loss:        0.128548
Test - acc:         0.828535 loss:        0.667310
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.958919 loss:        0.123627
Test - acc:         0.831083 loss:        0.669280
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.956384 loss:        0.132991
Test - acc:         0.817325 loss:        0.731610
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.954800 loss:        0.138719
Test - acc:         0.777325 loss:        0.928390
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.954694 loss:        0.135186
Test - acc:         0.821146 loss:        0.710479
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.957546 loss:        0.131595
Test - acc:         0.800764 loss:        0.779400
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.952899 loss:        0.142309
Test - acc:         0.826242 loss:        0.731965
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.956067 loss:        0.129215
Test - acc:         0.836688 loss:        0.627292
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.961981 loss:        0.117206
Test - acc:         0.782930 loss:        0.878153
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.951632 loss:        0.139666
Test - acc:         0.660892 loss:        1.437391
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.958496 loss:        0.131732
Test - acc:         0.837197 loss:        0.631705
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.959869 loss:        0.123706
Test - acc:         0.830318 loss:        0.675604
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.952688 loss:        0.139328
Test - acc:         0.809427 loss:        0.694570
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.953849 loss:        0.144085
Test - acc:         0.819108 loss:        0.664573
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.964410 loss:        0.108495
Test - acc:         0.806369 loss:        0.731599
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.957334 loss:        0.122124
Test - acc:         0.851720 loss:        0.585064
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.967156 loss:        0.105052
Test - acc:         0.827771 loss:        0.709461
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.969374 loss:        0.091474
Test - acc:         0.838217 loss:        0.635605
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.956490 loss:        0.129323
Test - acc:         0.762548 loss:        1.028909
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.946879 loss:        0.155814
Test - acc:         0.831847 loss:        0.696241
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.979618 loss:        0.065274
Test - acc:         0.898599 loss:        0.357452
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.995670 loss:        0.025365
Test - acc:         0.901911 loss:        0.348827
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.998310 loss:        0.018001
Test - acc:         0.901401 loss:        0.349102
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.998416 loss:        0.014164
Test - acc:         0.903694 loss:        0.338492
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.999050 loss:        0.011894
Test - acc:         0.907006 loss:        0.338618
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.999155 loss:        0.009926
Test - acc:         0.908790 loss:        0.340226
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.010093
Test - acc:         0.908025 loss:        0.345404
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.009227
Test - acc:         0.907771 loss:        0.343544
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.008318
Test - acc:         0.909554 loss:        0.341529
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.007613
Test - acc:         0.908280 loss:        0.347486
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.006877
Test - acc:         0.911592 loss:        0.341089
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.006673
Test - acc:         0.910064 loss:        0.348898
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.005670
Test - acc:         0.909809 loss:        0.351518
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005371
Test - acc:         0.909045 loss:        0.351490
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.005721
Test - acc:         0.908280 loss:        0.353865
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005696
Test - acc:         0.910064 loss:        0.346587
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.005312
Test - acc:         0.910573 loss:        0.350222
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004930
Test - acc:         0.913121 loss:        0.346192
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005111
Test - acc:         0.907516 loss:        0.356618
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004413
Test - acc:         0.910064 loss:        0.349031
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.004555
Test - acc:         0.910318 loss:        0.349404
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004274
Test - acc:         0.911083 loss:        0.351881
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003999
Test - acc:         0.908790 loss:        0.351467
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003976
Test - acc:         0.910064 loss:        0.347895
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003842
Test - acc:         0.910064 loss:        0.352655
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003669
Test - acc:         0.911847 loss:        0.350145
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003700
Test - acc:         0.912611 loss:        0.352254
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004040
Test - acc:         0.914904 loss:        0.347982
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003683
Test - acc:         0.912611 loss:        0.349823
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003341
Test - acc:         0.914395 loss:        0.344387
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003662
Test - acc:         0.913376 loss:        0.346190
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003618
Test - acc:         0.910828 loss:        0.347657
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003360
Test - acc:         0.911338 loss:        0.347669
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003733
Test - acc:         0.912357 loss:        0.346480
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003473
Test - acc:         0.912611 loss:        0.345901
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003070
Test - acc:         0.911592 loss:        0.346826
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003235
Test - acc:         0.910828 loss:        0.351898
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003234
Test - acc:         0.911083 loss:        0.353328
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002910
Test - acc:         0.916688 loss:        0.345881
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003264
Test - acc:         0.914650 loss:        0.343652
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002972
Test - acc:         0.914395 loss:        0.349085
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002784
Test - acc:         0.915159 loss:        0.346814
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003465
Test - acc:         0.914140 loss:        0.347684
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002871
Test - acc:         0.914395 loss:        0.345615
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002901
Test - acc:         0.912611 loss:        0.348066
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003184
Test - acc:         0.914395 loss:        0.344991
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003055
Test - acc:         0.915669 loss:        0.340762
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002900
Test - acc:         0.913631 loss:        0.343416
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003178
Test - acc:         0.913885 loss:        0.343483
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003080
Test - acc:         0.915159 loss:        0.340483
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003060
Test - acc:         0.912866 loss:        0.343939
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003215
Test - acc:         0.915669 loss:        0.337851
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003073
Test - acc:         0.914395 loss:        0.341928
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003062
Test - acc:         0.914140 loss:        0.339364
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003032
Test - acc:         0.913885 loss:        0.337378
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.002694
Test - acc:         0.916178 loss:        0.343154
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002870
Test - acc:         0.914140 loss:        0.340538
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002620
Test - acc:         0.914650 loss:        0.338722
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002590
Test - acc:         0.913885 loss:        0.341962
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002798
Test - acc:         0.913631 loss:        0.344168
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002675
Test - acc:         0.914140 loss:        0.341238
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002696
Test - acc:         0.917197 loss:        0.341507
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002690
Test - acc:         0.911847 loss:        0.342857
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002785
Test - acc:         0.914904 loss:        0.341732
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002648
Test - acc:         0.914904 loss:        0.342554
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002848
Test - acc:         0.914140 loss:        0.342179
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002786
Test - acc:         0.913885 loss:        0.339776
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002521
Test - acc:         0.913885 loss:        0.341231
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002803
Test - acc:         0.913631 loss:        0.342653
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003065
Test - acc:         0.913376 loss:        0.344821
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002413
Test - acc:         0.917452 loss:        0.339085
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002564
Test - acc:         0.916688 loss:        0.338423
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002439
Test - acc:         0.913885 loss:        0.342583
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002429
Test - acc:         0.916178 loss:        0.338017
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002499
Test - acc:         0.915924 loss:        0.336572
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002482
Test - acc:         0.915414 loss:        0.336736
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002328
Test - acc:         0.914650 loss:        0.343120
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002290
Test - acc:         0.916433 loss:        0.335937
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002434
Test - acc:         0.915669 loss:        0.339778
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002557
Test - acc:         0.915414 loss:        0.340241
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002275
Test - acc:         0.915414 loss:        0.339246
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002784
Test - acc:         0.915159 loss:        0.343724
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002710
Test - acc:         0.915159 loss:        0.338088
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002702
Test - acc:         0.915159 loss:        0.345363
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003244
Test - acc:         0.911338 loss:        0.340052
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003357
Test - acc:         0.912102 loss:        0.344103
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002965
Test - acc:         0.911847 loss:        0.339687
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003028
Test - acc:         0.913376 loss:        0.338409
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002872
Test - acc:         0.913121 loss:        0.338977
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002799
Test - acc:         0.913121 loss:        0.340081
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002865
Test - acc:         0.912102 loss:        0.338704
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002779
Test - acc:         0.912357 loss:        0.341905
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002511
Test - acc:         0.913376 loss:        0.342592
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002470
Test - acc:         0.912357 loss:        0.343330
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002848
Test - acc:         0.913376 loss:        0.341492
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003142
Test - acc:         0.914650 loss:        0.340334
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002845
Test - acc:         0.911847 loss:        0.341230
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002647
Test - acc:         0.911847 loss:        0.340580
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002781
Test - acc:         0.913631 loss:        0.345241
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002704
Test - acc:         0.911847 loss:        0.341204
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002827
Test - acc:         0.912357 loss:        0.338648
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002716
Test - acc:         0.913121 loss:        0.338869
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002503
Test - acc:         0.911338 loss:        0.341974
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002508
Test - acc:         0.911847 loss:        0.343868
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.002947
Test - acc:         0.912866 loss:        0.340624
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002494
Test - acc:         0.911083 loss:        0.336262
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002671
Test - acc:         0.911592 loss:        0.340979
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002660
Test - acc:         0.910573 loss:        0.343964
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002675
Test - acc:         0.912611 loss:        0.340474
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002768
Test - acc:         0.912357 loss:        0.340397
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002465
Test - acc:         0.911338 loss:        0.343487
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002510
Test - acc:         0.912611 loss:        0.340000
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002549
Test - acc:         0.911847 loss:        0.344090
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002650
Test - acc:         0.911847 loss:        0.340134
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002567
Test - acc:         0.912866 loss:        0.337332
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002626
Test - acc:         0.912611 loss:        0.337552
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002434
Test - acc:         0.912102 loss:        0.344666
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002829
Test - acc:         0.911592 loss:        0.344276
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002841
Test - acc:         0.912102 loss:        0.343903
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002571
Test - acc:         0.913121 loss:        0.340209
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002574
Test - acc:         0.910318 loss:        0.342935
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002649
Test - acc:         0.913121 loss:        0.339889
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002666
Test - acc:         0.913631 loss:        0.338324
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002402
Test - acc:         0.910573 loss:        0.336490
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002508
Test - acc:         0.913121 loss:        0.338798
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002466
Test - acc:         0.910828 loss:        0.338320
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002787
Test - acc:         0.911338 loss:        0.336383
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002360
Test - acc:         0.913376 loss:        0.340316
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002456
Test - acc:         0.912611 loss:        0.338724
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002397
Test - acc:         0.914140 loss:        0.339852
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002842
Test - acc:         0.913376 loss:        0.338712
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002677
Test - acc:         0.911592 loss:        0.338839
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002657
Test - acc:         0.911592 loss:        0.339707
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002615
Test - acc:         0.912357 loss:        0.343111
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002442
Test - acc:         0.912611 loss:        0.341145
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002596
Test - acc:         0.913885 loss:        0.337916
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002724
Test - acc:         0.912611 loss:        0.337854
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002842
Test - acc:         0.911083 loss:        0.343190
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002447
Test - acc:         0.912866 loss:        0.338566
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002540
Test - acc:         0.913376 loss:        0.342118
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002444
Test - acc:         0.912611 loss:        0.339070
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002819
Test - acc:         0.911338 loss:        0.340540
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002400
Test - acc:         0.911338 loss:        0.339133
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002427
Test - acc:         0.912357 loss:        0.339256
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002763
Test - acc:         0.911847 loss:        0.337383
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002395
Test - acc:         0.912357 loss:        0.337825
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002658
Test - acc:         0.913376 loss:        0.335137
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002637
Test - acc:         0.910828 loss:        0.339362
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002679
Test - acc:         0.913885 loss:        0.339294
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002622
Test - acc:         0.911338 loss:        0.337936
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.002971
Test - acc:         0.912357 loss:        0.335818
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002490
Test - acc:         0.911847 loss:        0.339100
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002531
Test - acc:         0.911592 loss:        0.339035
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002710
Test - acc:         0.913121 loss:        0.339878
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002630
Test - acc:         0.913121 loss:        0.341728
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002681
Test - acc:         0.913631 loss:        0.335596
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002756
Test - acc:         0.911847 loss:        0.337745
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002439
Test - acc:         0.912611 loss:        0.339436
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002722
Test - acc:         0.910828 loss:        0.341403
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002545
Test - acc:         0.912866 loss:        0.341419
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002499
Test - acc:         0.911847 loss:        0.340797
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002522
Test - acc:         0.912611 loss:        0.342217
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002239
Test - acc:         0.912611 loss:        0.341973
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002461
Test - acc:         0.911847 loss:        0.338979
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002366
Test - acc:         0.911592 loss:        0.338839
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002467
Test - acc:         0.913121 loss:        0.339599
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002477
Test - acc:         0.911847 loss:        0.343011
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002581
Test - acc:         0.911847 loss:        0.344117
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002715
Test - acc:         0.914140 loss:        0.338507
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002542
Test - acc:         0.912357 loss:        0.340703
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002645
Test - acc:         0.912611 loss:        0.340851
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002498
Test - acc:         0.912611 loss:        0.341792
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002536
Test - acc:         0.912611 loss:        0.336259
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002534
Test - acc:         0.912611 loss:        0.340995
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002266
Test - acc:         0.911847 loss:        0.342698
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002518
Test - acc:         0.911592 loss:        0.340586
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002296
Test - acc:         0.911592 loss:        0.340918
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002301
Test - acc:         0.911592 loss:        0.344488
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002694
Test - acc:         0.911083 loss:        0.340068
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002457
Test - acc:         0.911847 loss:        0.338496
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002471
Test - acc:         0.911592 loss:        0.343416
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002255
Test - acc:         0.913376 loss:        0.338957
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002445
Test - acc:         0.912102 loss:        0.340394
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002581
Test - acc:         0.912357 loss:        0.339265
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002769
Test - acc:         0.911338 loss:        0.343101
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002694
Test - acc:         0.912611 loss:        0.343522
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002552
Test - acc:         0.912102 loss:        0.341214
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002680
Test - acc:         0.912357 loss:        0.338604
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002386
Test - acc:         0.912357 loss:        0.337145
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002412
Test - acc:         0.912866 loss:        0.340892
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002597
Test - acc:         0.912611 loss:        0.341737
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002443
Test - acc:         0.912357 loss:        0.340127
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002351
Test - acc:         0.913376 loss:        0.337065
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002298
Test - acc:         0.911847 loss:        0.341229
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002376
Test - acc:         0.912357 loss:        0.338764
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002414
Test - acc:         0.913121 loss:        0.335252
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002481
Test - acc:         0.913631 loss:        0.336300
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002532
Test - acc:         0.912611 loss:        0.339122
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002684
Test - acc:         0.912357 loss:        0.341387
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002505
Test - acc:         0.911592 loss:        0.339275
Sparsity :          0.7500
Wdecay :        0.000500
