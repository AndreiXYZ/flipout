Running --model densenet121 --dataset imagenette --seed 44 --logdir=criterion_experiment_no_bias/densenet121 --prune_criterion weight_squared_div_flips --prune_freq 32 --prune_rate 0.5 --noise --comment=densenet121_crit=weight_squared_div_flips_pf=32_seed=44 --save_model=pre-finetune/densenet121_weight_squared_div_flips_pf32_s44
******************************
Running
{
    "model": "densenet121",
    "dataset": "imagenette",
    "batch_size": 128,
    "test_batch_size": 500,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/densenet121",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/densenet121_weight_squared_div_flips_pf32_s44",
    "load_model": null,
    "parallel": false
}
******************************
Total prunable params of model: 6880448
Model has 6964106 total params.
num_weights=6922272
num_biases=41834
num.prunable=6880448
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.333721 loss:        2.092967
Test - acc:         0.307771 loss:        6.056442
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.480199 loss:        1.621340
Test - acc:         0.505987 loss:        1.908920
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.573767 loss:        1.333350
Test - acc:         0.537070 loss:        1.505730
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.623614 loss:        1.201456
Test - acc:         0.503694 loss:        1.606702
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.653818 loss:        1.074715
Test - acc:         0.642548 loss:        1.140564
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.697223 loss:        0.927492
Test - acc:         0.683057 loss:        1.024992
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.722674 loss:        0.861224
Test - acc:         0.612484 loss:        1.328435
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.734502 loss:        0.817018
Test - acc:         0.656815 loss:        1.200333
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.757630 loss:        0.751320
Test - acc:         0.680764 loss:        0.998965
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.771993 loss:        0.702173
Test - acc:         0.716943 loss:        0.997309
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.786039 loss:        0.662253
Test - acc:         0.653248 loss:        1.191444
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.795438 loss:        0.622515
Test - acc:         0.749554 loss:        0.882538
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.812335 loss:        0.580071
Test - acc:         0.742420 loss:        0.863264
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.825114 loss:        0.533465
Test - acc:         0.702675 loss:        1.015203
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.834407 loss:        0.504240
Test - acc:         0.750064 loss:        0.772536
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845918 loss:        0.469809
Test - acc:         0.696815 loss:        1.170160
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.850037 loss:        0.444308
Test - acc:         0.740637 loss:        0.906648
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.857430 loss:        0.430658
Test - acc:         0.779873 loss:        0.737842
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.870736 loss:        0.387454
Test - acc:         0.675414 loss:        1.428247
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.860281 loss:        0.420546
Test - acc:         0.656306 loss:        1.565538
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.881297 loss:        0.360454
Test - acc:         0.784968 loss:        0.739873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.878340 loss:        0.382929
Test - acc:         0.776051 loss:        0.803206
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.884359 loss:        0.344970
Test - acc:         0.727134 loss:        1.069458
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.888795 loss:        0.346668
Test - acc:         0.771465 loss:        0.793985
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.895871 loss:        0.316962
Test - acc:         0.769427 loss:        0.810582
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.891118 loss:        0.326808
Test - acc:         0.769682 loss:        0.770453
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.901996 loss:        0.307741
Test - acc:         0.736306 loss:        0.998031
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.898828 loss:        0.298970
Test - acc:         0.796433 loss:        0.809667
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.899673 loss:        0.305613
Test - acc:         0.765096 loss:        0.852713
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.905587 loss:        0.286490
Test - acc:         0.676178 loss:        1.335155
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.896082 loss:        0.312915
Test - acc:         0.730955 loss:        0.970557
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.904003 loss:        0.281854
Test - acc:         0.766879 loss:        0.839994
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.940120 loss:        0.187401
Test - acc:         0.835414 loss:        0.595060
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.932094 loss:        0.192814
Test - acc:         0.804076 loss:        0.764592
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.927870 loss:        0.214125
Test - acc:         0.769936 loss:        0.886611
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.928715 loss:        0.211988
Test - acc:         0.795669 loss:        0.788882
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.930193 loss:        0.208816
Test - acc:         0.811210 loss:        0.690684
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.940543 loss:        0.179659
Test - acc:         0.781146 loss:        1.005978
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.935685 loss:        0.191692
Test - acc:         0.788790 loss:        0.875630
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.939170 loss:        0.186507
Test - acc:         0.763312 loss:        0.925928
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.917098 loss:        0.243866
Test - acc:         0.770955 loss:        0.941996
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.926814 loss:        0.224242
Test - acc:         0.767134 loss:        0.896668
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.927870 loss:        0.219902
Test - acc:         0.777834 loss:        0.822113
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.920900 loss:        0.226818
Test - acc:         0.758471 loss:        0.885351
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.932728 loss:        0.203560
Test - acc:         0.593885 loss:        1.913786
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.933467 loss:        0.198434
Test - acc:         0.583185 loss:        1.671071
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.909177 loss:        0.271711
Test - acc:         0.739873 loss:        1.019765
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.922695 loss:        0.233173
Test - acc:         0.748025 loss:        1.061281
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.923223 loss:        0.229076
Test - acc:         0.792866 loss:        0.760047
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.934101 loss:        0.209078
Test - acc:         0.764331 loss:        0.921225
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.924279 loss:        0.228741
Test - acc:         0.781656 loss:        0.802652
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.930616 loss:        0.213680
Test - acc:         0.741146 loss:        1.061158
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.930721 loss:        0.210755
Test - acc:         0.752102 loss:        1.042525
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.930510 loss:        0.214280
Test - acc:         0.788535 loss:        0.751566
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.925652 loss:        0.216803
Test - acc:         0.797707 loss:        0.670806
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.931883 loss:        0.203774
Test - acc:         0.800764 loss:        0.712956
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.940015 loss:        0.179660
Test - acc:         0.791592 loss:        0.855637
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.924068 loss:        0.222805
Test - acc:         0.744713 loss:        0.915110
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.939064 loss:        0.184112
Test - acc:         0.769682 loss:        1.001920
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.932411 loss:        0.202543
Test - acc:         0.762548 loss:        0.935134
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.926286 loss:        0.221080
Test - acc:         0.766369 loss:        0.897087
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.932728 loss:        0.204585
Test - acc:         0.738854 loss:        1.055874
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.933784 loss:        0.193684
Test - acc:         0.788280 loss:        0.784750
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.936952 loss:        0.191074
Test - acc:         0.820892 loss:        0.629346
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.964305 loss:        0.112636
Test - acc:         0.804331 loss:        0.761335
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.952793 loss:        0.134591
Test - acc:         0.707261 loss:        1.342621
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.947196 loss:        0.151648
Test - acc:         0.803567 loss:        0.799096
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.952582 loss:        0.149093
Test - acc:         0.745987 loss:        1.205689
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.949203 loss:        0.151799
Test - acc:         0.798217 loss:        0.810295
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.941810 loss:        0.177724
Test - acc:         0.791847 loss:        0.948732
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.942972 loss:        0.168576
Test - acc:         0.770446 loss:        0.994192
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.949519 loss:        0.150287
Test - acc:         0.776561 loss:        1.028920
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.952688 loss:        0.144366
Test - acc:         0.804586 loss:        0.778480
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.942021 loss:        0.180551
Test - acc:         0.744713 loss:        0.985001
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.951315 loss:        0.148612
Test - acc:         0.805350 loss:        0.759941
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.953638 loss:        0.142371
Test - acc:         0.797197 loss:        0.798246
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.950576 loss:        0.146768
Test - acc:         0.772229 loss:        0.907693
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.942444 loss:        0.167231
Test - acc:         0.803822 loss:        0.788780
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.939592 loss:        0.184646
Test - acc:         0.817580 loss:        0.682715
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.942021 loss:        0.171280
Test - acc:         0.762548 loss:        0.932877
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.941176 loss:        0.174045
Test - acc:         0.749299 loss:        1.043550
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.936424 loss:        0.191652
Test - acc:         0.787516 loss:        0.803274
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.947619 loss:        0.153568
Test - acc:         0.816306 loss:        0.722656
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.948147 loss:        0.153184
Test - acc:         0.703949 loss:        1.357568
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.930721 loss:        0.206966
Test - acc:         0.794395 loss:        0.778941
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.947619 loss:        0.160085
Test - acc:         0.721274 loss:        1.280225
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.947196 loss:        0.150425
Test - acc:         0.731210 loss:        1.388563
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.942655 loss:        0.170459
Test - acc:         0.788280 loss:        0.764147
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.951526 loss:        0.146630
Test - acc:         0.754904 loss:        1.166623
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.919632 loss:        0.236044
Test - acc:         0.598981 loss:        1.895824
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.916570 loss:        0.246623
Test - acc:         0.801783 loss:        0.706882
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.936319 loss:        0.190479
Test - acc:         0.769936 loss:        1.008013
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.942338 loss:        0.177221
Test - acc:         0.770446 loss:        0.987403
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.945823 loss:        0.161210
Test - acc:         0.769682 loss:        0.940742
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.937903 loss:        0.189168
Test - acc:         0.761783 loss:        1.055171
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.938431 loss:        0.186400
Test - acc:         0.827516 loss:        0.647223
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.964833 loss:        0.105013
Test - acc:         0.861146 loss:        0.533756
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.970324 loss:        0.086546
Test - acc:         0.804076 loss:        0.830597
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.950048 loss:        0.148018
Test - acc:         0.816561 loss:        0.785450
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.946246 loss:        0.160430
Test - acc:         0.797707 loss:        0.783832
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.961981 loss:        0.115116
Test - acc:         0.846115 loss:        0.623510
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.964621 loss:        0.113893
Test - acc:         0.766115 loss:        1.095509
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.951948 loss:        0.136300
Test - acc:         0.779873 loss:        0.973413
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.946246 loss:        0.158965
Test - acc:         0.736051 loss:        1.181095
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.952688 loss:        0.137320
Test - acc:         0.784459 loss:        0.990026
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.956490 loss:        0.128386
Test - acc:         0.845096 loss:        0.616579
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.954377 loss:        0.136729
Test - acc:         0.771720 loss:        1.050541
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.953638 loss:        0.139524
Test - acc:         0.795669 loss:        0.826194
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.944767 loss:        0.158729
Test - acc:         0.771210 loss:        1.149813
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.948780 loss:        0.149838
Test - acc:         0.819873 loss:        0.715967
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.958813 loss:        0.129181
Test - acc:         0.832357 loss:        0.658749
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.946457 loss:        0.156861
Test - acc:         0.767898 loss:        0.957944
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.953427 loss:        0.146231
Test - acc:         0.807134 loss:        0.845848
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.955539 loss:        0.132827
Test - acc:         0.786497 loss:        0.935164
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.946668 loss:        0.152492
Test - acc:         0.834904 loss:        0.645285
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.952265 loss:        0.142241
Test - acc:         0.770446 loss:        0.997524
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.948358 loss:        0.153285
Test - acc:         0.810955 loss:        0.828414
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.954166 loss:        0.138641
Test - acc:         0.775541 loss:        1.030584
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.943289 loss:        0.173314
Test - acc:         0.806879 loss:        0.795716
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.949942 loss:        0.142684
Test - acc:         0.788025 loss:        0.852994
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.956067 loss:        0.134399
Test - acc:         0.796688 loss:        0.813195
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.960186 loss:        0.122741
Test - acc:         0.813503 loss:        0.755762
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.956067 loss:        0.137966
Test - acc:         0.806369 loss:        0.819856
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.952582 loss:        0.142543
Test - acc:         0.827261 loss:        0.664276
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.953955 loss:        0.138180
Test - acc:         0.815032 loss:        0.791167
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.941916 loss:        0.165708
Test - acc:         0.808153 loss:        0.687478
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.957018 loss:        0.129752
Test - acc:         0.779618 loss:        0.872581
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.949414 loss:        0.159694
Test - acc:         0.830828 loss:        0.686390
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.975393 loss:        0.080805
Test - acc:         0.820382 loss:        0.779912
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.975288 loss:        0.076439
Test - acc:         0.837197 loss:        0.633469
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.964938 loss:        0.104875
Test - acc:         0.808408 loss:        0.788412
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.952688 loss:        0.144182
Test - acc:         0.836178 loss:        0.674656
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.958074 loss:        0.131132
Test - acc:         0.844586 loss:        0.614964
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.962932 loss:        0.107669
Test - acc:         0.816815 loss:        0.773036
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.955328 loss:        0.132632
Test - acc:         0.809172 loss:        0.729270
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.955434 loss:        0.130978
Test - acc:         0.754140 loss:        0.899898
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.962615 loss:        0.113548
Test - acc:         0.791338 loss:        0.822389
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.954166 loss:        0.138134
Test - acc:         0.831338 loss:        0.712627
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.960080 loss:        0.123949
Test - acc:         0.825478 loss:        0.690319
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.954166 loss:        0.137657
Test - acc:         0.722038 loss:        1.324017
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.957123 loss:        0.138064
Test - acc:         0.824713 loss:        0.668989
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.963037 loss:        0.117589
Test - acc:         0.803567 loss:        0.781703
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.962192 loss:        0.113080
Test - acc:         0.808917 loss:        0.859791
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.953744 loss:        0.143763
Test - acc:         0.751847 loss:        1.086715
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.950681 loss:        0.144431
Test - acc:         0.810701 loss:        0.795928
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.951209 loss:        0.139587
Test - acc:         0.757707 loss:        1.014696
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.947513 loss:        0.154571
Test - acc:         0.736561 loss:        1.227634
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.965994 loss:        0.106303
Test - acc:         0.837707 loss:        0.658045
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.964410 loss:        0.106733
Test - acc:         0.800000 loss:        0.909249
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.947196 loss:        0.157354
Test - acc:         0.807643 loss:        0.827914
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.982152 loss:        0.065617
Test - acc:         0.895541 loss:        0.383967
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.994508 loss:        0.030718
Test - acc:         0.900127 loss:        0.387824
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.997782 loss:        0.021815
Test - acc:         0.901401 loss:        0.380330
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.998310 loss:        0.017355
Test - acc:         0.902166 loss:        0.374057
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.998416 loss:        0.015264
Test - acc:         0.902930 loss:        0.372769
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.999155 loss:        0.012216
Test - acc:         0.902166 loss:        0.373785
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.998944 loss:        0.013256
Test - acc:         0.900637 loss:        0.378264
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.010815
Test - acc:         0.903694 loss:        0.380628
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.010686
Test - acc:         0.901146 loss:        0.382079
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.008737
Test - acc:         0.902420 loss:        0.383631
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.998099 loss:        0.017372
Test - acc:         0.896815 loss:        0.383676
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.998944 loss:        0.013528
Test - acc:         0.893503 loss:        0.388501
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.998838 loss:        0.011866
Test - acc:         0.896051 loss:        0.386181
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.011250
Test - acc:         0.897325 loss:        0.385374
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.010140
Test - acc:         0.896815 loss:        0.387632
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.010548
Test - acc:         0.897834 loss:        0.384730
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.009563
Test - acc:         0.898854 loss:        0.389732
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.008674
Test - acc:         0.900127 loss:        0.385128
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.008420
Test - acc:         0.897834 loss:        0.394525
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.007752
Test - acc:         0.898854 loss:        0.394046
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.007964
Test - acc:         0.898344 loss:        0.392801
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.007211
Test - acc:         0.900382 loss:        0.391753
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.007218
Test - acc:         0.900382 loss:        0.386848
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.006852
Test - acc:         0.900127 loss:        0.389410
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.006348
Test - acc:         0.898854 loss:        0.389788
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005810
Test - acc:         0.901401 loss:        0.390514
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005828
Test - acc:         0.899618 loss:        0.391196
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.007119
Test - acc:         0.898854 loss:        0.393192
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.006361
Test - acc:         0.900127 loss:        0.388348
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.005599
Test - acc:         0.899618 loss:        0.383744
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.005034
Test - acc:         0.898854 loss:        0.386494
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.005197
Test - acc:         0.898599 loss:        0.388465
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.005399
Test - acc:         0.898854 loss:        0.390459
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005855
Test - acc:         0.899618 loss:        0.390373
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.005748
Test - acc:         0.899363 loss:        0.402354
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005289
Test - acc:         0.900127 loss:        0.395119
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005172
Test - acc:         0.901146 loss:        0.388112
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004600
Test - acc:         0.898344 loss:        0.388168
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004528
Test - acc:         0.901146 loss:        0.385592
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004894
Test - acc:         0.900127 loss:        0.382873
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004875
Test - acc:         0.899873 loss:        0.388545
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004294
Test - acc:         0.901146 loss:        0.388419
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.982152 loss:        0.068201
Test - acc:         0.889172 loss:        0.426091
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.994297 loss:        0.034568
Test - acc:         0.892994 loss:        0.403836
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.996937 loss:        0.025686
Test - acc:         0.893248 loss:        0.414168
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.995776 loss:        0.024371
Test - acc:         0.895796 loss:        0.403506
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.998099 loss:        0.019299
Test - acc:         0.897325 loss:        0.410303
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.997782 loss:        0.017453
Test - acc:         0.898344 loss:        0.404875
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.997993 loss:        0.015500
Test - acc:         0.896306 loss:        0.407552
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.999050 loss:        0.012917
Test - acc:         0.894268 loss:        0.401588
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.998627 loss:        0.012847
Test - acc:         0.893248 loss:        0.406600
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.999050 loss:        0.011614
Test - acc:         0.897834 loss:        0.402228
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.998521 loss:        0.012303
Test - acc:         0.898599 loss:        0.410309
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.998521 loss:        0.011778
Test - acc:         0.900637 loss:        0.402732
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.010548
Test - acc:         0.896561 loss:        0.415335
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.008936
Test - acc:         0.895287 loss:        0.421218
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.999050 loss:        0.010120
Test - acc:         0.895796 loss:        0.410453
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.008325
Test - acc:         0.892739 loss:        0.432931
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.999155 loss:        0.008525
Test - acc:         0.895541 loss:        0.420561
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.998733 loss:        0.009883
Test - acc:         0.892739 loss:        0.427740
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.007921
Test - acc:         0.896561 loss:        0.420230
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.007201
Test - acc:         0.897834 loss:        0.418478
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.007484
Test - acc:         0.897325 loss:        0.409607
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.999155 loss:        0.007757
Test - acc:         0.895796 loss:        0.423656
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.007592
Test - acc:         0.897325 loss:        0.416714
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.999050 loss:        0.007948
Test - acc:         0.896051 loss:        0.421213
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.007000
Test - acc:         0.896306 loss:        0.423035
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.006960
Test - acc:         0.896815 loss:        0.422146
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.006870
Test - acc:         0.897070 loss:        0.419289
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.007416
Test - acc:         0.894522 loss:        0.426666
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.007080
Test - acc:         0.898344 loss:        0.421542
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.006709
Test - acc:         0.897070 loss:        0.413747
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.006028
Test - acc:         0.897070 loss:        0.423884
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.006336
Test - acc:         0.897325 loss:        0.421308
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.881931 loss:        0.363261
Test - acc:         0.829299 loss:        0.563284
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.946457 loss:        0.179108
Test - acc:         0.858599 loss:        0.475315
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.956173 loss:        0.139905
Test - acc:         0.845350 loss:        0.522806
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.965994 loss:        0.117427
Test - acc:         0.855287 loss:        0.502710
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.966206 loss:        0.108885
Test - acc:         0.873376 loss:        0.449828
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.973809 loss:        0.092393
Test - acc:         0.864713 loss:        0.495352
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.973915 loss:        0.088787
Test - acc:         0.873376 loss:        0.463207
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.974971 loss:        0.084747
Test - acc:         0.863694 loss:        0.496953
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.980251 loss:        0.073985
Test - acc:         0.861911 loss:        0.514029
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.983420 loss:        0.066816
Test - acc:         0.869809 loss:        0.489106
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.981307 loss:        0.064739
Test - acc:         0.871847 loss:        0.493029
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.980146 loss:        0.068815
Test - acc:         0.873121 loss:        0.492268
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.981624 loss:        0.064490
Test - acc:         0.855032 loss:        0.559270
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.982258 loss:        0.060777
Test - acc:         0.867771 loss:        0.501285
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.983525 loss:        0.060386
Test - acc:         0.867516 loss:        0.529933
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.982680 loss:        0.058339
Test - acc:         0.865732 loss:        0.541268
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.983842 loss:        0.055371
Test - acc:         0.873376 loss:        0.506603
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.985637 loss:        0.048634
Test - acc:         0.865223 loss:        0.525117
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.988278 loss:        0.043206
Test - acc:         0.878217 loss:        0.496777
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.991763 loss:        0.037256
Test - acc:         0.883567 loss:        0.480703
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.989650 loss:        0.042861
Test - acc:         0.868280 loss:        0.538548
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.983948 loss:        0.054727
Test - acc:         0.859873 loss:        0.537970
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.984159 loss:        0.049715
Test - acc:         0.862675 loss:        0.565774
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.989545 loss:        0.043483
Test - acc:         0.873631 loss:        0.509530
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.987749 loss:        0.043116
Test - acc:         0.873631 loss:        0.510000
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.986588 loss:        0.046849
Test - acc:         0.873121 loss:        0.546021
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.991446 loss:        0.036340
Test - acc:         0.886369 loss:        0.474669
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.994720 loss:        0.027551
Test - acc:         0.886369 loss:        0.460079
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.995142 loss:        0.021747
Test - acc:         0.885350 loss:        0.459448
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.996621 loss:        0.018327
Test - acc:         0.886879 loss:        0.459950
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.997465 loss:        0.019521
Test - acc:         0.888917 loss:        0.453629
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.997571 loss:        0.016981
Test - acc:         0.888917 loss:        0.454767
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.782554 loss:        0.680100
Test - acc:         0.810446 loss:        0.613251
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.871581 loss:        0.389384
Test - acc:         0.828535 loss:        0.545336
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.897877 loss:        0.323713
Test - acc:         0.837962 loss:        0.519062
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.909494 loss:        0.289672
Test - acc:         0.846115 loss:        0.486165
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.913296 loss:        0.266095
Test - acc:         0.852229 loss:        0.483530
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.922167 loss:        0.244196
Test - acc:         0.849936 loss:        0.480962
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.925335 loss:        0.234668
Test - acc:         0.857580 loss:        0.459917
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.933150 loss:        0.221217
Test - acc:         0.860382 loss:        0.456408
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.937269 loss:        0.207283
Test - acc:         0.861401 loss:        0.457469
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.939487 loss:        0.196785
Test - acc:         0.863949 loss:        0.448985
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.944345 loss:        0.186745
Test - acc:         0.864204 loss:        0.454451
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.946668 loss:        0.182122
Test - acc:         0.863694 loss:        0.449035
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.945084 loss:        0.178241
Test - acc:         0.864713 loss:        0.449581
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.949519 loss:        0.170707
Test - acc:         0.868535 loss:        0.448948
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.951632 loss:        0.160971
Test - acc:         0.862166 loss:        0.450392
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.952688 loss:        0.162829
Test - acc:         0.861656 loss:        0.455960
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.951104 loss:        0.160778
Test - acc:         0.863949 loss:        0.453731
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.957229 loss:        0.150161
Test - acc:         0.862166 loss:        0.454142
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.955011 loss:        0.147842
Test - acc:         0.865478 loss:        0.446099
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.957440 loss:        0.144014
Test - acc:         0.867006 loss:        0.448111
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.958074 loss:        0.145698
Test - acc:         0.869045 loss:        0.451231
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.962720 loss:        0.139416
Test - acc:         0.865732 loss:        0.448655
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.961348 loss:        0.134119
Test - acc:         0.867006 loss:        0.447676
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.960291 loss:        0.134828
Test - acc:         0.866242 loss:        0.455942
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.963460 loss:        0.132099
Test - acc:         0.868025 loss:        0.443899
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.962192 loss:        0.130602
Test - acc:         0.867261 loss:        0.454288
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.963565 loss:        0.122993
Test - acc:         0.863949 loss:        0.457748
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.967367 loss:        0.125444
Test - acc:         0.868025 loss:        0.446060
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.965255 loss:        0.121828
Test - acc:         0.867516 loss:        0.446074
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.962932 loss:        0.121952
Test - acc:         0.865223 loss:        0.460111
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.965149 loss:        0.120615
Test - acc:         0.864459 loss:        0.457078
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.964093 loss:        0.122998
Test - acc:         0.868790 loss:        0.449149
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.574717 loss:        1.331052
Test - acc:         0.706752 loss:        0.961913
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.724575 loss:        0.886590
Test - acc:         0.741911 loss:        0.828561
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.753934 loss:        0.792026
Test - acc:         0.768408 loss:        0.754779
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.773788 loss:        0.735676
Test - acc:         0.777580 loss:        0.715929
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.785088 loss:        0.700085
Test - acc:         0.781911 loss:        0.695010
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.794170 loss:        0.660979
Test - acc:         0.781911 loss:        0.680181
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.802619 loss:        0.644115
Test - acc:         0.792357 loss:        0.661709
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.810328 loss:        0.618562
Test - acc:         0.794650 loss:        0.651707
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.817299 loss:        0.611082
Test - acc:         0.800255 loss:        0.643367
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.818038 loss:        0.590219
Test - acc:         0.799745 loss:        0.625423
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.820467 loss:        0.584363
Test - acc:         0.801783 loss:        0.622142
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.820150 loss:        0.569576
Test - acc:         0.807643 loss:        0.603914
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.830711 loss:        0.558771
Test - acc:         0.801783 loss:        0.606766
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.827859 loss:        0.543536
Test - acc:         0.807134 loss:        0.596780
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.834513 loss:        0.531710
Test - acc:         0.813248 loss:        0.592361
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.840638 loss:        0.520611
Test - acc:         0.815796 loss:        0.585185
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.840110 loss:        0.517222
Test - acc:         0.812739 loss:        0.595633
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.841483 loss:        0.511974
Test - acc:         0.817070 loss:        0.575496
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.846129 loss:        0.504482
Test - acc:         0.815796 loss:        0.579878
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.844651 loss:        0.495354
Test - acc:         0.819108 loss:        0.563749
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.846658 loss:        0.485855
Test - acc:         0.819363 loss:        0.567610
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.849298 loss:        0.486474
Test - acc:         0.819873 loss:        0.557712
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.853100 loss:        0.476985
Test - acc:         0.821146 loss:        0.555541
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.853628 loss:        0.481114
Test - acc:         0.820637 loss:        0.548296
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.857746 loss:        0.468016
Test - acc:         0.825478 loss:        0.543730
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.853733 loss:        0.462051
Test - acc:         0.822930 loss:        0.542292
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.857218 loss:        0.456578
Test - acc:         0.822930 loss:        0.540234
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.858063 loss:        0.457150
Test - acc:         0.822930 loss:        0.557020
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.858908 loss:        0.456679
Test - acc:         0.830573 loss:        0.540239
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.859647 loss:        0.446352
Test - acc:         0.823185 loss:        0.542535
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.861971 loss:        0.443451
Test - acc:         0.829809 loss:        0.529413
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.862393 loss:        0.443690
Test - acc:         0.830828 loss:        0.546543
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.427817 loss:        1.693905
Test - acc:         0.550318 loss:        1.421350
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.586546 loss:        1.372904
Test - acc:         0.612229 loss:        1.286481
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.619918 loss:        1.280137
Test - acc:         0.650701 loss:        1.186616
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.642412 loss:        1.206271
Test - acc:         0.638217 loss:        1.178916
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.663534 loss:        1.152849
Test - acc:         0.504459 loss:        1.419586
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.668180 loss:        1.119711
Test - acc:         0.669554 loss:        1.079044
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.684127 loss:        1.081092
Test - acc:         0.205350 loss:        2.992529
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.686451 loss:        1.055031
Test - acc:         0.333758 loss:        2.024850
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.693209 loss:        1.039897
Test - acc:         0.164841 loss:        3.594496
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.700285 loss:        1.017003
Test - acc:         0.696815 loss:        0.998117
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.711163 loss:        0.994399
Test - acc:         0.707006 loss:        0.968548
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.711268 loss:        0.974422
Test - acc:         0.707006 loss:        0.974047
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.715915 loss:        0.961385
Test - acc:         0.657325 loss:        1.062469
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.716338 loss:        0.944858
Test - acc:         0.718726 loss:        0.938325
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.722040 loss:        0.932746
Test - acc:         0.607898 loss:        1.163323
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.727954 loss:        0.921068
Test - acc:         0.728917 loss:        0.902276
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.732601 loss:        0.906121
Test - acc:         0.732484 loss:        0.891397
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.733657 loss:        0.894634
Test - acc:         0.735287 loss:        0.880276
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.737248 loss:        0.879009
Test - acc:         0.735287 loss:        0.883606
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.741683 loss:        0.871403
Test - acc:         0.678217 loss:        1.003240
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.738515 loss:        0.870066
Test - acc:         0.730701 loss:        0.866783
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.741789 loss:        0.863533
Test - acc:         0.734522 loss:        0.868597
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.746964 loss:        0.845610
Test - acc:         0.740127 loss:        0.850525
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.745485 loss:        0.841516
Test - acc:         0.745223 loss:        0.839964
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.751294 loss:        0.832422
Test - acc:         0.720764 loss:        0.878355
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.756574 loss:        0.828251
Test - acc:         0.732484 loss:        0.853507
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.752983 loss:        0.826256
Test - acc:         0.746752 loss:        0.820534
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.755096 loss:        0.813538
Test - acc:         0.721783 loss:        0.879033
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.757102 loss:        0.812042
Test - acc:         0.759490 loss:        0.796851
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.760798 loss:        0.806211
Test - acc:         0.757452 loss:        0.808538
Sparsity :          0.9990
Wdecay :        0.000500
