Running --model densenet121 --dataset imagenette --seed 43 --logdir=criterion_experiment_no_bias/densenet121 --prune_criterion weight_squared_div_flips --prune_freq 39 --prune_rate 0.5 --noise --comment=densenet121_crit=weight_squared_div_flips_pf=39_seed=43 --save_model=pre-finetune/densenet121_weight_squared_div_flips_pf39_s43
******************************
Running
{
    "model": "densenet121",
    "dataset": "imagenette",
    "batch_size": 128,
    "test_batch_size": 500,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/densenet121",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/densenet121_weight_squared_div_flips_pf39_s43",
    "load_model": null,
    "parallel": false
}
******************************
Total prunable params of model: 6880448
Model has 6964106 total params.
num_weights=6922272
num_biases=41834
num.prunable=6880448
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.349773 loss:        2.060545
Test - acc:         0.464204 loss:        2.213250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.485585 loss:        1.615811
Test - acc:         0.464968 loss:        1.583937
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.557820 loss:        1.377145
Test - acc:         0.379873 loss:        5.230523
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.610730 loss:        1.211797
Test - acc:         0.612229 loss:        1.312108
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.673250 loss:        1.018220
Test - acc:         0.558981 loss:        3.248274
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.690569 loss:        0.968898
Test - acc:         0.641274 loss:        1.140370
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.724469 loss:        0.876802
Test - acc:         0.650955 loss:        1.158730
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.733552 loss:        0.826554
Test - acc:         0.597707 loss:        1.548129
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.762488 loss:        0.736475
Test - acc:         0.662930 loss:        1.179407
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.766396 loss:        0.710909
Test - acc:         0.670828 loss:        1.127672
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.789312 loss:        0.645313
Test - acc:         0.650446 loss:        1.270193
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.786778 loss:        0.655107
Test - acc:         0.701911 loss:        1.013121
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.809484 loss:        0.574430
Test - acc:         0.724841 loss:        0.893706
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.806843 loss:        0.586806
Test - acc:         0.715159 loss:        0.970227
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.826803 loss:        0.524665
Test - acc:         0.761019 loss:        0.762350
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.832612 loss:        0.507240
Test - acc:         0.746242 loss:        0.865811
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.837892 loss:        0.492861
Test - acc:         0.761019 loss:        0.791856
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.842222 loss:        0.474012
Test - acc:         0.767643 loss:        0.727084
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.854578 loss:        0.427188
Test - acc:         0.778599 loss:        0.729459
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.863977 loss:        0.413003
Test - acc:         0.802293 loss:        0.683786
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.869891 loss:        0.390734
Test - acc:         0.741146 loss:        0.951417
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.862182 loss:        0.419845
Test - acc:         0.756688 loss:        0.830134
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.872003 loss:        0.386279
Test - acc:         0.738854 loss:        0.971353
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.888373 loss:        0.338286
Test - acc:         0.753121 loss:        0.890835
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.881191 loss:        0.359579
Test - acc:         0.761783 loss:        0.870714
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.889006 loss:        0.331484
Test - acc:         0.766624 loss:        0.816003
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.891013 loss:        0.327791
Test - acc:         0.711338 loss:        1.099947
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.885416 loss:        0.344792
Test - acc:         0.723822 loss:        0.942273
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.886049 loss:        0.344339
Test - acc:         0.731975 loss:        0.986230
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.896293 loss:        0.309933
Test - acc:         0.806879 loss:        0.669766
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.902735 loss:        0.291633
Test - acc:         0.799745 loss:        0.692862
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.891330 loss:        0.325026
Test - acc:         0.783185 loss:        0.776448
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.905692 loss:        0.280567
Test - acc:         0.804586 loss:        0.737609
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.911184 loss:        0.267477
Test - acc:         0.792102 loss:        0.764655
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.907171 loss:        0.278837
Test - acc:         0.779108 loss:        0.831724
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.902841 loss:        0.286806
Test - acc:         0.770955 loss:        0.899808
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.911923 loss:        0.264657
Test - acc:         0.775541 loss:        0.868617
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.906326 loss:        0.283050
Test - acc:         0.792866 loss:        0.683969
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.911184 loss:        0.266550
Test - acc:         0.776561 loss:        0.815401
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.942549 loss:        0.180817
Test - acc:         0.836943 loss:        0.584518
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.946140 loss:        0.171189
Test - acc:         0.800255 loss:        0.749574
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.941493 loss:        0.182151
Test - acc:         0.827516 loss:        0.642741
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.938959 loss:        0.183323
Test - acc:         0.790318 loss:        0.781031
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.932517 loss:        0.199438
Test - acc:         0.822420 loss:        0.637979
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.938325 loss:        0.190441
Test - acc:         0.719236 loss:        1.090242
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.923540 loss:        0.228974
Test - acc:         0.758217 loss:        1.023200
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.936847 loss:        0.185239
Test - acc:         0.801274 loss:        0.731979
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.927236 loss:        0.217631
Test - acc:         0.747261 loss:        0.961059
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.911395 loss:        0.273714
Test - acc:         0.351847 loss:        4.567665
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.893759 loss:        0.315366
Test - acc:         0.804331 loss:        0.666052
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.934523 loss:        0.198550
Test - acc:         0.812994 loss:        0.673017
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.929982 loss:        0.212988
Test - acc:         0.765096 loss:        0.873979
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.928715 loss:        0.211854
Test - acc:         0.805350 loss:        0.690819
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.926814 loss:        0.218308
Test - acc:         0.840000 loss:        0.572149
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.932411 loss:        0.197708
Test - acc:         0.766879 loss:        0.880041
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.917732 loss:        0.250740
Test - acc:         0.775541 loss:        0.828651
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.926497 loss:        0.213581
Test - acc:         0.738599 loss:        0.983507
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.928926 loss:        0.206012
Test - acc:         0.767134 loss:        0.930193
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.930933 loss:        0.203106
Test - acc:         0.783949 loss:        0.965369
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.940015 loss:        0.180670
Test - acc:         0.754650 loss:        1.026467
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.925230 loss:        0.230133
Test - acc:         0.774268 loss:        0.912662
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.926814 loss:        0.220175
Test - acc:         0.787771 loss:        0.833184
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.939381 loss:        0.180852
Test - acc:         0.777834 loss:        0.863480
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.926075 loss:        0.215623
Test - acc:         0.819618 loss:        0.650246
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.940015 loss:        0.177256
Test - acc:         0.803567 loss:        0.771753
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.936847 loss:        0.191542
Test - acc:         0.807898 loss:        0.832505
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.917943 loss:        0.239977
Test - acc:         0.636178 loss:        1.714711
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.925441 loss:        0.225253
Test - acc:         0.767389 loss:        0.850280
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.937586 loss:        0.192273
Test - acc:         0.783439 loss:        0.843079
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.926497 loss:        0.217591
Test - acc:         0.767389 loss:        0.968721
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.924807 loss:        0.221575
Test - acc:         0.716688 loss:        1.171230
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.942127 loss:        0.177640
Test - acc:         0.801274 loss:        0.777167
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.934734 loss:        0.198501
Test - acc:         0.778854 loss:        0.887608
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.934206 loss:        0.193168
Test - acc:         0.766369 loss:        0.950002
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.935051 loss:        0.202819
Test - acc:         0.805350 loss:        0.728423
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.942549 loss:        0.171267
Test - acc:         0.823439 loss:        0.667285
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.939487 loss:        0.187176
Test - acc:         0.763312 loss:        0.931878
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.929243 loss:        0.214140
Test - acc:         0.787516 loss:        0.851534
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.962404 loss:        0.114191
Test - acc:         0.828535 loss:        0.670432
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.972120 loss:        0.086945
Test - acc:         0.805860 loss:        0.804779
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.951526 loss:        0.146305
Test - acc:         0.805350 loss:        0.812924
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.946351 loss:        0.154453
Test - acc:         0.761019 loss:        1.016363
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.953744 loss:        0.142588
Test - acc:         0.811975 loss:        0.698641
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.946034 loss:        0.158101
Test - acc:         0.772484 loss:        0.965696
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.954061 loss:        0.142181
Test - acc:         0.819618 loss:        0.778904
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.940965 loss:        0.176998
Test - acc:         0.799236 loss:        0.816833
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.954166 loss:        0.143430
Test - acc:         0.798981 loss:        0.867832
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.939381 loss:        0.177059
Test - acc:         0.794904 loss:        0.806674
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.947935 loss:        0.154113
Test - acc:         0.806115 loss:        0.774482
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.944662 loss:        0.160383
Test - acc:         0.793376 loss:        0.856662
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.952160 loss:        0.141691
Test - acc:         0.800510 loss:        0.836665
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.948041 loss:        0.152055
Test - acc:         0.843822 loss:        0.616976
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.947724 loss:        0.150176
Test - acc:         0.762803 loss:        0.996355
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.935262 loss:        0.187205
Test - acc:         0.792611 loss:        0.822107
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.944556 loss:        0.163827
Test - acc:         0.797707 loss:        0.790605
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.955011 loss:        0.133618
Test - acc:         0.810446 loss:        0.742833
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.953110 loss:        0.144692
Test - acc:         0.764586 loss:        1.073330
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.943289 loss:        0.166267
Test - acc:         0.774522 loss:        0.897787
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.948252 loss:        0.163462
Test - acc:         0.745478 loss:        1.000833
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.943817 loss:        0.175810
Test - acc:         0.766369 loss:        1.021204
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.946774 loss:        0.159616
Test - acc:         0.788535 loss:        0.804326
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.955539 loss:        0.136424
Test - acc:         0.786242 loss:        0.910040
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.949519 loss:        0.152967
Test - acc:         0.783694 loss:        0.936315
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.949836 loss:        0.151648
Test - acc:         0.809427 loss:        0.736247
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.950892 loss:        0.152637
Test - acc:         0.824459 loss:        0.725693
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.954800 loss:        0.133265
Test - acc:         0.686879 loss:        1.529390
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.945084 loss:        0.163006
Test - acc:         0.824459 loss:        0.688189
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.951843 loss:        0.144622
Test - acc:         0.676943 loss:        1.515838
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.951315 loss:        0.142131
Test - acc:         0.834650 loss:        0.656180
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.939698 loss:        0.176803
Test - acc:         0.801274 loss:        0.751628
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.947513 loss:        0.159474
Test - acc:         0.789809 loss:        0.837214
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.951420 loss:        0.143615
Test - acc:         0.807134 loss:        0.799283
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.942127 loss:        0.172517
Test - acc:         0.765605 loss:        1.023197
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.948463 loss:        0.151153
Test - acc:         0.809682 loss:        0.749634
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.947091 loss:        0.155528
Test - acc:         0.827771 loss:        0.694302
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.936213 loss:        0.182011
Test - acc:         0.822675 loss:        0.679959
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.954589 loss:        0.137367
Test - acc:         0.814777 loss:        0.738650
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.970641 loss:        0.094301
Test - acc:         0.801783 loss:        0.762678
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.971063 loss:        0.091933
Test - acc:         0.796943 loss:        0.872278
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.965361 loss:        0.105518
Test - acc:         0.791847 loss:        0.887529
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.960397 loss:        0.117997
Test - acc:         0.793885 loss:        0.845915
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.962298 loss:        0.111130
Test - acc:         0.840255 loss:        0.623655
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.959447 loss:        0.124297
Test - acc:         0.802548 loss:        0.778205
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.951104 loss:        0.141024
Test - acc:         0.777580 loss:        0.941970
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.957334 loss:        0.131939
Test - acc:         0.789299 loss:        0.905134
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.959447 loss:        0.124936
Test - acc:         0.788025 loss:        0.902768
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.961559 loss:        0.112645
Test - acc:         0.812994 loss:        0.756587
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.956912 loss:        0.126758
Test - acc:         0.202293 loss:        9.223161
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.936530 loss:        0.182150
Test - acc:         0.797707 loss:        0.814935
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.954905 loss:        0.138260
Test - acc:         0.835159 loss:        0.606586
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.956806 loss:        0.131556
Test - acc:         0.780637 loss:        0.895285
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.965044 loss:        0.104771
Test - acc:         0.777834 loss:        0.922714
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.957440 loss:        0.134586
Test - acc:         0.792866 loss:        0.870755
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.959975 loss:        0.129435
Test - acc:         0.825223 loss:        0.658041
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.964516 loss:        0.111873
Test - acc:         0.801274 loss:        0.794861
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.951104 loss:        0.144405
Test - acc:         0.795414 loss:        0.849933
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.944873 loss:        0.161094
Test - acc:         0.686879 loss:        1.443834
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.961348 loss:        0.115560
Test - acc:         0.775032 loss:        0.992200
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.964410 loss:        0.109444
Test - acc:         0.828025 loss:        0.697458
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.959552 loss:        0.127412
Test - acc:         0.820382 loss:        0.668791
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.946774 loss:        0.163732
Test - acc:         0.758217 loss:        0.969590
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.953533 loss:        0.134537
Test - acc:         0.342420 loss:        5.705038
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.952582 loss:        0.137187
Test - acc:         0.830318 loss:        0.640972
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.955856 loss:        0.128963
Test - acc:         0.802038 loss:        0.820439
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.952582 loss:        0.141953
Test - acc:         0.815796 loss:        0.810558
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.953110 loss:        0.140593
Test - acc:         0.801019 loss:        0.813602
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.961876 loss:        0.117973
Test - acc:         0.830828 loss:        0.735305
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.956384 loss:        0.131285
Test - acc:         0.809936 loss:        0.728311
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.952793 loss:        0.142746
Test - acc:         0.784968 loss:        0.852424
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.958285 loss:        0.131770
Test - acc:         0.803822 loss:        0.741382
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.983420 loss:        0.058579
Test - acc:         0.891720 loss:        0.387519
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.995881 loss:        0.026144
Test - acc:         0.897070 loss:        0.381649
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.997465 loss:        0.019203
Test - acc:         0.895796 loss:        0.382257
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.998733 loss:        0.015616
Test - acc:         0.897070 loss:        0.379283
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.998733 loss:        0.013241
Test - acc:         0.898344 loss:        0.382962
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.999050 loss:        0.011321
Test - acc:         0.897580 loss:        0.387231
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.998733 loss:        0.013787
Test - acc:         0.898854 loss:        0.385593
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.998627 loss:        0.012394
Test - acc:         0.898599 loss:        0.384551
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.010147
Test - acc:         0.897580 loss:        0.386368
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.009843
Test - acc:         0.900382 loss:        0.391023
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.008623
Test - acc:         0.897834 loss:        0.389517
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.008628
Test - acc:         0.897325 loss:        0.395803
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.008487
Test - acc:         0.898089 loss:        0.400413
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.007879
Test - acc:         0.900127 loss:        0.391296
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.999155 loss:        0.008015
Test - acc:         0.897580 loss:        0.397920
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.007760
Test - acc:         0.899618 loss:        0.397128
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.006887
Test - acc:         0.899363 loss:        0.389293
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.006249
Test - acc:         0.896561 loss:        0.391543
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.007208
Test - acc:         0.898089 loss:        0.397137
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.007178
Test - acc:         0.898599 loss:        0.392726
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005938
Test - acc:         0.897070 loss:        0.394938
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005647
Test - acc:         0.898089 loss:        0.396030
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005650
Test - acc:         0.897325 loss:        0.401591
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005428
Test - acc:         0.894522 loss:        0.400463
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005443
Test - acc:         0.896306 loss:        0.402778
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.005162
Test - acc:         0.897580 loss:        0.398739
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.005263
Test - acc:         0.897070 loss:        0.398708
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.005117
Test - acc:         0.898344 loss:        0.392888
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004545
Test - acc:         0.897834 loss:        0.397732
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004834
Test - acc:         0.898854 loss:        0.392893
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.004912
Test - acc:         0.900382 loss:        0.387777
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004825
Test - acc:         0.897580 loss:        0.386068
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004838
Test - acc:         0.898089 loss:        0.389429
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004530
Test - acc:         0.896051 loss:        0.392448
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004576
Test - acc:         0.897325 loss:        0.392905
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004258
Test - acc:         0.899108 loss:        0.390077
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004418
Test - acc:         0.897834 loss:        0.389030
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004430
Test - acc:         0.897325 loss:        0.393011
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004575
Test - acc:         0.897070 loss:        0.395735
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004953
Test - acc:         0.897580 loss:        0.394127
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004347
Test - acc:         0.897070 loss:        0.396000
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003885
Test - acc:         0.895796 loss:        0.400478
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004322
Test - acc:         0.898599 loss:        0.390888
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004222
Test - acc:         0.897834 loss:        0.385409
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004278
Test - acc:         0.895796 loss:        0.396188
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.996304 loss:        0.023602
Test - acc:         0.892229 loss:        0.418385
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.998733 loss:        0.014091
Test - acc:         0.895032 loss:        0.403156
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.999050 loss:        0.011596
Test - acc:         0.897834 loss:        0.399268
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.010596
Test - acc:         0.895287 loss:        0.402070
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.009981
Test - acc:         0.896815 loss:        0.401733
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.998838 loss:        0.009476
Test - acc:         0.896561 loss:        0.402205
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.008719
Test - acc:         0.896561 loss:        0.408024
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.007185
Test - acc:         0.896306 loss:        0.407493
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.007781
Test - acc:         0.895032 loss:        0.407896
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.006455
Test - acc:         0.897070 loss:        0.411603
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.006011
Test - acc:         0.895541 loss:        0.407798
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.006386
Test - acc:         0.898089 loss:        0.400306
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.006287
Test - acc:         0.896051 loss:        0.406693
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.005725
Test - acc:         0.894777 loss:        0.409639
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.006540
Test - acc:         0.897834 loss:        0.408442
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.005933
Test - acc:         0.899108 loss:        0.404825
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005213
Test - acc:         0.898089 loss:        0.402683
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.999366 loss:        0.006231
Test - acc:         0.896815 loss:        0.408832
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004939
Test - acc:         0.896815 loss:        0.415714
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.005546
Test - acc:         0.899108 loss:        0.396664
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005061
Test - acc:         0.896815 loss:        0.404859
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004612
Test - acc:         0.899618 loss:        0.402763
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004416
Test - acc:         0.897580 loss:        0.404523
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004129
Test - acc:         0.898089 loss:        0.410075
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.004697
Test - acc:         0.899363 loss:        0.401440
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004910
Test - acc:         0.898089 loss:        0.410390
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004473
Test - acc:         0.900382 loss:        0.413932
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.004517
Test - acc:         0.899363 loss:        0.405140
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004035
Test - acc:         0.898854 loss:        0.402730
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004358
Test - acc:         0.899108 loss:        0.408443
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.004356
Test - acc:         0.901146 loss:        0.403154
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004620
Test - acc:         0.900892 loss:        0.399531
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003984
Test - acc:         0.898854 loss:        0.405079
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003873
Test - acc:         0.899618 loss:        0.402532
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003882
Test - acc:         0.898854 loss:        0.407250
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004273
Test - acc:         0.899108 loss:        0.395680
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003633
Test - acc:         0.900892 loss:        0.402041
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003948
Test - acc:         0.897070 loss:        0.406291
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003837
Test - acc:         0.898089 loss:        0.403658
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.962404 loss:        0.126752
Test - acc:         0.857325 loss:        0.526535
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.983842 loss:        0.061410
Test - acc:         0.881274 loss:        0.441993
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.992185 loss:        0.037012
Test - acc:         0.888917 loss:        0.417975
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.992396 loss:        0.033543
Test - acc:         0.887389 loss:        0.434459
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.995459 loss:        0.025347
Test - acc:         0.885096 loss:        0.453078
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.994297 loss:        0.028405
Test - acc:         0.890955 loss:        0.435051
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.994931 loss:        0.024742
Test - acc:         0.881019 loss:        0.479595
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.995459 loss:        0.021673
Test - acc:         0.883312 loss:        0.470543
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.997677 loss:        0.017116
Test - acc:         0.891210 loss:        0.426607
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.997254 loss:        0.015862
Test - acc:         0.893248 loss:        0.432371
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.998310 loss:        0.015333
Test - acc:         0.889936 loss:        0.443325
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.997360 loss:        0.016685
Test - acc:         0.888408 loss:        0.467229
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.998310 loss:        0.014703
Test - acc:         0.883312 loss:        0.473360
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.999050 loss:        0.012139
Test - acc:         0.890701 loss:        0.447967
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.998944 loss:        0.011535
Test - acc:         0.882803 loss:        0.496294
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.997782 loss:        0.011745
Test - acc:         0.889172 loss:        0.457246
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.998205 loss:        0.011495
Test - acc:         0.895032 loss:        0.435447
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.999050 loss:        0.009114
Test - acc:         0.896306 loss:        0.429848
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.999261 loss:        0.008280
Test - acc:         0.893248 loss:        0.433069
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.999366 loss:        0.007690
Test - acc:         0.892484 loss:        0.434850
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.999155 loss:        0.008276
Test - acc:         0.895032 loss:        0.431347
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.999261 loss:        0.007390
Test - acc:         0.893503 loss:        0.432180
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.999683 loss:        0.006297
Test - acc:         0.896306 loss:        0.426136
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.999683 loss:        0.007287
Test - acc:         0.895287 loss:        0.433796
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.999578 loss:        0.007768
Test - acc:         0.895541 loss:        0.433078
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.006036
Test - acc:         0.894522 loss:        0.433728
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.999366 loss:        0.006922
Test - acc:         0.896561 loss:        0.431888
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.999155 loss:        0.006451
Test - acc:         0.895541 loss:        0.429379
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.006923
Test - acc:         0.894013 loss:        0.432878
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.999366 loss:        0.006066
Test - acc:         0.895541 loss:        0.431155
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.999578 loss:        0.006236
Test - acc:         0.896051 loss:        0.432555
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.006359
Test - acc:         0.896561 loss:        0.432265
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.005836
Test - acc:         0.894268 loss:        0.437901
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999261 loss:        0.006830
Test - acc:         0.894268 loss:        0.440465
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999366 loss:        0.006305
Test - acc:         0.894777 loss:        0.436238
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.006051
Test - acc:         0.897325 loss:        0.430443
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999683 loss:        0.005760
Test - acc:         0.896815 loss:        0.437344
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.005223
Test - acc:         0.899363 loss:        0.429263
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.005803
Test - acc:         0.894013 loss:        0.436361
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.859014 loss:        0.454255
Test - acc:         0.838471 loss:        0.542759
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.929876 loss:        0.237476
Test - acc:         0.855541 loss:        0.495076
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.941493 loss:        0.198353
Test - acc:         0.862166 loss:        0.472350
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.953744 loss:        0.164668
Test - acc:         0.864204 loss:        0.455677
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.959869 loss:        0.148339
Test - acc:         0.865732 loss:        0.452398
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.966522 loss:        0.128518
Test - acc:         0.869809 loss:        0.447478
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.967473 loss:        0.120196
Test - acc:         0.872102 loss:        0.442945
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.971803 loss:        0.108039
Test - acc:         0.869809 loss:        0.456569
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.975077 loss:        0.103273
Test - acc:         0.872357 loss:        0.442366
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.976238 loss:        0.098649
Test - acc:         0.875414 loss:        0.432777
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.979723 loss:        0.089409
Test - acc:         0.876688 loss:        0.437379
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.981307 loss:        0.082380
Test - acc:         0.873885 loss:        0.439339
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.981096 loss:        0.082770
Test - acc:         0.874140 loss:        0.435335
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.983631 loss:        0.078212
Test - acc:         0.874904 loss:        0.443667
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.982258 loss:        0.073279
Test - acc:         0.877962 loss:        0.432011
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.983736 loss:        0.071098
Test - acc:         0.877707 loss:        0.439344
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.987961 loss:        0.063591
Test - acc:         0.875924 loss:        0.440660
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.985849 loss:        0.064245
Test - acc:         0.874904 loss:        0.446736
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.986693 loss:        0.061171
Test - acc:         0.874650 loss:        0.444311
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.987221 loss:        0.060546
Test - acc:         0.876943 loss:        0.439344
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.986693 loss:        0.060453
Test - acc:         0.875924 loss:        0.442838
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.988066 loss:        0.057467
Test - acc:         0.873631 loss:        0.440777
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.987855 loss:        0.056343
Test - acc:         0.875669 loss:        0.441794
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.989967 loss:        0.052052
Test - acc:         0.874650 loss:        0.450247
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.990284 loss:        0.051809
Test - acc:         0.876943 loss:        0.446073
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.989017 loss:        0.050819
Test - acc:         0.878217 loss:        0.448704
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.989862 loss:        0.051128
Test - acc:         0.872611 loss:        0.448563
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.992607 loss:        0.044488
Test - acc:         0.875669 loss:        0.451725
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.991235 loss:        0.047422
Test - acc:         0.875159 loss:        0.450864
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.991868 loss:        0.043603
Test - acc:         0.876688 loss:        0.450567
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.990178 loss:        0.045619
Test - acc:         0.877452 loss:        0.456809
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.990601 loss:        0.045044
Test - acc:         0.875669 loss:        0.462121
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.991868 loss:        0.042857
Test - acc:         0.880510 loss:        0.448910
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.991446 loss:        0.039656
Test - acc:         0.872866 loss:        0.462393
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.993347 loss:        0.039048
Test - acc:         0.876943 loss:        0.457281
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.993241 loss:        0.037694
Test - acc:         0.875414 loss:        0.456855
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.994297 loss:        0.037277
Test - acc:         0.876943 loss:        0.461928
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.993664 loss:        0.037394
Test - acc:         0.880255 loss:        0.456061
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.993241 loss:        0.038685
Test - acc:         0.877197 loss:        0.461427
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.715598 loss:        0.900727
Test - acc:         0.772994 loss:        0.717337
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.825642 loss:        0.563144
Test - acc:         0.804076 loss:        0.612070
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.854261 loss:        0.484999
Test - acc:         0.814013 loss:        0.574816
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.861337 loss:        0.447459
Test - acc:         0.822675 loss:        0.549500
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.874432 loss:        0.408981
Test - acc:         0.833631 loss:        0.532607
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.878551 loss:        0.385611
Test - acc:         0.831847 loss:        0.516162
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.885521 loss:        0.372112
Test - acc:         0.838981 loss:        0.515765
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.896293 loss:        0.347410
Test - acc:         0.834395 loss:        0.505636
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.897560 loss:        0.334637
Test - acc:         0.839745 loss:        0.493002
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.900834 loss:        0.326393
Test - acc:         0.850191 loss:        0.478319
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.901362 loss:        0.318050
Test - acc:         0.849936 loss:        0.484577
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.909177 loss:        0.293671
Test - acc:         0.846624 loss:        0.481545
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.911184 loss:        0.291459
Test - acc:         0.848153 loss:        0.474689
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.914035 loss:        0.285500
Test - acc:         0.850701 loss:        0.477444
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.918260 loss:        0.277981
Test - acc:         0.847898 loss:        0.481966
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.918365 loss:        0.266303
Test - acc:         0.850191 loss:        0.474478
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.923012 loss:        0.259287
Test - acc:         0.853758 loss:        0.471992
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.922801 loss:        0.257479
Test - acc:         0.853248 loss:        0.468824
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.924913 loss:        0.248570
Test - acc:         0.845350 loss:        0.487931
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.925652 loss:        0.248455
Test - acc:         0.853248 loss:        0.474522
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.931883 loss:        0.234390
Test - acc:         0.849427 loss:        0.470550
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.930827 loss:        0.241363
Test - acc:         0.848917 loss:        0.467960
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.935368 loss:        0.225136
Test - acc:         0.853758 loss:        0.461125
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.938219 loss:        0.215607
Test - acc:         0.855032 loss:        0.463548
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.934629 loss:        0.216145
Test - acc:         0.855541 loss:        0.470808
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.939276 loss:        0.208709
Test - acc:         0.856815 loss:        0.462851
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.935474 loss:        0.217951
Test - acc:         0.855287 loss:        0.465247
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.940226 loss:        0.206520
Test - acc:         0.853758 loss:        0.465897
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.941071 loss:        0.201450
Test - acc:         0.856815 loss:        0.467613
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.938642 loss:        0.200631
Test - acc:         0.860127 loss:        0.462061
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.939804 loss:        0.198583
Test - acc:         0.857070 loss:        0.466085
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.938325 loss:        0.202933
Test - acc:         0.859108 loss:        0.461466
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.943289 loss:        0.191230
Test - acc:         0.861401 loss:        0.459994
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.943922 loss:        0.188543
Test - acc:         0.857070 loss:        0.464870
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.942127 loss:        0.188205
Test - acc:         0.861656 loss:        0.466441
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.945190 loss:        0.188217
Test - acc:         0.859618 loss:        0.456967
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.946140 loss:        0.180817
Test - acc:         0.860382 loss:        0.455907
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.948041 loss:        0.181534
Test - acc:         0.859108 loss:        0.471475
Sparsity :          0.9961
Wdecay :        0.000500
