Running --model densenet121 --dataset imagenette --seed 44 --logdir=criterion_experiment_no_bias/densenet121 --prune_criterion weight_squared_div_flips --prune_freq 70 --prune_rate 0.5 --noise --comment=densenet121_crit=weight_squared_div_flips_pf=70_seed=44 --save_model=pre-finetune/densenet121_weight_squared_div_flips_pf70_s44
******************************
Running
{
    "model": "densenet121",
    "dataset": "imagenette",
    "batch_size": 128,
    "test_batch_size": 500,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 70,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/densenet121",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/densenet121_weight_squared_div_flips_pf70_s44",
    "load_model": null,
    "parallel": false
}
******************************
Total prunable params of model: 6880448
Model has 6964106 total params.
num_weights=6922272
num_biases=41834
num.prunable=6880448
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.333721 loss:        2.092967
Test - acc:         0.307771 loss:        6.056442
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.480199 loss:        1.621340
Test - acc:         0.505987 loss:        1.908920
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.573767 loss:        1.333350
Test - acc:         0.537070 loss:        1.505730
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.623614 loss:        1.201456
Test - acc:         0.503694 loss:        1.606702
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.653818 loss:        1.074715
Test - acc:         0.642548 loss:        1.140564
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.697223 loss:        0.927492
Test - acc:         0.683057 loss:        1.024992
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.722674 loss:        0.861224
Test - acc:         0.612484 loss:        1.328435
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.734502 loss:        0.817018
Test - acc:         0.656815 loss:        1.200333
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.757630 loss:        0.751320
Test - acc:         0.680764 loss:        0.998965
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.771993 loss:        0.702173
Test - acc:         0.716943 loss:        0.997309
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.786039 loss:        0.662253
Test - acc:         0.653248 loss:        1.191444
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.795438 loss:        0.622515
Test - acc:         0.749554 loss:        0.882538
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.812335 loss:        0.580071
Test - acc:         0.742420 loss:        0.863264
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.825114 loss:        0.533465
Test - acc:         0.702675 loss:        1.015203
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.834407 loss:        0.504240
Test - acc:         0.750064 loss:        0.772536
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845918 loss:        0.469809
Test - acc:         0.696815 loss:        1.170160
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.850037 loss:        0.444308
Test - acc:         0.740637 loss:        0.906648
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.857430 loss:        0.430658
Test - acc:         0.779873 loss:        0.737842
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.870736 loss:        0.387454
Test - acc:         0.675414 loss:        1.428247
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.860281 loss:        0.420546
Test - acc:         0.656306 loss:        1.565538
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.881297 loss:        0.360454
Test - acc:         0.784968 loss:        0.739873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.878340 loss:        0.382929
Test - acc:         0.776051 loss:        0.803206
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.884359 loss:        0.344970
Test - acc:         0.727134 loss:        1.069458
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.888795 loss:        0.346668
Test - acc:         0.771465 loss:        0.793985
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.895871 loss:        0.316962
Test - acc:         0.769427 loss:        0.810582
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.891118 loss:        0.326808
Test - acc:         0.769682 loss:        0.770453
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.901996 loss:        0.307741
Test - acc:         0.736306 loss:        0.998031
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.898828 loss:        0.298970
Test - acc:         0.796433 loss:        0.809667
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.899673 loss:        0.305613
Test - acc:         0.765096 loss:        0.852713
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.905587 loss:        0.286490
Test - acc:         0.676178 loss:        1.335155
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.896082 loss:        0.312915
Test - acc:         0.730955 loss:        0.970557
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.904003 loss:        0.281854
Test - acc:         0.766879 loss:        0.839994
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.913613 loss:        0.262699
Test - acc:         0.801274 loss:        0.716547
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.911923 loss:        0.261046
Test - acc:         0.761529 loss:        0.948662
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.912662 loss:        0.265486
Test - acc:         0.780892 loss:        0.799466
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.908966 loss:        0.273010
Test - acc:         0.758981 loss:        0.971180
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.916464 loss:        0.252723
Test - acc:         0.743694 loss:        0.879393
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.916992 loss:        0.247641
Test - acc:         0.651975 loss:        1.671991
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.917943 loss:        0.238841
Test - acc:         0.743694 loss:        0.962231
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.920266 loss:        0.238883
Test - acc:         0.647389 loss:        1.702062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.923751 loss:        0.234690
Test - acc:         0.790828 loss:        0.822939
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.929137 loss:        0.212244
Test - acc:         0.701911 loss:        1.229443
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.907804 loss:        0.273883
Test - acc:         0.802803 loss:        0.698272
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.919632 loss:        0.241945
Test - acc:         0.798726 loss:        0.701875
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.918471 loss:        0.243604
Test - acc:         0.792866 loss:        0.816701
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.925863 loss:        0.221479
Test - acc:         0.711338 loss:        1.276467
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.921850 loss:        0.241697
Test - acc:         0.802548 loss:        0.746040
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.910656 loss:        0.273963
Test - acc:         0.426242 loss:        2.942865
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.888373 loss:        0.332485
Test - acc:         0.783185 loss:        0.749481
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.927870 loss:        0.221475
Test - acc:         0.771465 loss:        0.908757
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.911923 loss:        0.259537
Test - acc:         0.789554 loss:        0.747312
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.920055 loss:        0.241351
Test - acc:         0.798471 loss:        0.760223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.921111 loss:        0.237593
Test - acc:         0.770446 loss:        0.862252
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.925969 loss:        0.222965
Test - acc:         0.784713 loss:        0.795751
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.931883 loss:        0.211062
Test - acc:         0.806369 loss:        0.698372
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.920161 loss:        0.241977
Test - acc:         0.830828 loss:        0.611886
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.933150 loss:        0.200237
Test - acc:         0.832611 loss:        0.587423
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.925230 loss:        0.220845
Test - acc:         0.693758 loss:        1.176126
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.922801 loss:        0.229710
Test - acc:         0.803567 loss:        0.752749
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.935051 loss:        0.196382
Test - acc:         0.792611 loss:        0.772364
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.926391 loss:        0.222342
Test - acc:         0.819363 loss:        0.653327
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.925969 loss:        0.211836
Test - acc:         0.720000 loss:        1.101422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.931249 loss:        0.206102
Test - acc:         0.815287 loss:        0.705944
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.925758 loss:        0.215592
Test - acc:         0.816815 loss:        0.650783
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.938325 loss:        0.189618
Test - acc:         0.793376 loss:        0.804342
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.922061 loss:        0.232599
Test - acc:         0.730701 loss:        1.218470
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.936741 loss:        0.190754
Test - acc:         0.826497 loss:        0.643881
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.927236 loss:        0.218581
Test - acc:         0.734268 loss:        1.079342
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.926286 loss:        0.219830
Test - acc:         0.769682 loss:        0.926411
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.924807 loss:        0.226449
Test - acc:         0.801019 loss:        0.790222
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.960186 loss:        0.123844
Test - acc:         0.819873 loss:        0.696482
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.967473 loss:        0.098963
Test - acc:         0.831847 loss:        0.659811
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.965783 loss:        0.107420
Test - acc:         0.812484 loss:        0.770241
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.956490 loss:        0.132857
Test - acc:         0.818599 loss:        0.720391
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.952582 loss:        0.140232
Test - acc:         0.812484 loss:        0.673672
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.950048 loss:        0.151643
Test - acc:         0.794395 loss:        0.780485
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.948147 loss:        0.158208
Test - acc:         0.797452 loss:        0.814514
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.943500 loss:        0.169012
Test - acc:         0.790318 loss:        0.892627
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.950787 loss:        0.148289
Test - acc:         0.821146 loss:        0.682400
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.934206 loss:        0.191949
Test - acc:         0.776306 loss:        0.823489
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.939064 loss:        0.180641
Test - acc:         0.818089 loss:        0.693693
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.940226 loss:        0.180298
Test - acc:         0.784204 loss:        0.833946
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.955011 loss:        0.138385
Test - acc:         0.835924 loss:        0.629348
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.944662 loss:        0.160330
Test - acc:         0.781656 loss:        0.870345
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.951420 loss:        0.150553
Test - acc:         0.795669 loss:        0.808449
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.941282 loss:        0.170377
Test - acc:         0.733503 loss:        1.341444
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.944450 loss:        0.167095
Test - acc:         0.798471 loss:        0.798560
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.955117 loss:        0.139638
Test - acc:         0.767898 loss:        0.848796
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.946246 loss:        0.158969
Test - acc:         0.831592 loss:        0.677766
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.940754 loss:        0.180864
Test - acc:         0.809936 loss:        0.760213
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.951315 loss:        0.150490
Test - acc:         0.829809 loss:        0.650065
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.952054 loss:        0.146991
Test - acc:         0.629045 loss:        2.042000
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.944978 loss:        0.168227
Test - acc:         0.733248 loss:        1.134325
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.943605 loss:        0.172687
Test - acc:         0.705478 loss:        1.415435
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.944556 loss:        0.157473
Test - acc:         0.756433 loss:        1.045973
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.946879 loss:        0.160199
Test - acc:         0.812994 loss:        0.702593
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.949731 loss:        0.159123
Test - acc:         0.815287 loss:        0.697312
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.950787 loss:        0.145282
Test - acc:         0.799745 loss:        0.781160
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.952688 loss:        0.144255
Test - acc:         0.829045 loss:        0.672371
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.954905 loss:        0.138397
Test - acc:         0.801529 loss:        0.771915
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.944767 loss:        0.164051
Test - acc:         0.829809 loss:        0.627587
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.942549 loss:        0.178975
Test - acc:         0.816051 loss:        0.712983
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.950787 loss:        0.147965
Test - acc:         0.788790 loss:        0.816257
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.950259 loss:        0.151333
Test - acc:         0.795414 loss:        0.784627
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.948886 loss:        0.153256
Test - acc:         0.814522 loss:        0.710339
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.942021 loss:        0.173451
Test - acc:         0.851975 loss:        0.579016
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.947724 loss:        0.155119
Test - acc:         0.798471 loss:        0.798240
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.952371 loss:        0.145825
Test - acc:         0.763822 loss:        0.907534
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.953110 loss:        0.138724
Test - acc:         0.814777 loss:        0.755036
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.943183 loss:        0.180351
Test - acc:         0.768662 loss:        0.951656
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.948780 loss:        0.160778
Test - acc:         0.782675 loss:        0.935518
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.953110 loss:        0.139407
Test - acc:         0.824713 loss:        0.765091
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.954483 loss:        0.144856
Test - acc:         0.798726 loss:        0.844193
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.954483 loss:        0.134174
Test - acc:         0.811975 loss:        0.762579
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.941493 loss:        0.171655
Test - acc:         0.790318 loss:        0.794042
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.950470 loss:        0.148127
Test - acc:         0.813248 loss:        0.730139
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.954377 loss:        0.136863
Test - acc:         0.831083 loss:        0.677760
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.954800 loss:        0.136109
Test - acc:         0.745478 loss:        1.177355
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.954483 loss:        0.140313
Test - acc:         0.804586 loss:        0.788687
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.951104 loss:        0.151729
Test - acc:         0.778854 loss:        0.859310
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.946562 loss:        0.149865
Test - acc:         0.657834 loss:        1.570369
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.939698 loss:        0.180989
Test - acc:         0.736051 loss:        1.076691
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.943077 loss:        0.166328
Test - acc:         0.818344 loss:        0.662325
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.953533 loss:        0.136441
Test - acc:         0.845860 loss:        0.598768
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.956278 loss:        0.132169
Test - acc:         0.807389 loss:        0.729363
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.947513 loss:        0.151687
Test - acc:         0.786242 loss:        0.912705
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.956067 loss:        0.133368
Test - acc:         0.808153 loss:        0.727182
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.951315 loss:        0.142702
Test - acc:         0.821656 loss:        0.706747
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.944028 loss:        0.162767
Test - acc:         0.825478 loss:        0.671023
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.954483 loss:        0.134206
Test - acc:         0.834140 loss:        0.656120
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.947407 loss:        0.157188
Test - acc:         0.817834 loss:        0.690950
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.946668 loss:        0.162198
Test - acc:         0.783694 loss:        0.803650
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.950048 loss:        0.152424
Test - acc:         0.779618 loss:        0.922287
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.959447 loss:        0.124091
Test - acc:         0.738854 loss:        1.167293
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.951948 loss:        0.142179
Test - acc:         0.814013 loss:        0.662693
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.953110 loss:        0.136512
Test - acc:         0.831847 loss:        0.657423
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.947619 loss:        0.166476
Test - acc:         0.835414 loss:        0.638812
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.957123 loss:        0.131546
Test - acc:         0.835924 loss:        0.622746
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.965994 loss:        0.104923
Test - acc:         0.780892 loss:        0.796867
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.954272 loss:        0.134164
Test - acc:         0.705732 loss:        1.358217
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.967367 loss:        0.097715
Test - acc:         0.855032 loss:        0.565911
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.982786 loss:        0.054750
Test - acc:         0.845096 loss:        0.622496
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.971063 loss:        0.089112
Test - acc:         0.813248 loss:        0.753044
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.964410 loss:        0.108783
Test - acc:         0.848662 loss:        0.585897
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.974443 loss:        0.077338
Test - acc:         0.807898 loss:        0.850205
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.969268 loss:        0.089779
Test - acc:         0.839236 loss:        0.682320
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.955434 loss:        0.139158
Test - acc:         0.792102 loss:        0.867095
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.959235 loss:        0.125702
Test - acc:         0.836433 loss:        0.703787
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.962509 loss:        0.118027
Test - acc:         0.845096 loss:        0.565622
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.960397 loss:        0.117144
Test - acc:         0.832102 loss:        0.685397
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.989017 loss:        0.040935
Test - acc:         0.894268 loss:        0.377558
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.997993 loss:        0.016869
Test - acc:         0.896815 loss:        0.364213
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.998944 loss:        0.011968
Test - acc:         0.900382 loss:        0.359246
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.999050 loss:        0.010575
Test - acc:         0.901656 loss:        0.352442
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.999261 loss:        0.008950
Test - acc:         0.900382 loss:        0.353525
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.007479
Test - acc:         0.902420 loss:        0.352121
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.999472 loss:        0.008040
Test - acc:         0.904968 loss:        0.352076
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.007266
Test - acc:         0.902420 loss:        0.347728
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.006707
Test - acc:         0.904204 loss:        0.348908
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.006260
Test - acc:         0.906752 loss:        0.351097
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005673
Test - acc:         0.905732 loss:        0.348030
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005860
Test - acc:         0.906242 loss:        0.350418
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004710
Test - acc:         0.906752 loss:        0.351270
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.005049
Test - acc:         0.906242 loss:        0.347744
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.004867
Test - acc:         0.904459 loss:        0.353380
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004634
Test - acc:         0.907516 loss:        0.344839
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.004346
Test - acc:         0.905732 loss:        0.350151
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.004561
Test - acc:         0.906242 loss:        0.345163
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.999578 loss:        0.004728
Test - acc:         0.903949 loss:        0.355978
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003955
Test - acc:         0.904459 loss:        0.348945
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003914
Test - acc:         0.906242 loss:        0.346949
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003810
Test - acc:         0.905732 loss:        0.347535
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003780
Test - acc:         0.905478 loss:        0.345187
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003406
Test - acc:         0.903949 loss:        0.342707
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003962
Test - acc:         0.906497 loss:        0.348842
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003339
Test - acc:         0.907006 loss:        0.345066
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003424
Test - acc:         0.906752 loss:        0.344117
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.999683 loss:        0.003990
Test - acc:         0.908025 loss:        0.343258
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003702
Test - acc:         0.905732 loss:        0.344593
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003491
Test - acc:         0.907771 loss:        0.341056
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003548
Test - acc:         0.906752 loss:        0.341169
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003569
Test - acc:         0.906752 loss:        0.344641
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003144
Test - acc:         0.906242 loss:        0.342953
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003437
Test - acc:         0.907006 loss:        0.346479
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003564
Test - acc:         0.906497 loss:        0.352641
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003138
Test - acc:         0.906497 loss:        0.349255
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003183
Test - acc:         0.906752 loss:        0.350803
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002855
Test - acc:         0.907516 loss:        0.346395
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002791
Test - acc:         0.907261 loss:        0.341145
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003063
Test - acc:         0.908280 loss:        0.338516
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003030
Test - acc:         0.905987 loss:        0.340920
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002917
Test - acc:         0.908790 loss:        0.340845
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003162
Test - acc:         0.908025 loss:        0.340685
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002761
Test - acc:         0.908535 loss:        0.338882
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002872
Test - acc:         0.908025 loss:        0.345844
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003159
Test - acc:         0.909299 loss:        0.341654
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002747
Test - acc:         0.908535 loss:        0.339458
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002678
Test - acc:         0.908025 loss:        0.338677
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003055
Test - acc:         0.908025 loss:        0.342236
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003575
Test - acc:         0.906752 loss:        0.341814
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003079
Test - acc:         0.909299 loss:        0.341921
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003320
Test - acc:         0.911847 loss:        0.337547
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003012
Test - acc:         0.911083 loss:        0.339187
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002934
Test - acc:         0.909299 loss:        0.336136
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003016
Test - acc:         0.909299 loss:        0.334585
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002733
Test - acc:         0.911847 loss:        0.338356
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003072
Test - acc:         0.910318 loss:        0.335878
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002774
Test - acc:         0.911592 loss:        0.334155
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002609
Test - acc:         0.910828 loss:        0.334375
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002861
Test - acc:         0.908790 loss:        0.339233
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.004657
Test - acc:         0.907771 loss:        0.341112
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003723
Test - acc:         0.908280 loss:        0.339527
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003776
Test - acc:         0.910064 loss:        0.338894
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003684
Test - acc:         0.910064 loss:        0.341526
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003710
Test - acc:         0.909045 loss:        0.344138
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003734
Test - acc:         0.909299 loss:        0.344474
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003775
Test - acc:         0.908280 loss:        0.345700
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003398
Test - acc:         0.908535 loss:        0.340646
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003560
Test - acc:         0.910318 loss:        0.340033
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003871
Test - acc:         0.907516 loss:        0.340598
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003136
Test - acc:         0.910064 loss:        0.338946
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003005
Test - acc:         0.909809 loss:        0.337771
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003267
Test - acc:         0.909045 loss:        0.340011
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003049
Test - acc:         0.908280 loss:        0.336402
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003091
Test - acc:         0.907771 loss:        0.337965
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003106
Test - acc:         0.910064 loss:        0.336408
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002899
Test - acc:         0.909809 loss:        0.343242
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002926
Test - acc:         0.909809 loss:        0.337126
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003091
Test - acc:         0.908790 loss:        0.338055
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002848
Test - acc:         0.909809 loss:        0.337572
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002815
Test - acc:         0.909554 loss:        0.338352
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003280
Test - acc:         0.909809 loss:        0.339443
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002951
Test - acc:         0.908790 loss:        0.335432
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003147
Test - acc:         0.909299 loss:        0.341746
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002894
Test - acc:         0.909045 loss:        0.341828
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.999789 loss:        0.003609
Test - acc:         0.910318 loss:        0.340527
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002955
Test - acc:         0.910318 loss:        0.341448
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003067
Test - acc:         0.909809 loss:        0.333545
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003075
Test - acc:         0.911592 loss:        0.336666
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002931
Test - acc:         0.911338 loss:        0.339458
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.003081
Test - acc:         0.910828 loss:        0.337376
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002803
Test - acc:         0.909299 loss:        0.334880
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.999894 loss:        0.002614
Test - acc:         0.911338 loss:        0.338790
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002626
Test - acc:         0.912102 loss:        0.338637
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002798
Test - acc:         0.908790 loss:        0.337785
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.003111
Test - acc:         0.911338 loss:        0.339585
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002863
Test - acc:         0.910573 loss:        0.335861
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002613
Test - acc:         0.911338 loss:        0.336386
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        1.000000 loss:        0.002881
Test - acc:         0.909299 loss:        0.344864
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002802
Test - acc:         0.910064 loss:        0.338884
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.003026
Test - acc:         0.910064 loss:        0.335035
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002907
Test - acc:         0.911083 loss:        0.335119
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002608
Test - acc:         0.909554 loss:        0.337776
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002674
Test - acc:         0.910828 loss:        0.339334
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.003080
Test - acc:         0.911338 loss:        0.337024
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002734
Test - acc:         0.911847 loss:        0.331848
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002767
Test - acc:         0.911083 loss:        0.338606
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002565
Test - acc:         0.911083 loss:        0.338729
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002450
Test - acc:         0.910318 loss:        0.337806
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002866
Test - acc:         0.911847 loss:        0.337181
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002544
Test - acc:         0.910828 loss:        0.342697
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002610
Test - acc:         0.910828 loss:        0.335243
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002626
Test - acc:         0.910318 loss:        0.338744
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002816
Test - acc:         0.911083 loss:        0.336910
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.002948
Test - acc:         0.912866 loss:        0.334407
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002619
Test - acc:         0.911083 loss:        0.333339
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002504
Test - acc:         0.909554 loss:        0.340093
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.003003
Test - acc:         0.910318 loss:        0.340826
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002745
Test - acc:         0.910064 loss:        0.340773
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.002781
Test - acc:         0.911083 loss:        0.337098
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002545
Test - acc:         0.911083 loss:        0.339138
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002541
Test - acc:         0.910828 loss:        0.337447
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002892
Test - acc:         0.910828 loss:        0.334232
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002493
Test - acc:         0.912102 loss:        0.332570
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002656
Test - acc:         0.912102 loss:        0.334232
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002352
Test - acc:         0.910828 loss:        0.335771
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002744
Test - acc:         0.912611 loss:        0.333282
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002579
Test - acc:         0.910064 loss:        0.338515
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002513
Test - acc:         0.911592 loss:        0.338370
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002458
Test - acc:         0.912357 loss:        0.337470
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.997360 loss:        0.017342
Test - acc:         0.898854 loss:        0.372848
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.998521 loss:        0.013215
Test - acc:         0.900637 loss:        0.368387
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.998521 loss:        0.012603
Test - acc:         0.901146 loss:        0.368562
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.998944 loss:        0.011575
Test - acc:         0.899363 loss:        0.366117
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.009659
Test - acc:         0.900127 loss:        0.365770
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.010068
Test - acc:         0.902675 loss:        0.361361
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.999261 loss:        0.009725
Test - acc:         0.902420 loss:        0.361366
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.999578 loss:        0.009977
Test - acc:         0.901146 loss:        0.366722
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.999683 loss:        0.008289
Test - acc:         0.902675 loss:        0.360329
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.999261 loss:        0.008856
Test - acc:         0.903185 loss:        0.360721
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.999578 loss:        0.007910
Test - acc:         0.904204 loss:        0.359413
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.009044
Test - acc:         0.904459 loss:        0.360710
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.007134
Test - acc:         0.903694 loss:        0.360326
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.999578 loss:        0.007814
Test - acc:         0.903439 loss:        0.357216
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.999578 loss:        0.008065
Test - acc:         0.903949 loss:        0.356082
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.006735
Test - acc:         0.903185 loss:        0.358088
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.007564
Test - acc:         0.902930 loss:        0.355427
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.007749
Test - acc:         0.902675 loss:        0.357733
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.007136
Test - acc:         0.901401 loss:        0.358674
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999683 loss:        0.007056
Test - acc:         0.904459 loss:        0.358611
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.007597
Test - acc:         0.906242 loss:        0.354319
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.999472 loss:        0.007181
Test - acc:         0.904713 loss:        0.356223
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.999578 loss:        0.006740
Test - acc:         0.902930 loss:        0.358281
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.006357
Test - acc:         0.903185 loss:        0.358546
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.006134
Test - acc:         0.905732 loss:        0.358413
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999578 loss:        0.006844
Test - acc:         0.905478 loss:        0.352047
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999578 loss:        0.007092
Test - acc:         0.903694 loss:        0.356737
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.006044
Test - acc:         0.902675 loss:        0.357710
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.006475
Test - acc:         0.904968 loss:        0.364373
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.005952
Test - acc:         0.904713 loss:        0.362108
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.005960
Test - acc:         0.904713 loss:        0.358276
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.005558
Test - acc:         0.902420 loss:        0.363474
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.005426
Test - acc:         0.904204 loss:        0.361542
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.005895
Test - acc:         0.904968 loss:        0.357867
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.005301
Test - acc:         0.905223 loss:        0.358052
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.005638
Test - acc:         0.908280 loss:        0.356367
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.999578 loss:        0.005992
Test - acc:         0.904459 loss:        0.358934
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.005076
Test - acc:         0.904204 loss:        0.357698
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.005756
Test - acc:         0.903949 loss:        0.354365
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.005630
Test - acc:         0.907516 loss:        0.358524
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.005870
Test - acc:         0.904713 loss:        0.358542
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.005590
Test - acc:         0.904204 loss:        0.361267
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.005182
Test - acc:         0.904968 loss:        0.354333
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.005425
Test - acc:         0.904713 loss:        0.356327
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.004698
Test - acc:         0.904713 loss:        0.361191
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.004778
Test - acc:         0.904713 loss:        0.357378
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.005122
Test - acc:         0.906242 loss:        0.358792
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.999683 loss:        0.005056
Test - acc:         0.901911 loss:        0.363562
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.005317
Test - acc:         0.907516 loss:        0.359754
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.005438
Test - acc:         0.905732 loss:        0.356687
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.004711
Test - acc:         0.902930 loss:        0.362506
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.004347
Test - acc:         0.906497 loss:        0.357530
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.004773
Test - acc:         0.905732 loss:        0.359276
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.004650
Test - acc:         0.904459 loss:        0.357401
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999683 loss:        0.005626
Test - acc:         0.906497 loss:        0.356823
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.005290
Test - acc:         0.903949 loss:        0.361312
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999683 loss:        0.004943
Test - acc:         0.905478 loss:        0.359250
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999578 loss:        0.005197
Test - acc:         0.907261 loss:        0.356558
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.004475
Test - acc:         0.905478 loss:        0.353779
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.004806
Test - acc:         0.907261 loss:        0.360416
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.004399
Test - acc:         0.906497 loss:        0.359923
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.004565
Test - acc:         0.907261 loss:        0.357410
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999683 loss:        0.004848
Test - acc:         0.905732 loss:        0.356598
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.004359
Test - acc:         0.907516 loss:        0.358279
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.004468
Test - acc:         0.907516 loss:        0.354874
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.004233
Test - acc:         0.904459 loss:        0.352544
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.004596
Test - acc:         0.906752 loss:        0.354731
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.004279
Test - acc:         0.904713 loss:        0.355957
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.999894 loss:        0.004992
Test - acc:         0.906752 loss:        0.357522
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999789 loss:        0.004502
Test - acc:         0.906752 loss:        0.353285
Sparsity :          0.9375
Wdecay :        0.000500
