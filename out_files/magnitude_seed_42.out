******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "magnitude",
    "prune_freq": 117,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf117_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.325920 loss:        1.847682
Test - acc:         0.433400 loss:        1.508943
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.514980 loss:        1.326493
Test - acc:         0.576200 loss:        1.166692
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.637620 loss:        1.009011
Test - acc:         0.606500 loss:        1.140515
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.717740 loss:        0.806052
Test - acc:         0.696100 loss:        0.896292
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.760000 loss:        0.691428
Test - acc:         0.742900 loss:        0.751321
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.787060 loss:        0.611490
Test - acc:         0.756600 loss:        0.710299
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.806060 loss:        0.564509
Test - acc:         0.769000 loss:        0.710991
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.817920 loss:        0.529806
Test - acc:         0.787400 loss:        0.621394
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.824420 loss:        0.509206
Test - acc:         0.772600 loss:        0.684377
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.833860 loss:        0.483304
Test - acc:         0.775600 loss:        0.667925
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.838920 loss:        0.469585
Test - acc:         0.767600 loss:        0.754009
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.844060 loss:        0.454590
Test - acc:         0.796900 loss:        0.618867
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.850060 loss:        0.440107
Test - acc:         0.763200 loss:        0.735417
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.851640 loss:        0.434035
Test - acc:         0.793900 loss:        0.660835
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.855280 loss:        0.423293
Test - acc:         0.832300 loss:        0.496892
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.858580 loss:        0.415450
Test - acc:         0.831400 loss:        0.510164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.859460 loss:        0.408462
Test - acc:         0.820900 loss:        0.522441
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.860880 loss:        0.400126
Test - acc:         0.834400 loss:        0.483465
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.864860 loss:        0.393520
Test - acc:         0.804400 loss:        0.618786
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.868280 loss:        0.385265
Test - acc:         0.807600 loss:        0.623200
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.869220 loss:        0.381626
Test - acc:         0.814300 loss:        0.590255
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870500 loss:        0.379530
Test - acc:         0.798800 loss:        0.641787
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.871620 loss:        0.377487
Test - acc:         0.811600 loss:        0.574784
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.875440 loss:        0.368226
Test - acc:         0.821200 loss:        0.538520
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.877080 loss:        0.364041
Test - acc:         0.827800 loss:        0.496110
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.874160 loss:        0.366042
Test - acc:         0.818700 loss:        0.529624
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.877320 loss:        0.360246
Test - acc:         0.830300 loss:        0.521060
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.875840 loss:        0.361626
Test - acc:         0.829000 loss:        0.518092
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.878240 loss:        0.352239
Test - acc:         0.821600 loss:        0.544881
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.876260 loss:        0.358935
Test - acc:         0.835600 loss:        0.488176
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.880360 loss:        0.350550
Test - acc:         0.844600 loss:        0.479743
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.880840 loss:        0.348789
Test - acc:         0.799400 loss:        0.643922
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.882680 loss:        0.344050
Test - acc:         0.856300 loss:        0.439701
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.881020 loss:        0.349389
Test - acc:         0.861500 loss:        0.419639
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.881940 loss:        0.344315
Test - acc:         0.810400 loss:        0.605949
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.882960 loss:        0.341227
Test - acc:         0.855700 loss:        0.442214
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.883060 loss:        0.340363
Test - acc:         0.845100 loss:        0.467937
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.882780 loss:        0.341310
Test - acc:         0.812600 loss:        0.630480
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.885200 loss:        0.338045
Test - acc:         0.834200 loss:        0.483156
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.885040 loss:        0.338235
Test - acc:         0.864300 loss:        0.393861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.882280 loss:        0.342935
Test - acc:         0.835700 loss:        0.519533
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.887920 loss:        0.329464
Test - acc:         0.842400 loss:        0.484434
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.887960 loss:        0.328591
Test - acc:         0.835800 loss:        0.475898
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.888020 loss:        0.327734
Test - acc:         0.835600 loss:        0.509635
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.885720 loss:        0.336958
Test - acc:         0.831700 loss:        0.497432
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.886360 loss:        0.334090
Test - acc:         0.843100 loss:        0.469549
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.888340 loss:        0.329442
Test - acc:         0.828700 loss:        0.518911
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.885980 loss:        0.331101
Test - acc:         0.818900 loss:        0.543475
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.887960 loss:        0.327193
Test - acc:         0.813600 loss:        0.590675
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.886560 loss:        0.332759
Test - acc:         0.844200 loss:        0.453877
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.888840 loss:        0.326604
Test - acc:         0.849000 loss:        0.455457
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.890940 loss:        0.324219
Test - acc:         0.866300 loss:        0.399992
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.890300 loss:        0.327441
Test - acc:         0.849900 loss:        0.477880
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.889060 loss:        0.325174
Test - acc:         0.853900 loss:        0.436648
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.888320 loss:        0.326879
Test - acc:         0.842000 loss:        0.473023
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.890220 loss:        0.324462
Test - acc:         0.765700 loss:        0.721840
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.889780 loss:        0.323409
Test - acc:         0.841300 loss:        0.492251
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.889440 loss:        0.323690
Test - acc:         0.836300 loss:        0.492271
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.890180 loss:        0.322190
Test - acc:         0.866900 loss:        0.402989
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.890920 loss:        0.321626
Test - acc:         0.854900 loss:        0.441502
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.891540 loss:        0.318806
Test - acc:         0.869200 loss:        0.409789
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.888320 loss:        0.325977
Test - acc:         0.809500 loss:        0.589435
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.891780 loss:        0.316914
Test - acc:         0.814400 loss:        0.548158
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.890480 loss:        0.321112
Test - acc:         0.824300 loss:        0.562069
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.890260 loss:        0.321836
Test - acc:         0.835700 loss:        0.484339
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.890780 loss:        0.322657
Test - acc:         0.837100 loss:        0.501047
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.890920 loss:        0.317687
Test - acc:         0.834500 loss:        0.482315
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.892680 loss:        0.316245
Test - acc:         0.802800 loss:        0.677386
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.891800 loss:        0.318122
Test - acc:         0.839700 loss:        0.505710
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.890020 loss:        0.322447
Test - acc:         0.847000 loss:        0.469302
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.891000 loss:        0.319200
Test - acc:         0.844200 loss:        0.481440
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.893220 loss:        0.316158
Test - acc:         0.817600 loss:        0.552311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.892660 loss:        0.315162
Test - acc:         0.847100 loss:        0.481526
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.893540 loss:        0.315739
Test - acc:         0.852100 loss:        0.453969
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.892600 loss:        0.311690
Test - acc:         0.851400 loss:        0.467760
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.891160 loss:        0.317758
Test - acc:         0.841100 loss:        0.495059
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.893740 loss:        0.314208
Test - acc:         0.840100 loss:        0.480453
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.893260 loss:        0.316344
Test - acc:         0.856000 loss:        0.437006
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.893020 loss:        0.316026
Test - acc:         0.840600 loss:        0.492237
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.893300 loss:        0.313632
Test - acc:         0.853700 loss:        0.453069
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.893940 loss:        0.314349
Test - acc:         0.844000 loss:        0.511161
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.894180 loss:        0.311707
Test - acc:         0.851800 loss:        0.456814
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.895200 loss:        0.311285
Test - acc:         0.863900 loss:        0.416785
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.893460 loss:        0.313377
Test - acc:         0.857100 loss:        0.431436
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.895080 loss:        0.311268
Test - acc:         0.847100 loss:        0.473754
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.892420 loss:        0.314244
Test - acc:         0.799600 loss:        0.692643
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.892120 loss:        0.318428
Test - acc:         0.837200 loss:        0.513208
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.894960 loss:        0.308684
Test - acc:         0.841200 loss:        0.477324
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.894680 loss:        0.311076
Test - acc:         0.837000 loss:        0.488150
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.895720 loss:        0.308183
Test - acc:         0.818900 loss:        0.571047
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.891480 loss:        0.315237
Test - acc:         0.817100 loss:        0.603858
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.896080 loss:        0.308371
Test - acc:         0.829100 loss:        0.524826
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.895040 loss:        0.311140
Test - acc:         0.844400 loss:        0.488078
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.892720 loss:        0.314943
Test - acc:         0.852800 loss:        0.447448
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.894500 loss:        0.311513
Test - acc:         0.873900 loss:        0.371224
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.894180 loss:        0.308375
Test - acc:         0.796400 loss:        0.683448
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.892300 loss:        0.317814
Test - acc:         0.845400 loss:        0.491270
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.894060 loss:        0.313063
Test - acc:         0.817500 loss:        0.584605
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.893220 loss:        0.312436
Test - acc:         0.842900 loss:        0.508549
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.893500 loss:        0.308674
Test - acc:         0.835800 loss:        0.506185
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.895420 loss:        0.309118
Test - acc:         0.855700 loss:        0.441999
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.892380 loss:        0.311453
Test - acc:         0.848100 loss:        0.468522
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.894260 loss:        0.313469
Test - acc:         0.834700 loss:        0.512115
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.895020 loss:        0.308316
Test - acc:         0.842900 loss:        0.486786
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.896280 loss:        0.307339
Test - acc:         0.864000 loss:        0.416849
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.894220 loss:        0.314614
Test - acc:         0.820000 loss:        0.578944
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.895560 loss:        0.307326
Test - acc:         0.848900 loss:        0.461513
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.895040 loss:        0.310405
Test - acc:         0.831700 loss:        0.531827
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.894740 loss:        0.310948
Test - acc:         0.843500 loss:        0.481606
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.896260 loss:        0.303323
Test - acc:         0.819700 loss:        0.551166
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.894520 loss:        0.308364
Test - acc:         0.824900 loss:        0.514837
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.895640 loss:        0.308051
Test - acc:         0.826200 loss:        0.519885
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.894160 loss:        0.313164
Test - acc:         0.838100 loss:        0.502093
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.893220 loss:        0.313084
Test - acc:         0.844100 loss:        0.490844
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.895620 loss:        0.307287
Test - acc:         0.817200 loss:        0.576853
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.892480 loss:        0.315124
Test - acc:         0.851300 loss:        0.448466
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.895580 loss:        0.306133
Test - acc:         0.860500 loss:        0.420976
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.912540 loss:        0.256632
Test - acc:         0.844500 loss:        0.480008
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.909420 loss:        0.265330
Test - acc:         0.853400 loss:        0.439927
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.907940 loss:        0.270567
Test - acc:         0.864100 loss:        0.427773
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.908420 loss:        0.268699
Test - acc:         0.826200 loss:        0.558390
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.906320 loss:        0.275165
Test - acc:         0.837900 loss:        0.510434
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.907940 loss:        0.269231
Test - acc:         0.855000 loss:        0.455338
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.905440 loss:        0.275902
Test - acc:         0.860000 loss:        0.437155
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.910240 loss:        0.265851
Test - acc:         0.843300 loss:        0.482870
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.906240 loss:        0.272681
Test - acc:         0.865700 loss:        0.406569
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.906980 loss:        0.268964
Test - acc:         0.811300 loss:        0.599355
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.907860 loss:        0.270738
Test - acc:         0.803700 loss:        0.633094
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.909560 loss:        0.268062
Test - acc:         0.871300 loss:        0.407919
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.909800 loss:        0.266515
Test - acc:         0.838700 loss:        0.507694
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.905540 loss:        0.274541
Test - acc:         0.852400 loss:        0.463619
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.905680 loss:        0.276735
Test - acc:         0.868000 loss:        0.401544
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.907400 loss:        0.269652
Test - acc:         0.838300 loss:        0.501961
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.906820 loss:        0.273215
Test - acc:         0.848600 loss:        0.481984
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.904720 loss:        0.273302
Test - acc:         0.882000 loss:        0.362941
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.910120 loss:        0.265723
Test - acc:         0.828900 loss:        0.524801
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.908160 loss:        0.268395
Test - acc:         0.859800 loss:        0.429990
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.906800 loss:        0.271130
Test - acc:         0.859300 loss:        0.411193
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.907500 loss:        0.270811
Test - acc:         0.846200 loss:        0.484922
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.907500 loss:        0.271501
Test - acc:         0.853000 loss:        0.450013
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.908600 loss:        0.267958
Test - acc:         0.844000 loss:        0.508808
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.909740 loss:        0.265038
Test - acc:         0.847400 loss:        0.469345
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.908220 loss:        0.268638
Test - acc:         0.875500 loss:        0.376227
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.908620 loss:        0.265111
Test - acc:         0.854700 loss:        0.473356
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.906400 loss:        0.273188
Test - acc:         0.857100 loss:        0.453451
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.908440 loss:        0.267743
Test - acc:         0.858600 loss:        0.412634
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.908480 loss:        0.266265
Test - acc:         0.842500 loss:        0.477087
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.908960 loss:        0.265622
Test - acc:         0.859100 loss:        0.436792
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.909640 loss:        0.268196
Test - acc:         0.871300 loss:        0.390779
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.912740 loss:        0.258190
Test - acc:         0.869700 loss:        0.418899
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.955740 loss:        0.133471
Test - acc:         0.935300 loss:        0.188637
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.969200 loss:        0.093794
Test - acc:         0.939400 loss:        0.182405
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.974380 loss:        0.078171
Test - acc:         0.941200 loss:        0.179679
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.977940 loss:        0.067463
Test - acc:         0.941800 loss:        0.179752
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.980660 loss:        0.059714
Test - acc:         0.940800 loss:        0.185792
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.983100 loss:        0.051087
Test - acc:         0.941300 loss:        0.189547
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.984820 loss:        0.047073
Test - acc:         0.945200 loss:        0.184362
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.985980 loss:        0.043498
Test - acc:         0.946400 loss:        0.180148
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.987260 loss:        0.039539
Test - acc:         0.940100 loss:        0.196400
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.989240 loss:        0.033735
Test - acc:         0.941000 loss:        0.195049
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.989580 loss:        0.032908
Test - acc:         0.942800 loss:        0.198908
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.991320 loss:        0.029123
Test - acc:         0.941600 loss:        0.205865
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.990460 loss:        0.030797
Test - acc:         0.941900 loss:        0.200348
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.991820 loss:        0.026934
Test - acc:         0.943800 loss:        0.198029
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.992340 loss:        0.025203
Test - acc:         0.941700 loss:        0.210351
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.992680 loss:        0.024466
Test - acc:         0.940800 loss:        0.212999
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.991500 loss:        0.025360
Test - acc:         0.941000 loss:        0.218480
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.992720 loss:        0.024449
Test - acc:         0.942500 loss:        0.212255
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.992740 loss:        0.024224
Test - acc:         0.940600 loss:        0.220844
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.992080 loss:        0.024678
Test - acc:         0.939000 loss:        0.222131
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.992620 loss:        0.024460
Test - acc:         0.942800 loss:        0.204755
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.992840 loss:        0.024672
Test - acc:         0.943000 loss:        0.209526
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.991540 loss:        0.026065
Test - acc:         0.937000 loss:        0.237236
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.991660 loss:        0.026272
Test - acc:         0.943100 loss:        0.213474
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.991000 loss:        0.027313
Test - acc:         0.940500 loss:        0.219197
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.992000 loss:        0.026438
Test - acc:         0.942100 loss:        0.221577
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.992180 loss:        0.025131
Test - acc:         0.932200 loss:        0.242955
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.991080 loss:        0.028705
Test - acc:         0.937800 loss:        0.241718
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.991080 loss:        0.027709
Test - acc:         0.935000 loss:        0.232098
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.991720 loss:        0.026430
Test - acc:         0.940000 loss:        0.218585
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.991320 loss:        0.028051
Test - acc:         0.934100 loss:        0.236580
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.988960 loss:        0.033775
Test - acc:         0.935500 loss:        0.229107
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.989080 loss:        0.032993
Test - acc:         0.931500 loss:        0.246774
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.990140 loss:        0.031439
Test - acc:         0.930800 loss:        0.249590
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.987580 loss:        0.037681
Test - acc:         0.933400 loss:        0.236877
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.988740 loss:        0.035391
Test - acc:         0.933200 loss:        0.243592
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.987520 loss:        0.038279
Test - acc:         0.925600 loss:        0.266863
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.988280 loss:        0.035163
Test - acc:         0.929400 loss:        0.258698
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.988200 loss:        0.036162
Test - acc:         0.934400 loss:        0.235432
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.986320 loss:        0.040285
Test - acc:         0.927800 loss:        0.264284
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.988660 loss:        0.035082
Test - acc:         0.930500 loss:        0.253092
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.987400 loss:        0.038261
Test - acc:         0.927700 loss:        0.269929
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.985880 loss:        0.043714
Test - acc:         0.934300 loss:        0.241644
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.987380 loss:        0.038717
Test - acc:         0.932900 loss:        0.241720
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.986000 loss:        0.041901
Test - acc:         0.926600 loss:        0.273894
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.985840 loss:        0.042886
Test - acc:         0.927200 loss:        0.259671
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.986860 loss:        0.040180
Test - acc:         0.927200 loss:        0.271901
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.985360 loss:        0.044040
Test - acc:         0.931100 loss:        0.257108
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.986320 loss:        0.042126
Test - acc:         0.931300 loss:        0.260152
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.984740 loss:        0.046599
Test - acc:         0.930900 loss:        0.243822
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.986300 loss:        0.041454
Test - acc:         0.933200 loss:        0.247647
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.986500 loss:        0.040919
Test - acc:         0.927400 loss:        0.263905
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.987300 loss:        0.039202
Test - acc:         0.923300 loss:        0.278691
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.986320 loss:        0.041476
Test - acc:         0.926800 loss:        0.275006
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.985040 loss:        0.043672
Test - acc:         0.928200 loss:        0.252700
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.986560 loss:        0.041432
Test - acc:         0.928100 loss:        0.260965
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.985040 loss:        0.046792
Test - acc:         0.919400 loss:        0.278817
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.988020 loss:        0.036874
Test - acc:         0.927200 loss:        0.280355
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.986660 loss:        0.040210
Test - acc:         0.924900 loss:        0.277432
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.984380 loss:        0.045480
Test - acc:         0.929500 loss:        0.257256
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.985160 loss:        0.045176
Test - acc:         0.930000 loss:        0.262157
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.986320 loss:        0.043901
Test - acc:         0.929600 loss:        0.252459
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.988040 loss:        0.037473
Test - acc:         0.921500 loss:        0.287237
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.985460 loss:        0.043695
Test - acc:         0.929500 loss:        0.256508
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.986800 loss:        0.041521
Test - acc:         0.929000 loss:        0.264306
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.988760 loss:        0.035571
Test - acc:         0.929800 loss:        0.275892
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.986460 loss:        0.042708
Test - acc:         0.927600 loss:        0.258374
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.984520 loss:        0.045633
Test - acc:         0.935300 loss:        0.235639
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.986560 loss:        0.041010
Test - acc:         0.930700 loss:        0.257057
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.986180 loss:        0.043219
Test - acc:         0.932700 loss:        0.243058
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.987300 loss:        0.040358
Test - acc:         0.925400 loss:        0.280414
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.985960 loss:        0.042785
Test - acc:         0.923400 loss:        0.282678
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.985440 loss:        0.043150
Test - acc:         0.927500 loss:        0.261617
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.986040 loss:        0.041596
Test - acc:         0.929400 loss:        0.256036
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.987660 loss:        0.039707
Test - acc:         0.935600 loss:        0.231984
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.987220 loss:        0.039776
Test - acc:         0.928900 loss:        0.271310
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.986720 loss:        0.039708
Test - acc:         0.924900 loss:        0.268922
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.986120 loss:        0.042338
Test - acc:         0.923100 loss:        0.283797
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.986840 loss:        0.040348
Test - acc:         0.917300 loss:        0.308114
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.984920 loss:        0.044788
Test - acc:         0.923100 loss:        0.289025
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.984260 loss:        0.046859
Test - acc:         0.931200 loss:        0.253736
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.986520 loss:        0.040683
Test - acc:         0.923800 loss:        0.271816
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.986780 loss:        0.040794
Test - acc:         0.919000 loss:        0.286381
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.986440 loss:        0.041810
Test - acc:         0.919800 loss:        0.310478
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.990680 loss:        0.031695
Test - acc:         0.933900 loss:        0.239567
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.992660 loss:        0.025133
Test - acc:         0.933200 loss:        0.250877
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.993480 loss:        0.022176
Test - acc:         0.935300 loss:        0.234792
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.993440 loss:        0.020974
Test - acc:         0.928400 loss:        0.271642
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.991840 loss:        0.026453
Test - acc:         0.935200 loss:        0.250374
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.991320 loss:        0.027664
Test - acc:         0.935900 loss:        0.247715
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.992260 loss:        0.025140
Test - acc:         0.934300 loss:        0.241498
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.991280 loss:        0.027934
Test - acc:         0.931400 loss:        0.262976
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.992060 loss:        0.025175
Test - acc:         0.934600 loss:        0.253970
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.990140 loss:        0.030055
Test - acc:         0.928500 loss:        0.278849
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.991440 loss:        0.027152
Test - acc:         0.934600 loss:        0.257017
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.990940 loss:        0.027913
Test - acc:         0.928600 loss:        0.276810
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.992620 loss:        0.026596
Test - acc:         0.929400 loss:        0.268300
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.990380 loss:        0.029658
Test - acc:         0.930600 loss:        0.260494
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.990340 loss:        0.032209
Test - acc:         0.926200 loss:        0.281381
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.989960 loss:        0.030903
Test - acc:         0.930000 loss:        0.265217
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.995660 loss:        0.016122
Test - acc:         0.943900 loss:        0.214852
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.997920 loss:        0.009289
Test - acc:         0.945600 loss:        0.208111
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.998100 loss:        0.007888
Test - acc:         0.946600 loss:        0.207553
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.999160 loss:        0.005994
Test - acc:         0.946600 loss:        0.206619
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.005073
Test - acc:         0.947500 loss:        0.207886
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.999100 loss:        0.005184
Test - acc:         0.946700 loss:        0.206366
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.999180 loss:        0.004961
Test - acc:         0.947200 loss:        0.204841
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.004519
Test - acc:         0.947000 loss:        0.203461
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.999260 loss:        0.004551
Test - acc:         0.946800 loss:        0.205233
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.004084
Test - acc:         0.946700 loss:        0.202685
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.003696
Test - acc:         0.947500 loss:        0.204769
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.003917
Test - acc:         0.948200 loss:        0.201750
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.003610
Test - acc:         0.948100 loss:        0.203090
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.003486
Test - acc:         0.949000 loss:        0.203041
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.003217
Test - acc:         0.948700 loss:        0.202492
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.003522
Test - acc:         0.948100 loss:        0.203051
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.003340
Test - acc:         0.949600 loss:        0.203233
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.003387
Test - acc:         0.948000 loss:        0.202018
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.003093
Test - acc:         0.949000 loss:        0.199679
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.002870
Test - acc:         0.948400 loss:        0.199996
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.002727
Test - acc:         0.949000 loss:        0.199438
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003076
Test - acc:         0.949500 loss:        0.200551
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002822
Test - acc:         0.948600 loss:        0.200457
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002891
Test - acc:         0.948700 loss:        0.199491
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002642
Test - acc:         0.949500 loss:        0.199341
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002584
Test - acc:         0.948000 loss:        0.199951
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002588
Test - acc:         0.949100 loss:        0.201192
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.002808
Test - acc:         0.949700 loss:        0.198434
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002587
Test - acc:         0.949300 loss:        0.198103
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002529
Test - acc:         0.949300 loss:        0.200149
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002354
Test - acc:         0.948200 loss:        0.199118
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.002770
Test - acc:         0.948200 loss:        0.199003
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002589
Test - acc:         0.948900 loss:        0.197662
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002533
Test - acc:         0.949100 loss:        0.197198
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.002534
Test - acc:         0.949500 loss:        0.196173
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002513
Test - acc:         0.950400 loss:        0.195436
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002297
Test - acc:         0.950400 loss:        0.196441
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002213
Test - acc:         0.950300 loss:        0.195020
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002360
Test - acc:         0.949500 loss:        0.194811
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002319
Test - acc:         0.949900 loss:        0.194888
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002246
Test - acc:         0.949900 loss:        0.195508
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002243
Test - acc:         0.950100 loss:        0.194200
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002365
Test - acc:         0.950400 loss:        0.195577
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002337
Test - acc:         0.948700 loss:        0.197481
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002376
Test - acc:         0.949500 loss:        0.196463
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002224
Test - acc:         0.949700 loss:        0.197432
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002390
Test - acc:         0.950800 loss:        0.194144
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002190
Test - acc:         0.950900 loss:        0.192615
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002181
Test - acc:         0.950200 loss:        0.193520
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002182
Test - acc:         0.949600 loss:        0.195638
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002210
Test - acc:         0.949800 loss:        0.193079
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002275
Test - acc:         0.950500 loss:        0.192979
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002262
Test - acc:         0.949800 loss:        0.194022
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002178
Test - acc:         0.950000 loss:        0.194625
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002211
Test - acc:         0.950200 loss:        0.193064
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002146
Test - acc:         0.950500 loss:        0.193185
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002218
Test - acc:         0.950400 loss:        0.192717
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002108
Test - acc:         0.950200 loss:        0.193205
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002136
Test - acc:         0.950200 loss:        0.193855
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002186
Test - acc:         0.950400 loss:        0.193420
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002286
Test - acc:         0.950200 loss:        0.194188
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002187
Test - acc:         0.950500 loss:        0.192713
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002152
Test - acc:         0.950400 loss:        0.192002
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002097
Test - acc:         0.950800 loss:        0.192537
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002179
Test - acc:         0.950700 loss:        0.190900
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002106
Test - acc:         0.951100 loss:        0.190517
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002120
Test - acc:         0.951000 loss:        0.190817
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002289
Test - acc:         0.950500 loss:        0.191357
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002060
Test - acc:         0.950200 loss:        0.191919
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002102
Test - acc:         0.950400 loss:        0.191874
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002140
Test - acc:         0.950300 loss:        0.189204
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002283
Test - acc:         0.949700 loss:        0.192301
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002186
Test - acc:         0.951200 loss:        0.190170
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002052
Test - acc:         0.951600 loss:        0.189496
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002182
Test - acc:         0.950900 loss:        0.191083
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002193
Test - acc:         0.951200 loss:        0.188898
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002161
Test - acc:         0.950400 loss:        0.189833
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002062
Test - acc:         0.951200 loss:        0.188717
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002034
Test - acc:         0.950500 loss:        0.191131
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002067
Test - acc:         0.950900 loss:        0.189794
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002103
Test - acc:         0.950700 loss:        0.189771
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002207
Test - acc:         0.951000 loss:        0.191098
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002123
Test - acc:         0.951200 loss:        0.189670
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.001984
Test - acc:         0.951400 loss:        0.189494
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001983
Test - acc:         0.950700 loss:        0.189308
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002146
Test - acc:         0.951500 loss:        0.188651
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002168
Test - acc:         0.950900 loss:        0.190081
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002124
Test - acc:         0.951700 loss:        0.187995
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002134
Test - acc:         0.952500 loss:        0.186609
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002173
Test - acc:         0.952100 loss:        0.187919
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002049
Test - acc:         0.952600 loss:        0.187430
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002014
Test - acc:         0.951500 loss:        0.188087
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002166
Test - acc:         0.951700 loss:        0.186935
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002143
Test - acc:         0.951700 loss:        0.187513
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002090
Test - acc:         0.951900 loss:        0.187722
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002082
Test - acc:         0.953000 loss:        0.187455
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002033
Test - acc:         0.951800 loss:        0.186016
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002140
Test - acc:         0.952500 loss:        0.187159
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002025
Test - acc:         0.952400 loss:        0.186714
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002120
Test - acc:         0.951800 loss:        0.186691
Sparsity :          0.7500
Wdecay :        0.000500
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "magnitude",
    "prune_freq": 70,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf70_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.325920 loss:        1.847682
Test - acc:         0.433400 loss:        1.508943
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.514980 loss:        1.326493
Test - acc:         0.576200 loss:        1.166692
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.637620 loss:        1.009011
Test - acc:         0.606500 loss:        1.140515
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.717740 loss:        0.806052
Test - acc:         0.696100 loss:        0.896292
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.760000 loss:        0.691428
Test - acc:         0.742900 loss:        0.751321
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.787060 loss:        0.611490
Test - acc:         0.756600 loss:        0.710299
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.806060 loss:        0.564509
Test - acc:         0.769000 loss:        0.710991
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.817920 loss:        0.529806
Test - acc:         0.787400 loss:        0.621394
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.824420 loss:        0.509206
Test - acc:         0.772600 loss:        0.684377
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.833860 loss:        0.483304
Test - acc:         0.775600 loss:        0.667925
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.838920 loss:        0.469585
Test - acc:         0.767600 loss:        0.754009
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.844060 loss:        0.454590
Test - acc:         0.796900 loss:        0.618867
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.850060 loss:        0.440107
Test - acc:         0.763200 loss:        0.735417
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.851640 loss:        0.434035
Test - acc:         0.793900 loss:        0.660835
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.855280 loss:        0.423293
Test - acc:         0.832300 loss:        0.496892
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.858580 loss:        0.415450
Test - acc:         0.831400 loss:        0.510164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.859460 loss:        0.408462
Test - acc:         0.820900 loss:        0.522441
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.860880 loss:        0.400126
Test - acc:         0.834400 loss:        0.483465
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.864860 loss:        0.393520
Test - acc:         0.804400 loss:        0.618786
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.868280 loss:        0.385265
Test - acc:         0.807600 loss:        0.623200
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.869220 loss:        0.381626
Test - acc:         0.814300 loss:        0.590255
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870500 loss:        0.379530
Test - acc:         0.798800 loss:        0.641787
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.871620 loss:        0.377487
Test - acc:         0.811600 loss:        0.574784
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.875440 loss:        0.368226
Test - acc:         0.821200 loss:        0.538520
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.877080 loss:        0.364041
Test - acc:         0.827800 loss:        0.496110
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.874160 loss:        0.366042
Test - acc:         0.818700 loss:        0.529624
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.877320 loss:        0.360246
Test - acc:         0.830300 loss:        0.521060
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.875840 loss:        0.361626
Test - acc:         0.829000 loss:        0.518092
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.878240 loss:        0.352239
Test - acc:         0.821600 loss:        0.544881
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.876260 loss:        0.358935
Test - acc:         0.835600 loss:        0.488176
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.880360 loss:        0.350550
Test - acc:         0.844600 loss:        0.479743
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.880840 loss:        0.348789
Test - acc:         0.799400 loss:        0.643922
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.882680 loss:        0.344050
Test - acc:         0.856300 loss:        0.439701
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.881020 loss:        0.349389
Test - acc:         0.861500 loss:        0.419639
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.881940 loss:        0.344315
Test - acc:         0.810400 loss:        0.605949
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.882960 loss:        0.341227
Test - acc:         0.855700 loss:        0.442214
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.883060 loss:        0.340363
Test - acc:         0.845100 loss:        0.467937
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.882780 loss:        0.341310
Test - acc:         0.812600 loss:        0.630480
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.885200 loss:        0.338045
Test - acc:         0.834200 loss:        0.483156
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.885040 loss:        0.338235
Test - acc:         0.864300 loss:        0.393861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.882280 loss:        0.342935
Test - acc:         0.835700 loss:        0.519533
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.887920 loss:        0.329464
Test - acc:         0.842400 loss:        0.484434
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.887960 loss:        0.328591
Test - acc:         0.835800 loss:        0.475898
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.888020 loss:        0.327734
Test - acc:         0.835600 loss:        0.509635
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.885720 loss:        0.336958
Test - acc:         0.831700 loss:        0.497432
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.886360 loss:        0.334090
Test - acc:         0.843100 loss:        0.469549
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.888340 loss:        0.329442
Test - acc:         0.828700 loss:        0.518911
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.885980 loss:        0.331101
Test - acc:         0.818900 loss:        0.543475
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.887960 loss:        0.327193
Test - acc:         0.813600 loss:        0.590675
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.886560 loss:        0.332759
Test - acc:         0.844200 loss:        0.453877
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.888840 loss:        0.326604
Test - acc:         0.849000 loss:        0.455457
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.890940 loss:        0.324219
Test - acc:         0.866300 loss:        0.399992
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.890300 loss:        0.327441
Test - acc:         0.849900 loss:        0.477880
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.889060 loss:        0.325174
Test - acc:         0.853900 loss:        0.436648
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.888320 loss:        0.326879
Test - acc:         0.842000 loss:        0.473023
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.890220 loss:        0.324462
Test - acc:         0.765700 loss:        0.721840
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.889780 loss:        0.323409
Test - acc:         0.841300 loss:        0.492251
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.889440 loss:        0.323690
Test - acc:         0.836300 loss:        0.492271
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.890180 loss:        0.322190
Test - acc:         0.866900 loss:        0.402989
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.890920 loss:        0.321626
Test - acc:         0.854900 loss:        0.441502
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.891540 loss:        0.318806
Test - acc:         0.869200 loss:        0.409789
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.888320 loss:        0.325977
Test - acc:         0.809500 loss:        0.589435
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.891780 loss:        0.316914
Test - acc:         0.814400 loss:        0.548158
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.890480 loss:        0.321112
Test - acc:         0.824300 loss:        0.562069
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.890260 loss:        0.321836
Test - acc:         0.835700 loss:        0.484339
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.890780 loss:        0.322657
Test - acc:         0.837100 loss:        0.501047
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.890920 loss:        0.317687
Test - acc:         0.834500 loss:        0.482315
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.892680 loss:        0.316245
Test - acc:         0.802800 loss:        0.677386
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.891800 loss:        0.318122
Test - acc:         0.839700 loss:        0.505710
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.890020 loss:        0.322447
Test - acc:         0.847000 loss:        0.469302
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.910080 loss:        0.269629
Test - acc:         0.870700 loss:        0.385488
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.908540 loss:        0.268896
Test - acc:         0.862400 loss:        0.424273
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.908340 loss:        0.271888
Test - acc:         0.853300 loss:        0.450935
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.904200 loss:        0.277636
Test - acc:         0.864900 loss:        0.415966
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.905720 loss:        0.277065
Test - acc:         0.851700 loss:        0.472101
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.903120 loss:        0.282086
Test - acc:         0.860300 loss:        0.421282
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.905940 loss:        0.277519
Test - acc:         0.842900 loss:        0.494600
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.906240 loss:        0.274720
Test - acc:         0.856300 loss:        0.426001
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.905900 loss:        0.277478
Test - acc:         0.852300 loss:        0.430217
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.908480 loss:        0.273947
Test - acc:         0.855400 loss:        0.448409
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.907500 loss:        0.274838
Test - acc:         0.836900 loss:        0.505961
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.906720 loss:        0.273260
Test - acc:         0.838900 loss:        0.511375
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.907300 loss:        0.274252
Test - acc:         0.865000 loss:        0.427244
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.907800 loss:        0.271680
Test - acc:         0.836900 loss:        0.519544
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.903880 loss:        0.281171
Test - acc:         0.859000 loss:        0.427902
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.906240 loss:        0.273695
Test - acc:         0.873900 loss:        0.387407
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.908340 loss:        0.271419
Test - acc:         0.875400 loss:        0.375224
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.909540 loss:        0.269680
Test - acc:         0.819700 loss:        0.539275
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.904760 loss:        0.275412
Test - acc:         0.871000 loss:        0.399641
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.906480 loss:        0.270794
Test - acc:         0.855200 loss:        0.458944
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.905400 loss:        0.277007
Test - acc:         0.870700 loss:        0.392303
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.905680 loss:        0.276775
Test - acc:         0.839100 loss:        0.488701
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.905060 loss:        0.273720
Test - acc:         0.852000 loss:        0.475272
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.905140 loss:        0.277655
Test - acc:         0.864600 loss:        0.404226
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.907960 loss:        0.269389
Test - acc:         0.864400 loss:        0.414007
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.906820 loss:        0.275470
Test - acc:         0.859800 loss:        0.435886
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.905780 loss:        0.275831
Test - acc:         0.808300 loss:        0.659157
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.905100 loss:        0.275872
Test - acc:         0.843900 loss:        0.496854
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.905720 loss:        0.272065
Test - acc:         0.863400 loss:        0.407047
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.905520 loss:        0.273421
Test - acc:         0.844400 loss:        0.476647
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.907920 loss:        0.270086
Test - acc:         0.881200 loss:        0.373907
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.906740 loss:        0.273435
Test - acc:         0.848300 loss:        0.463406
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.905520 loss:        0.275133
Test - acc:         0.856200 loss:        0.433781
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.907340 loss:        0.272287
Test - acc:         0.849300 loss:        0.460139
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.907080 loss:        0.272825
Test - acc:         0.865200 loss:        0.413952
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.906620 loss:        0.276745
Test - acc:         0.846100 loss:        0.488385
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.908180 loss:        0.270574
Test - acc:         0.858200 loss:        0.447760
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.907340 loss:        0.268241
Test - acc:         0.862700 loss:        0.445240
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.906980 loss:        0.270142
Test - acc:         0.842200 loss:        0.488333
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.904240 loss:        0.276847
Test - acc:         0.846900 loss:        0.492650
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.907140 loss:        0.269216
Test - acc:         0.829300 loss:        0.513238
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.906420 loss:        0.273285
Test - acc:         0.865500 loss:        0.397593
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.904380 loss:        0.279363
Test - acc:         0.814000 loss:        0.614287
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.906520 loss:        0.271612
Test - acc:         0.858000 loss:        0.435704
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.907860 loss:        0.269835
Test - acc:         0.860300 loss:        0.418940
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.905440 loss:        0.274014
Test - acc:         0.845300 loss:        0.494068
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.907340 loss:        0.269917
Test - acc:         0.862600 loss:        0.439392
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.908780 loss:        0.267441
Test - acc:         0.842200 loss:        0.488905
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.906720 loss:        0.269046
Test - acc:         0.825200 loss:        0.530216
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.904580 loss:        0.277357
Test - acc:         0.862800 loss:        0.415180
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.909620 loss:        0.267988
Test - acc:         0.841400 loss:        0.526582
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.906420 loss:        0.273900
Test - acc:         0.836900 loss:        0.503737
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.906140 loss:        0.273403
Test - acc:         0.852300 loss:        0.444186
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.907340 loss:        0.269806
Test - acc:         0.867700 loss:        0.414710
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.908100 loss:        0.269584
Test - acc:         0.855000 loss:        0.463656
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.907020 loss:        0.274187
Test - acc:         0.858900 loss:        0.420227
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.909020 loss:        0.268613
Test - acc:         0.822800 loss:        0.582315
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.906580 loss:        0.271531
Test - acc:         0.739800 loss:        0.897886
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.906620 loss:        0.270465
Test - acc:         0.830400 loss:        0.533321
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.906160 loss:        0.272611
Test - acc:         0.866800 loss:        0.411808
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.908040 loss:        0.271152
Test - acc:         0.847500 loss:        0.465845
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.905880 loss:        0.273493
Test - acc:         0.841800 loss:        0.460811
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.906920 loss:        0.269890
Test - acc:         0.831200 loss:        0.522643
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.907660 loss:        0.268942
Test - acc:         0.827400 loss:        0.556873
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.905980 loss:        0.275233
Test - acc:         0.861300 loss:        0.424604
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.908020 loss:        0.269498
Test - acc:         0.826700 loss:        0.519633
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.905100 loss:        0.274617
Test - acc:         0.864100 loss:        0.409002
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.908740 loss:        0.268618
Test - acc:         0.870200 loss:        0.397642
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.905220 loss:        0.274952
Test - acc:         0.837600 loss:        0.505896
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.905280 loss:        0.276870
Test - acc:         0.832100 loss:        0.534504
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.920800 loss:        0.235723
Test - acc:         0.884700 loss:        0.370860
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.919720 loss:        0.233473
Test - acc:         0.850500 loss:        0.486675
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.917280 loss:        0.245488
Test - acc:         0.884500 loss:        0.343419
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.915800 loss:        0.245620
Test - acc:         0.881600 loss:        0.369350
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.914420 loss:        0.249494
Test - acc:         0.875800 loss:        0.378394
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.915520 loss:        0.248394
Test - acc:         0.874600 loss:        0.376717
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.916000 loss:        0.246293
Test - acc:         0.879300 loss:        0.367693
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.914340 loss:        0.248157
Test - acc:         0.881500 loss:        0.368653
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.915760 loss:        0.245927
Test - acc:         0.858500 loss:        0.441170
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.916020 loss:        0.247541
Test - acc:         0.861700 loss:        0.438197
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.958360 loss:        0.127055
Test - acc:         0.936000 loss:        0.190159
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.970700 loss:        0.089247
Test - acc:         0.938900 loss:        0.185874
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.974740 loss:        0.077386
Test - acc:         0.941800 loss:        0.180881
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.978720 loss:        0.065993
Test - acc:         0.940500 loss:        0.181925
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.981860 loss:        0.057528
Test - acc:         0.944200 loss:        0.178215
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.983240 loss:        0.053566
Test - acc:         0.941100 loss:        0.188767
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.984480 loss:        0.047636
Test - acc:         0.943100 loss:        0.184108
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.986900 loss:        0.042850
Test - acc:         0.941200 loss:        0.184973
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.987280 loss:        0.039714
Test - acc:         0.942000 loss:        0.191000
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.988620 loss:        0.036427
Test - acc:         0.943900 loss:        0.193448
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.988880 loss:        0.035187
Test - acc:         0.942900 loss:        0.189804
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.989640 loss:        0.032392
Test - acc:         0.941800 loss:        0.205848
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.990240 loss:        0.030539
Test - acc:         0.942200 loss:        0.196130
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.990780 loss:        0.029869
Test - acc:         0.943000 loss:        0.200270
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.991780 loss:        0.026459
Test - acc:         0.944400 loss:        0.197522
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.991660 loss:        0.027227
Test - acc:         0.941600 loss:        0.206656
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.991880 loss:        0.025622
Test - acc:         0.937800 loss:        0.219664
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.991020 loss:        0.027661
Test - acc:         0.940500 loss:        0.209496
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.992180 loss:        0.025914
Test - acc:         0.941000 loss:        0.214997
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.990880 loss:        0.027877
Test - acc:         0.941300 loss:        0.214861
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.992100 loss:        0.025719
Test - acc:         0.940500 loss:        0.210861
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.992100 loss:        0.024941
Test - acc:         0.942400 loss:        0.209410
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.992120 loss:        0.024328
Test - acc:         0.937700 loss:        0.222920
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.992880 loss:        0.024456
Test - acc:         0.942100 loss:        0.225492
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.991580 loss:        0.027063
Test - acc:         0.940700 loss:        0.221223
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.991720 loss:        0.026742
Test - acc:         0.939500 loss:        0.218280
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.991020 loss:        0.028829
Test - acc:         0.939400 loss:        0.218468
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.990780 loss:        0.029335
Test - acc:         0.937700 loss:        0.225737
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.990540 loss:        0.029301
Test - acc:         0.937200 loss:        0.223778
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.990280 loss:        0.030401
Test - acc:         0.935700 loss:        0.233716
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.991180 loss:        0.028394
Test - acc:         0.935900 loss:        0.235460
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.989180 loss:        0.032973
Test - acc:         0.928300 loss:        0.255277
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.989400 loss:        0.032642
Test - acc:         0.934700 loss:        0.234820
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.988640 loss:        0.035174
Test - acc:         0.934600 loss:        0.241848
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.988860 loss:        0.034929
Test - acc:         0.929400 loss:        0.250109
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.987260 loss:        0.039236
Test - acc:         0.933400 loss:        0.247652
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.987800 loss:        0.037071
Test - acc:         0.931900 loss:        0.240701
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.989280 loss:        0.035466
Test - acc:         0.933700 loss:        0.230938
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.988160 loss:        0.035858
Test - acc:         0.929800 loss:        0.259361
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.987880 loss:        0.037007
Test - acc:         0.928200 loss:        0.257574
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.987980 loss:        0.037328
Test - acc:         0.937100 loss:        0.232035
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.986760 loss:        0.039077
Test - acc:         0.932600 loss:        0.243756
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.986320 loss:        0.041566
Test - acc:         0.932700 loss:        0.241142
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.986640 loss:        0.040787
Test - acc:         0.932700 loss:        0.244431
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.986440 loss:        0.041395
Test - acc:         0.933300 loss:        0.240075
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.987860 loss:        0.039359
Test - acc:         0.931600 loss:        0.247118
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.987180 loss:        0.038184
Test - acc:         0.927100 loss:        0.267819
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.986680 loss:        0.040761
Test - acc:         0.918100 loss:        0.299684
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.986500 loss:        0.041720
Test - acc:         0.933200 loss:        0.237020
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.987840 loss:        0.038061
Test - acc:         0.926700 loss:        0.262777
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.987220 loss:        0.040195
Test - acc:         0.925600 loss:        0.285553
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.986720 loss:        0.041867
Test - acc:         0.929000 loss:        0.258080
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.986760 loss:        0.041290
Test - acc:         0.928800 loss:        0.261316
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.986940 loss:        0.040768
Test - acc:         0.924400 loss:        0.273201
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.986840 loss:        0.040663
Test - acc:         0.930300 loss:        0.242090
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.985600 loss:        0.043161
Test - acc:         0.930900 loss:        0.234126
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.984780 loss:        0.046412
Test - acc:         0.927700 loss:        0.274206
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.986760 loss:        0.040396
Test - acc:         0.927300 loss:        0.265685
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.986940 loss:        0.040256
Test - acc:         0.930900 loss:        0.262622
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.986660 loss:        0.041249
Test - acc:         0.925900 loss:        0.270307
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.987040 loss:        0.044485
Test - acc:         0.936700 loss:        0.216218
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.991140 loss:        0.032400
Test - acc:         0.936200 loss:        0.236245
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.990960 loss:        0.030638
Test - acc:         0.929700 loss:        0.259757
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.990120 loss:        0.031897
Test - acc:         0.932200 loss:        0.253379
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.990320 loss:        0.031251
Test - acc:         0.933200 loss:        0.259262
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.990480 loss:        0.030309
Test - acc:         0.929900 loss:        0.263930
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.988460 loss:        0.036300
Test - acc:         0.926700 loss:        0.280116
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.989300 loss:        0.033789
Test - acc:         0.930100 loss:        0.268434
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.988900 loss:        0.034766
Test - acc:         0.930900 loss:        0.258939
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.988960 loss:        0.035331
Test - acc:         0.932100 loss:        0.249089
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.989820 loss:        0.032203
Test - acc:         0.933000 loss:        0.253899
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.988000 loss:        0.035712
Test - acc:         0.929500 loss:        0.278111
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.987460 loss:        0.038658
Test - acc:         0.931500 loss:        0.256902
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.989440 loss:        0.033567
Test - acc:         0.933300 loss:        0.259646
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.989160 loss:        0.032993
Test - acc:         0.933200 loss:        0.252672
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.989240 loss:        0.033958
Test - acc:         0.930100 loss:        0.266176
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.988560 loss:        0.036125
Test - acc:         0.928000 loss:        0.272048
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.988160 loss:        0.035681
Test - acc:         0.921700 loss:        0.289395
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.986680 loss:        0.039778
Test - acc:         0.928200 loss:        0.257706
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.989240 loss:        0.034779
Test - acc:         0.924900 loss:        0.276084
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.987260 loss:        0.037603
Test - acc:         0.923300 loss:        0.285954
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.988120 loss:        0.036431
Test - acc:         0.924700 loss:        0.287462
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.987180 loss:        0.038496
Test - acc:         0.925100 loss:        0.290977
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.988260 loss:        0.037449
Test - acc:         0.936000 loss:        0.245392
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.988420 loss:        0.035813
Test - acc:         0.931800 loss:        0.257799
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.987140 loss:        0.039444
Test - acc:         0.926100 loss:        0.275072
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.987940 loss:        0.037801
Test - acc:         0.935200 loss:        0.242523
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.988400 loss:        0.036594
Test - acc:         0.930700 loss:        0.263628
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.988800 loss:        0.035755
Test - acc:         0.931100 loss:        0.261879
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.987760 loss:        0.038007
Test - acc:         0.927700 loss:        0.271753
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.988300 loss:        0.036376
Test - acc:         0.930000 loss:        0.262955
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.988580 loss:        0.036361
Test - acc:         0.926700 loss:        0.282083
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.988220 loss:        0.035942
Test - acc:         0.921100 loss:        0.285879
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.989700 loss:        0.032926
Test - acc:         0.926400 loss:        0.281810
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.987460 loss:        0.038992
Test - acc:         0.927100 loss:        0.281694
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.988020 loss:        0.036682
Test - acc:         0.920600 loss:        0.298765
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.990240 loss:        0.031847
Test - acc:         0.929300 loss:        0.286048
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.987120 loss:        0.037794
Test - acc:         0.923100 loss:        0.284076
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.987640 loss:        0.038027
Test - acc:         0.929900 loss:        0.258237
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.989820 loss:        0.032929
Test - acc:         0.927100 loss:        0.268610
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.994440 loss:        0.019444
Test - acc:         0.940500 loss:        0.215328
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.997440 loss:        0.010982
Test - acc:         0.942800 loss:        0.210745
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.997960 loss:        0.009259
Test - acc:         0.945100 loss:        0.210432
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.998320 loss:        0.008422
Test - acc:         0.944700 loss:        0.209163
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.998660 loss:        0.007002
Test - acc:         0.944500 loss:        0.208277
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.999180 loss:        0.005992
Test - acc:         0.946300 loss:        0.207818
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.999100 loss:        0.006079
Test - acc:         0.945100 loss:        0.209184
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.005539
Test - acc:         0.945100 loss:        0.209060
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.999100 loss:        0.005450
Test - acc:         0.945500 loss:        0.209783
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.004821
Test - acc:         0.945500 loss:        0.209030
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.999280 loss:        0.005209
Test - acc:         0.946000 loss:        0.208501
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.999180 loss:        0.005143
Test - acc:         0.947900 loss:        0.210145
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.004588
Test - acc:         0.947100 loss:        0.208250
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.004366
Test - acc:         0.947500 loss:        0.208195
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.004232
Test - acc:         0.946700 loss:        0.209561
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.999380 loss:        0.004373
Test - acc:         0.947900 loss:        0.208762
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.004131
Test - acc:         0.948300 loss:        0.206412
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.004152
Test - acc:         0.946900 loss:        0.206095
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.004068
Test - acc:         0.948200 loss:        0.206824
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.003918
Test - acc:         0.948500 loss:        0.205837
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.003495
Test - acc:         0.948000 loss:        0.205906
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.003886
Test - acc:         0.948300 loss:        0.206343
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.003513
Test - acc:         0.947700 loss:        0.205454
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.003864
Test - acc:         0.947100 loss:        0.205711
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.003450
Test - acc:         0.948200 loss:        0.207012
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.003382
Test - acc:         0.948500 loss:        0.206903
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003460
Test - acc:         0.947500 loss:        0.206669
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.003476
Test - acc:         0.948800 loss:        0.205290
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.003305
Test - acc:         0.947100 loss:        0.205921
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.003519
Test - acc:         0.947800 loss:        0.209691
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.990780 loss:        0.043328
Test - acc:         0.936100 loss:        0.219752
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.995540 loss:        0.028569
Test - acc:         0.935500 loss:        0.214804
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.996580 loss:        0.022730
Test - acc:         0.938300 loss:        0.212067
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.996900 loss:        0.020213
Test - acc:         0.937800 loss:        0.212151
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.997000 loss:        0.019361
Test - acc:         0.937300 loss:        0.216754
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.997660 loss:        0.016633
Test - acc:         0.939700 loss:        0.215785
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.997660 loss:        0.015614
Test - acc:         0.938700 loss:        0.214698
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.998080 loss:        0.014040
Test - acc:         0.940100 loss:        0.213839
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.998200 loss:        0.013708
Test - acc:         0.941200 loss:        0.211873
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.997980 loss:        0.013515
Test - acc:         0.941300 loss:        0.214886
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.998520 loss:        0.012132
Test - acc:         0.941000 loss:        0.215030
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.998320 loss:        0.011458
Test - acc:         0.942300 loss:        0.212462
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.998540 loss:        0.010991
Test - acc:         0.940500 loss:        0.214608
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.998380 loss:        0.010632
Test - acc:         0.941000 loss:        0.216261
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.998700 loss:        0.010838
Test - acc:         0.942700 loss:        0.215577
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.998740 loss:        0.009352
Test - acc:         0.941700 loss:        0.220123
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.998700 loss:        0.009699
Test - acc:         0.944100 loss:        0.216128
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.998920 loss:        0.009067
Test - acc:         0.944000 loss:        0.214229
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.998860 loss:        0.009034
Test - acc:         0.943600 loss:        0.215216
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.998780 loss:        0.008723
Test - acc:         0.942300 loss:        0.217340
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.998880 loss:        0.008462
Test - acc:         0.941000 loss:        0.217024
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.998820 loss:        0.008103
Test - acc:         0.941600 loss:        0.219365
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.008053
Test - acc:         0.941800 loss:        0.219760
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999000 loss:        0.007807
Test - acc:         0.942100 loss:        0.218333
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.998980 loss:        0.007615
Test - acc:         0.940700 loss:        0.221812
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999100 loss:        0.007402
Test - acc:         0.941800 loss:        0.220030
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999140 loss:        0.007531
Test - acc:         0.941800 loss:        0.219705
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.006624
Test - acc:         0.942300 loss:        0.220065
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999140 loss:        0.006939
Test - acc:         0.941500 loss:        0.222739
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.006291
Test - acc:         0.942900 loss:        0.219533
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999260 loss:        0.006454
Test - acc:         0.941600 loss:        0.222818
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.006222
Test - acc:         0.942900 loss:        0.219601
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.006265
Test - acc:         0.942300 loss:        0.222554
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999260 loss:        0.006203
Test - acc:         0.943600 loss:        0.217669
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999280 loss:        0.006364
Test - acc:         0.943100 loss:        0.218342
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999280 loss:        0.006031
Test - acc:         0.943500 loss:        0.218835
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.999140 loss:        0.006131
Test - acc:         0.942000 loss:        0.219453
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.006087
Test - acc:         0.942100 loss:        0.223467
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999160 loss:        0.005938
Test - acc:         0.942800 loss:        0.220871
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.005682
Test - acc:         0.943100 loss:        0.225412
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.005368
Test - acc:         0.944400 loss:        0.220881
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.005492
Test - acc:         0.943900 loss:        0.222239
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.999380 loss:        0.005399
Test - acc:         0.944200 loss:        0.220249
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.005349
Test - acc:         0.944600 loss:        0.220277
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999380 loss:        0.005450
Test - acc:         0.944900 loss:        0.222650
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.005400
Test - acc:         0.943700 loss:        0.221469
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.005260
Test - acc:         0.944400 loss:        0.225167
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.004778
Test - acc:         0.944000 loss:        0.222321
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.004963
Test - acc:         0.944700 loss:        0.224668
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.005028
Test - acc:         0.943900 loss:        0.225360
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.004761
Test - acc:         0.944500 loss:        0.223321
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.004891
Test - acc:         0.943700 loss:        0.225716
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.005008
Test - acc:         0.944400 loss:        0.224353
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.004430
Test - acc:         0.942700 loss:        0.225481
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.004181
Test - acc:         0.943200 loss:        0.223643
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.004685
Test - acc:         0.944400 loss:        0.224741
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.004752
Test - acc:         0.942600 loss:        0.224545
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.004611
Test - acc:         0.945200 loss:        0.224123
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.004618
Test - acc:         0.945400 loss:        0.218322
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.004381
Test - acc:         0.943600 loss:        0.221447
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.004616
Test - acc:         0.944300 loss:        0.220518
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.004349
Test - acc:         0.944200 loss:        0.221976
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.004468
Test - acc:         0.944400 loss:        0.222611
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.004464
Test - acc:         0.944300 loss:        0.219994
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.004006
Test - acc:         0.945300 loss:        0.220717
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004367
Test - acc:         0.944500 loss:        0.220708
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.004029
Test - acc:         0.944800 loss:        0.220603
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003972
Test - acc:         0.944500 loss:        0.221775
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.004382
Test - acc:         0.944200 loss:        0.220254
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.004389
Test - acc:         0.945500 loss:        0.220028
Sparsity :          0.9375
Wdecay :        0.000500
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "magnitude",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf50_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.325920 loss:        1.847682
Test - acc:         0.433400 loss:        1.508943
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.514980 loss:        1.326493
Test - acc:         0.576200 loss:        1.166692
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.637620 loss:        1.009011
Test - acc:         0.606500 loss:        1.140515
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.717740 loss:        0.806052
Test - acc:         0.696100 loss:        0.896292
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.760000 loss:        0.691428
Test - acc:         0.742900 loss:        0.751321
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.787060 loss:        0.611490
Test - acc:         0.756600 loss:        0.710299
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.806060 loss:        0.564509
Test - acc:         0.769000 loss:        0.710991
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.817920 loss:        0.529806
Test - acc:         0.787400 loss:        0.621394
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.824420 loss:        0.509206
Test - acc:         0.772600 loss:        0.684377
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.833860 loss:        0.483304
Test - acc:         0.775600 loss:        0.667925
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.838920 loss:        0.469585
Test - acc:         0.767600 loss:        0.754009
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.844060 loss:        0.454590
Test - acc:         0.796900 loss:        0.618867
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.850060 loss:        0.440107
Test - acc:         0.763200 loss:        0.735417
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.851640 loss:        0.434035
Test - acc:         0.793900 loss:        0.660835
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.855280 loss:        0.423293
Test - acc:         0.832300 loss:        0.496892
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.858580 loss:        0.415450
Test - acc:         0.831400 loss:        0.510164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.859460 loss:        0.408462
Test - acc:         0.820900 loss:        0.522441
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.860880 loss:        0.400126
Test - acc:         0.834400 loss:        0.483465
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.864860 loss:        0.393520
Test - acc:         0.804400 loss:        0.618786
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.868280 loss:        0.385265
Test - acc:         0.807600 loss:        0.623200
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.869220 loss:        0.381626
Test - acc:         0.814300 loss:        0.590255
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870500 loss:        0.379530
Test - acc:         0.798800 loss:        0.641787
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.871620 loss:        0.377487
Test - acc:         0.811600 loss:        0.574784
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.875440 loss:        0.368226
Test - acc:         0.821200 loss:        0.538520
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.877080 loss:        0.364041
Test - acc:         0.827800 loss:        0.496110
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.874160 loss:        0.366042
Test - acc:         0.818700 loss:        0.529624
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.877320 loss:        0.360246
Test - acc:         0.830300 loss:        0.521060
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.875840 loss:        0.361626
Test - acc:         0.829000 loss:        0.518092
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.878240 loss:        0.352239
Test - acc:         0.821600 loss:        0.544881
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.876260 loss:        0.358935
Test - acc:         0.835600 loss:        0.488176
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.880360 loss:        0.350550
Test - acc:         0.844600 loss:        0.479743
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.880840 loss:        0.348789
Test - acc:         0.799400 loss:        0.643922
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.882680 loss:        0.344050
Test - acc:         0.856300 loss:        0.439701
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.881020 loss:        0.349389
Test - acc:         0.861500 loss:        0.419639
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.881940 loss:        0.344315
Test - acc:         0.810400 loss:        0.605949
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.882960 loss:        0.341227
Test - acc:         0.855700 loss:        0.442214
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.883060 loss:        0.340363
Test - acc:         0.845100 loss:        0.467937
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.882780 loss:        0.341310
Test - acc:         0.812600 loss:        0.630480
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.885200 loss:        0.338045
Test - acc:         0.834200 loss:        0.483156
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.885040 loss:        0.338235
Test - acc:         0.864300 loss:        0.393861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.882280 loss:        0.342935
Test - acc:         0.835700 loss:        0.519533
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.887920 loss:        0.329464
Test - acc:         0.842400 loss:        0.484434
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.887960 loss:        0.328591
Test - acc:         0.835800 loss:        0.475898
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.888020 loss:        0.327734
Test - acc:         0.835600 loss:        0.509635
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.885720 loss:        0.336958
Test - acc:         0.831700 loss:        0.497432
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.886360 loss:        0.334090
Test - acc:         0.843100 loss:        0.469549
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.888340 loss:        0.329442
Test - acc:         0.828700 loss:        0.518911
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.885980 loss:        0.331101
Test - acc:         0.818900 loss:        0.543475
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.887960 loss:        0.327193
Test - acc:         0.813600 loss:        0.590675
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.886560 loss:        0.332759
Test - acc:         0.844200 loss:        0.453877
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.907020 loss:        0.274932
Test - acc:         0.839400 loss:        0.513190
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.904420 loss:        0.280728
Test - acc:         0.854500 loss:        0.424551
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.904500 loss:        0.280921
Test - acc:         0.864600 loss:        0.407203
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.901860 loss:        0.288437
Test - acc:         0.849200 loss:        0.457191
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.900660 loss:        0.288683
Test - acc:         0.852200 loss:        0.453717
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.902040 loss:        0.287156
Test - acc:         0.818500 loss:        0.578802
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.900720 loss:        0.287087
Test - acc:         0.854200 loss:        0.430760
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.902900 loss:        0.283281
Test - acc:         0.756100 loss:        0.816017
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.904940 loss:        0.280001
Test - acc:         0.866900 loss:        0.389249
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.901660 loss:        0.287212
Test - acc:         0.855300 loss:        0.428982
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.904100 loss:        0.283115
Test - acc:         0.882000 loss:        0.355066
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.903500 loss:        0.285760
Test - acc:         0.792500 loss:        0.698368
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.902660 loss:        0.282588
Test - acc:         0.853500 loss:        0.443402
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.906160 loss:        0.279417
Test - acc:         0.819100 loss:        0.605245
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.902300 loss:        0.285271
Test - acc:         0.849500 loss:        0.458071
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.905860 loss:        0.277859
Test - acc:         0.840600 loss:        0.483993
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.903820 loss:        0.281738
Test - acc:         0.863700 loss:        0.415153
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.902540 loss:        0.282678
Test - acc:         0.855700 loss:        0.428401
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.904540 loss:        0.279292
Test - acc:         0.853500 loss:        0.468766
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.904000 loss:        0.280133
Test - acc:         0.846600 loss:        0.468406
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.904620 loss:        0.280621
Test - acc:         0.853100 loss:        0.456102
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.902380 loss:        0.282521
Test - acc:         0.839300 loss:        0.476182
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.905380 loss:        0.278287
Test - acc:         0.859800 loss:        0.429629
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.904240 loss:        0.279626
Test - acc:         0.829600 loss:        0.512573
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.905900 loss:        0.274159
Test - acc:         0.780800 loss:        0.715817
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.904520 loss:        0.277750
Test - acc:         0.863600 loss:        0.405060
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.902480 loss:        0.283443
Test - acc:         0.851100 loss:        0.446317
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.906880 loss:        0.271878
Test - acc:         0.820000 loss:        0.543497
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.903100 loss:        0.281695
Test - acc:         0.822000 loss:        0.542785
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.905520 loss:        0.275152
Test - acc:         0.872700 loss:        0.401387
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.904220 loss:        0.282039
Test - acc:         0.873000 loss:        0.397177
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.906560 loss:        0.270490
Test - acc:         0.878800 loss:        0.369067
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.906120 loss:        0.276150
Test - acc:         0.843700 loss:        0.505500
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.905660 loss:        0.275659
Test - acc:         0.848000 loss:        0.484293
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.904820 loss:        0.279573
Test - acc:         0.831900 loss:        0.486625
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.904120 loss:        0.279917
Test - acc:         0.847800 loss:        0.472046
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.904460 loss:        0.275761
Test - acc:         0.866700 loss:        0.397596
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.906960 loss:        0.274543
Test - acc:         0.856100 loss:        0.433434
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.905040 loss:        0.277326
Test - acc:         0.851200 loss:        0.461939
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.904780 loss:        0.279537
Test - acc:         0.856100 loss:        0.444259
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.903840 loss:        0.278173
Test - acc:         0.859700 loss:        0.425858
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.902240 loss:        0.283893
Test - acc:         0.865200 loss:        0.405198
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.905940 loss:        0.272831
Test - acc:         0.817300 loss:        0.574123
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.902060 loss:        0.284164
Test - acc:         0.872300 loss:        0.385388
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.902660 loss:        0.281571
Test - acc:         0.862200 loss:        0.430267
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.903620 loss:        0.280629
Test - acc:         0.825500 loss:        0.558231
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.905220 loss:        0.279104
Test - acc:         0.851800 loss:        0.465749
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.903720 loss:        0.278424
Test - acc:         0.856500 loss:        0.448709
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.905180 loss:        0.273777
Test - acc:         0.859600 loss:        0.441524
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.904380 loss:        0.281988
Test - acc:         0.841300 loss:        0.496771
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.919580 loss:        0.236544
Test - acc:         0.879300 loss:        0.364033
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.915940 loss:        0.245675
Test - acc:         0.858300 loss:        0.426265
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.915220 loss:        0.249432
Test - acc:         0.874700 loss:        0.370558
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.915180 loss:        0.250538
Test - acc:         0.868900 loss:        0.428361
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.912880 loss:        0.251149
Test - acc:         0.877200 loss:        0.373519
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.911600 loss:        0.256748
Test - acc:         0.850100 loss:        0.469239
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.913560 loss:        0.251124
Test - acc:         0.840300 loss:        0.531003
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.914760 loss:        0.247574
Test - acc:         0.824200 loss:        0.568282
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.914920 loss:        0.248737
Test - acc:         0.862200 loss:        0.437299
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.910540 loss:        0.258445
Test - acc:         0.846600 loss:        0.474051
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.914200 loss:        0.248756
Test - acc:         0.824400 loss:        0.531878
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.912700 loss:        0.252687
Test - acc:         0.873800 loss:        0.373418
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.913120 loss:        0.252205
Test - acc:         0.865200 loss:        0.418683
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.915420 loss:        0.248916
Test - acc:         0.882900 loss:        0.345916
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.914780 loss:        0.249999
Test - acc:         0.857400 loss:        0.434158
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.914960 loss:        0.250725
Test - acc:         0.872700 loss:        0.388929
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.914820 loss:        0.248466
Test - acc:         0.885700 loss:        0.348520
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.914360 loss:        0.248684
Test - acc:         0.868100 loss:        0.412692
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.915260 loss:        0.248949
Test - acc:         0.828200 loss:        0.553482
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.913740 loss:        0.252338
Test - acc:         0.879100 loss:        0.359146
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.914820 loss:        0.250391
Test - acc:         0.875300 loss:        0.364773
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.914200 loss:        0.249969
Test - acc:         0.880900 loss:        0.354075
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.914560 loss:        0.248460
Test - acc:         0.867800 loss:        0.401256
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.913840 loss:        0.250950
Test - acc:         0.836500 loss:        0.535264
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.916320 loss:        0.246798
Test - acc:         0.786200 loss:        0.732157
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.913920 loss:        0.249430
Test - acc:         0.862000 loss:        0.408253
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.913960 loss:        0.249282
Test - acc:         0.863600 loss:        0.415137
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.912740 loss:        0.251345
Test - acc:         0.852900 loss:        0.451848
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.914080 loss:        0.250159
Test - acc:         0.863800 loss:        0.412598
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.914460 loss:        0.250843
Test - acc:         0.860700 loss:        0.425186
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.913960 loss:        0.250430
Test - acc:         0.870800 loss:        0.395372
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.913980 loss:        0.249001
Test - acc:         0.857700 loss:        0.424005
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.913180 loss:        0.251298
Test - acc:         0.846900 loss:        0.462053
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.915440 loss:        0.247465
Test - acc:         0.852800 loss:        0.449699
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.911600 loss:        0.252201
Test - acc:         0.863100 loss:        0.427765
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.915680 loss:        0.246024
Test - acc:         0.834100 loss:        0.537236
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.914640 loss:        0.248134
Test - acc:         0.848400 loss:        0.483480
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.913860 loss:        0.249915
Test - acc:         0.815400 loss:        0.577840
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.911940 loss:        0.251769
Test - acc:         0.853600 loss:        0.450603
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.911240 loss:        0.254149
Test - acc:         0.835700 loss:        0.517143
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.913800 loss:        0.249590
Test - acc:         0.879900 loss:        0.365109
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.913900 loss:        0.247849
Test - acc:         0.880500 loss:        0.374816
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.914000 loss:        0.249637
Test - acc:         0.865600 loss:        0.405038
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.913240 loss:        0.250407
Test - acc:         0.831400 loss:        0.520795
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.916260 loss:        0.246700
Test - acc:         0.864100 loss:        0.421051
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.913640 loss:        0.252717
Test - acc:         0.871400 loss:        0.392250
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.914000 loss:        0.250310
Test - acc:         0.875000 loss:        0.376821
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.916400 loss:        0.247456
Test - acc:         0.872400 loss:        0.388493
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.913680 loss:        0.248187
Test - acc:         0.881000 loss:        0.354711
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.914200 loss:        0.248020
Test - acc:         0.850900 loss:        0.471191
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.947000 loss:        0.166981
Test - acc:         0.928400 loss:        0.207794
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.964320 loss:        0.113052
Test - acc:         0.931700 loss:        0.198126
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.968860 loss:        0.096930
Test - acc:         0.936100 loss:        0.192358
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.972120 loss:        0.083576
Test - acc:         0.935600 loss:        0.194169
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.975760 loss:        0.076494
Test - acc:         0.936500 loss:        0.197051
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.977900 loss:        0.069592
Test - acc:         0.937200 loss:        0.193925
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.980440 loss:        0.062294
Test - acc:         0.937900 loss:        0.194472
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.059998
Test - acc:         0.937100 loss:        0.192712
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.981660 loss:        0.056730
Test - acc:         0.936200 loss:        0.202624
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.984720 loss:        0.050005
Test - acc:         0.936500 loss:        0.202328
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.984260 loss:        0.048575
Test - acc:         0.938800 loss:        0.200199
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.984800 loss:        0.046762
Test - acc:         0.939000 loss:        0.207319
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.987020 loss:        0.041192
Test - acc:         0.936900 loss:        0.211375
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.987240 loss:        0.039247
Test - acc:         0.936700 loss:        0.213301
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.987880 loss:        0.037760
Test - acc:         0.939400 loss:        0.211588
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.988320 loss:        0.037286
Test - acc:         0.937400 loss:        0.210768
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.988240 loss:        0.036243
Test - acc:         0.936700 loss:        0.212088
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.988720 loss:        0.034377
Test - acc:         0.935800 loss:        0.224800
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.989400 loss:        0.033860
Test - acc:         0.936300 loss:        0.217041
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.989580 loss:        0.033840
Test - acc:         0.936200 loss:        0.223552
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.990000 loss:        0.031037
Test - acc:         0.936700 loss:        0.223638
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.989120 loss:        0.033052
Test - acc:         0.938800 loss:        0.216971
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.989440 loss:        0.032709
Test - acc:         0.931800 loss:        0.248457
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.990480 loss:        0.030134
Test - acc:         0.937900 loss:        0.228610
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.989800 loss:        0.031401
Test - acc:         0.935300 loss:        0.234448
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.989840 loss:        0.031466
Test - acc:         0.935900 loss:        0.231727
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.989780 loss:        0.031419
Test - acc:         0.934200 loss:        0.233268
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.990060 loss:        0.030187
Test - acc:         0.934300 loss:        0.229533
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.989460 loss:        0.033620
Test - acc:         0.931800 loss:        0.252152
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.989200 loss:        0.032872
Test - acc:         0.937200 loss:        0.232197
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.989040 loss:        0.033393
Test - acc:         0.935000 loss:        0.232218
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.989020 loss:        0.033977
Test - acc:         0.927100 loss:        0.268206
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.988620 loss:        0.035171
Test - acc:         0.932400 loss:        0.251935
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.988000 loss:        0.035839
Test - acc:         0.929100 loss:        0.273946
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.987700 loss:        0.038051
Test - acc:         0.934600 loss:        0.236294
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.987680 loss:        0.037175
Test - acc:         0.936700 loss:        0.234290
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.988600 loss:        0.035591
Test - acc:         0.923800 loss:        0.281670
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.987740 loss:        0.037708
Test - acc:         0.936900 loss:        0.233893
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.987300 loss:        0.038954
Test - acc:         0.934000 loss:        0.237311
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.986800 loss:        0.040213
Test - acc:         0.925300 loss:        0.288170
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.986100 loss:        0.041796
Test - acc:         0.935500 loss:        0.221324
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.986800 loss:        0.039767
Test - acc:         0.929800 loss:        0.255445
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.987260 loss:        0.040061
Test - acc:         0.926900 loss:        0.256210
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.986740 loss:        0.041688
Test - acc:         0.927300 loss:        0.259740
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.987880 loss:        0.037945
Test - acc:         0.931300 loss:        0.250085
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.985620 loss:        0.043649
Test - acc:         0.934500 loss:        0.228164
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.987060 loss:        0.039623
Test - acc:         0.930200 loss:        0.240636
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.986580 loss:        0.040479
Test - acc:         0.925400 loss:        0.267796
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.985400 loss:        0.042755
Test - acc:         0.931800 loss:        0.244174
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.984380 loss:        0.046709
Test - acc:         0.921400 loss:        0.285231
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.975700 loss:        0.077432
Test - acc:         0.922000 loss:        0.262400
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.982500 loss:        0.057054
Test - acc:         0.922900 loss:        0.271885
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.983760 loss:        0.050969
Test - acc:         0.919800 loss:        0.291472
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.982500 loss:        0.052978
Test - acc:         0.926500 loss:        0.259131
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.983300 loss:        0.049907
Test - acc:         0.929800 loss:        0.244851
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.983300 loss:        0.051099
Test - acc:         0.923300 loss:        0.275831
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.982520 loss:        0.053323
Test - acc:         0.923600 loss:        0.268465
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.984080 loss:        0.049786
Test - acc:         0.923800 loss:        0.283815
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.985540 loss:        0.045300
Test - acc:         0.929400 loss:        0.261069
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.984440 loss:        0.047091
Test - acc:         0.925300 loss:        0.280262
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.983240 loss:        0.050500
Test - acc:         0.926500 loss:        0.264253
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.983760 loss:        0.049202
Test - acc:         0.932600 loss:        0.252053
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.985160 loss:        0.046143
Test - acc:         0.928700 loss:        0.264264
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.984940 loss:        0.047750
Test - acc:         0.921800 loss:        0.281342
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.984380 loss:        0.047783
Test - acc:         0.925200 loss:        0.265167
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.983920 loss:        0.048830
Test - acc:         0.919400 loss:        0.287370
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.984640 loss:        0.047903
Test - acc:         0.929100 loss:        0.256712
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.986240 loss:        0.041369
Test - acc:         0.928000 loss:        0.262087
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.984780 loss:        0.046774
Test - acc:         0.929000 loss:        0.270427
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.985440 loss:        0.044810
Test - acc:         0.924500 loss:        0.288266
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.985220 loss:        0.046045
Test - acc:         0.931900 loss:        0.260422
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.985460 loss:        0.044753
Test - acc:         0.928000 loss:        0.258444
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.984780 loss:        0.046221
Test - acc:         0.925800 loss:        0.268916
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.984160 loss:        0.047257
Test - acc:         0.917000 loss:        0.296073
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.984120 loss:        0.047265
Test - acc:         0.926100 loss:        0.273812
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.985060 loss:        0.046194
Test - acc:         0.931100 loss:        0.264937
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.984920 loss:        0.045068
Test - acc:         0.929000 loss:        0.259245
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.984580 loss:        0.045233
Test - acc:         0.930000 loss:        0.256994
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.985220 loss:        0.044764
Test - acc:         0.930500 loss:        0.257851
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.985660 loss:        0.043492
Test - acc:         0.926100 loss:        0.265051
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.985080 loss:        0.044967
Test - acc:         0.929300 loss:        0.263306
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.986180 loss:        0.043277
Test - acc:         0.918200 loss:        0.306724
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.984140 loss:        0.047266
Test - acc:         0.927800 loss:        0.254766
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.984640 loss:        0.046782
Test - acc:         0.929700 loss:        0.261927
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.985640 loss:        0.043327
Test - acc:         0.927200 loss:        0.287395
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.984920 loss:        0.045783
Test - acc:         0.928200 loss:        0.282805
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.986260 loss:        0.042995
Test - acc:         0.925700 loss:        0.281538
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.984940 loss:        0.046251
Test - acc:         0.927100 loss:        0.275186
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.985660 loss:        0.043515
Test - acc:         0.922300 loss:        0.285632
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.984860 loss:        0.046546
Test - acc:         0.929700 loss:        0.265681
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.985380 loss:        0.045171
Test - acc:         0.924100 loss:        0.283890
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.985480 loss:        0.043569
Test - acc:         0.927500 loss:        0.271679
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.983940 loss:        0.046930
Test - acc:         0.921400 loss:        0.285657
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.984720 loss:        0.045643
Test - acc:         0.923100 loss:        0.283471
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.984380 loss:        0.045748
Test - acc:         0.922900 loss:        0.277665
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.984560 loss:        0.046859
Test - acc:         0.920500 loss:        0.303052
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.984680 loss:        0.047029
Test - acc:         0.922800 loss:        0.277195
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.985440 loss:        0.044916
Test - acc:         0.931000 loss:        0.254407
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.985380 loss:        0.044257
Test - acc:         0.929000 loss:        0.260610
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.985480 loss:        0.044254
Test - acc:         0.928500 loss:        0.267361
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.958840 loss:        0.141345
Test - acc:         0.923700 loss:        0.247398
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.979000 loss:        0.080486
Test - acc:         0.927900 loss:        0.235853
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.983820 loss:        0.065721
Test - acc:         0.929400 loss:        0.229070
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.984880 loss:        0.057416
Test - acc:         0.929500 loss:        0.227454
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.987700 loss:        0.050832
Test - acc:         0.930900 loss:        0.227590
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.989020 loss:        0.045668
Test - acc:         0.932800 loss:        0.225083
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.989760 loss:        0.041683
Test - acc:         0.931300 loss:        0.226771
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.990620 loss:        0.039157
Test - acc:         0.932200 loss:        0.228696
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.990860 loss:        0.037016
Test - acc:         0.932300 loss:        0.228864
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.991860 loss:        0.034627
Test - acc:         0.933100 loss:        0.226848
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.992360 loss:        0.031738
Test - acc:         0.933900 loss:        0.226887
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.992600 loss:        0.031480
Test - acc:         0.933600 loss:        0.225412
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.993260 loss:        0.028135
Test - acc:         0.933100 loss:        0.223547
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.993700 loss:        0.028083
Test - acc:         0.934400 loss:        0.226864
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.993640 loss:        0.026743
Test - acc:         0.936500 loss:        0.225698
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.993620 loss:        0.026343
Test - acc:         0.934100 loss:        0.226275
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.994260 loss:        0.024298
Test - acc:         0.935400 loss:        0.227947
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.994740 loss:        0.023231
Test - acc:         0.935500 loss:        0.229596
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.994640 loss:        0.023155
Test - acc:         0.935500 loss:        0.227164
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.995320 loss:        0.021018
Test - acc:         0.935100 loss:        0.228441
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.995320 loss:        0.020552
Test - acc:         0.935000 loss:        0.230937
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.995580 loss:        0.020031
Test - acc:         0.936400 loss:        0.229745
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.995280 loss:        0.020356
Test - acc:         0.935900 loss:        0.230963
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.995040 loss:        0.020532
Test - acc:         0.935200 loss:        0.234341
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.996080 loss:        0.018527
Test - acc:         0.936600 loss:        0.228956
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.995620 loss:        0.019526
Test - acc:         0.935800 loss:        0.229658
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.996240 loss:        0.017645
Test - acc:         0.936400 loss:        0.232672
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.995580 loss:        0.018324
Test - acc:         0.935000 loss:        0.233336
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.996520 loss:        0.016536
Test - acc:         0.935000 loss:        0.232705
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.996520 loss:        0.016819
Test - acc:         0.936100 loss:        0.234290
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.996500 loss:        0.015949
Test - acc:         0.937300 loss:        0.232640
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.996540 loss:        0.015955
Test - acc:         0.936600 loss:        0.236748
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.996540 loss:        0.016126
Test - acc:         0.936800 loss:        0.235048
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.996760 loss:        0.015096
Test - acc:         0.936700 loss:        0.236054
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.996880 loss:        0.014478
Test - acc:         0.936700 loss:        0.238269
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.996800 loss:        0.014777
Test - acc:         0.937200 loss:        0.240242
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.997000 loss:        0.014668
Test - acc:         0.937200 loss:        0.237064
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.997080 loss:        0.014234
Test - acc:         0.937300 loss:        0.236546
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.997000 loss:        0.013970
Test - acc:         0.937700 loss:        0.235290
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.996620 loss:        0.014486
Test - acc:         0.937100 loss:        0.234599
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.997300 loss:        0.013221
Test - acc:         0.936800 loss:        0.236651
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.997100 loss:        0.013124
Test - acc:         0.936500 loss:        0.238518
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.997720 loss:        0.012839
Test - acc:         0.934300 loss:        0.238709
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.996980 loss:        0.013283
Test - acc:         0.937200 loss:        0.240154
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.997260 loss:        0.013169
Test - acc:         0.934500 loss:        0.240923
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.997480 loss:        0.012277
Test - acc:         0.936400 loss:        0.239545
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.997760 loss:        0.012136
Test - acc:         0.937400 loss:        0.236473
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.997500 loss:        0.012078
Test - acc:         0.937000 loss:        0.237355
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.997260 loss:        0.012735
Test - acc:         0.937300 loss:        0.238969
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.997720 loss:        0.011545
Test - acc:         0.936100 loss:        0.240194
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.884620 loss:        0.374390
Test - acc:         0.891400 loss:        0.351170
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.933420 loss:        0.214838
Test - acc:         0.900700 loss:        0.321744
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.944700 loss:        0.178176
Test - acc:         0.906300 loss:        0.303414
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.951320 loss:        0.157391
Test - acc:         0.911100 loss:        0.290820
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.957080 loss:        0.137396
Test - acc:         0.912300 loss:        0.289244
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.960000 loss:        0.127337
Test - acc:         0.913300 loss:        0.287809
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.961540 loss:        0.121515
Test - acc:         0.916000 loss:        0.282414
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.965760 loss:        0.111538
Test - acc:         0.916200 loss:        0.282165
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.964740 loss:        0.109216
Test - acc:         0.917900 loss:        0.282552
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.966900 loss:        0.104725
Test - acc:         0.918500 loss:        0.282615
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.969240 loss:        0.098137
Test - acc:         0.919100 loss:        0.281334
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.970040 loss:        0.094527
Test - acc:         0.919900 loss:        0.281502
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.970940 loss:        0.090977
Test - acc:         0.918800 loss:        0.280092
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.971540 loss:        0.088979
Test - acc:         0.921800 loss:        0.274370
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.972640 loss:        0.087650
Test - acc:         0.921300 loss:        0.280452
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.974260 loss:        0.082106
Test - acc:         0.920500 loss:        0.279994
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.974080 loss:        0.080920
Test - acc:         0.922900 loss:        0.276421
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.973940 loss:        0.081689
Test - acc:         0.921500 loss:        0.276539
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.975860 loss:        0.076919
Test - acc:         0.922300 loss:        0.272331
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.976060 loss:        0.075161
Test - acc:         0.921200 loss:        0.274398
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.975820 loss:        0.075825
Test - acc:         0.922600 loss:        0.272689
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.976900 loss:        0.073498
Test - acc:         0.920400 loss:        0.280081
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.977880 loss:        0.070122
Test - acc:         0.920500 loss:        0.281293
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.978060 loss:        0.068964
Test - acc:         0.922600 loss:        0.280236
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.978120 loss:        0.067537
Test - acc:         0.924500 loss:        0.278790
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.977300 loss:        0.070533
Test - acc:         0.920300 loss:        0.284270
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.978300 loss:        0.067349
Test - acc:         0.924500 loss:        0.278566
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.978680 loss:        0.065730
Test - acc:         0.922400 loss:        0.281720
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.979120 loss:        0.064782
Test - acc:         0.924300 loss:        0.279966
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.979320 loss:        0.065915
Test - acc:         0.922000 loss:        0.284199
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.979180 loss:        0.064348
Test - acc:         0.921800 loss:        0.276651
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.980340 loss:        0.061435
Test - acc:         0.921000 loss:        0.285514
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.980620 loss:        0.061708
Test - acc:         0.923700 loss:        0.280628
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.981020 loss:        0.059639
Test - acc:         0.923100 loss:        0.281124
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.982120 loss:        0.057150
Test - acc:         0.922900 loss:        0.290518
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.980780 loss:        0.058284
Test - acc:         0.924700 loss:        0.276268
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.981160 loss:        0.059096
Test - acc:         0.924100 loss:        0.283793
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.981320 loss:        0.057135
Test - acc:         0.921600 loss:        0.286660
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.982860 loss:        0.056354
Test - acc:         0.924900 loss:        0.281428
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.982160 loss:        0.056308
Test - acc:         0.924300 loss:        0.283971
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.982360 loss:        0.056285
Test - acc:         0.921700 loss:        0.289818
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.981920 loss:        0.055796
Test - acc:         0.922100 loss:        0.288860
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.982720 loss:        0.055143
Test - acc:         0.923100 loss:        0.281329
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.983140 loss:        0.052970
Test - acc:         0.924800 loss:        0.278064
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.982580 loss:        0.054785
Test - acc:         0.925500 loss:        0.279969
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.984180 loss:        0.051915
Test - acc:         0.927000 loss:        0.278618
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.983700 loss:        0.051160
Test - acc:         0.924500 loss:        0.286644
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.982660 loss:        0.053587
Test - acc:         0.926700 loss:        0.284717
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.983340 loss:        0.052770
Test - acc:         0.927900 loss:        0.276472
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.983380 loss:        0.052625
Test - acc:         0.926100 loss:        0.286333
Sparsity :          0.9844
Wdecay :        0.000500
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "magnitude",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf39_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.325920 loss:        1.847682
Test - acc:         0.433400 loss:        1.508943
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.514980 loss:        1.326493
Test - acc:         0.576200 loss:        1.166692
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.637620 loss:        1.009011
Test - acc:         0.606500 loss:        1.140515
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.717740 loss:        0.806052
Test - acc:         0.696100 loss:        0.896292
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.760000 loss:        0.691428
Test - acc:         0.742900 loss:        0.751321
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.787060 loss:        0.611490
Test - acc:         0.756600 loss:        0.710299
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.806060 loss:        0.564509
Test - acc:         0.769000 loss:        0.710991
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.817920 loss:        0.529806
Test - acc:         0.787400 loss:        0.621394
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.824420 loss:        0.509206
Test - acc:         0.772600 loss:        0.684377
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.833860 loss:        0.483304
Test - acc:         0.775600 loss:        0.667925
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.838920 loss:        0.469585
Test - acc:         0.767600 loss:        0.754009
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.844060 loss:        0.454590
Test - acc:         0.796900 loss:        0.618867
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.850060 loss:        0.440107
Test - acc:         0.763200 loss:        0.735417
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.851640 loss:        0.434035
Test - acc:         0.793900 loss:        0.660835
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.855280 loss:        0.423293
Test - acc:         0.832300 loss:        0.496892
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.858580 loss:        0.415450
Test - acc:         0.831400 loss:        0.510164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.859460 loss:        0.408462
Test - acc:         0.820900 loss:        0.522441
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.860880 loss:        0.400126
Test - acc:         0.834400 loss:        0.483465
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.864860 loss:        0.393520
Test - acc:         0.804400 loss:        0.618786
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.868280 loss:        0.385265
Test - acc:         0.807600 loss:        0.623200
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.869220 loss:        0.381626
Test - acc:         0.814300 loss:        0.590255
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870500 loss:        0.379530
Test - acc:         0.798800 loss:        0.641787
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.871620 loss:        0.377487
Test - acc:         0.811600 loss:        0.574784
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.875440 loss:        0.368226
Test - acc:         0.821200 loss:        0.538520
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.877080 loss:        0.364041
Test - acc:         0.827800 loss:        0.496110
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.874160 loss:        0.366042
Test - acc:         0.818700 loss:        0.529624
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.877320 loss:        0.360246
Test - acc:         0.830300 loss:        0.521060
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.875840 loss:        0.361626
Test - acc:         0.829000 loss:        0.518092
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.878240 loss:        0.352239
Test - acc:         0.821600 loss:        0.544881
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.876260 loss:        0.358935
Test - acc:         0.835600 loss:        0.488176
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.880360 loss:        0.350550
Test - acc:         0.844600 loss:        0.479743
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.880840 loss:        0.348789
Test - acc:         0.799400 loss:        0.643922
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.882680 loss:        0.344050
Test - acc:         0.856300 loss:        0.439701
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.881020 loss:        0.349389
Test - acc:         0.861500 loss:        0.419639
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.881940 loss:        0.344315
Test - acc:         0.810400 loss:        0.605949
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.882960 loss:        0.341227
Test - acc:         0.855700 loss:        0.442214
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.883060 loss:        0.340363
Test - acc:         0.845100 loss:        0.467937
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.882780 loss:        0.341310
Test - acc:         0.812600 loss:        0.630480
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.885200 loss:        0.338045
Test - acc:         0.834200 loss:        0.483156
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.901760 loss:        0.288392
Test - acc:         0.844600 loss:        0.485026
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.900020 loss:        0.292022
Test - acc:         0.850200 loss:        0.469623
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.898920 loss:        0.291284
Test - acc:         0.850000 loss:        0.446221
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.900260 loss:        0.291747
Test - acc:         0.802600 loss:        0.652551
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.898200 loss:        0.294477
Test - acc:         0.863200 loss:        0.409705
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.898080 loss:        0.298317
Test - acc:         0.850400 loss:        0.435596
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.900300 loss:        0.290059
Test - acc:         0.871400 loss:        0.384102
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.898780 loss:        0.295217
Test - acc:         0.831800 loss:        0.532309
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.898940 loss:        0.296457
Test - acc:         0.843300 loss:        0.477787
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.900180 loss:        0.293219
Test - acc:         0.778300 loss:        0.742952
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.901360 loss:        0.289188
Test - acc:         0.870300 loss:        0.391010
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.899600 loss:        0.292423
Test - acc:         0.845400 loss:        0.502908
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.902560 loss:        0.287298
Test - acc:         0.860900 loss:        0.415213
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.901860 loss:        0.288379
Test - acc:         0.815200 loss:        0.560551
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.901980 loss:        0.288230
Test - acc:         0.839800 loss:        0.491795
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.900420 loss:        0.285322
Test - acc:         0.846200 loss:        0.470161
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.901300 loss:        0.286988
Test - acc:         0.861700 loss:        0.426023
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.901520 loss:        0.287483
Test - acc:         0.847800 loss:        0.457891
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.903380 loss:        0.283584
Test - acc:         0.824100 loss:        0.554656
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.901960 loss:        0.285457
Test - acc:         0.862700 loss:        0.425073
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.901800 loss:        0.288435
Test - acc:         0.854700 loss:        0.430824
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.902640 loss:        0.282668
Test - acc:         0.808800 loss:        0.617960
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.900260 loss:        0.290341
Test - acc:         0.862500 loss:        0.425602
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.901040 loss:        0.286001
Test - acc:         0.852000 loss:        0.443942
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.900880 loss:        0.286678
Test - acc:         0.846100 loss:        0.475130
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.900620 loss:        0.290456
Test - acc:         0.847000 loss:        0.466210
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.903180 loss:        0.280284
Test - acc:         0.856400 loss:        0.453501
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.902960 loss:        0.285750
Test - acc:         0.864600 loss:        0.405555
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.902940 loss:        0.286764
Test - acc:         0.857200 loss:        0.444188
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.903480 loss:        0.279774
Test - acc:         0.853600 loss:        0.454554
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.902500 loss:        0.287487
Test - acc:         0.845800 loss:        0.473374
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.900640 loss:        0.289260
Test - acc:         0.846200 loss:        0.480221
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.903040 loss:        0.282221
Test - acc:         0.859500 loss:        0.426983
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.903940 loss:        0.283563
Test - acc:         0.838500 loss:        0.487106
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.904980 loss:        0.280368
Test - acc:         0.870100 loss:        0.392415
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.903240 loss:        0.280425
Test - acc:         0.830100 loss:        0.567841
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.902740 loss:        0.284127
Test - acc:         0.851700 loss:        0.465176
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.904620 loss:        0.284038
Test - acc:         0.837500 loss:        0.489424
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.903860 loss:        0.280782
Test - acc:         0.791700 loss:        0.627577
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.918340 loss:        0.244209
Test - acc:         0.838200 loss:        0.518503
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.917340 loss:        0.243885
Test - acc:         0.858700 loss:        0.449506
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.911240 loss:        0.259153
Test - acc:         0.853600 loss:        0.475628
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.912280 loss:        0.255253
Test - acc:         0.826900 loss:        0.561852
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.912300 loss:        0.255151
Test - acc:         0.863600 loss:        0.414945
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.913920 loss:        0.250693
Test - acc:         0.875900 loss:        0.390948
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.912320 loss:        0.259378
Test - acc:         0.866800 loss:        0.405110
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.911420 loss:        0.257620
Test - acc:         0.862800 loss:        0.420760
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.913760 loss:        0.253423
Test - acc:         0.856900 loss:        0.450510
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.912500 loss:        0.254207
Test - acc:         0.854400 loss:        0.441307
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.912800 loss:        0.255340
Test - acc:         0.870500 loss:        0.390099
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.913600 loss:        0.254180
Test - acc:         0.862200 loss:        0.433312
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.911900 loss:        0.259805
Test - acc:         0.852800 loss:        0.441121
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.913580 loss:        0.254743
Test - acc:         0.869700 loss:        0.397574
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.914140 loss:        0.253676
Test - acc:         0.866300 loss:        0.409899
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.911460 loss:        0.259780
Test - acc:         0.852800 loss:        0.443120
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.913600 loss:        0.252853
Test - acc:         0.866600 loss:        0.404019
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.913080 loss:        0.253641
Test - acc:         0.834800 loss:        0.508727
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.913320 loss:        0.254508
Test - acc:         0.863600 loss:        0.439865
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.913920 loss:        0.254786
Test - acc:         0.860200 loss:        0.422019
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.912160 loss:        0.256619
Test - acc:         0.866000 loss:        0.417452
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.911380 loss:        0.257993
Test - acc:         0.876500 loss:        0.368357
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.913360 loss:        0.251714
Test - acc:         0.840100 loss:        0.495050
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.912200 loss:        0.256767
Test - acc:         0.880200 loss:        0.364390
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.912820 loss:        0.252194
Test - acc:         0.853700 loss:        0.448173
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.910000 loss:        0.259926
Test - acc:         0.850000 loss:        0.456943
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.912500 loss:        0.251443
Test - acc:         0.858900 loss:        0.448312
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.911360 loss:        0.256853
Test - acc:         0.852500 loss:        0.489807
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.913860 loss:        0.253321
Test - acc:         0.841400 loss:        0.506625
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.912620 loss:        0.253691
Test - acc:         0.846600 loss:        0.480724
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.913040 loss:        0.253308
Test - acc:         0.876100 loss:        0.379027
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.910960 loss:        0.259377
Test - acc:         0.862400 loss:        0.418384
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.914020 loss:        0.252249
Test - acc:         0.850000 loss:        0.462209
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.912900 loss:        0.254058
Test - acc:         0.860900 loss:        0.422791
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.914140 loss:        0.251622
Test - acc:         0.875200 loss:        0.394447
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.912780 loss:        0.255737
Test - acc:         0.887400 loss:        0.331793
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.914400 loss:        0.251181
Test - acc:         0.839700 loss:        0.489372
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.911440 loss:        0.259141
Test - acc:         0.847700 loss:        0.519947
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.913280 loss:        0.251717
Test - acc:         0.887300 loss:        0.338609
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.920420 loss:        0.231676
Test - acc:         0.856000 loss:        0.440184
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.919640 loss:        0.232859
Test - acc:         0.862200 loss:        0.416361
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.915820 loss:        0.243399
Test - acc:         0.843200 loss:        0.514357
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.918920 loss:        0.235676
Test - acc:         0.863100 loss:        0.422741
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.917100 loss:        0.240634
Test - acc:         0.866000 loss:        0.410070
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.919220 loss:        0.233170
Test - acc:         0.864600 loss:        0.401631
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.915600 loss:        0.243526
Test - acc:         0.862800 loss:        0.422833
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.916900 loss:        0.237560
Test - acc:         0.867500 loss:        0.406977
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.917380 loss:        0.237942
Test - acc:         0.861800 loss:        0.411288
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.921100 loss:        0.234222
Test - acc:         0.875300 loss:        0.402314
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.917300 loss:        0.239603
Test - acc:         0.854200 loss:        0.438187
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.917420 loss:        0.239048
Test - acc:         0.862300 loss:        0.424861
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.918860 loss:        0.236037
Test - acc:         0.863300 loss:        0.442666
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.916700 loss:        0.241593
Test - acc:         0.891100 loss:        0.333596
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.917340 loss:        0.240344
Test - acc:         0.885800 loss:        0.345291
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.916440 loss:        0.238992
Test - acc:         0.848500 loss:        0.477068
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.918820 loss:        0.235454
Test - acc:         0.855700 loss:        0.468332
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.915500 loss:        0.241706
Test - acc:         0.877800 loss:        0.384685
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.919180 loss:        0.235249
Test - acc:         0.832300 loss:        0.522045
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.917760 loss:        0.238607
Test - acc:         0.873400 loss:        0.376538
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.919200 loss:        0.238569
Test - acc:         0.878600 loss:        0.376349
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.916320 loss:        0.243374
Test - acc:         0.830700 loss:        0.529467
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.916360 loss:        0.240487
Test - acc:         0.857400 loss:        0.452865
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.918400 loss:        0.238345
Test - acc:         0.886200 loss:        0.347937
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.919600 loss:        0.235195
Test - acc:         0.860100 loss:        0.444915
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.917340 loss:        0.240851
Test - acc:         0.875000 loss:        0.383933
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.919140 loss:        0.235111
Test - acc:         0.867100 loss:        0.416008
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.917920 loss:        0.239734
Test - acc:         0.884300 loss:        0.351856
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.918980 loss:        0.238178
Test - acc:         0.880900 loss:        0.378291
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.918480 loss:        0.236350
Test - acc:         0.870200 loss:        0.386620
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.918720 loss:        0.238184
Test - acc:         0.863600 loss:        0.429834
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.918180 loss:        0.239044
Test - acc:         0.860900 loss:        0.432737
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.918280 loss:        0.236468
Test - acc:         0.861100 loss:        0.416319
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.957520 loss:        0.129508
Test - acc:         0.933600 loss:        0.194076
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.968980 loss:        0.094412
Test - acc:         0.935300 loss:        0.190295
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.974360 loss:        0.080150
Test - acc:         0.937100 loss:        0.184836
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.977400 loss:        0.068731
Test - acc:         0.938400 loss:        0.186270
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.979020 loss:        0.063641
Test - acc:         0.939400 loss:        0.182793
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.980460 loss:        0.058916
Test - acc:         0.938300 loss:        0.189550
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.974280 loss:        0.085506
Test - acc:         0.936700 loss:        0.194331
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.979900 loss:        0.066730
Test - acc:         0.936400 loss:        0.196321
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.980840 loss:        0.060663
Test - acc:         0.933700 loss:        0.204821
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.981380 loss:        0.057109
Test - acc:         0.934500 loss:        0.206417
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.983680 loss:        0.051831
Test - acc:         0.938400 loss:        0.204178
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.983580 loss:        0.050231
Test - acc:         0.934500 loss:        0.216719
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.984440 loss:        0.047353
Test - acc:         0.938000 loss:        0.208799
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.985120 loss:        0.044967
Test - acc:         0.935900 loss:        0.217616
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.986380 loss:        0.042409
Test - acc:         0.937600 loss:        0.213770
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.985620 loss:        0.043959
Test - acc:         0.936500 loss:        0.214979
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.986420 loss:        0.041776
Test - acc:         0.936300 loss:        0.225607
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.987140 loss:        0.040020
Test - acc:         0.936200 loss:        0.228162
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.987360 loss:        0.038619
Test - acc:         0.934600 loss:        0.226127
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.986860 loss:        0.040918
Test - acc:         0.934000 loss:        0.234914
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.987460 loss:        0.039132
Test - acc:         0.935800 loss:        0.224949
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.987740 loss:        0.038190
Test - acc:         0.935500 loss:        0.219345
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.988120 loss:        0.037898
Test - acc:         0.932400 loss:        0.223447
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.987440 loss:        0.038349
Test - acc:         0.935000 loss:        0.233654
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.987220 loss:        0.039147
Test - acc:         0.935300 loss:        0.236539
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.987420 loss:        0.038325
Test - acc:         0.936900 loss:        0.223070
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.986620 loss:        0.040344
Test - acc:         0.935400 loss:        0.235508
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.986020 loss:        0.042327
Test - acc:         0.932900 loss:        0.242445
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.986420 loss:        0.041142
Test - acc:         0.932800 loss:        0.226804
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.986000 loss:        0.041008
Test - acc:         0.937300 loss:        0.222238
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.986640 loss:        0.041495
Test - acc:         0.934200 loss:        0.230941
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.986600 loss:        0.040975
Test - acc:         0.927300 loss:        0.249844
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.986500 loss:        0.041852
Test - acc:         0.929600 loss:        0.249463
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.985820 loss:        0.042957
Test - acc:         0.927400 loss:        0.259814
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.986000 loss:        0.043133
Test - acc:         0.922500 loss:        0.281142
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.984340 loss:        0.044713
Test - acc:         0.928800 loss:        0.260514
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.984840 loss:        0.045723
Test - acc:         0.927900 loss:        0.249070
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.984340 loss:        0.046233
Test - acc:         0.927400 loss:        0.267269
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.984780 loss:        0.045831
Test - acc:         0.925000 loss:        0.269173
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.983420 loss:        0.049841
Test - acc:         0.919200 loss:        0.277941
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.984960 loss:        0.045886
Test - acc:         0.932900 loss:        0.239384
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.985540 loss:        0.044581
Test - acc:         0.927300 loss:        0.270794
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.982540 loss:        0.050927
Test - acc:         0.931100 loss:        0.246673
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.983760 loss:        0.048217
Test - acc:         0.932100 loss:        0.240235
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.982880 loss:        0.050409
Test - acc:         0.927300 loss:        0.260181
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.953640 loss:        0.140366
Test - acc:         0.914600 loss:        0.271097
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.966880 loss:        0.100613
Test - acc:         0.911300 loss:        0.290756
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.967580 loss:        0.095986
Test - acc:         0.902800 loss:        0.342259
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.969460 loss:        0.092234
Test - acc:         0.917400 loss:        0.287166
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.971700 loss:        0.084583
Test - acc:         0.920500 loss:        0.265801
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.973960 loss:        0.079195
Test - acc:         0.915300 loss:        0.294455
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.971900 loss:        0.084057
Test - acc:         0.916300 loss:        0.296452
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.974420 loss:        0.076689
Test - acc:         0.913700 loss:        0.284970
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.973800 loss:        0.077551
Test - acc:         0.918200 loss:        0.272251
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.973700 loss:        0.077518
Test - acc:         0.920400 loss:        0.269193
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.972820 loss:        0.079005
Test - acc:         0.921400 loss:        0.280301
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.973940 loss:        0.075051
Test - acc:         0.916100 loss:        0.289111
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.975120 loss:        0.073506
Test - acc:         0.914300 loss:        0.300719
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.975420 loss:        0.072531
Test - acc:         0.917300 loss:        0.304701
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.976460 loss:        0.069258
Test - acc:         0.918800 loss:        0.294026
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.974560 loss:        0.074104
Test - acc:         0.916300 loss:        0.296642
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.975740 loss:        0.071529
Test - acc:         0.918800 loss:        0.283649
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.975760 loss:        0.071116
Test - acc:         0.915700 loss:        0.301024
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.975500 loss:        0.070548
Test - acc:         0.923900 loss:        0.278479
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.976340 loss:        0.068848
Test - acc:         0.920600 loss:        0.274036
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.976780 loss:        0.068476
Test - acc:         0.917500 loss:        0.284618
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.977040 loss:        0.067233
Test - acc:         0.926200 loss:        0.260845
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.976320 loss:        0.069092
Test - acc:         0.925800 loss:        0.263511
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.977280 loss:        0.067459
Test - acc:         0.921400 loss:        0.283546
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.976480 loss:        0.067355
Test - acc:         0.912100 loss:        0.310652
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.975460 loss:        0.070350
Test - acc:         0.925500 loss:        0.271233
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.975780 loss:        0.070135
Test - acc:         0.922100 loss:        0.278239
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.977220 loss:        0.068633
Test - acc:         0.917000 loss:        0.304106
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.976720 loss:        0.067929
Test - acc:         0.922700 loss:        0.272252
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.976980 loss:        0.067209
Test - acc:         0.920400 loss:        0.278054
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.977180 loss:        0.067364
Test - acc:         0.920900 loss:        0.287078
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.977420 loss:        0.067267
Test - acc:         0.919200 loss:        0.285190
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.976600 loss:        0.069862
Test - acc:         0.920500 loss:        0.274957
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.979080 loss:        0.063610
Test - acc:         0.920400 loss:        0.289490
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.977720 loss:        0.065894
Test - acc:         0.922800 loss:        0.269368
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.977600 loss:        0.065775
Test - acc:         0.921000 loss:        0.279537
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.978940 loss:        0.063235
Test - acc:         0.926200 loss:        0.258702
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.976740 loss:        0.068066
Test - acc:         0.917900 loss:        0.275477
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.977300 loss:        0.067766
Test - acc:         0.918900 loss:        0.281961
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.932440 loss:        0.205790
Test - acc:         0.896300 loss:        0.315752
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.946740 loss:        0.156498
Test - acc:         0.902700 loss:        0.312202
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.949100 loss:        0.147296
Test - acc:         0.898400 loss:        0.337117
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.951840 loss:        0.141286
Test - acc:         0.905600 loss:        0.306876
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.952760 loss:        0.137217
Test - acc:         0.901800 loss:        0.320842
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.955900 loss:        0.129220
Test - acc:         0.907300 loss:        0.296971
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.956160 loss:        0.128403
Test - acc:         0.910900 loss:        0.296762
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.955840 loss:        0.125880
Test - acc:         0.902900 loss:        0.325046
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.956200 loss:        0.125507
Test - acc:         0.910600 loss:        0.293561
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.956860 loss:        0.122909
Test - acc:         0.913600 loss:        0.281722
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.959480 loss:        0.117873
Test - acc:         0.904300 loss:        0.313145
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.958660 loss:        0.117467
Test - acc:         0.901500 loss:        0.319860
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.959240 loss:        0.117118
Test - acc:         0.910400 loss:        0.296531
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.958840 loss:        0.118397
Test - acc:         0.911600 loss:        0.278293
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.959540 loss:        0.116651
Test - acc:         0.904700 loss:        0.297480
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.958560 loss:        0.116719
Test - acc:         0.909500 loss:        0.298103
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.971540 loss:        0.085021
Test - acc:         0.924900 loss:        0.241062
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.978700 loss:        0.066063
Test - acc:         0.929000 loss:        0.234740
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.981660 loss:        0.058131
Test - acc:         0.928000 loss:        0.234896
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.982460 loss:        0.056839
Test - acc:         0.928000 loss:        0.234351
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.983340 loss:        0.054701
Test - acc:         0.928200 loss:        0.237035
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.984600 loss:        0.050444
Test - acc:         0.929500 loss:        0.238247
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.985100 loss:        0.048698
Test - acc:         0.929400 loss:        0.234691
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.984840 loss:        0.047751
Test - acc:         0.929100 loss:        0.234571
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.985920 loss:        0.046380
Test - acc:         0.929400 loss:        0.236874
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.987140 loss:        0.043341
Test - acc:         0.930100 loss:        0.234844
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.987480 loss:        0.042810
Test - acc:         0.928700 loss:        0.234796
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.988040 loss:        0.041491
Test - acc:         0.929500 loss:        0.238009
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.987140 loss:        0.041727
Test - acc:         0.930300 loss:        0.237503
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.987100 loss:        0.041231
Test - acc:         0.929200 loss:        0.237327
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.987940 loss:        0.040232
Test - acc:         0.931300 loss:        0.236490
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.987980 loss:        0.039620
Test - acc:         0.931600 loss:        0.235602
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.988760 loss:        0.038450
Test - acc:         0.928500 loss:        0.241671
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.989180 loss:        0.038243
Test - acc:         0.929200 loss:        0.238912
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.988800 loss:        0.036925
Test - acc:         0.930500 loss:        0.240351
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.989740 loss:        0.036611
Test - acc:         0.930800 loss:        0.237198
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.990060 loss:        0.034120
Test - acc:         0.929600 loss:        0.241039
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.990260 loss:        0.034897
Test - acc:         0.929700 loss:        0.241721
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.990080 loss:        0.034391
Test - acc:         0.930100 loss:        0.239490
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.822560 loss:        0.554542
Test - acc:         0.850100 loss:        0.452171
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.887480 loss:        0.344055
Test - acc:         0.865800 loss:        0.409769
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.902880 loss:        0.297423
Test - acc:         0.879600 loss:        0.367862
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.912000 loss:        0.265277
Test - acc:         0.882900 loss:        0.357526
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.917360 loss:        0.248634
Test - acc:         0.886400 loss:        0.344792
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.922500 loss:        0.234517
Test - acc:         0.890400 loss:        0.338241
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.925720 loss:        0.225010
Test - acc:         0.891900 loss:        0.331479
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.928840 loss:        0.215206
Test - acc:         0.892900 loss:        0.330662
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.931580 loss:        0.205020
Test - acc:         0.892800 loss:        0.339747
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.934620 loss:        0.196000
Test - acc:         0.896900 loss:        0.327590
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.935040 loss:        0.193075
Test - acc:         0.897100 loss:        0.325938
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.934740 loss:        0.192334
Test - acc:         0.898400 loss:        0.322996
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.935880 loss:        0.190305
Test - acc:         0.896900 loss:        0.323629
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.938320 loss:        0.182741
Test - acc:         0.898400 loss:        0.321720
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.939600 loss:        0.179729
Test - acc:         0.899300 loss:        0.321001
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.940660 loss:        0.176356
Test - acc:         0.902000 loss:        0.318742
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.942000 loss:        0.173542
Test - acc:         0.897100 loss:        0.326218
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.940540 loss:        0.173755
Test - acc:         0.897800 loss:        0.322025
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.943400 loss:        0.169016
Test - acc:         0.900700 loss:        0.314357
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.943460 loss:        0.168410
Test - acc:         0.900300 loss:        0.316904
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.944620 loss:        0.164324
Test - acc:         0.899900 loss:        0.319717
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.943740 loss:        0.165566
Test - acc:         0.900400 loss:        0.314947
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.945500 loss:        0.160506
Test - acc:         0.899500 loss:        0.322467
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.944840 loss:        0.161206
Test - acc:         0.898800 loss:        0.324718
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.945260 loss:        0.157337
Test - acc:         0.900400 loss:        0.314353
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.946940 loss:        0.157085
Test - acc:         0.900700 loss:        0.313949
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.946800 loss:        0.155853
Test - acc:         0.901300 loss:        0.319135
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.946500 loss:        0.154633
Test - acc:         0.899700 loss:        0.318024
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.947500 loss:        0.151442
Test - acc:         0.899200 loss:        0.317046
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.948560 loss:        0.150277
Test - acc:         0.898400 loss:        0.326080
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.948440 loss:        0.148775
Test - acc:         0.902100 loss:        0.319445
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.948180 loss:        0.148908
Test - acc:         0.898600 loss:        0.327904
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.948900 loss:        0.149959
Test - acc:         0.901700 loss:        0.322296
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.949680 loss:        0.147371
Test - acc:         0.900000 loss:        0.323108
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.949600 loss:        0.145886
Test - acc:         0.904200 loss:        0.313844
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.949240 loss:        0.146297
Test - acc:         0.898500 loss:        0.322989
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.950720 loss:        0.144260
Test - acc:         0.899500 loss:        0.322653
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.951320 loss:        0.142481
Test - acc:         0.903100 loss:        0.320409
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.952060 loss:        0.139069
Test - acc:         0.901900 loss:        0.317352
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.610880 loss:        1.258006
Test - acc:         0.704500 loss:        0.920089
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.730180 loss:        0.843701
Test - acc:         0.753500 loss:        0.790803
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.769480 loss:        0.751150
Test - acc:         0.775300 loss:        0.733006
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.793540 loss:        0.688250
Test - acc:         0.771500 loss:        0.724814
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.807220 loss:        0.641801
Test - acc:         0.795100 loss:        0.666824
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.817560 loss:        0.605723
Test - acc:         0.809600 loss:        0.628922
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.824040 loss:        0.577722
Test - acc:         0.816300 loss:        0.605765
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.828580 loss:        0.558351
Test - acc:         0.821700 loss:        0.579420
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.832580 loss:        0.539325
Test - acc:         0.829400 loss:        0.554799
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.837680 loss:        0.522427
Test - acc:         0.834000 loss:        0.539505
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.838160 loss:        0.511336
Test - acc:         0.828300 loss:        0.557462
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.841200 loss:        0.499146
Test - acc:         0.828600 loss:        0.550213
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.843180 loss:        0.489743
Test - acc:         0.834000 loss:        0.533700
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.843760 loss:        0.486671
Test - acc:         0.833900 loss:        0.531074
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.849380 loss:        0.468379
Test - acc:         0.827700 loss:        0.551126
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.850460 loss:        0.463117
Test - acc:         0.834100 loss:        0.522451
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.852580 loss:        0.455130
Test - acc:         0.839800 loss:        0.496833
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.851080 loss:        0.456286
Test - acc:         0.844000 loss:        0.496952
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.852320 loss:        0.448721
Test - acc:         0.839700 loss:        0.496289
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.855380 loss:        0.446538
Test - acc:         0.833200 loss:        0.519692
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.853640 loss:        0.445606
Test - acc:         0.840600 loss:        0.498210
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.859120 loss:        0.430780
Test - acc:         0.841000 loss:        0.493099
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.859460 loss:        0.427932
Test - acc:         0.845300 loss:        0.490905
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.857120 loss:        0.429599
Test - acc:         0.844500 loss:        0.481834
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.858400 loss:        0.429474
Test - acc:         0.844900 loss:        0.479953
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.863300 loss:        0.420445
Test - acc:         0.844800 loss:        0.476687
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.860480 loss:        0.422118
Test - acc:         0.844300 loss:        0.479623
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.862620 loss:        0.416764
Test - acc:         0.841700 loss:        0.482873
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.862200 loss:        0.412379
Test - acc:         0.842400 loss:        0.482452
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.863320 loss:        0.410182
Test - acc:         0.847500 loss:        0.473946
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.864180 loss:        0.406906
Test - acc:         0.843500 loss:        0.485396
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.866260 loss:        0.404935
Test - acc:         0.849100 loss:        0.458915
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.866760 loss:        0.402388
Test - acc:         0.843900 loss:        0.476888
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.865300 loss:        0.401962
Test - acc:         0.845400 loss:        0.489366
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.868600 loss:        0.395077
Test - acc:         0.832400 loss:        0.523301
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.865140 loss:        0.399448
Test - acc:         0.848700 loss:        0.463898
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.866620 loss:        0.396399
Test - acc:         0.852500 loss:        0.454749
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.867360 loss:        0.397208
Test - acc:         0.849300 loss:        0.463828
Sparsity :          0.9961
Wdecay :        0.000500
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 10000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "magnitude",
    "prune_freq": 117,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf117_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
Traceback (most recent call last):
  File "main.py", line 229, in <module>
    main()
  File "main.py", line 147, in main
    model, opt = train(config, writer)
  File "main.py", line 79, in train
    test_acc, test_loss = epoch(epoch_num, test_loader, test_dataset_size, model, opt, writer, config)
  File "/home/andreia/thesis/utils/epoch_funcs.py", line 115, in regular_epoch
    out = model.forward(x)
  File "/home/andreia/thesis/models/cifar10_models.py", line 90, in forward
    out = self.layer1(out)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/andreia/thesis/models/cifar10_models.py", line 33, in forward
    out = self.bn2(self.conv2(out))
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py", line 107, in forward
    exponential_average_factor, self.eps)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py", line 1670, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: CUDA out of memory. Tried to allocate 2.44 GiB (GPU 0; 10.92 GiB total capacity; 10.06 GiB already allocated; 281.50 MiB free; 10.12 GiB reserved in total by PyTorch)
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 10000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "magnitude",
    "prune_freq": 70,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf70_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
Traceback (most recent call last):
  File "main.py", line 229, in <module>
    main()
  File "main.py", line 147, in main
    model, opt = train(config, writer)
  File "main.py", line 79, in train
    test_acc, test_loss = epoch(epoch_num, test_loader, test_dataset_size, model, opt, writer, config)
  File "/home/andreia/thesis/utils/epoch_funcs.py", line 115, in regular_epoch
    out = model.forward(x)
  File "/home/andreia/thesis/models/cifar10_models.py", line 90, in forward
    out = self.layer1(out)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/andreia/thesis/models/cifar10_models.py", line 33, in forward
    out = self.bn2(self.conv2(out))
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py", line 107, in forward
    exponential_average_factor, self.eps)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py", line 1670, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: CUDA out of memory. Tried to allocate 2.44 GiB (GPU 0; 10.92 GiB total capacity; 10.06 GiB already allocated; 281.50 MiB free; 10.12 GiB reserved in total by PyTorch)
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 10000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "magnitude",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf50_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
Traceback (most recent call last):
  File "main.py", line 229, in <module>
    main()
  File "main.py", line 147, in main
    model, opt = train(config, writer)
  File "main.py", line 79, in train
    test_acc, test_loss = epoch(epoch_num, test_loader, test_dataset_size, model, opt, writer, config)
  File "/home/andreia/thesis/utils/epoch_funcs.py", line 115, in regular_epoch
    out = model.forward(x)
  File "/home/andreia/thesis/models/cifar10_models.py", line 90, in forward
    out = self.layer1(out)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/andreia/thesis/models/cifar10_models.py", line 33, in forward
    out = self.bn2(self.conv2(out))
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py", line 107, in forward
    exponential_average_factor, self.eps)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py", line 1670, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: CUDA out of memory. Tried to allocate 2.44 GiB (GPU 0; 10.92 GiB total capacity; 10.06 GiB already allocated; 281.50 MiB free; 10.12 GiB reserved in total by PyTorch)
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 10000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "magnitude",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf39_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
Traceback (most recent call last):
  File "main.py", line 229, in <module>
    main()
  File "main.py", line 147, in main
    model, opt = train(config, writer)
  File "main.py", line 79, in train
    test_acc, test_loss = epoch(epoch_num, test_loader, test_dataset_size, model, opt, writer, config)
  File "/home/andreia/thesis/utils/epoch_funcs.py", line 115, in regular_epoch
    out = model.forward(x)
  File "/home/andreia/thesis/models/cifar10_models.py", line 90, in forward
    out = self.layer1(out)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/andreia/thesis/models/cifar10_models.py", line 33, in forward
    out = self.bn2(self.conv2(out))
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py", line 107, in forward
    exponential_average_factor, self.eps)
  File "/home/andreia/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py", line 1670, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: CUDA out of memory. Tried to allocate 2.44 GiB (GPU 0; 10.92 GiB total capacity; 10.06 GiB already allocated; 281.50 MiB free; 10.12 GiB reserved in total by PyTorch)
