******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "magnitude",
    "prune_freq": 117,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf117_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.335980 loss:        1.834952
Test - acc:         0.434500 loss:        1.516146
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.499920 loss:        1.365646
Test - acc:         0.537700 loss:        1.276935
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.611440 loss:        1.086855
Test - acc:         0.651300 loss:        0.967936
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.677260 loss:        0.902607
Test - acc:         0.710500 loss:        0.822695
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.738000 loss:        0.748496
Test - acc:         0.700700 loss:        0.876529
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.774800 loss:        0.643551
Test - acc:         0.701600 loss:        0.851181
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.794800 loss:        0.592851
Test - acc:         0.783100 loss:        0.633620
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.811260 loss:        0.547022
Test - acc:         0.764600 loss:        0.729727
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.822840 loss:        0.515704
Test - acc:         0.782200 loss:        0.629174
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.827640 loss:        0.501411
Test - acc:         0.744500 loss:        0.794391
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.834920 loss:        0.481459
Test - acc:         0.783700 loss:        0.669560
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.840020 loss:        0.466377
Test - acc:         0.816200 loss:        0.550512
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.846700 loss:        0.446303
Test - acc:         0.817500 loss:        0.551231
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.847620 loss:        0.440282
Test - acc:         0.804700 loss:        0.591621
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.853220 loss:        0.429697
Test - acc:         0.825900 loss:        0.523532
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.858960 loss:        0.417897
Test - acc:         0.814100 loss:        0.563547
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.860980 loss:        0.406157
Test - acc:         0.809000 loss:        0.589957
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.862760 loss:        0.401622
Test - acc:         0.820900 loss:        0.561647
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.861200 loss:        0.399493
Test - acc:         0.792100 loss:        0.670461
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.866400 loss:        0.388288
Test - acc:         0.829800 loss:        0.510664
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.868420 loss:        0.386417
Test - acc:         0.737400 loss:        0.798181
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870620 loss:        0.378511
Test - acc:         0.778200 loss:        0.700896
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.868920 loss:        0.384943
Test - acc:         0.764300 loss:        0.800272
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.873520 loss:        0.372131
Test - acc:         0.841100 loss:        0.478396
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.873820 loss:        0.372261
Test - acc:         0.790300 loss:        0.657671
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.874320 loss:        0.366219
Test - acc:         0.805200 loss:        0.577301
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.875640 loss:        0.366298
Test - acc:         0.829900 loss:        0.527510
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.874740 loss:        0.368855
Test - acc:         0.843700 loss:        0.463200
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.876060 loss:        0.360821
Test - acc:         0.801300 loss:        0.613367
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.879120 loss:        0.356276
Test - acc:         0.811800 loss:        0.579423
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.877900 loss:        0.356128
Test - acc:         0.798700 loss:        0.640035
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.878860 loss:        0.352785
Test - acc:         0.834200 loss:        0.500045
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.880520 loss:        0.351507
Test - acc:         0.837000 loss:        0.483760
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.879500 loss:        0.350178
Test - acc:         0.833800 loss:        0.509795
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.880420 loss:        0.350848
Test - acc:         0.821100 loss:        0.567117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.881360 loss:        0.344487
Test - acc:         0.839800 loss:        0.479123
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.880760 loss:        0.348547
Test - acc:         0.821200 loss:        0.535436
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.881960 loss:        0.344331
Test - acc:         0.849900 loss:        0.454892
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.884680 loss:        0.337788
Test - acc:         0.850800 loss:        0.467627
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.882920 loss:        0.343065
Test - acc:         0.835100 loss:        0.487859
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.884920 loss:        0.336744
Test - acc:         0.853300 loss:        0.450791
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.883660 loss:        0.341292
Test - acc:         0.845400 loss:        0.463925
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.885240 loss:        0.340414
Test - acc:         0.835100 loss:        0.508432
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.883960 loss:        0.336862
Test - acc:         0.851500 loss:        0.469349
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.886720 loss:        0.334101
Test - acc:         0.829700 loss:        0.517003
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.886760 loss:        0.337058
Test - acc:         0.808500 loss:        0.583414
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.887880 loss:        0.330029
Test - acc:         0.842600 loss:        0.472905
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.888580 loss:        0.331149
Test - acc:         0.854200 loss:        0.419408
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.886040 loss:        0.335991
Test - acc:         0.850400 loss:        0.454128
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.890120 loss:        0.324418
Test - acc:         0.845300 loss:        0.457349
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.886900 loss:        0.332056
Test - acc:         0.822500 loss:        0.541397
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.889560 loss:        0.328144
Test - acc:         0.824900 loss:        0.528527
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.887640 loss:        0.331694
Test - acc:         0.843700 loss:        0.469096
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.890300 loss:        0.325172
Test - acc:         0.779800 loss:        0.704603
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.889980 loss:        0.324385
Test - acc:         0.836200 loss:        0.490818
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.888400 loss:        0.327677
Test - acc:         0.764300 loss:        0.732546
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.889960 loss:        0.326095
Test - acc:         0.826200 loss:        0.528966
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.888180 loss:        0.327213
Test - acc:         0.867100 loss:        0.412080
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.890920 loss:        0.322995
Test - acc:         0.851700 loss:        0.456365
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.888960 loss:        0.325928
Test - acc:         0.839000 loss:        0.471037
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.889240 loss:        0.322839
Test - acc:         0.832700 loss:        0.511367
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.889180 loss:        0.324538
Test - acc:         0.852900 loss:        0.429188
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.890900 loss:        0.321132
Test - acc:         0.848300 loss:        0.463419
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.888240 loss:        0.325699
Test - acc:         0.829800 loss:        0.533614
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.889000 loss:        0.321882
Test - acc:         0.843700 loss:        0.478733
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.891200 loss:        0.320338
Test - acc:         0.835300 loss:        0.509598
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.888600 loss:        0.322581
Test - acc:         0.776000 loss:        0.733588
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.894340 loss:        0.315268
Test - acc:         0.812000 loss:        0.607886
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.891020 loss:        0.319830
Test - acc:         0.839500 loss:        0.496077
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.889160 loss:        0.327541
Test - acc:         0.817300 loss:        0.560130
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.890440 loss:        0.323284
Test - acc:         0.862200 loss:        0.427684
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.892160 loss:        0.316462
Test - acc:         0.876900 loss:        0.374392
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.891900 loss:        0.321718
Test - acc:         0.862800 loss:        0.416596
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.891240 loss:        0.322974
Test - acc:         0.794300 loss:        0.641058
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.891520 loss:        0.319810
Test - acc:         0.870300 loss:        0.373097
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.893980 loss:        0.314348
Test - acc:         0.869000 loss:        0.404470
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.892200 loss:        0.320358
Test - acc:         0.855000 loss:        0.442243
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.894060 loss:        0.312094
Test - acc:         0.858200 loss:        0.432529
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.892500 loss:        0.316941
Test - acc:         0.861500 loss:        0.415334
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.890120 loss:        0.323339
Test - acc:         0.825200 loss:        0.571354
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.891700 loss:        0.317806
Test - acc:         0.826200 loss:        0.525665
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.891960 loss:        0.317302
Test - acc:         0.836200 loss:        0.508369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.893060 loss:        0.319321
Test - acc:         0.861400 loss:        0.422518
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.893340 loss:        0.311649
Test - acc:         0.843600 loss:        0.478975
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.893040 loss:        0.314951
Test - acc:         0.849300 loss:        0.470103
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.892100 loss:        0.317747
Test - acc:         0.807200 loss:        0.633317
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.891380 loss:        0.318067
Test - acc:         0.819300 loss:        0.576206
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.891300 loss:        0.319368
Test - acc:         0.860700 loss:        0.423628
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.893540 loss:        0.312058
Test - acc:         0.842300 loss:        0.485303
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.894100 loss:        0.311809
Test - acc:         0.839700 loss:        0.487034
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.893480 loss:        0.312225
Test - acc:         0.810300 loss:        0.569116
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.893180 loss:        0.313282
Test - acc:         0.861000 loss:        0.429070
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.893460 loss:        0.313290
Test - acc:         0.839700 loss:        0.481364
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.894680 loss:        0.311358
Test - acc:         0.826300 loss:        0.556854
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.891900 loss:        0.315955
Test - acc:         0.807900 loss:        0.574278
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.893840 loss:        0.312283
Test - acc:         0.858000 loss:        0.424965
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.893140 loss:        0.312021
Test - acc:         0.823900 loss:        0.537903
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.893240 loss:        0.311579
Test - acc:         0.842400 loss:        0.502984
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.895580 loss:        0.307271
Test - acc:         0.847200 loss:        0.469832
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.893640 loss:        0.312167
Test - acc:         0.849300 loss:        0.452280
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.893260 loss:        0.312878
Test - acc:         0.839400 loss:        0.496554
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.894660 loss:        0.308718
Test - acc:         0.846100 loss:        0.468981
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.894700 loss:        0.311763
Test - acc:         0.800000 loss:        0.668806
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.895000 loss:        0.312719
Test - acc:         0.860200 loss:        0.416721
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.894300 loss:        0.308365
Test - acc:         0.856600 loss:        0.438748
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.894600 loss:        0.309460
Test - acc:         0.873300 loss:        0.380011
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.894320 loss:        0.311125
Test - acc:         0.841100 loss:        0.482505
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.896340 loss:        0.306162
Test - acc:         0.844500 loss:        0.478318
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.894260 loss:        0.310715
Test - acc:         0.720300 loss:        0.943107
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.894560 loss:        0.309806
Test - acc:         0.839600 loss:        0.481686
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.895900 loss:        0.309142
Test - acc:         0.863400 loss:        0.422594
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.893260 loss:        0.312585
Test - acc:         0.852400 loss:        0.438799
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.895260 loss:        0.311534
Test - acc:         0.845400 loss:        0.470561
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.895680 loss:        0.310607
Test - acc:         0.851700 loss:        0.443347
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.895800 loss:        0.307366
Test - acc:         0.832900 loss:        0.540493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.894860 loss:        0.308342
Test - acc:         0.866200 loss:        0.399040
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.893520 loss:        0.313218
Test - acc:         0.824500 loss:        0.547688
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.913080 loss:        0.255744
Test - acc:         0.877500 loss:        0.374256
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.908600 loss:        0.265334
Test - acc:         0.868700 loss:        0.395801
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.907020 loss:        0.271635
Test - acc:         0.843300 loss:        0.500252
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.905880 loss:        0.276082
Test - acc:         0.863100 loss:        0.431558
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.908480 loss:        0.272892
Test - acc:         0.850100 loss:        0.474191
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.906360 loss:        0.274709
Test - acc:         0.838900 loss:        0.489812
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.906460 loss:        0.276079
Test - acc:         0.850000 loss:        0.452748
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.906560 loss:        0.270831
Test - acc:         0.873600 loss:        0.374552
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.905880 loss:        0.277073
Test - acc:         0.770200 loss:        0.780505
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.907700 loss:        0.272841
Test - acc:         0.864300 loss:        0.418420
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.904220 loss:        0.280661
Test - acc:         0.858500 loss:        0.421210
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.907560 loss:        0.268856
Test - acc:         0.852300 loss:        0.467261
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.909180 loss:        0.266323
Test - acc:         0.846000 loss:        0.465807
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.906740 loss:        0.270553
Test - acc:         0.814300 loss:        0.577388
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.906460 loss:        0.273561
Test - acc:         0.852500 loss:        0.463608
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.911160 loss:        0.263932
Test - acc:         0.833700 loss:        0.530966
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.909780 loss:        0.267758
Test - acc:         0.852200 loss:        0.446428
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.908580 loss:        0.270837
Test - acc:         0.857800 loss:        0.442613
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.907900 loss:        0.272004
Test - acc:         0.882000 loss:        0.359026
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.906600 loss:        0.272785
Test - acc:         0.858400 loss:        0.441917
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.909120 loss:        0.266250
Test - acc:         0.839000 loss:        0.502400
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.908300 loss:        0.269434
Test - acc:         0.840700 loss:        0.503492
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.905720 loss:        0.275185
Test - acc:         0.862400 loss:        0.417843
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.908220 loss:        0.268643
Test - acc:         0.869100 loss:        0.397970
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.905640 loss:        0.276192
Test - acc:         0.869600 loss:        0.385175
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.907500 loss:        0.270997
Test - acc:         0.864500 loss:        0.406543
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.908040 loss:        0.269531
Test - acc:         0.879700 loss:        0.353709
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.907540 loss:        0.270028
Test - acc:         0.873000 loss:        0.392179
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.907920 loss:        0.269170
Test - acc:         0.852000 loss:        0.467655
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.910080 loss:        0.264588
Test - acc:         0.849400 loss:        0.461113
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.909600 loss:        0.263136
Test - acc:         0.882600 loss:        0.354227
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.906680 loss:        0.270617
Test - acc:         0.831100 loss:        0.533833
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.907060 loss:        0.270355
Test - acc:         0.853700 loss:        0.433036
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.954440 loss:        0.137731
Test - acc:         0.936100 loss:        0.190019
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.968680 loss:        0.095546
Test - acc:         0.938600 loss:        0.183025
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.973940 loss:        0.078152
Test - acc:         0.941600 loss:        0.180275
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.976560 loss:        0.070228
Test - acc:         0.939800 loss:        0.177250
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.979980 loss:        0.061384
Test - acc:         0.942800 loss:        0.177079
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.982360 loss:        0.053849
Test - acc:         0.940300 loss:        0.188126
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.984380 loss:        0.048040
Test - acc:         0.941900 loss:        0.186413
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.986100 loss:        0.043656
Test - acc:         0.942700 loss:        0.191181
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.987280 loss:        0.040120
Test - acc:         0.944200 loss:        0.186118
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.988120 loss:        0.036874
Test - acc:         0.943000 loss:        0.183372
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.989900 loss:        0.032119
Test - acc:         0.941500 loss:        0.202090
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.990200 loss:        0.030951
Test - acc:         0.941500 loss:        0.200437
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.991120 loss:        0.029145
Test - acc:         0.939900 loss:        0.204116
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.991400 loss:        0.027744
Test - acc:         0.940200 loss:        0.208454
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.991440 loss:        0.027118
Test - acc:         0.938000 loss:        0.218665
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.992740 loss:        0.024706
Test - acc:         0.941500 loss:        0.208377
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.992040 loss:        0.024951
Test - acc:         0.942300 loss:        0.211115
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.992260 loss:        0.024942
Test - acc:         0.941700 loss:        0.211392
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.993400 loss:        0.022868
Test - acc:         0.939800 loss:        0.222383
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.992720 loss:        0.024761
Test - acc:         0.940300 loss:        0.218211
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.993000 loss:        0.023168
Test - acc:         0.941600 loss:        0.210265
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.991200 loss:        0.026742
Test - acc:         0.941200 loss:        0.212582
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.992060 loss:        0.024922
Test - acc:         0.938700 loss:        0.218615
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.992920 loss:        0.023268
Test - acc:         0.934800 loss:        0.229559
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.991900 loss:        0.025095
Test - acc:         0.940200 loss:        0.214500
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.991540 loss:        0.026492
Test - acc:         0.940300 loss:        0.218601
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.991940 loss:        0.025903
Test - acc:         0.935500 loss:        0.231666
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.990780 loss:        0.028440
Test - acc:         0.937700 loss:        0.227256
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.991760 loss:        0.027161
Test - acc:         0.934100 loss:        0.232158
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.991180 loss:        0.028600
Test - acc:         0.936500 loss:        0.229883
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.989180 loss:        0.033372
Test - acc:         0.935900 loss:        0.226379
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.989280 loss:        0.032939
Test - acc:         0.935200 loss:        0.225763
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.990700 loss:        0.029552
Test - acc:         0.932500 loss:        0.250232
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.989140 loss:        0.033840
Test - acc:         0.936200 loss:        0.232483
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.987600 loss:        0.037158
Test - acc:         0.931000 loss:        0.252974
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.989260 loss:        0.033711
Test - acc:         0.935400 loss:        0.240317
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.988700 loss:        0.036580
Test - acc:         0.931500 loss:        0.250254
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.988720 loss:        0.034618
Test - acc:         0.925700 loss:        0.265676
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.988280 loss:        0.037051
Test - acc:         0.933600 loss:        0.245327
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.987560 loss:        0.038205
Test - acc:         0.932700 loss:        0.242201
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.987160 loss:        0.038587
Test - acc:         0.926000 loss:        0.256557
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.988800 loss:        0.035287
Test - acc:         0.925100 loss:        0.268426
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.987480 loss:        0.037928
Test - acc:         0.931900 loss:        0.253065
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.987140 loss:        0.039845
Test - acc:         0.929500 loss:        0.252414
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.984760 loss:        0.045243
Test - acc:         0.924700 loss:        0.273945
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.986080 loss:        0.042443
Test - acc:         0.929300 loss:        0.252708
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.987920 loss:        0.039270
Test - acc:         0.931100 loss:        0.244254
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.987120 loss:        0.038844
Test - acc:         0.931600 loss:        0.247416
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.987840 loss:        0.037919
Test - acc:         0.931300 loss:        0.249347
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.987180 loss:        0.039423
Test - acc:         0.926800 loss:        0.259435
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.987500 loss:        0.038755
Test - acc:         0.926800 loss:        0.261110
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.987500 loss:        0.039683
Test - acc:         0.930000 loss:        0.262027
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.985640 loss:        0.043191
Test - acc:         0.931900 loss:        0.249232
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.984580 loss:        0.045281
Test - acc:         0.928900 loss:        0.255247
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.985240 loss:        0.044748
Test - acc:         0.921600 loss:        0.289228
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.985160 loss:        0.044588
Test - acc:         0.923300 loss:        0.283645
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.987260 loss:        0.039422
Test - acc:         0.925800 loss:        0.274387
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.987600 loss:        0.039189
Test - acc:         0.934700 loss:        0.237873
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.986360 loss:        0.041412
Test - acc:         0.931600 loss:        0.244325
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.987200 loss:        0.040924
Test - acc:         0.931100 loss:        0.261383
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.986420 loss:        0.043236
Test - acc:         0.928100 loss:        0.261713
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.986680 loss:        0.041107
Test - acc:         0.923200 loss:        0.269888
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.986060 loss:        0.042434
Test - acc:         0.927700 loss:        0.259388
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.986280 loss:        0.042498
Test - acc:         0.917700 loss:        0.298566
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.986040 loss:        0.042414
Test - acc:         0.932400 loss:        0.252457
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.987240 loss:        0.039053
Test - acc:         0.930300 loss:        0.255754
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.987100 loss:        0.039414
Test - acc:         0.929700 loss:        0.257158
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.986080 loss:        0.041637
Test - acc:         0.928900 loss:        0.243229
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.986380 loss:        0.042046
Test - acc:         0.928900 loss:        0.252113
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.987400 loss:        0.039483
Test - acc:         0.930200 loss:        0.244437
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.987300 loss:        0.039902
Test - acc:         0.923000 loss:        0.282457
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.987280 loss:        0.039095
Test - acc:         0.931200 loss:        0.253701
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.985540 loss:        0.043860
Test - acc:         0.933200 loss:        0.230712
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.986800 loss:        0.040986
Test - acc:         0.928100 loss:        0.247257
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.987180 loss:        0.039648
Test - acc:         0.929100 loss:        0.251962
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.985380 loss:        0.043290
Test - acc:         0.921300 loss:        0.286803
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.986720 loss:        0.040909
Test - acc:         0.924700 loss:        0.285889
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.985640 loss:        0.043277
Test - acc:         0.927600 loss:        0.267893
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.985800 loss:        0.043117
Test - acc:         0.923500 loss:        0.268856
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.986740 loss:        0.040542
Test - acc:         0.925500 loss:        0.274751
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.985940 loss:        0.041977
Test - acc:         0.924100 loss:        0.278952
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.985820 loss:        0.043324
Test - acc:         0.926300 loss:        0.270937
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.987660 loss:        0.037921
Test - acc:         0.926800 loss:        0.253054
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.987480 loss:        0.039966
Test - acc:         0.930700 loss:        0.258777
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.991320 loss:        0.031044
Test - acc:         0.937000 loss:        0.238221
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.994160 loss:        0.020967
Test - acc:         0.936700 loss:        0.235190
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.993360 loss:        0.023235
Test - acc:         0.936300 loss:        0.243956
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.993360 loss:        0.022599
Test - acc:         0.938700 loss:        0.235918
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.993860 loss:        0.022040
Test - acc:         0.938300 loss:        0.238322
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.992820 loss:        0.023874
Test - acc:         0.933000 loss:        0.255258
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.991800 loss:        0.026613
Test - acc:         0.926800 loss:        0.261459
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.991800 loss:        0.026952
Test - acc:         0.934000 loss:        0.255557
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.992220 loss:        0.024476
Test - acc:         0.929000 loss:        0.283231
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.992320 loss:        0.026050
Test - acc:         0.929200 loss:        0.266687
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.991220 loss:        0.028365
Test - acc:         0.932500 loss:        0.249748
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.990560 loss:        0.029984
Test - acc:         0.930200 loss:        0.266778
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.991180 loss:        0.029281
Test - acc:         0.922500 loss:        0.287307
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.990480 loss:        0.030991
Test - acc:         0.928400 loss:        0.268017
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.989100 loss:        0.033793
Test - acc:         0.925800 loss:        0.272330
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.991680 loss:        0.026832
Test - acc:         0.931400 loss:        0.270953
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.996600 loss:        0.013001
Test - acc:         0.946700 loss:        0.208441
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.998200 loss:        0.007615
Test - acc:         0.947100 loss:        0.204888
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.998860 loss:        0.006264
Test - acc:         0.948200 loss:        0.204319
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.999160 loss:        0.005603
Test - acc:         0.948300 loss:        0.201990
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.999180 loss:        0.005223
Test - acc:         0.949800 loss:        0.199397
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.004662
Test - acc:         0.948700 loss:        0.199359
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.004226
Test - acc:         0.949900 loss:        0.195372
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.003932
Test - acc:         0.949600 loss:        0.196536
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.003825
Test - acc:         0.949700 loss:        0.194705
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.003601
Test - acc:         0.950400 loss:        0.195407
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.003702
Test - acc:         0.950300 loss:        0.193587
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.003253
Test - acc:         0.949800 loss:        0.194006
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.003398
Test - acc:         0.950600 loss:        0.195677
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.003142
Test - acc:         0.951400 loss:        0.195211
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.003203
Test - acc:         0.950500 loss:        0.194353
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.002984
Test - acc:         0.951000 loss:        0.190729
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.003126
Test - acc:         0.951500 loss:        0.192551
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002775
Test - acc:         0.950900 loss:        0.190007
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.003106
Test - acc:         0.951600 loss:        0.191420
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003176
Test - acc:         0.950600 loss:        0.190861
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.002824
Test - acc:         0.951400 loss:        0.191489
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.002768
Test - acc:         0.951200 loss:        0.190474
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002625
Test - acc:         0.951000 loss:        0.189533
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.002815
Test - acc:         0.952000 loss:        0.188979
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002525
Test - acc:         0.951300 loss:        0.191250
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002505
Test - acc:         0.951800 loss:        0.188239
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002548
Test - acc:         0.952700 loss:        0.186538
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002356
Test - acc:         0.953500 loss:        0.187060
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002515
Test - acc:         0.952200 loss:        0.187445
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002374
Test - acc:         0.953500 loss:        0.188475
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002438
Test - acc:         0.952800 loss:        0.187109
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002508
Test - acc:         0.952300 loss:        0.188390
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002268
Test - acc:         0.952500 loss:        0.186885
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002484
Test - acc:         0.952800 loss:        0.188417
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002482
Test - acc:         0.953500 loss:        0.185862
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002289
Test - acc:         0.953800 loss:        0.184686
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002535
Test - acc:         0.952700 loss:        0.185540
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002363
Test - acc:         0.952500 loss:        0.186450
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002319
Test - acc:         0.952800 loss:        0.186769
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002156
Test - acc:         0.953200 loss:        0.183680
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002355
Test - acc:         0.954500 loss:        0.184421
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002259
Test - acc:         0.953400 loss:        0.184204
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002229
Test - acc:         0.953600 loss:        0.184345
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002271
Test - acc:         0.952000 loss:        0.186154
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002300
Test - acc:         0.952400 loss:        0.185123
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002348
Test - acc:         0.952500 loss:        0.183171
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002255
Test - acc:         0.953100 loss:        0.184535
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002176
Test - acc:         0.953100 loss:        0.184463
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002315
Test - acc:         0.952600 loss:        0.183062
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002212
Test - acc:         0.953300 loss:        0.184804
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002134
Test - acc:         0.953400 loss:        0.185028
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002214
Test - acc:         0.953100 loss:        0.184183
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002198
Test - acc:         0.953600 loss:        0.183418
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002426
Test - acc:         0.953100 loss:        0.183331
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002153
Test - acc:         0.954800 loss:        0.181853
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002186
Test - acc:         0.953700 loss:        0.181822
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002257
Test - acc:         0.953100 loss:        0.184019
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002234
Test - acc:         0.953100 loss:        0.181807
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002082
Test - acc:         0.953500 loss:        0.181575
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002139
Test - acc:         0.953500 loss:        0.183509
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002262
Test - acc:         0.953500 loss:        0.183398
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002274
Test - acc:         0.952800 loss:        0.184924
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002327
Test - acc:         0.952900 loss:        0.181741
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002177
Test - acc:         0.953200 loss:        0.182468
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002145
Test - acc:         0.953800 loss:        0.182230
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002076
Test - acc:         0.953100 loss:        0.182175
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002020
Test - acc:         0.952700 loss:        0.182970
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002113
Test - acc:         0.954400 loss:        0.181076
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002250
Test - acc:         0.953400 loss:        0.181278
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002128
Test - acc:         0.953400 loss:        0.181493
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002100
Test - acc:         0.952500 loss:        0.181771
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002157
Test - acc:         0.953100 loss:        0.182606
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002240
Test - acc:         0.953100 loss:        0.182879
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002129
Test - acc:         0.953400 loss:        0.180207
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002124
Test - acc:         0.953100 loss:        0.181805
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002125
Test - acc:         0.954600 loss:        0.181110
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002096
Test - acc:         0.952900 loss:        0.180747
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002047
Test - acc:         0.952900 loss:        0.180194
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002132
Test - acc:         0.952900 loss:        0.180150
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002177
Test - acc:         0.951400 loss:        0.180912
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002062
Test - acc:         0.952900 loss:        0.180427
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002138
Test - acc:         0.953800 loss:        0.180342
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002260
Test - acc:         0.953200 loss:        0.181544
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002239
Test - acc:         0.953200 loss:        0.181600
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002138
Test - acc:         0.953100 loss:        0.181298
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002085
Test - acc:         0.953300 loss:        0.181811
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002070
Test - acc:         0.952700 loss:        0.180515
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002023
Test - acc:         0.953400 loss:        0.179271
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002056
Test - acc:         0.953300 loss:        0.178905
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002121
Test - acc:         0.953000 loss:        0.179634
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002145
Test - acc:         0.953600 loss:        0.178524
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002116
Test - acc:         0.954500 loss:        0.177917
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002281
Test - acc:         0.953300 loss:        0.178870
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002184
Test - acc:         0.953100 loss:        0.180460
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002226
Test - acc:         0.954600 loss:        0.178918
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002210
Test - acc:         0.954800 loss:        0.176595
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002144
Test - acc:         0.953900 loss:        0.179559
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002147
Test - acc:         0.954600 loss:        0.180132
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002091
Test - acc:         0.954700 loss:        0.176906
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002125
Test - acc:         0.954800 loss:        0.176235
Sparsity :          0.7500
Wdecay :        0.000500
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "magnitude",
    "prune_freq": 70,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf70_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.335980 loss:        1.834952
Test - acc:         0.434500 loss:        1.516146
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.499920 loss:        1.365646
Test - acc:         0.537700 loss:        1.276935
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.611440 loss:        1.086855
Test - acc:         0.651300 loss:        0.967936
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.677260 loss:        0.902607
Test - acc:         0.710500 loss:        0.822695
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.738000 loss:        0.748496
Test - acc:         0.700700 loss:        0.876529
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.774800 loss:        0.643551
Test - acc:         0.701600 loss:        0.851181
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.794800 loss:        0.592851
Test - acc:         0.783100 loss:        0.633620
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.811260 loss:        0.547022
Test - acc:         0.764600 loss:        0.729727
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.822840 loss:        0.515704
Test - acc:         0.782200 loss:        0.629174
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.827640 loss:        0.501411
Test - acc:         0.744500 loss:        0.794391
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.834920 loss:        0.481459
Test - acc:         0.783700 loss:        0.669560
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.840020 loss:        0.466377
Test - acc:         0.816200 loss:        0.550512
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.846700 loss:        0.446303
Test - acc:         0.817500 loss:        0.551231
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.847620 loss:        0.440282
Test - acc:         0.804700 loss:        0.591621
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.853220 loss:        0.429697
Test - acc:         0.825900 loss:        0.523532
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.858960 loss:        0.417897
Test - acc:         0.814100 loss:        0.563547
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.860980 loss:        0.406157
Test - acc:         0.809000 loss:        0.589957
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.862760 loss:        0.401622
Test - acc:         0.820900 loss:        0.561647
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.861200 loss:        0.399493
Test - acc:         0.792100 loss:        0.670461
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.866400 loss:        0.388288
Test - acc:         0.829800 loss:        0.510664
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.868420 loss:        0.386417
Test - acc:         0.737400 loss:        0.798181
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870620 loss:        0.378511
Test - acc:         0.778200 loss:        0.700896
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.868920 loss:        0.384943
Test - acc:         0.764300 loss:        0.800272
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.873520 loss:        0.372131
Test - acc:         0.841100 loss:        0.478396
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.873820 loss:        0.372261
Test - acc:         0.790300 loss:        0.657671
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.874320 loss:        0.366219
Test - acc:         0.805200 loss:        0.577301
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.875640 loss:        0.366298
Test - acc:         0.829900 loss:        0.527510
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.874740 loss:        0.368855
Test - acc:         0.843700 loss:        0.463200
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.876060 loss:        0.360821
Test - acc:         0.801300 loss:        0.613367
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.879120 loss:        0.356276
Test - acc:         0.811800 loss:        0.579423
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.877900 loss:        0.356128
Test - acc:         0.798700 loss:        0.640035
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.878860 loss:        0.352785
Test - acc:         0.834200 loss:        0.500045
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.880520 loss:        0.351507
Test - acc:         0.837000 loss:        0.483760
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.879500 loss:        0.350178
Test - acc:         0.833800 loss:        0.509795
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.880420 loss:        0.350848
Test - acc:         0.821100 loss:        0.567117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.881360 loss:        0.344487
Test - acc:         0.839800 loss:        0.479123
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.880760 loss:        0.348547
Test - acc:         0.821200 loss:        0.535436
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.881960 loss:        0.344331
Test - acc:         0.849900 loss:        0.454892
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.884680 loss:        0.337788
Test - acc:         0.850800 loss:        0.467627
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.882920 loss:        0.343065
Test - acc:         0.835100 loss:        0.487859
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.884920 loss:        0.336744
Test - acc:         0.853300 loss:        0.450791
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.883660 loss:        0.341292
Test - acc:         0.845400 loss:        0.463925
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.885240 loss:        0.340414
Test - acc:         0.835100 loss:        0.508432
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.883960 loss:        0.336862
Test - acc:         0.851500 loss:        0.469349
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.886720 loss:        0.334101
Test - acc:         0.829700 loss:        0.517003
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.886760 loss:        0.337058
Test - acc:         0.808500 loss:        0.583414
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.887880 loss:        0.330029
Test - acc:         0.842600 loss:        0.472905
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.888580 loss:        0.331149
Test - acc:         0.854200 loss:        0.419408
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.886040 loss:        0.335991
Test - acc:         0.850400 loss:        0.454128
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.890120 loss:        0.324418
Test - acc:         0.845300 loss:        0.457349
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.886900 loss:        0.332056
Test - acc:         0.822500 loss:        0.541397
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.889560 loss:        0.328144
Test - acc:         0.824900 loss:        0.528527
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.887640 loss:        0.331694
Test - acc:         0.843700 loss:        0.469096
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.890300 loss:        0.325172
Test - acc:         0.779800 loss:        0.704603
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.889980 loss:        0.324385
Test - acc:         0.836200 loss:        0.490818
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.888400 loss:        0.327677
Test - acc:         0.764300 loss:        0.732546
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.889960 loss:        0.326095
Test - acc:         0.826200 loss:        0.528966
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.888180 loss:        0.327213
Test - acc:         0.867100 loss:        0.412080
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.890920 loss:        0.322995
Test - acc:         0.851700 loss:        0.456365
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.888960 loss:        0.325928
Test - acc:         0.839000 loss:        0.471037
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.889240 loss:        0.322839
Test - acc:         0.832700 loss:        0.511367
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.889180 loss:        0.324538
Test - acc:         0.852900 loss:        0.429188
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.890900 loss:        0.321132
Test - acc:         0.848300 loss:        0.463419
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.888240 loss:        0.325699
Test - acc:         0.829800 loss:        0.533614
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.889000 loss:        0.321882
Test - acc:         0.843700 loss:        0.478733
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.891200 loss:        0.320338
Test - acc:         0.835300 loss:        0.509598
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.888600 loss:        0.322581
Test - acc:         0.776000 loss:        0.733588
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.894340 loss:        0.315268
Test - acc:         0.812000 loss:        0.607886
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.891020 loss:        0.319830
Test - acc:         0.839500 loss:        0.496077
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.889160 loss:        0.327541
Test - acc:         0.817300 loss:        0.560130
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.908860 loss:        0.269610
Test - acc:         0.825100 loss:        0.583784
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.908260 loss:        0.270933
Test - acc:         0.878600 loss:        0.367938
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.905440 loss:        0.279573
Test - acc:         0.861700 loss:        0.411692
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.903760 loss:        0.281358
Test - acc:         0.845500 loss:        0.470938
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.903700 loss:        0.283667
Test - acc:         0.857700 loss:        0.431324
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.905200 loss:        0.277474
Test - acc:         0.836900 loss:        0.507650
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.905880 loss:        0.277853
Test - acc:         0.858000 loss:        0.434299
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.904380 loss:        0.279827
Test - acc:         0.864500 loss:        0.392483
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.902020 loss:        0.281412
Test - acc:         0.862600 loss:        0.409933
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.902120 loss:        0.286217
Test - acc:         0.875500 loss:        0.386653
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.904640 loss:        0.279522
Test - acc:         0.856300 loss:        0.427833
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.906200 loss:        0.275241
Test - acc:         0.838100 loss:        0.524511
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.905380 loss:        0.274923
Test - acc:         0.884800 loss:        0.347528
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.906160 loss:        0.273776
Test - acc:         0.864800 loss:        0.412819
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.903280 loss:        0.279783
Test - acc:         0.838400 loss:        0.512622
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.905820 loss:        0.276740
Test - acc:         0.814600 loss:        0.624257
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.906140 loss:        0.276357
Test - acc:         0.863400 loss:        0.419026
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.904700 loss:        0.276122
Test - acc:         0.857700 loss:        0.436694
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.904020 loss:        0.275592
Test - acc:         0.848100 loss:        0.478576
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.904420 loss:        0.275733
Test - acc:         0.875800 loss:        0.370199
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.905980 loss:        0.273274
Test - acc:         0.853200 loss:        0.441871
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.907200 loss:        0.273152
Test - acc:         0.862100 loss:        0.442529
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.904080 loss:        0.279080
Test - acc:         0.844000 loss:        0.478822
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.907040 loss:        0.273320
Test - acc:         0.823800 loss:        0.555126
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.905860 loss:        0.277675
Test - acc:         0.830600 loss:        0.498657
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.905260 loss:        0.273313
Test - acc:         0.841400 loss:        0.493363
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.906660 loss:        0.272867
Test - acc:         0.835800 loss:        0.519688
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.906200 loss:        0.274928
Test - acc:         0.826000 loss:        0.523660
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.908320 loss:        0.270181
Test - acc:         0.846900 loss:        0.485885
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.909520 loss:        0.267110
Test - acc:         0.836100 loss:        0.486528
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.906220 loss:        0.274769
Test - acc:         0.858400 loss:        0.444887
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.909620 loss:        0.269655
Test - acc:         0.857900 loss:        0.428496
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.908240 loss:        0.273997
Test - acc:         0.843700 loss:        0.470665
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.905620 loss:        0.277373
Test - acc:         0.861000 loss:        0.425923
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.904440 loss:        0.277284
Test - acc:         0.815200 loss:        0.628198
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.905840 loss:        0.271872
Test - acc:         0.859600 loss:        0.418325
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.905360 loss:        0.276326
Test - acc:         0.862200 loss:        0.407616
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.907540 loss:        0.269902
Test - acc:         0.799800 loss:        0.683630
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.907800 loss:        0.268410
Test - acc:         0.813700 loss:        0.588970
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.906400 loss:        0.272759
Test - acc:         0.815100 loss:        0.589567
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.905060 loss:        0.275621
Test - acc:         0.850800 loss:        0.471727
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.907660 loss:        0.274451
Test - acc:         0.871600 loss:        0.381289
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.905600 loss:        0.274832
Test - acc:         0.873600 loss:        0.394810
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.906500 loss:        0.274008
Test - acc:         0.867800 loss:        0.394853
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.905700 loss:        0.277344
Test - acc:         0.857700 loss:        0.434653
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.907360 loss:        0.272461
Test - acc:         0.871200 loss:        0.383757
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.905620 loss:        0.274930
Test - acc:         0.842600 loss:        0.512448
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.905520 loss:        0.275412
Test - acc:         0.866400 loss:        0.399685
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.906580 loss:        0.271077
Test - acc:         0.824300 loss:        0.536827
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.906220 loss:        0.276652
Test - acc:         0.853300 loss:        0.430436
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.904120 loss:        0.278554
Test - acc:         0.828100 loss:        0.526518
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.907880 loss:        0.271718
Test - acc:         0.859500 loss:        0.434178
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.907360 loss:        0.271402
Test - acc:         0.854000 loss:        0.441701
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.904580 loss:        0.279756
Test - acc:         0.807300 loss:        0.579119
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.907880 loss:        0.268530
Test - acc:         0.872500 loss:        0.388076
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.907360 loss:        0.273671
Test - acc:         0.821700 loss:        0.573605
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.905440 loss:        0.275075
Test - acc:         0.848500 loss:        0.479046
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.904400 loss:        0.277937
Test - acc:         0.839700 loss:        0.506686
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.907100 loss:        0.270170
Test - acc:         0.846100 loss:        0.474563
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.907440 loss:        0.274128
Test - acc:         0.841800 loss:        0.490083
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.905320 loss:        0.273083
Test - acc:         0.865600 loss:        0.406328
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.908440 loss:        0.271056
Test - acc:         0.832000 loss:        0.533492
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.906360 loss:        0.272263
Test - acc:         0.844800 loss:        0.488233
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.909160 loss:        0.267492
Test - acc:         0.830700 loss:        0.517014
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.904600 loss:        0.278165
Test - acc:         0.830200 loss:        0.507579
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.908100 loss:        0.269672
Test - acc:         0.865700 loss:        0.408295
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.906060 loss:        0.274003
Test - acc:         0.871800 loss:        0.400831
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.906580 loss:        0.272927
Test - acc:         0.853400 loss:        0.447302
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.907260 loss:        0.272483
Test - acc:         0.864800 loss:        0.411428
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.905820 loss:        0.275958
Test - acc:         0.864100 loss:        0.405042
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.920920 loss:        0.231694
Test - acc:         0.850900 loss:        0.462538
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.917200 loss:        0.243722
Test - acc:         0.861100 loss:        0.429060
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.917940 loss:        0.241390
Test - acc:         0.887000 loss:        0.339516
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.913240 loss:        0.250742
Test - acc:         0.866300 loss:        0.417268
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.916240 loss:        0.243624
Test - acc:         0.875500 loss:        0.385853
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.913560 loss:        0.250006
Test - acc:         0.863500 loss:        0.414348
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.915240 loss:        0.246625
Test - acc:         0.820200 loss:        0.597063
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.914000 loss:        0.248373
Test - acc:         0.877700 loss:        0.382503
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.913000 loss:        0.252105
Test - acc:         0.885500 loss:        0.352668
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.913820 loss:        0.249839
Test - acc:         0.874800 loss:        0.384561
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.956840 loss:        0.128707
Test - acc:         0.932300 loss:        0.189826
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.970760 loss:        0.090406
Test - acc:         0.936800 loss:        0.183606
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.974520 loss:        0.076158
Test - acc:         0.937600 loss:        0.181955
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.977760 loss:        0.067480
Test - acc:         0.941200 loss:        0.179497
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.980440 loss:        0.061111
Test - acc:         0.940000 loss:        0.181506
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.982640 loss:        0.054774
Test - acc:         0.941300 loss:        0.186033
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.985240 loss:        0.046536
Test - acc:         0.940400 loss:        0.188192
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.985900 loss:        0.043577
Test - acc:         0.941500 loss:        0.185630
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.986840 loss:        0.040403
Test - acc:         0.943100 loss:        0.188179
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.988420 loss:        0.035511
Test - acc:         0.941800 loss:        0.196550
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.988820 loss:        0.035441
Test - acc:         0.942300 loss:        0.196251
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.990340 loss:        0.031542
Test - acc:         0.941300 loss:        0.197023
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.990360 loss:        0.030799
Test - acc:         0.943500 loss:        0.198306
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.991240 loss:        0.029176
Test - acc:         0.942900 loss:        0.199968
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.990980 loss:        0.028682
Test - acc:         0.940600 loss:        0.209866
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.991260 loss:        0.027845
Test - acc:         0.942200 loss:        0.200322
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.991400 loss:        0.027897
Test - acc:         0.939900 loss:        0.208475
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.992360 loss:        0.024987
Test - acc:         0.939900 loss:        0.211424
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.992520 loss:        0.024401
Test - acc:         0.938400 loss:        0.209495
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.992800 loss:        0.023798
Test - acc:         0.942400 loss:        0.204639
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.992600 loss:        0.023936
Test - acc:         0.940800 loss:        0.213760
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.992000 loss:        0.025926
Test - acc:         0.941900 loss:        0.205614
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.992100 loss:        0.025712
Test - acc:         0.937600 loss:        0.225677
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.992880 loss:        0.023716
Test - acc:         0.938700 loss:        0.217805
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.992320 loss:        0.024614
Test - acc:         0.937700 loss:        0.238090
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.990780 loss:        0.027520
Test - acc:         0.937600 loss:        0.237015
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.990480 loss:        0.030276
Test - acc:         0.941800 loss:        0.207912
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.990740 loss:        0.029116
Test - acc:         0.938200 loss:        0.227733
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.990380 loss:        0.030225
Test - acc:         0.937600 loss:        0.228658
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.991260 loss:        0.028614
Test - acc:         0.933900 loss:        0.229674
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.991320 loss:        0.029007
Test - acc:         0.939100 loss:        0.222677
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.991280 loss:        0.028519
Test - acc:         0.935100 loss:        0.247226
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.990740 loss:        0.029775
Test - acc:         0.937000 loss:        0.241192
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.989900 loss:        0.031574
Test - acc:         0.934400 loss:        0.230869
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.988580 loss:        0.034901
Test - acc:         0.931500 loss:        0.250196
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.988720 loss:        0.035070
Test - acc:         0.932100 loss:        0.237166
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.990500 loss:        0.030597
Test - acc:         0.926700 loss:        0.274233
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.988320 loss:        0.036040
Test - acc:         0.930400 loss:        0.238420
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.987720 loss:        0.037880
Test - acc:         0.934300 loss:        0.228693
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.988080 loss:        0.037438
Test - acc:         0.934500 loss:        0.229714
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.988240 loss:        0.037556
Test - acc:         0.932100 loss:        0.232678
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.987000 loss:        0.039888
Test - acc:         0.911300 loss:        0.331459
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.987060 loss:        0.038528
Test - acc:         0.937100 loss:        0.229038
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.987880 loss:        0.037506
Test - acc:         0.933200 loss:        0.240363
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.987900 loss:        0.037135
Test - acc:         0.926300 loss:        0.266946
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.988420 loss:        0.035845
Test - acc:         0.931400 loss:        0.239634
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.986520 loss:        0.041644
Test - acc:         0.929000 loss:        0.244664
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.986640 loss:        0.041143
Test - acc:         0.931800 loss:        0.243056
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.987040 loss:        0.040596
Test - acc:         0.930100 loss:        0.242035
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.985700 loss:        0.043138
Test - acc:         0.934000 loss:        0.229430
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.986480 loss:        0.042473
Test - acc:         0.936400 loss:        0.231389
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.987800 loss:        0.037127
Test - acc:         0.932400 loss:        0.250892
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.987800 loss:        0.037778
Test - acc:         0.927900 loss:        0.267701
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.987220 loss:        0.040207
Test - acc:         0.929500 loss:        0.259419
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.985760 loss:        0.042705
Test - acc:         0.929500 loss:        0.266394
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.985740 loss:        0.043975
Test - acc:         0.926600 loss:        0.257369
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.986320 loss:        0.041318
Test - acc:         0.922600 loss:        0.287463
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.985900 loss:        0.042960
Test - acc:         0.932700 loss:        0.241834
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.986160 loss:        0.042967
Test - acc:         0.928400 loss:        0.245046
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.985880 loss:        0.042774
Test - acc:         0.929500 loss:        0.251234
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.988520 loss:        0.040609
Test - acc:         0.936300 loss:        0.219133
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.992100 loss:        0.028411
Test - acc:         0.931500 loss:        0.254469
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.992820 loss:        0.025464
Test - acc:         0.934000 loss:        0.249135
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.991940 loss:        0.027559
Test - acc:         0.935900 loss:        0.242973
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.990500 loss:        0.030414
Test - acc:         0.924300 loss:        0.276109
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.989780 loss:        0.033307
Test - acc:         0.933600 loss:        0.242747
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.990960 loss:        0.029598
Test - acc:         0.929700 loss:        0.268196
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.989660 loss:        0.032915
Test - acc:         0.934200 loss:        0.242690
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.989960 loss:        0.031329
Test - acc:         0.928700 loss:        0.268668
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.989220 loss:        0.034022
Test - acc:         0.922400 loss:        0.286532
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.989180 loss:        0.033677
Test - acc:         0.933000 loss:        0.243424
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.989880 loss:        0.031594
Test - acc:         0.925600 loss:        0.274107
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.989560 loss:        0.032910
Test - acc:         0.932900 loss:        0.251445
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.989080 loss:        0.035390
Test - acc:         0.931400 loss:        0.251030
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.988380 loss:        0.036741
Test - acc:         0.925800 loss:        0.266686
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.989000 loss:        0.035497
Test - acc:         0.926700 loss:        0.270862
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.989100 loss:        0.035107
Test - acc:         0.936400 loss:        0.235087
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.990540 loss:        0.029853
Test - acc:         0.927800 loss:        0.268068
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.987500 loss:        0.038208
Test - acc:         0.936400 loss:        0.237400
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.989300 loss:        0.033094
Test - acc:         0.922300 loss:        0.293970
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.987000 loss:        0.040047
Test - acc:         0.925900 loss:        0.266104
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.988240 loss:        0.036639
Test - acc:         0.935900 loss:        0.244550
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.989100 loss:        0.035567
Test - acc:         0.925500 loss:        0.266997
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.988920 loss:        0.033610
Test - acc:         0.932800 loss:        0.253113
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.986920 loss:        0.040563
Test - acc:         0.916900 loss:        0.308867
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.988740 loss:        0.035721
Test - acc:         0.928000 loss:        0.268940
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.989280 loss:        0.033797
Test - acc:         0.924200 loss:        0.287061
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.989320 loss:        0.033997
Test - acc:         0.928300 loss:        0.254480
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.990100 loss:        0.031989
Test - acc:         0.933500 loss:        0.251789
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.988960 loss:        0.035355
Test - acc:         0.931800 loss:        0.256958
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.986900 loss:        0.039046
Test - acc:         0.931400 loss:        0.251252
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.988880 loss:        0.035229
Test - acc:         0.931200 loss:        0.253278
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.987820 loss:        0.037164
Test - acc:         0.921700 loss:        0.286775
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.987980 loss:        0.036303
Test - acc:         0.928700 loss:        0.265697
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.989080 loss:        0.034956
Test - acc:         0.934900 loss:        0.237871
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.990240 loss:        0.031921
Test - acc:         0.928700 loss:        0.262612
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.989380 loss:        0.032426
Test - acc:         0.929500 loss:        0.277486
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.988020 loss:        0.036685
Test - acc:         0.924800 loss:        0.284912
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.988360 loss:        0.036329
Test - acc:         0.920400 loss:        0.298414
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.990100 loss:        0.033345
Test - acc:         0.931200 loss:        0.267456
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.995200 loss:        0.017347
Test - acc:         0.944900 loss:        0.207821
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.997420 loss:        0.011101
Test - acc:         0.947100 loss:        0.204447
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.998360 loss:        0.008487
Test - acc:         0.946700 loss:        0.204070
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.998600 loss:        0.007277
Test - acc:         0.947800 loss:        0.204510
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.998840 loss:        0.006982
Test - acc:         0.947100 loss:        0.200927
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.998860 loss:        0.006290
Test - acc:         0.947600 loss:        0.201241
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.999140 loss:        0.005946
Test - acc:         0.948400 loss:        0.199077
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.999260 loss:        0.005451
Test - acc:         0.947400 loss:        0.201623
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.005300
Test - acc:         0.947100 loss:        0.199213
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.004825
Test - acc:         0.946500 loss:        0.202639
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.004946
Test - acc:         0.948800 loss:        0.200923
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.004283
Test - acc:         0.948300 loss:        0.201242
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.004752
Test - acc:         0.948700 loss:        0.199240
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.004530
Test - acc:         0.949300 loss:        0.198301
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.004327
Test - acc:         0.948600 loss:        0.199621
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.003778
Test - acc:         0.948000 loss:        0.197095
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.004273
Test - acc:         0.948600 loss:        0.199178
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.004257
Test - acc:         0.948000 loss:        0.198619
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.003787
Test - acc:         0.949600 loss:        0.199990
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.004185
Test - acc:         0.949400 loss:        0.197328
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.003574
Test - acc:         0.949200 loss:        0.197713
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.003763
Test - acc:         0.950200 loss:        0.197130
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.003516
Test - acc:         0.949800 loss:        0.196415
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.003697
Test - acc:         0.948500 loss:        0.196788
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003606
Test - acc:         0.947700 loss:        0.196717
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.003436
Test - acc:         0.947800 loss:        0.195925
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.003456
Test - acc:         0.949900 loss:        0.192640
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.003468
Test - acc:         0.948800 loss:        0.194521
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.003301
Test - acc:         0.950200 loss:        0.195067
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.003211
Test - acc:         0.949400 loss:        0.194461
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.991400 loss:        0.041266
Test - acc:         0.941000 loss:        0.204263
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.995600 loss:        0.026663
Test - acc:         0.941500 loss:        0.199173
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.996320 loss:        0.023066
Test - acc:         0.941100 loss:        0.197526
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.996980 loss:        0.020801
Test - acc:         0.941400 loss:        0.195832
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.997500 loss:        0.018122
Test - acc:         0.941900 loss:        0.195062
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.997940 loss:        0.016351
Test - acc:         0.942600 loss:        0.197055
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.997520 loss:        0.016198
Test - acc:         0.943600 loss:        0.196273
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.997960 loss:        0.014936
Test - acc:         0.944500 loss:        0.194757
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.998200 loss:        0.013653
Test - acc:         0.943800 loss:        0.197450
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.998280 loss:        0.012751
Test - acc:         0.944900 loss:        0.194581
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.998500 loss:        0.012180
Test - acc:         0.945500 loss:        0.196231
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.998600 loss:        0.011822
Test - acc:         0.944900 loss:        0.196623
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.998640 loss:        0.010829
Test - acc:         0.944800 loss:        0.197146
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.998660 loss:        0.010675
Test - acc:         0.944300 loss:        0.198096
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.010124
Test - acc:         0.945400 loss:        0.197634
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.998820 loss:        0.009753
Test - acc:         0.944400 loss:        0.199012
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.998560 loss:        0.010096
Test - acc:         0.944200 loss:        0.200036
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.009631
Test - acc:         0.945400 loss:        0.198734
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.998740 loss:        0.009361
Test - acc:         0.946000 loss:        0.196726
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.998780 loss:        0.009049
Test - acc:         0.945200 loss:        0.198027
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.999220 loss:        0.008107
Test - acc:         0.946200 loss:        0.197964
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.999100 loss:        0.008154
Test - acc:         0.946000 loss:        0.200623
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.998920 loss:        0.008075
Test - acc:         0.946700 loss:        0.198383
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.998940 loss:        0.008027
Test - acc:         0.947100 loss:        0.197759
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.007522
Test - acc:         0.948000 loss:        0.196096
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.007020
Test - acc:         0.947800 loss:        0.197555
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.007193
Test - acc:         0.947600 loss:        0.199242
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999000 loss:        0.007581
Test - acc:         0.947000 loss:        0.196788
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.006844
Test - acc:         0.946400 loss:        0.201184
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.999180 loss:        0.007085
Test - acc:         0.947800 loss:        0.198446
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.006390
Test - acc:         0.947100 loss:        0.199547
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999260 loss:        0.006589
Test - acc:         0.947200 loss:        0.200024
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.006227
Test - acc:         0.948000 loss:        0.197740
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.006493
Test - acc:         0.947300 loss:        0.201183
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.006254
Test - acc:         0.947300 loss:        0.199772
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.006158
Test - acc:         0.946400 loss:        0.199860
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.005908
Test - acc:         0.946500 loss:        0.199938
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.006006
Test - acc:         0.947200 loss:        0.198414
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.005503
Test - acc:         0.946700 loss:        0.199844
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.005557
Test - acc:         0.947000 loss:        0.200789
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.005687
Test - acc:         0.946500 loss:        0.201198
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.005500
Test - acc:         0.945800 loss:        0.201681
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004997
Test - acc:         0.947100 loss:        0.200611
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.005696
Test - acc:         0.946700 loss:        0.201439
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.005570
Test - acc:         0.947400 loss:        0.200597
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.005322
Test - acc:         0.947700 loss:        0.200556
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.004920
Test - acc:         0.946700 loss:        0.203530
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.004968
Test - acc:         0.947900 loss:        0.200258
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.004723
Test - acc:         0.947700 loss:        0.201342
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.004961
Test - acc:         0.948000 loss:        0.202881
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.004756
Test - acc:         0.947600 loss:        0.202430
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.004844
Test - acc:         0.948300 loss:        0.200318
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.004991
Test - acc:         0.945800 loss:        0.204545
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.005032
Test - acc:         0.947900 loss:        0.200726
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.004939
Test - acc:         0.946800 loss:        0.204120
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.004622
Test - acc:         0.946300 loss:        0.201497
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.004784
Test - acc:         0.947100 loss:        0.202706
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.004691
Test - acc:         0.947700 loss:        0.201671
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004373
Test - acc:         0.947800 loss:        0.201869
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.004679
Test - acc:         0.947400 loss:        0.201960
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.004385
Test - acc:         0.947800 loss:        0.201652
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.004221
Test - acc:         0.947600 loss:        0.199796
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.004448
Test - acc:         0.948500 loss:        0.199950
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.004492
Test - acc:         0.948200 loss:        0.202623
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.004360
Test - acc:         0.949100 loss:        0.203129
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.004315
Test - acc:         0.949300 loss:        0.201298
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.004374
Test - acc:         0.949300 loss:        0.202823
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.004010
Test - acc:         0.948300 loss:        0.202664
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004150
Test - acc:         0.947600 loss:        0.203064
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.004126
Test - acc:         0.948300 loss:        0.202088
Sparsity :          0.9375
Wdecay :        0.000500
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "magnitude",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf50_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.335980 loss:        1.834952
Test - acc:         0.434500 loss:        1.516146
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.499920 loss:        1.365646
Test - acc:         0.537700 loss:        1.276935
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.611440 loss:        1.086855
Test - acc:         0.651300 loss:        0.967936
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.677260 loss:        0.902607
Test - acc:         0.710500 loss:        0.822695
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.738000 loss:        0.748496
Test - acc:         0.700700 loss:        0.876529
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.774800 loss:        0.643551
Test - acc:         0.701600 loss:        0.851181
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.794800 loss:        0.592851
Test - acc:         0.783100 loss:        0.633620
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.811260 loss:        0.547022
Test - acc:         0.764600 loss:        0.729727
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.822840 loss:        0.515704
Test - acc:         0.782200 loss:        0.629174
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.827640 loss:        0.501411
Test - acc:         0.744500 loss:        0.794391
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.834920 loss:        0.481459
Test - acc:         0.783700 loss:        0.669560
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.840020 loss:        0.466377
Test - acc:         0.816200 loss:        0.550512
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.846700 loss:        0.446303
Test - acc:         0.817500 loss:        0.551231
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.847620 loss:        0.440282
Test - acc:         0.804700 loss:        0.591621
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.853220 loss:        0.429697
Test - acc:         0.825900 loss:        0.523532
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.858960 loss:        0.417897
Test - acc:         0.814100 loss:        0.563547
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.860980 loss:        0.406157
Test - acc:         0.809000 loss:        0.589957
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.862760 loss:        0.401622
Test - acc:         0.820900 loss:        0.561647
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.861200 loss:        0.399493
Test - acc:         0.792100 loss:        0.670461
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.866400 loss:        0.388288
Test - acc:         0.829800 loss:        0.510664
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.868420 loss:        0.386417
Test - acc:         0.737400 loss:        0.798181
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870620 loss:        0.378511
Test - acc:         0.778200 loss:        0.700896
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.868920 loss:        0.384943
Test - acc:         0.764300 loss:        0.800272
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.873520 loss:        0.372131
Test - acc:         0.841100 loss:        0.478396
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.873820 loss:        0.372261
Test - acc:         0.790300 loss:        0.657671
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.874320 loss:        0.366219
Test - acc:         0.805200 loss:        0.577301
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.875640 loss:        0.366298
Test - acc:         0.829900 loss:        0.527510
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.874740 loss:        0.368855
Test - acc:         0.843700 loss:        0.463200
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.876060 loss:        0.360821
Test - acc:         0.801300 loss:        0.613367
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.879120 loss:        0.356276
Test - acc:         0.811800 loss:        0.579423
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.877900 loss:        0.356128
Test - acc:         0.798700 loss:        0.640035
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.878860 loss:        0.352785
Test - acc:         0.834200 loss:        0.500045
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.880520 loss:        0.351507
Test - acc:         0.837000 loss:        0.483760
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.879500 loss:        0.350178
Test - acc:         0.833800 loss:        0.509795
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.880420 loss:        0.350848
Test - acc:         0.821100 loss:        0.567117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.881360 loss:        0.344487
Test - acc:         0.839800 loss:        0.479123
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.880760 loss:        0.348547
Test - acc:         0.821200 loss:        0.535436
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.881960 loss:        0.344331
Test - acc:         0.849900 loss:        0.454892
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.884680 loss:        0.337788
Test - acc:         0.850800 loss:        0.467627
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.882920 loss:        0.343065
Test - acc:         0.835100 loss:        0.487859
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.884920 loss:        0.336744
Test - acc:         0.853300 loss:        0.450791
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.883660 loss:        0.341292
Test - acc:         0.845400 loss:        0.463925
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.885240 loss:        0.340414
Test - acc:         0.835100 loss:        0.508432
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.883960 loss:        0.336862
Test - acc:         0.851500 loss:        0.469349
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.886720 loss:        0.334101
Test - acc:         0.829700 loss:        0.517003
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.886760 loss:        0.337058
Test - acc:         0.808500 loss:        0.583414
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.887880 loss:        0.330029
Test - acc:         0.842600 loss:        0.472905
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.888580 loss:        0.331149
Test - acc:         0.854200 loss:        0.419408
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.886040 loss:        0.335991
Test - acc:         0.850400 loss:        0.454128
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.890120 loss:        0.324418
Test - acc:         0.845300 loss:        0.457349
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.904760 loss:        0.279017
Test - acc:         0.879000 loss:        0.365259
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.904140 loss:        0.282057
Test - acc:         0.847300 loss:        0.473975
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.901520 loss:        0.286388
Test - acc:         0.839300 loss:        0.513480
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.902100 loss:        0.290481
Test - acc:         0.866400 loss:        0.381486
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.900680 loss:        0.292692
Test - acc:         0.868700 loss:        0.392461
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.901360 loss:        0.286970
Test - acc:         0.790700 loss:        0.695833
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.901600 loss:        0.287263
Test - acc:         0.797100 loss:        0.657355
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.901900 loss:        0.287680
Test - acc:         0.872300 loss:        0.389182
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.901600 loss:        0.287248
Test - acc:         0.841900 loss:        0.489507
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.903040 loss:        0.286507
Test - acc:         0.827200 loss:        0.509336
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.902340 loss:        0.286534
Test - acc:         0.827500 loss:        0.551357
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.904780 loss:        0.284393
Test - acc:         0.847500 loss:        0.458204
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.902720 loss:        0.287430
Test - acc:         0.834700 loss:        0.517257
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.903960 loss:        0.283505
Test - acc:         0.836800 loss:        0.525538
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.903780 loss:        0.284853
Test - acc:         0.850200 loss:        0.462144
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.902280 loss:        0.285906
Test - acc:         0.819900 loss:        0.604253
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.903800 loss:        0.283627
Test - acc:         0.845500 loss:        0.478558
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.903280 loss:        0.280476
Test - acc:         0.837100 loss:        0.514262
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.902680 loss:        0.283088
Test - acc:         0.853100 loss:        0.437906
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.901320 loss:        0.286448
Test - acc:         0.837000 loss:        0.519127
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.901980 loss:        0.283224
Test - acc:         0.868500 loss:        0.402015
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.903940 loss:        0.279758
Test - acc:         0.849800 loss:        0.451143
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.901400 loss:        0.287749
Test - acc:         0.868700 loss:        0.389461
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.903700 loss:        0.281912
Test - acc:         0.865600 loss:        0.410128
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.905540 loss:        0.277045
Test - acc:         0.865900 loss:        0.400473
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.903700 loss:        0.281294
Test - acc:         0.859400 loss:        0.439887
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.904880 loss:        0.279693
Test - acc:         0.874500 loss:        0.376947
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.903620 loss:        0.278879
Test - acc:         0.871100 loss:        0.385337
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.903740 loss:        0.281698
Test - acc:         0.851000 loss:        0.446972
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.901940 loss:        0.286465
Test - acc:         0.820000 loss:        0.577082
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.903740 loss:        0.279950
Test - acc:         0.863000 loss:        0.417225
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.903960 loss:        0.277605
Test - acc:         0.832200 loss:        0.532370
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.904060 loss:        0.280551
Test - acc:         0.865700 loss:        0.399208
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.903600 loss:        0.282418
Test - acc:         0.858700 loss:        0.441716
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.904440 loss:        0.284195
Test - acc:         0.859500 loss:        0.428757
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.904720 loss:        0.277798
Test - acc:         0.837500 loss:        0.515922
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.904480 loss:        0.280591
Test - acc:         0.877800 loss:        0.383894
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.903280 loss:        0.280076
Test - acc:         0.835000 loss:        0.526088
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.901580 loss:        0.285409
Test - acc:         0.837700 loss:        0.508109
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.902720 loss:        0.284136
Test - acc:         0.839700 loss:        0.509769
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.904200 loss:        0.279838
Test - acc:         0.856400 loss:        0.439979
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.902160 loss:        0.284825
Test - acc:         0.837800 loss:        0.517596
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.905440 loss:        0.278828
Test - acc:         0.853000 loss:        0.457314
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.904440 loss:        0.279204
Test - acc:         0.854200 loss:        0.460775
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.903180 loss:        0.281775
Test - acc:         0.859000 loss:        0.417057
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.906540 loss:        0.275960
Test - acc:         0.838200 loss:        0.494493
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.905240 loss:        0.281219
Test - acc:         0.866000 loss:        0.403376
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.903880 loss:        0.281606
Test - acc:         0.740500 loss:        0.900838
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.904680 loss:        0.277666
Test - acc:         0.868300 loss:        0.415981
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.906040 loss:        0.275385
Test - acc:         0.838400 loss:        0.475072
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.917540 loss:        0.237549
Test - acc:         0.882800 loss:        0.350728
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.915920 loss:        0.244012
Test - acc:         0.851600 loss:        0.456460
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.914840 loss:        0.247107
Test - acc:         0.819900 loss:        0.629188
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.912080 loss:        0.256091
Test - acc:         0.842600 loss:        0.521873
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.910780 loss:        0.259177
Test - acc:         0.867400 loss:        0.404308
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.910660 loss:        0.254013
Test - acc:         0.876600 loss:        0.375252
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.913760 loss:        0.249811
Test - acc:         0.880200 loss:        0.358771
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.915180 loss:        0.249782
Test - acc:         0.865100 loss:        0.426657
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.914180 loss:        0.252927
Test - acc:         0.788500 loss:        0.710032
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.912740 loss:        0.252269
Test - acc:         0.847500 loss:        0.491111
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.912700 loss:        0.254873
Test - acc:         0.880600 loss:        0.365676
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.913400 loss:        0.252167
Test - acc:         0.849900 loss:        0.467576
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.915020 loss:        0.248819
Test - acc:         0.871700 loss:        0.403555
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.912600 loss:        0.255589
Test - acc:         0.858500 loss:        0.443433
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.913560 loss:        0.248986
Test - acc:         0.834300 loss:        0.516896
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.915500 loss:        0.250897
Test - acc:         0.877400 loss:        0.359225
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.912980 loss:        0.251055
Test - acc:         0.846700 loss:        0.467122
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.913840 loss:        0.252356
Test - acc:         0.872200 loss:        0.385401
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.913700 loss:        0.251587
Test - acc:         0.869000 loss:        0.395068
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.914980 loss:        0.249785
Test - acc:         0.852100 loss:        0.467870
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.912460 loss:        0.254692
Test - acc:         0.840600 loss:        0.517693
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.913880 loss:        0.250637
Test - acc:         0.855500 loss:        0.440076
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.911760 loss:        0.257627
Test - acc:         0.867200 loss:        0.408439
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.912360 loss:        0.252511
Test - acc:         0.858700 loss:        0.414915
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.915020 loss:        0.250075
Test - acc:         0.889300 loss:        0.342424
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.913400 loss:        0.255410
Test - acc:         0.855300 loss:        0.446240
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.916300 loss:        0.245848
Test - acc:         0.839100 loss:        0.539815
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.913040 loss:        0.253270
Test - acc:         0.868400 loss:        0.409341
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.913020 loss:        0.250905
Test - acc:         0.869700 loss:        0.420988
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.913240 loss:        0.253316
Test - acc:         0.850600 loss:        0.483376
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.914300 loss:        0.249174
Test - acc:         0.860000 loss:        0.420768
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.913940 loss:        0.250466
Test - acc:         0.863200 loss:        0.450642
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.914220 loss:        0.249829
Test - acc:         0.850200 loss:        0.465155
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.914540 loss:        0.250307
Test - acc:         0.854000 loss:        0.453599
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.912900 loss:        0.252828
Test - acc:         0.862500 loss:        0.426807
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.916260 loss:        0.248064
Test - acc:         0.877300 loss:        0.359179
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.912040 loss:        0.253923
Test - acc:         0.858500 loss:        0.434436
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.913520 loss:        0.251333
Test - acc:         0.814600 loss:        0.599191
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.913260 loss:        0.253520
Test - acc:         0.864900 loss:        0.412188
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.912300 loss:        0.251871
Test - acc:         0.834000 loss:        0.509715
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.913540 loss:        0.251304
Test - acc:         0.863500 loss:        0.419285
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.912120 loss:        0.253959
Test - acc:         0.875100 loss:        0.374860
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.914000 loss:        0.249625
Test - acc:         0.880800 loss:        0.370051
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.912960 loss:        0.247989
Test - acc:         0.858400 loss:        0.437531
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.912900 loss:        0.251104
Test - acc:         0.867000 loss:        0.410812
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.913720 loss:        0.252785
Test - acc:         0.863000 loss:        0.421253
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.915480 loss:        0.246922
Test - acc:         0.860600 loss:        0.423434
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.912500 loss:        0.252741
Test - acc:         0.856600 loss:        0.454924
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.913700 loss:        0.251485
Test - acc:         0.864200 loss:        0.426430
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.914600 loss:        0.251603
Test - acc:         0.860500 loss:        0.414295
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.948060 loss:        0.161288
Test - acc:         0.931700 loss:        0.200267
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.963760 loss:        0.110769
Test - acc:         0.935900 loss:        0.192537
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.970460 loss:        0.090600
Test - acc:         0.937400 loss:        0.190127
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.972480 loss:        0.082340
Test - acc:         0.938600 loss:        0.185834
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.974880 loss:        0.075827
Test - acc:         0.938800 loss:        0.188157
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.978080 loss:        0.067269
Test - acc:         0.938500 loss:        0.188556
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.979780 loss:        0.061413
Test - acc:         0.938200 loss:        0.192836
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.981460 loss:        0.056378
Test - acc:         0.940600 loss:        0.189324
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.982620 loss:        0.053232
Test - acc:         0.940200 loss:        0.188053
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.983900 loss:        0.048948
Test - acc:         0.938900 loss:        0.199495
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.985540 loss:        0.045432
Test - acc:         0.940300 loss:        0.195089
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.986900 loss:        0.042320
Test - acc:         0.941000 loss:        0.197175
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.986400 loss:        0.041932
Test - acc:         0.941100 loss:        0.195603
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.987780 loss:        0.038881
Test - acc:         0.937100 loss:        0.216479
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.988080 loss:        0.036962
Test - acc:         0.941000 loss:        0.201848
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.988620 loss:        0.036116
Test - acc:         0.940100 loss:        0.201009
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.988000 loss:        0.037212
Test - acc:         0.940700 loss:        0.200506
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.989160 loss:        0.033285
Test - acc:         0.939200 loss:        0.211186
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.990720 loss:        0.030205
Test - acc:         0.940100 loss:        0.212106
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.991180 loss:        0.028709
Test - acc:         0.939200 loss:        0.212047
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.990720 loss:        0.029752
Test - acc:         0.940300 loss:        0.216254
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.989160 loss:        0.032141
Test - acc:         0.938700 loss:        0.227161
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.989660 loss:        0.032480
Test - acc:         0.939800 loss:        0.214651
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.990800 loss:        0.030178
Test - acc:         0.936600 loss:        0.219841
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.990320 loss:        0.030228
Test - acc:         0.938000 loss:        0.224875
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.990100 loss:        0.030126
Test - acc:         0.938200 loss:        0.230346
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.989700 loss:        0.032302
Test - acc:         0.935300 loss:        0.236310
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.989920 loss:        0.031859
Test - acc:         0.936800 loss:        0.233381
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.990960 loss:        0.028397
Test - acc:         0.937200 loss:        0.235214
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.990400 loss:        0.030525
Test - acc:         0.937600 loss:        0.226222
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.991020 loss:        0.029835
Test - acc:         0.936400 loss:        0.237263
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.988880 loss:        0.034706
Test - acc:         0.931300 loss:        0.243199
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.988920 loss:        0.034364
Test - acc:         0.933800 loss:        0.239236
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.989800 loss:        0.033020
Test - acc:         0.936700 loss:        0.228622
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.988860 loss:        0.034249
Test - acc:         0.938600 loss:        0.221551
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.989820 loss:        0.031888
Test - acc:         0.936500 loss:        0.236901
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.988160 loss:        0.036550
Test - acc:         0.935700 loss:        0.225329
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.988020 loss:        0.036509
Test - acc:         0.929900 loss:        0.250961
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.987400 loss:        0.037998
Test - acc:         0.927000 loss:        0.268882
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.988160 loss:        0.037179
Test - acc:         0.937700 loss:        0.224583
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.987900 loss:        0.037828
Test - acc:         0.933600 loss:        0.237222
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.986900 loss:        0.039704
Test - acc:         0.918400 loss:        0.308595
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.988760 loss:        0.036660
Test - acc:         0.932800 loss:        0.236899
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.988340 loss:        0.038006
Test - acc:         0.932400 loss:        0.247060
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.986440 loss:        0.039925
Test - acc:         0.932000 loss:        0.246601
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.986620 loss:        0.041445
Test - acc:         0.928500 loss:        0.256106
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.984500 loss:        0.046693
Test - acc:         0.933000 loss:        0.240206
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.986120 loss:        0.042591
Test - acc:         0.927100 loss:        0.257208
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.986900 loss:        0.039332
Test - acc:         0.933900 loss:        0.242549
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.985800 loss:        0.043163
Test - acc:         0.927900 loss:        0.258953
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.974980 loss:        0.079950
Test - acc:         0.924700 loss:        0.258370
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.983380 loss:        0.054323
Test - acc:         0.928100 loss:        0.246554
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.982060 loss:        0.054209
Test - acc:         0.927900 loss:        0.252268
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.984040 loss:        0.050984
Test - acc:         0.928200 loss:        0.255193
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.983400 loss:        0.050102
Test - acc:         0.923800 loss:        0.281386
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.984020 loss:        0.048743
Test - acc:         0.931000 loss:        0.252468
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.985220 loss:        0.045905
Test - acc:         0.925200 loss:        0.266250
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.986720 loss:        0.043036
Test - acc:         0.926900 loss:        0.265149
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.983560 loss:        0.048855
Test - acc:         0.929300 loss:        0.252427
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.983880 loss:        0.049256
Test - acc:         0.922100 loss:        0.269046
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.983860 loss:        0.048208
Test - acc:         0.924400 loss:        0.275675
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.985420 loss:        0.043993
Test - acc:         0.924700 loss:        0.272989
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.983900 loss:        0.048079
Test - acc:         0.928900 loss:        0.257468
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.985280 loss:        0.047020
Test - acc:         0.928500 loss:        0.260840
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.984280 loss:        0.046611
Test - acc:         0.931400 loss:        0.243561
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.984420 loss:        0.046564
Test - acc:         0.926700 loss:        0.252434
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.985600 loss:        0.043390
Test - acc:         0.929000 loss:        0.258550
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.986340 loss:        0.043518
Test - acc:         0.923000 loss:        0.280004
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.985660 loss:        0.043653
Test - acc:         0.928800 loss:        0.261847
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.985840 loss:        0.043037
Test - acc:         0.930200 loss:        0.246842
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.986420 loss:        0.042603
Test - acc:         0.927400 loss:        0.251149
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.985820 loss:        0.044224
Test - acc:         0.930600 loss:        0.240973
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.985320 loss:        0.043407
Test - acc:         0.930300 loss:        0.256359
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.984600 loss:        0.046609
Test - acc:         0.924200 loss:        0.261553
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.985160 loss:        0.044323
Test - acc:         0.933000 loss:        0.240236
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.985120 loss:        0.045095
Test - acc:         0.925000 loss:        0.280348
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.985960 loss:        0.042800
Test - acc:         0.923900 loss:        0.284661
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.985860 loss:        0.042789
Test - acc:         0.926300 loss:        0.267136
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.984240 loss:        0.048252
Test - acc:         0.924900 loss:        0.268448
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.985500 loss:        0.044325
Test - acc:         0.923600 loss:        0.269072
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.986500 loss:        0.042355
Test - acc:         0.928100 loss:        0.256905
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.986340 loss:        0.042592
Test - acc:         0.922900 loss:        0.277838
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.985000 loss:        0.045662
Test - acc:         0.924700 loss:        0.284109
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.985340 loss:        0.044911
Test - acc:         0.924900 loss:        0.272387
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.986800 loss:        0.042508
Test - acc:         0.926500 loss:        0.259526
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.986020 loss:        0.042736
Test - acc:         0.929500 loss:        0.249046
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.985180 loss:        0.044428
Test - acc:         0.927500 loss:        0.261400
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.986660 loss:        0.041522
Test - acc:         0.926700 loss:        0.274529
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.985760 loss:        0.043729
Test - acc:         0.928300 loss:        0.256952
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.986600 loss:        0.041855
Test - acc:         0.928800 loss:        0.251314
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.986200 loss:        0.042081
Test - acc:         0.927900 loss:        0.257774
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.985120 loss:        0.045215
Test - acc:         0.923200 loss:        0.280162
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.985400 loss:        0.045142
Test - acc:         0.926400 loss:        0.268210
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.985460 loss:        0.044673
Test - acc:         0.930500 loss:        0.250801
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.985280 loss:        0.043293
Test - acc:         0.928700 loss:        0.256761
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.986980 loss:        0.040886
Test - acc:         0.928000 loss:        0.274216
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.985920 loss:        0.042541
Test - acc:         0.923800 loss:        0.275668
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.986060 loss:        0.042780
Test - acc:         0.926700 loss:        0.266767
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.985460 loss:        0.044205
Test - acc:         0.927300 loss:        0.267688
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.985420 loss:        0.045528
Test - acc:         0.929700 loss:        0.269921
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.956140 loss:        0.145460
Test - acc:         0.924600 loss:        0.231649
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.979260 loss:        0.080330
Test - acc:         0.929800 loss:        0.215304
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.983760 loss:        0.065465
Test - acc:         0.931600 loss:        0.213129
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.986480 loss:        0.055896
Test - acc:         0.933400 loss:        0.210335
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.987120 loss:        0.051832
Test - acc:         0.933300 loss:        0.207507
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.989760 loss:        0.044433
Test - acc:         0.935400 loss:        0.204764
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.988820 loss:        0.043307
Test - acc:         0.935500 loss:        0.206997
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.990260 loss:        0.039981
Test - acc:         0.937100 loss:        0.204215
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.991640 loss:        0.035453
Test - acc:         0.936800 loss:        0.208439
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.992120 loss:        0.033881
Test - acc:         0.934900 loss:        0.205947
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.992340 loss:        0.031775
Test - acc:         0.936500 loss:        0.208440
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.992980 loss:        0.030147
Test - acc:         0.936100 loss:        0.207886
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.992900 loss:        0.029978
Test - acc:         0.936500 loss:        0.208029
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.993580 loss:        0.028141
Test - acc:         0.936700 loss:        0.206896
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.993520 loss:        0.027461
Test - acc:         0.937700 loss:        0.208627
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.994060 loss:        0.025737
Test - acc:         0.936000 loss:        0.208835
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.994500 loss:        0.024459
Test - acc:         0.938100 loss:        0.209785
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.994580 loss:        0.024391
Test - acc:         0.937400 loss:        0.209317
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.994700 loss:        0.023511
Test - acc:         0.936900 loss:        0.212887
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.994860 loss:        0.022588
Test - acc:         0.935900 loss:        0.211817
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.994840 loss:        0.022073
Test - acc:         0.936500 loss:        0.212688
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.995380 loss:        0.021083
Test - acc:         0.937800 loss:        0.215395
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.995760 loss:        0.020159
Test - acc:         0.937800 loss:        0.211569
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.995800 loss:        0.019760
Test - acc:         0.938200 loss:        0.213670
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.995800 loss:        0.019518
Test - acc:         0.937900 loss:        0.215978
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.995840 loss:        0.019085
Test - acc:         0.938500 loss:        0.218469
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.996240 loss:        0.017853
Test - acc:         0.939200 loss:        0.214114
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.996460 loss:        0.017087
Test - acc:         0.939600 loss:        0.215760
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.996560 loss:        0.016929
Test - acc:         0.939000 loss:        0.217485
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.996340 loss:        0.017599
Test - acc:         0.939900 loss:        0.214523
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.996720 loss:        0.015849
Test - acc:         0.939800 loss:        0.217461
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.997040 loss:        0.016073
Test - acc:         0.938900 loss:        0.217147
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.996900 loss:        0.015855
Test - acc:         0.939000 loss:        0.214243
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.996480 loss:        0.015618
Test - acc:         0.939500 loss:        0.217059
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.997000 loss:        0.015029
Test - acc:         0.940000 loss:        0.215865
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.997000 loss:        0.014968
Test - acc:         0.939000 loss:        0.216165
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.997220 loss:        0.014756
Test - acc:         0.939200 loss:        0.217191
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.997120 loss:        0.014667
Test - acc:         0.939100 loss:        0.216587
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.997480 loss:        0.013762
Test - acc:         0.939200 loss:        0.220926
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.997260 loss:        0.013596
Test - acc:         0.939200 loss:        0.217478
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.997680 loss:        0.013270
Test - acc:         0.939200 loss:        0.219044
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.997000 loss:        0.013826
Test - acc:         0.937400 loss:        0.221503
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.997640 loss:        0.012679
Test - acc:         0.938500 loss:        0.220463
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.997540 loss:        0.012298
Test - acc:         0.939000 loss:        0.218464
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.997780 loss:        0.012353
Test - acc:         0.939300 loss:        0.222228
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.997820 loss:        0.012098
Test - acc:         0.938200 loss:        0.221761
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.997700 loss:        0.012550
Test - acc:         0.939200 loss:        0.223786
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.997960 loss:        0.011997
Test - acc:         0.939500 loss:        0.222000
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.997360 loss:        0.012712
Test - acc:         0.940200 loss:        0.220429
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.997580 loss:        0.011729
Test - acc:         0.940500 loss:        0.219427
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.888460 loss:        0.365349
Test - acc:         0.890100 loss:        0.350155
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.936880 loss:        0.205010
Test - acc:         0.899600 loss:        0.316041
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.949640 loss:        0.166075
Test - acc:         0.911000 loss:        0.295485
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.955560 loss:        0.146972
Test - acc:         0.909800 loss:        0.288940
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.959660 loss:        0.133015
Test - acc:         0.912100 loss:        0.289901
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.961640 loss:        0.123064
Test - acc:         0.910300 loss:        0.286998
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.961920 loss:        0.118686
Test - acc:         0.916300 loss:        0.286506
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.965400 loss:        0.109347
Test - acc:         0.916100 loss:        0.275688
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.966620 loss:        0.104758
Test - acc:         0.917700 loss:        0.275635
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.967600 loss:        0.101856
Test - acc:         0.920100 loss:        0.271770
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.969100 loss:        0.096478
Test - acc:         0.917100 loss:        0.279800
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.969380 loss:        0.095936
Test - acc:         0.920200 loss:        0.272298
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.972000 loss:        0.088786
Test - acc:         0.920700 loss:        0.268501
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.973660 loss:        0.085409
Test - acc:         0.916200 loss:        0.275946
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.973920 loss:        0.084088
Test - acc:         0.922000 loss:        0.266488
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.973240 loss:        0.083248
Test - acc:         0.921700 loss:        0.267673
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.976120 loss:        0.077002
Test - acc:         0.920700 loss:        0.275081
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.975240 loss:        0.078267
Test - acc:         0.920900 loss:        0.273151
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.975280 loss:        0.076344
Test - acc:         0.920100 loss:        0.273317
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.977780 loss:        0.072497
Test - acc:         0.921100 loss:        0.274792
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.978140 loss:        0.069594
Test - acc:         0.920900 loss:        0.275382
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.978480 loss:        0.070212
Test - acc:         0.920500 loss:        0.276581
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.977980 loss:        0.069959
Test - acc:         0.922500 loss:        0.269942
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.978940 loss:        0.066093
Test - acc:         0.922700 loss:        0.273102
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.978620 loss:        0.068381
Test - acc:         0.922700 loss:        0.268950
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.978880 loss:        0.066800
Test - acc:         0.920400 loss:        0.279525
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.980140 loss:        0.064828
Test - acc:         0.921700 loss:        0.273442
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.978640 loss:        0.065288
Test - acc:         0.921400 loss:        0.278260
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.980760 loss:        0.062203
Test - acc:         0.922900 loss:        0.271661
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.980540 loss:        0.062017
Test - acc:         0.922300 loss:        0.276828
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.980440 loss:        0.060761
Test - acc:         0.923900 loss:        0.275328
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.981520 loss:        0.058138
Test - acc:         0.922300 loss:        0.277119
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.981740 loss:        0.059269
Test - acc:         0.922700 loss:        0.272637
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.981360 loss:        0.058107
Test - acc:         0.922600 loss:        0.277489
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.981720 loss:        0.057652
Test - acc:         0.919900 loss:        0.279630
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.982200 loss:        0.057743
Test - acc:         0.921000 loss:        0.279405
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.982260 loss:        0.055908
Test - acc:         0.923400 loss:        0.279885
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.983180 loss:        0.054524
Test - acc:         0.925000 loss:        0.277831
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.982600 loss:        0.054586
Test - acc:         0.921200 loss:        0.284081
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.982560 loss:        0.054894
Test - acc:         0.923500 loss:        0.286511
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.983520 loss:        0.052264
Test - acc:         0.922700 loss:        0.280809
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.983560 loss:        0.052732
Test - acc:         0.920600 loss:        0.283074
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.983540 loss:        0.051830
Test - acc:         0.923000 loss:        0.285344
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.984420 loss:        0.051066
Test - acc:         0.920400 loss:        0.280630
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.984660 loss:        0.049648
Test - acc:         0.923600 loss:        0.278453
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.982960 loss:        0.052205
Test - acc:         0.922800 loss:        0.280035
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.982960 loss:        0.051021
Test - acc:         0.920400 loss:        0.288158
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.983580 loss:        0.052181
Test - acc:         0.922000 loss:        0.283479
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.985260 loss:        0.047287
Test - acc:         0.918500 loss:        0.286755
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.985040 loss:        0.047757
Test - acc:         0.923200 loss:        0.283420
Sparsity :          0.9844
Wdecay :        0.000500
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "magnitude",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf39_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.335980 loss:        1.834952
Test - acc:         0.434500 loss:        1.516146
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.499920 loss:        1.365646
Test - acc:         0.537700 loss:        1.276935
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.611440 loss:        1.086855
Test - acc:         0.651300 loss:        0.967936
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.677260 loss:        0.902607
Test - acc:         0.710500 loss:        0.822695
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.738000 loss:        0.748496
Test - acc:         0.700700 loss:        0.876529
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.774800 loss:        0.643551
Test - acc:         0.701600 loss:        0.851181
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.794800 loss:        0.592851
Test - acc:         0.783100 loss:        0.633620
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.811260 loss:        0.547022
Test - acc:         0.764600 loss:        0.729727
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.822840 loss:        0.515704
Test - acc:         0.782200 loss:        0.629174
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.827640 loss:        0.501411
Test - acc:         0.744500 loss:        0.794391
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.834920 loss:        0.481459
Test - acc:         0.783700 loss:        0.669560
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.840020 loss:        0.466377
Test - acc:         0.816200 loss:        0.550512
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.846700 loss:        0.446303
Test - acc:         0.817500 loss:        0.551231
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.847620 loss:        0.440282
Test - acc:         0.804700 loss:        0.591621
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.853220 loss:        0.429697
Test - acc:         0.825900 loss:        0.523532
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.858960 loss:        0.417897
Test - acc:         0.814100 loss:        0.563547
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.860980 loss:        0.406157
Test - acc:         0.809000 loss:        0.589957
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.862760 loss:        0.401622
Test - acc:         0.820900 loss:        0.561647
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.861200 loss:        0.399493
Test - acc:         0.792100 loss:        0.670461
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.866400 loss:        0.388288
Test - acc:         0.829800 loss:        0.510664
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.868420 loss:        0.386417
Test - acc:         0.737400 loss:        0.798181
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870620 loss:        0.378511
Test - acc:         0.778200 loss:        0.700896
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.868920 loss:        0.384943
Test - acc:         0.764300 loss:        0.800272
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.873520 loss:        0.372131
Test - acc:         0.841100 loss:        0.478396
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.873820 loss:        0.372261
Test - acc:         0.790300 loss:        0.657671
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.874320 loss:        0.366219
Test - acc:         0.805200 loss:        0.577301
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.875640 loss:        0.366298
Test - acc:         0.829900 loss:        0.527510
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.874740 loss:        0.368855
Test - acc:         0.843700 loss:        0.463200
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.876060 loss:        0.360821
Test - acc:         0.801300 loss:        0.613367
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.879120 loss:        0.356276
Test - acc:         0.811800 loss:        0.579423
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.877900 loss:        0.356128
Test - acc:         0.798700 loss:        0.640035
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.878860 loss:        0.352785
Test - acc:         0.834200 loss:        0.500045
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.880520 loss:        0.351507
Test - acc:         0.837000 loss:        0.483760
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.879500 loss:        0.350178
Test - acc:         0.833800 loss:        0.509795
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.880420 loss:        0.350848
Test - acc:         0.821100 loss:        0.567117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.881360 loss:        0.344487
Test - acc:         0.839800 loss:        0.479123
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.880760 loss:        0.348547
Test - acc:         0.821200 loss:        0.535436
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.881960 loss:        0.344331
Test - acc:         0.849900 loss:        0.454892
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.884680 loss:        0.337788
Test - acc:         0.850800 loss:        0.467627
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.901420 loss:        0.289277
Test - acc:         0.865900 loss:        0.411499
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.900940 loss:        0.290159
Test - acc:         0.859600 loss:        0.432294
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.899960 loss:        0.293875
Test - acc:         0.852800 loss:        0.437998
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.896380 loss:        0.302045
Test - acc:         0.854800 loss:        0.435105
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.897660 loss:        0.295870
Test - acc:         0.866700 loss:        0.397880
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.899200 loss:        0.293951
Test - acc:         0.853500 loss:        0.453386
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.900660 loss:        0.292917
Test - acc:         0.743100 loss:        0.816920
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.897100 loss:        0.295773
Test - acc:         0.842600 loss:        0.476527
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.900640 loss:        0.294947
Test - acc:         0.838600 loss:        0.490426
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.899240 loss:        0.295488
Test - acc:         0.875300 loss:        0.374660
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.902080 loss:        0.289457
Test - acc:         0.837700 loss:        0.477909
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.898520 loss:        0.294270
Test - acc:         0.845300 loss:        0.465828
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.902300 loss:        0.286433
Test - acc:         0.865900 loss:        0.397958
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.899340 loss:        0.291284
Test - acc:         0.822800 loss:        0.556012
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.903600 loss:        0.285462
Test - acc:         0.845200 loss:        0.468828
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.899760 loss:        0.291726
Test - acc:         0.868200 loss:        0.404566
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.900400 loss:        0.290185
Test - acc:         0.821400 loss:        0.541675
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.902320 loss:        0.285484
Test - acc:         0.816000 loss:        0.571329
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.900980 loss:        0.290208
Test - acc:         0.845400 loss:        0.466381
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.902200 loss:        0.286162
Test - acc:         0.846100 loss:        0.472503
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.901820 loss:        0.288876
Test - acc:         0.859600 loss:        0.429073
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.899920 loss:        0.290888
Test - acc:         0.850000 loss:        0.452762
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.903060 loss:        0.285022
Test - acc:         0.870600 loss:        0.391022
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.902020 loss:        0.284253
Test - acc:         0.850800 loss:        0.455038
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.899480 loss:        0.289696
Test - acc:         0.853000 loss:        0.457731
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.902760 loss:        0.286069
Test - acc:         0.875000 loss:        0.384102
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.900280 loss:        0.290293
Test - acc:         0.851800 loss:        0.443085
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.900840 loss:        0.286404
Test - acc:         0.854400 loss:        0.449525
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.901600 loss:        0.284137
Test - acc:         0.848600 loss:        0.460615
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.901780 loss:        0.284940
Test - acc:         0.849600 loss:        0.459119
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.902420 loss:        0.284134
Test - acc:         0.819800 loss:        0.585325
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.903820 loss:        0.284105
Test - acc:         0.818700 loss:        0.589696
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.902680 loss:        0.284008
Test - acc:         0.873000 loss:        0.372134
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.901680 loss:        0.287543
Test - acc:         0.840600 loss:        0.481116
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.901520 loss:        0.287993
Test - acc:         0.812800 loss:        0.603674
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.903140 loss:        0.283279
Test - acc:         0.867300 loss:        0.402218
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.902160 loss:        0.285252
Test - acc:         0.838700 loss:        0.521530
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.904240 loss:        0.281875
Test - acc:         0.868100 loss:        0.395821
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.903280 loss:        0.282105
Test - acc:         0.869900 loss:        0.389134
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.916600 loss:        0.247191
Test - acc:         0.854700 loss:        0.437670
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.911280 loss:        0.256334
Test - acc:         0.873800 loss:        0.388853
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.911520 loss:        0.254630
Test - acc:         0.869000 loss:        0.394476
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.910300 loss:        0.256015
Test - acc:         0.864100 loss:        0.416502
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.911900 loss:        0.256936
Test - acc:         0.875000 loss:        0.381775
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.910680 loss:        0.261168
Test - acc:         0.856000 loss:        0.451300
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.913000 loss:        0.255542
Test - acc:         0.843100 loss:        0.479283
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.909420 loss:        0.262086
Test - acc:         0.870300 loss:        0.405417
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.911900 loss:        0.254404
Test - acc:         0.856500 loss:        0.444538
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.912900 loss:        0.257549
Test - acc:         0.839300 loss:        0.527468
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.909160 loss:        0.260856
Test - acc:         0.837800 loss:        0.483935
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.912860 loss:        0.253414
Test - acc:         0.865800 loss:        0.417553
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.912160 loss:        0.253503
Test - acc:         0.823400 loss:        0.522727
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.910300 loss:        0.257759
Test - acc:         0.857000 loss:        0.429188
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.912800 loss:        0.255266
Test - acc:         0.870700 loss:        0.390461
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.912140 loss:        0.255434
Test - acc:         0.840600 loss:        0.522795
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.912220 loss:        0.255933
Test - acc:         0.803500 loss:        0.627120
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.912340 loss:        0.255272
Test - acc:         0.848100 loss:        0.461692
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.913700 loss:        0.255002
Test - acc:         0.868000 loss:        0.404351
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.911660 loss:        0.255499
Test - acc:         0.818300 loss:        0.597285
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.913920 loss:        0.252959
Test - acc:         0.857000 loss:        0.468898
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.913640 loss:        0.250129
Test - acc:         0.865700 loss:        0.408769
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.913920 loss:        0.254543
Test - acc:         0.855600 loss:        0.433242
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.912120 loss:        0.255765
Test - acc:         0.864100 loss:        0.405361
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.912740 loss:        0.254129
Test - acc:         0.829100 loss:        0.533668
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.911860 loss:        0.254940
Test - acc:         0.844100 loss:        0.503266
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.915320 loss:        0.248049
Test - acc:         0.857800 loss:        0.453376
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.911100 loss:        0.256355
Test - acc:         0.876000 loss:        0.366716
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.910680 loss:        0.258364
Test - acc:         0.889700 loss:        0.329257
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.913720 loss:        0.252435
Test - acc:         0.871600 loss:        0.381720
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.913580 loss:        0.250958
Test - acc:         0.814700 loss:        0.597418
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.913020 loss:        0.253866
Test - acc:         0.816700 loss:        0.595857
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.911480 loss:        0.255523
Test - acc:         0.874200 loss:        0.384623
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.912720 loss:        0.252287
Test - acc:         0.809500 loss:        0.630164
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.911840 loss:        0.255005
Test - acc:         0.867100 loss:        0.407057
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.912220 loss:        0.257944
Test - acc:         0.854200 loss:        0.445894
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.912060 loss:        0.251927
Test - acc:         0.866900 loss:        0.409182
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.912440 loss:        0.257283
Test - acc:         0.873100 loss:        0.396932
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.913560 loss:        0.250215
Test - acc:         0.840700 loss:        0.502832
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.920760 loss:        0.232038
Test - acc:         0.871600 loss:        0.385561
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.918740 loss:        0.235238
Test - acc:         0.861100 loss:        0.425195
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.917120 loss:        0.241018
Test - acc:         0.838600 loss:        0.508352
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.918160 loss:        0.240098
Test - acc:         0.854900 loss:        0.474716
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.918960 loss:        0.237494
Test - acc:         0.883300 loss:        0.362125
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.916960 loss:        0.239274
Test - acc:         0.879600 loss:        0.356744
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.918920 loss:        0.239791
Test - acc:         0.862800 loss:        0.416767
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.918400 loss:        0.238382
Test - acc:         0.880700 loss:        0.358489
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.919980 loss:        0.236485
Test - acc:         0.842400 loss:        0.506792
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.918960 loss:        0.237849
Test - acc:         0.866600 loss:        0.416991
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.918020 loss:        0.239426
Test - acc:         0.876400 loss:        0.369046
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.918560 loss:        0.236176
Test - acc:         0.856200 loss:        0.463882
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.918040 loss:        0.239320
Test - acc:         0.845500 loss:        0.506363
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.915040 loss:        0.244040
Test - acc:         0.860600 loss:        0.408914
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.919840 loss:        0.233406
Test - acc:         0.856800 loss:        0.471788
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.916500 loss:        0.241198
Test - acc:         0.881700 loss:        0.356017
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.918060 loss:        0.235596
Test - acc:         0.870400 loss:        0.378872
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.919560 loss:        0.236490
Test - acc:         0.857600 loss:        0.432918
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.917740 loss:        0.240248
Test - acc:         0.870800 loss:        0.382312
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.919440 loss:        0.234862
Test - acc:         0.845400 loss:        0.490245
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.916940 loss:        0.240364
Test - acc:         0.834400 loss:        0.529653
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.919460 loss:        0.236684
Test - acc:         0.850100 loss:        0.452008
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.918000 loss:        0.241408
Test - acc:         0.850300 loss:        0.467913
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.918500 loss:        0.236471
Test - acc:         0.891200 loss:        0.328962
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.917340 loss:        0.238979
Test - acc:         0.877500 loss:        0.378618
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.918980 loss:        0.236612
Test - acc:         0.871500 loss:        0.395894
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.916700 loss:        0.241116
Test - acc:         0.872800 loss:        0.396043
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.918480 loss:        0.237163
Test - acc:         0.848600 loss:        0.490633
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.918880 loss:        0.237433
Test - acc:         0.881700 loss:        0.349043
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.918280 loss:        0.237087
Test - acc:         0.882000 loss:        0.358136
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.917700 loss:        0.236765
Test - acc:         0.871600 loss:        0.390321
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.917940 loss:        0.239843
Test - acc:         0.877400 loss:        0.370710
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.917820 loss:        0.239182
Test - acc:         0.876800 loss:        0.373381
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.958260 loss:        0.126823
Test - acc:         0.935700 loss:        0.192277
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.968960 loss:        0.093970
Test - acc:         0.937900 loss:        0.188914
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.974140 loss:        0.077798
Test - acc:         0.938700 loss:        0.186183
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.977300 loss:        0.069747
Test - acc:         0.940100 loss:        0.183074
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.979140 loss:        0.063974
Test - acc:         0.939200 loss:        0.187841
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.981480 loss:        0.057030
Test - acc:         0.942700 loss:        0.185586
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.974960 loss:        0.081125
Test - acc:         0.935900 loss:        0.195289
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.979820 loss:        0.065529
Test - acc:         0.940200 loss:        0.189366
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.980320 loss:        0.059752
Test - acc:         0.941500 loss:        0.190059
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.982700 loss:        0.054516
Test - acc:         0.937800 loss:        0.199194
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.983740 loss:        0.050952
Test - acc:         0.940200 loss:        0.196837
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.984800 loss:        0.047327
Test - acc:         0.940300 loss:        0.199335
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.985940 loss:        0.045633
Test - acc:         0.941200 loss:        0.203820
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.985800 loss:        0.044166
Test - acc:         0.938500 loss:        0.209539
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.986900 loss:        0.042465
Test - acc:         0.940000 loss:        0.210706
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.987220 loss:        0.040807
Test - acc:         0.937700 loss:        0.214924
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.987100 loss:        0.040376
Test - acc:         0.939800 loss:        0.208321
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.986800 loss:        0.040640
Test - acc:         0.937700 loss:        0.221742
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.987720 loss:        0.038143
Test - acc:         0.935300 loss:        0.218050
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.987980 loss:        0.037427
Test - acc:         0.938800 loss:        0.213388
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.989000 loss:        0.035451
Test - acc:         0.938400 loss:        0.217881
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.986760 loss:        0.038115
Test - acc:         0.937800 loss:        0.213827
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.987220 loss:        0.038763
Test - acc:         0.934900 loss:        0.226830
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.989140 loss:        0.034288
Test - acc:         0.934900 loss:        0.233273
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.987500 loss:        0.037542
Test - acc:         0.932700 loss:        0.231175
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.987860 loss:        0.037472
Test - acc:         0.934700 loss:        0.231157
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.987040 loss:        0.038684
Test - acc:         0.931200 loss:        0.248402
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.987080 loss:        0.038938
Test - acc:         0.934200 loss:        0.241237
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.988340 loss:        0.036747
Test - acc:         0.932700 loss:        0.242287
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.987080 loss:        0.039215
Test - acc:         0.931700 loss:        0.241662
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.986960 loss:        0.040053
Test - acc:         0.934700 loss:        0.253949
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.987200 loss:        0.039782
Test - acc:         0.934300 loss:        0.238346
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.986580 loss:        0.041366
Test - acc:         0.935300 loss:        0.233494
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.987260 loss:        0.040862
Test - acc:         0.933000 loss:        0.247411
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.984280 loss:        0.046970
Test - acc:         0.933700 loss:        0.234994
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.986700 loss:        0.041207
Test - acc:         0.934600 loss:        0.224149
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.985640 loss:        0.043917
Test - acc:         0.929300 loss:        0.251112
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.986740 loss:        0.041689
Test - acc:         0.929400 loss:        0.249230
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.984460 loss:        0.045358
Test - acc:         0.933700 loss:        0.241273
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.985620 loss:        0.043404
Test - acc:         0.931800 loss:        0.241584
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.984300 loss:        0.047759
Test - acc:         0.929700 loss:        0.240767
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.984380 loss:        0.047494
Test - acc:         0.928700 loss:        0.257095
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.983980 loss:        0.046406
Test - acc:         0.937300 loss:        0.214926
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.984780 loss:        0.046402
Test - acc:         0.918500 loss:        0.274398
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.984980 loss:        0.045990
Test - acc:         0.930500 loss:        0.252709
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.955180 loss:        0.139336
Test - acc:         0.911900 loss:        0.291373
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.967640 loss:        0.098691
Test - acc:         0.911000 loss:        0.297327
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.971440 loss:        0.087011
Test - acc:         0.913400 loss:        0.293806
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.970040 loss:        0.088221
Test - acc:         0.918400 loss:        0.275197
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.973120 loss:        0.079993
Test - acc:         0.921800 loss:        0.272441
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.971480 loss:        0.082876
Test - acc:         0.918400 loss:        0.267036
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.975200 loss:        0.072909
Test - acc:         0.920000 loss:        0.259035
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.975040 loss:        0.073225
Test - acc:         0.906000 loss:        0.320609
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.973740 loss:        0.076314
Test - acc:         0.919700 loss:        0.279509
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.974520 loss:        0.073819
Test - acc:         0.912400 loss:        0.310697
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.974060 loss:        0.075961
Test - acc:         0.919400 loss:        0.283858
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.976600 loss:        0.068536
Test - acc:         0.919500 loss:        0.279952
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.976880 loss:        0.068985
Test - acc:         0.913600 loss:        0.294082
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.976220 loss:        0.071334
Test - acc:         0.917900 loss:        0.278100
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.975620 loss:        0.072030
Test - acc:         0.916600 loss:        0.299406
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.975700 loss:        0.071594
Test - acc:         0.919200 loss:        0.274800
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.976720 loss:        0.068731
Test - acc:         0.917300 loss:        0.288295
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.978020 loss:        0.064105
Test - acc:         0.918400 loss:        0.287400
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.976780 loss:        0.068713
Test - acc:         0.918200 loss:        0.287240
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.977220 loss:        0.068707
Test - acc:         0.916200 loss:        0.294804
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.978360 loss:        0.065289
Test - acc:         0.924800 loss:        0.261950
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.977360 loss:        0.067062
Test - acc:         0.926400 loss:        0.269965
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.977740 loss:        0.065261
Test - acc:         0.918300 loss:        0.286625
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.978600 loss:        0.064816
Test - acc:         0.921200 loss:        0.273263
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.979040 loss:        0.062514
Test - acc:         0.920100 loss:        0.280809
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.976280 loss:        0.069363
Test - acc:         0.916300 loss:        0.289346
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.978660 loss:        0.064002
Test - acc:         0.923300 loss:        0.260118
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.978140 loss:        0.063536
Test - acc:         0.920600 loss:        0.268438
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.977460 loss:        0.065755
Test - acc:         0.917600 loss:        0.287999
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.978080 loss:        0.064342
Test - acc:         0.919400 loss:        0.282044
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.977140 loss:        0.067365
Test - acc:         0.920300 loss:        0.271721
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.978360 loss:        0.065775
Test - acc:         0.917300 loss:        0.291327
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.979140 loss:        0.061870
Test - acc:         0.921300 loss:        0.282739
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.976800 loss:        0.066101
Test - acc:         0.921700 loss:        0.275200
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.979900 loss:        0.060576
Test - acc:         0.920100 loss:        0.281937
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.978280 loss:        0.064846
Test - acc:         0.922800 loss:        0.275917
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.977800 loss:        0.067564
Test - acc:         0.918100 loss:        0.289516
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.977860 loss:        0.065189
Test - acc:         0.921700 loss:        0.265888
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.979040 loss:        0.062156
Test - acc:         0.925500 loss:        0.266465
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.937880 loss:        0.191956
Test - acc:         0.907200 loss:        0.293196
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.948700 loss:        0.151258
Test - acc:         0.896700 loss:        0.326148
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.952700 loss:        0.140193
Test - acc:         0.899400 loss:        0.325661
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.955080 loss:        0.130569
Test - acc:         0.910200 loss:        0.291969
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.954280 loss:        0.130701
Test - acc:         0.911100 loss:        0.291536
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.956200 loss:        0.125836
Test - acc:         0.909900 loss:        0.288735
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.956440 loss:        0.126970
Test - acc:         0.911700 loss:        0.286273
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.958600 loss:        0.119168
Test - acc:         0.908100 loss:        0.302144
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.959260 loss:        0.118463
Test - acc:         0.906600 loss:        0.301632
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.958860 loss:        0.121047
Test - acc:         0.909400 loss:        0.297290
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.956780 loss:        0.121243
Test - acc:         0.907400 loss:        0.307149
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.960060 loss:        0.116519
Test - acc:         0.911500 loss:        0.291151
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.962640 loss:        0.110846
Test - acc:         0.903900 loss:        0.325751
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.961360 loss:        0.112679
Test - acc:         0.906100 loss:        0.297656
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.958800 loss:        0.116430
Test - acc:         0.911500 loss:        0.285882
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.962760 loss:        0.110225
Test - acc:         0.908100 loss:        0.296181
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.976340 loss:        0.073987
Test - acc:         0.926700 loss:        0.239346
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.980760 loss:        0.060930
Test - acc:         0.928500 loss:        0.235692
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.982340 loss:        0.056117
Test - acc:         0.928800 loss:        0.233242
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.984260 loss:        0.051264
Test - acc:         0.931500 loss:        0.230180
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.984720 loss:        0.050317
Test - acc:         0.931000 loss:        0.231156
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.985440 loss:        0.047740
Test - acc:         0.932300 loss:        0.229784
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.986360 loss:        0.045509
Test - acc:         0.931200 loss:        0.229224
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.986920 loss:        0.044198
Test - acc:         0.931200 loss:        0.231832
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.987120 loss:        0.043423
Test - acc:         0.931600 loss:        0.230364
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.987560 loss:        0.041848
Test - acc:         0.930600 loss:        0.234536
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.988520 loss:        0.039590
Test - acc:         0.932100 loss:        0.232860
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.988760 loss:        0.039779
Test - acc:         0.931600 loss:        0.232058
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.989300 loss:        0.038113
Test - acc:         0.934300 loss:        0.230854
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.989480 loss:        0.037157
Test - acc:         0.932700 loss:        0.232661
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.989240 loss:        0.037667
Test - acc:         0.931400 loss:        0.234675
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.989360 loss:        0.037631
Test - acc:         0.933100 loss:        0.232953
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.989440 loss:        0.036370
Test - acc:         0.932800 loss:        0.233987
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.989860 loss:        0.035306
Test - acc:         0.932600 loss:        0.234088
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.989840 loss:        0.034874
Test - acc:         0.931200 loss:        0.235521
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.990000 loss:        0.035167
Test - acc:         0.932100 loss:        0.234211
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.990600 loss:        0.033530
Test - acc:         0.932400 loss:        0.234064
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.990660 loss:        0.032811
Test - acc:         0.931900 loss:        0.234553
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.991500 loss:        0.031264
Test - acc:         0.932700 loss:        0.234526
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.833060 loss:        0.544295
Test - acc:         0.860000 loss:        0.419474
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.897940 loss:        0.319409
Test - acc:         0.877900 loss:        0.371524
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.913800 loss:        0.269355
Test - acc:         0.884700 loss:        0.353722
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.918240 loss:        0.249242
Test - acc:         0.883600 loss:        0.357639
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.923540 loss:        0.231524
Test - acc:         0.891100 loss:        0.339018
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.928320 loss:        0.217342
Test - acc:         0.887400 loss:        0.335517
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.931020 loss:        0.208756
Test - acc:         0.895300 loss:        0.323706
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.933620 loss:        0.201663
Test - acc:         0.892200 loss:        0.324426
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.935720 loss:        0.193433
Test - acc:         0.898100 loss:        0.316545
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.934700 loss:        0.193304
Test - acc:         0.897300 loss:        0.322974
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.937600 loss:        0.187498
Test - acc:         0.896900 loss:        0.319909
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.940240 loss:        0.180888
Test - acc:         0.897500 loss:        0.316051
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.941040 loss:        0.174667
Test - acc:         0.897500 loss:        0.315424
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.940380 loss:        0.175863
Test - acc:         0.897900 loss:        0.317506
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.941360 loss:        0.173572
Test - acc:         0.899400 loss:        0.313903
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.942300 loss:        0.172572
Test - acc:         0.898700 loss:        0.316786
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.945020 loss:        0.163068
Test - acc:         0.902000 loss:        0.309617
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.946080 loss:        0.161133
Test - acc:         0.902200 loss:        0.313470
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.946900 loss:        0.157060
Test - acc:         0.900600 loss:        0.305754
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.945300 loss:        0.159597
Test - acc:         0.902300 loss:        0.306427
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.946020 loss:        0.156046
Test - acc:         0.903700 loss:        0.303818
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.948300 loss:        0.152756
Test - acc:         0.903800 loss:        0.309781
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.947520 loss:        0.154558
Test - acc:         0.901400 loss:        0.312276
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.949780 loss:        0.150261
Test - acc:         0.897000 loss:        0.323351
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.949000 loss:        0.151205
Test - acc:         0.904100 loss:        0.307400
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.948280 loss:        0.147491
Test - acc:         0.902600 loss:        0.311606
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.949020 loss:        0.149433
Test - acc:         0.905200 loss:        0.300916
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.949320 loss:        0.147051
Test - acc:         0.903900 loss:        0.302544
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.950020 loss:        0.146275
Test - acc:         0.904800 loss:        0.308155
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.952240 loss:        0.140256
Test - acc:         0.904500 loss:        0.312346
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.950600 loss:        0.142714
Test - acc:         0.905000 loss:        0.304626
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.952780 loss:        0.138421
Test - acc:         0.904300 loss:        0.304423
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.951660 loss:        0.138212
Test - acc:         0.906100 loss:        0.307450
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.953220 loss:        0.138282
Test - acc:         0.903600 loss:        0.319377
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.953440 loss:        0.135907
Test - acc:         0.906200 loss:        0.306886
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.954740 loss:        0.132697
Test - acc:         0.905100 loss:        0.310259
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.953940 loss:        0.134074
Test - acc:         0.905300 loss:        0.315281
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.953360 loss:        0.133697
Test - acc:         0.905700 loss:        0.306136
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.953020 loss:        0.135563
Test - acc:         0.904900 loss:        0.317182
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.439640 loss:        1.745728
Test - acc:         0.488700 loss:        1.658751
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.588280 loss:        1.234512
Test - acc:         0.610600 loss:        1.158764
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.644500 loss:        1.066594
Test - acc:         0.584000 loss:        1.383866
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.680180 loss:        0.975435
Test - acc:         0.701900 loss:        0.913736
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.701440 loss:        0.910919
Test - acc:         0.681900 loss:        0.972529
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.716520 loss:        0.866163
Test - acc:         0.691600 loss:        0.949207
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.728400 loss:        0.827762
Test - acc:         0.735000 loss:        0.825999
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.735660 loss:        0.800471
Test - acc:         0.743200 loss:        0.801689
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.745820 loss:        0.774392
Test - acc:         0.729000 loss:        0.843002
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.754320 loss:        0.753040
Test - acc:         0.733700 loss:        0.823946
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.758160 loss:        0.734898
Test - acc:         0.747900 loss:        0.767115
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.760660 loss:        0.723955
Test - acc:         0.761300 loss:        0.730544
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.763780 loss:        0.709321
Test - acc:         0.733600 loss:        0.807407
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.769200 loss:        0.694198
Test - acc:         0.765000 loss:        0.723670
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.772080 loss:        0.687269
Test - acc:         0.756900 loss:        0.735762
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.773620 loss:        0.675621
Test - acc:         0.770200 loss:        0.702305
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.781360 loss:        0.660151
Test - acc:         0.776500 loss:        0.679109
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.778740 loss:        0.660584
Test - acc:         0.775600 loss:        0.692042
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.781580 loss:        0.652036
Test - acc:         0.763600 loss:        0.723678
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.784180 loss:        0.647026
Test - acc:         0.776900 loss:        0.673711
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.784700 loss:        0.642149
Test - acc:         0.738200 loss:        0.799047
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.789820 loss:        0.632528
Test - acc:         0.787200 loss:        0.637122
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.788460 loss:        0.625918
Test - acc:         0.787000 loss:        0.648555
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.791680 loss:        0.622121
Test - acc:         0.774900 loss:        0.674192
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.787660 loss:        0.624089
Test - acc:         0.780800 loss:        0.647159
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.792000 loss:        0.614588
Test - acc:         0.786400 loss:        0.642949
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.795500 loss:        0.605731
Test - acc:         0.790300 loss:        0.640947
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.797480 loss:        0.604074
Test - acc:         0.770700 loss:        0.683245
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.798880 loss:        0.597150
Test - acc:         0.797500 loss:        0.615076
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.798640 loss:        0.599043
Test - acc:         0.789400 loss:        0.615794
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.799660 loss:        0.595041
Test - acc:         0.796800 loss:        0.609461
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.801580 loss:        0.588286
Test - acc:         0.790000 loss:        0.623836
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.802740 loss:        0.582729
Test - acc:         0.788900 loss:        0.622083
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.801580 loss:        0.581549
Test - acc:         0.797200 loss:        0.603466
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.803940 loss:        0.579155
Test - acc:         0.791700 loss:        0.632550
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.807180 loss:        0.571812
Test - acc:         0.801000 loss:        0.596656
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.805660 loss:        0.571879
Test - acc:         0.795100 loss:        0.606660
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.806280 loss:        0.569335
Test - acc:         0.796000 loss:        0.603730
Sparsity :          0.9961
Wdecay :        0.000500
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "magnitude",
    "prune_freq": 117,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf117_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.321120 loss:        1.854424
Test - acc:         0.411200 loss:        1.565847
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.503360 loss:        1.355529
Test - acc:         0.531400 loss:        1.293555
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.617780 loss:        1.072467
Test - acc:         0.591400 loss:        1.217174
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.680140 loss:        0.906641
Test - acc:         0.678000 loss:        0.939917
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.732700 loss:        0.765037
Test - acc:         0.736100 loss:        0.773973
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.771620 loss:        0.656548
Test - acc:         0.757200 loss:        0.711667
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.798220 loss:        0.586788
Test - acc:         0.761500 loss:        0.711917
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.810080 loss:        0.548399
Test - acc:         0.749500 loss:        0.790651
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.822320 loss:        0.513472
Test - acc:         0.744300 loss:        0.789349
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.831620 loss:        0.493238
Test - acc:         0.786600 loss:        0.631747
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.840040 loss:        0.468217
Test - acc:         0.771300 loss:        0.675180
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.843080 loss:        0.456581
Test - acc:         0.778400 loss:        0.638347
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.848780 loss:        0.444750
Test - acc:         0.790000 loss:        0.639025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.852020 loss:        0.428776
Test - acc:         0.774900 loss:        0.715545
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.856340 loss:        0.424426
Test - acc:         0.792900 loss:        0.618070
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.857340 loss:        0.412202
Test - acc:         0.822300 loss:        0.529257
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.861340 loss:        0.406203
Test - acc:         0.732400 loss:        0.895132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.864500 loss:        0.396459
Test - acc:         0.814100 loss:        0.557998
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.865940 loss:        0.393389
Test - acc:         0.792200 loss:        0.626124
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.868100 loss:        0.385620
Test - acc:         0.859600 loss:        0.433525
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.869140 loss:        0.378930
Test - acc:         0.775600 loss:        0.673397
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.872320 loss:        0.374244
Test - acc:         0.820300 loss:        0.542755
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.868560 loss:        0.377958
Test - acc:         0.832300 loss:        0.520596
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.874120 loss:        0.367059
Test - acc:         0.836200 loss:        0.490914
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.875740 loss:        0.365345
Test - acc:         0.793600 loss:        0.673231
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.877340 loss:        0.363791
Test - acc:         0.796600 loss:        0.618069
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.877120 loss:        0.358088
Test - acc:         0.835000 loss:        0.508686
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.877520 loss:        0.356427
Test - acc:         0.802200 loss:        0.692358
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.879100 loss:        0.354404
Test - acc:         0.821900 loss:        0.532333
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.880980 loss:        0.354309
Test - acc:         0.815300 loss:        0.532912
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.880820 loss:        0.349731
Test - acc:         0.829100 loss:        0.504460
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.880720 loss:        0.348386
Test - acc:         0.840200 loss:        0.468264
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.883940 loss:        0.344529
Test - acc:         0.749700 loss:        0.876263
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.883580 loss:        0.341230
Test - acc:         0.802100 loss:        0.621983
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.883180 loss:        0.345867
Test - acc:         0.813300 loss:        0.591781
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.885000 loss:        0.341380
Test - acc:         0.856500 loss:        0.435875
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.883020 loss:        0.344775
Test - acc:         0.851100 loss:        0.454024
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.884900 loss:        0.336330
Test - acc:         0.813600 loss:        0.574401
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.885520 loss:        0.338197
Test - acc:         0.852600 loss:        0.461663
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.882320 loss:        0.345838
Test - acc:         0.822000 loss:        0.520061
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.883080 loss:        0.342687
Test - acc:         0.787700 loss:        0.688354
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.885640 loss:        0.333763
Test - acc:         0.834800 loss:        0.509691
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.885820 loss:        0.333605
Test - acc:         0.836500 loss:        0.501145
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.887380 loss:        0.330377
Test - acc:         0.853400 loss:        0.438513
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.888040 loss:        0.330697
Test - acc:         0.851500 loss:        0.454154
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.889620 loss:        0.326160
Test - acc:         0.865700 loss:        0.417851
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.887000 loss:        0.331534
Test - acc:         0.872000 loss:        0.384061
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.887280 loss:        0.330618
Test - acc:         0.842200 loss:        0.476856
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.888100 loss:        0.330641
Test - acc:         0.814700 loss:        0.589198
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.886980 loss:        0.330561
Test - acc:         0.838200 loss:        0.506258
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.888860 loss:        0.326933
Test - acc:         0.843900 loss:        0.483417
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.889800 loss:        0.325366
Test - acc:         0.839700 loss:        0.473554
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.888920 loss:        0.324661
Test - acc:         0.813800 loss:        0.596887
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.889620 loss:        0.327166
Test - acc:         0.834800 loss:        0.496957
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.890800 loss:        0.323371
Test - acc:         0.773100 loss:        0.728922
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.887840 loss:        0.328113
Test - acc:         0.826600 loss:        0.537099
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.889540 loss:        0.322512
Test - acc:         0.824400 loss:        0.573904
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.888940 loss:        0.326143
Test - acc:         0.865400 loss:        0.410687
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.891040 loss:        0.326104
Test - acc:         0.857700 loss:        0.430683
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.890460 loss:        0.321243
Test - acc:         0.789500 loss:        0.701078
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.891420 loss:        0.322934
Test - acc:         0.875300 loss:        0.385217
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.888880 loss:        0.326754
Test - acc:         0.855600 loss:        0.427254
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.889160 loss:        0.322752
Test - acc:         0.842200 loss:        0.471284
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.891220 loss:        0.320879
Test - acc:         0.804200 loss:        0.580389
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.888680 loss:        0.319731
Test - acc:         0.823400 loss:        0.538242
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.892700 loss:        0.318083
Test - acc:         0.823000 loss:        0.548749
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.891480 loss:        0.320668
Test - acc:         0.850700 loss:        0.439127
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.891920 loss:        0.318147
Test - acc:         0.871900 loss:        0.381574
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.890080 loss:        0.324433
Test - acc:         0.781400 loss:        0.685586
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.896000 loss:        0.310720
Test - acc:         0.850400 loss:        0.456744
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.889420 loss:        0.323392
Test - acc:         0.841500 loss:        0.504343
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.889840 loss:        0.320312
Test - acc:         0.849800 loss:        0.439365
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.892340 loss:        0.316681
Test - acc:         0.851000 loss:        0.440290
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.892240 loss:        0.318255
Test - acc:         0.845500 loss:        0.463180
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.890840 loss:        0.318793
Test - acc:         0.800600 loss:        0.609927
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.893240 loss:        0.315762
Test - acc:         0.808700 loss:        0.607719
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.893540 loss:        0.313544
Test - acc:         0.778100 loss:        0.729867
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.891200 loss:        0.322050
Test - acc:         0.801000 loss:        0.610584
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.893480 loss:        0.317786
Test - acc:         0.810100 loss:        0.590460
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.891780 loss:        0.315627
Test - acc:         0.801900 loss:        0.641178
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.892440 loss:        0.314542
Test - acc:         0.830700 loss:        0.524342
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.891920 loss:        0.318172
Test - acc:         0.823800 loss:        0.555275
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.892600 loss:        0.313521
Test - acc:         0.831600 loss:        0.504801
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.891980 loss:        0.318466
Test - acc:         0.865100 loss:        0.413420
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.894040 loss:        0.315548
Test - acc:         0.844400 loss:        0.472022
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.891840 loss:        0.316131
Test - acc:         0.830500 loss:        0.521337
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.895380 loss:        0.307626
Test - acc:         0.808400 loss:        0.595936
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.895980 loss:        0.309864
Test - acc:         0.855100 loss:        0.434891
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.893460 loss:        0.313407
Test - acc:         0.852300 loss:        0.433351
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.892600 loss:        0.315624
Test - acc:         0.846500 loss:        0.470418
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.894000 loss:        0.315946
Test - acc:         0.853700 loss:        0.441212
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.894620 loss:        0.310261
Test - acc:         0.856500 loss:        0.443590
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.894620 loss:        0.305512
Test - acc:         0.854600 loss:        0.441779
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.893580 loss:        0.313377
Test - acc:         0.857000 loss:        0.449762
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.892360 loss:        0.316049
Test - acc:         0.811200 loss:        0.560664
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.895860 loss:        0.308468
Test - acc:         0.845600 loss:        0.462129
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.893980 loss:        0.311713
Test - acc:         0.836900 loss:        0.521558
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.893760 loss:        0.314413
Test - acc:         0.844800 loss:        0.460874
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.893920 loss:        0.309475
Test - acc:         0.838700 loss:        0.504757
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.892740 loss:        0.315857
Test - acc:         0.832500 loss:        0.522787
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.895640 loss:        0.308341
Test - acc:         0.812800 loss:        0.603413
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.895180 loss:        0.312317
Test - acc:         0.821900 loss:        0.580595
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.896820 loss:        0.305439
Test - acc:         0.862100 loss:        0.406890
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.893620 loss:        0.312043
Test - acc:         0.865000 loss:        0.414013
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.892480 loss:        0.313400
Test - acc:         0.836700 loss:        0.523754
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.894360 loss:        0.312396
Test - acc:         0.849100 loss:        0.471249
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.892520 loss:        0.317069
Test - acc:         0.815500 loss:        0.542778
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.893640 loss:        0.315077
Test - acc:         0.821600 loss:        0.546125
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.894320 loss:        0.312687
Test - acc:         0.847400 loss:        0.463050
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.896960 loss:        0.305789
Test - acc:         0.858200 loss:        0.419646
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.894000 loss:        0.309196
Test - acc:         0.832900 loss:        0.504291
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.894540 loss:        0.310137
Test - acc:         0.827600 loss:        0.543247
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.894160 loss:        0.309922
Test - acc:         0.708900 loss:        1.041109
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.893060 loss:        0.314644
Test - acc:         0.853400 loss:        0.441122
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.895940 loss:        0.308968
Test - acc:         0.734200 loss:        0.900486
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.896060 loss:        0.307191
Test - acc:         0.846100 loss:        0.473978
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.894740 loss:        0.307830
Test - acc:         0.822100 loss:        0.543210
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.910360 loss:        0.265127
Test - acc:         0.872900 loss:        0.383297
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.911000 loss:        0.260564
Test - acc:         0.836400 loss:        0.525866
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.907900 loss:        0.270707
Test - acc:         0.878500 loss:        0.362948
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.906400 loss:        0.272348
Test - acc:         0.861400 loss:        0.428933
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.906440 loss:        0.277146
Test - acc:         0.846700 loss:        0.482945
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.903060 loss:        0.284363
Test - acc:         0.885600 loss:        0.350342
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.906640 loss:        0.276709
Test - acc:         0.857000 loss:        0.452265
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.907920 loss:        0.274195
Test - acc:         0.847400 loss:        0.483517
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.908260 loss:        0.270320
Test - acc:         0.866800 loss:        0.416303
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.907460 loss:        0.269324
Test - acc:         0.870200 loss:        0.382411
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.907400 loss:        0.272985
Test - acc:         0.845600 loss:        0.461243
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.906120 loss:        0.272469
Test - acc:         0.841500 loss:        0.488191
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.908280 loss:        0.269816
Test - acc:         0.834400 loss:        0.529252
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.907740 loss:        0.270272
Test - acc:         0.848700 loss:        0.471860
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.906840 loss:        0.271283
Test - acc:         0.877700 loss:        0.367107
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.906440 loss:        0.275034
Test - acc:         0.869100 loss:        0.391158
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.906660 loss:        0.272110
Test - acc:         0.839300 loss:        0.514173
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.907520 loss:        0.270283
Test - acc:         0.870900 loss:        0.385069
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.909900 loss:        0.262931
Test - acc:         0.848800 loss:        0.453869
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.906120 loss:        0.275555
Test - acc:         0.872900 loss:        0.404264
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.908140 loss:        0.269283
Test - acc:         0.866800 loss:        0.411592
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.907640 loss:        0.272924
Test - acc:         0.860200 loss:        0.423932
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.907520 loss:        0.271249
Test - acc:         0.833600 loss:        0.519986
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.909240 loss:        0.264570
Test - acc:         0.821900 loss:        0.600350
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.906720 loss:        0.270199
Test - acc:         0.876200 loss:        0.380543
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.907400 loss:        0.269632
Test - acc:         0.843100 loss:        0.491797
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.905800 loss:        0.272841
Test - acc:         0.853800 loss:        0.441214
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.906660 loss:        0.270308
Test - acc:         0.847400 loss:        0.468569
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.908360 loss:        0.267758
Test - acc:         0.830600 loss:        0.498083
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.907060 loss:        0.270199
Test - acc:         0.860800 loss:        0.424558
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.910040 loss:        0.264503
Test - acc:         0.864800 loss:        0.411527
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.907400 loss:        0.269191
Test - acc:         0.848200 loss:        0.467098
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.909420 loss:        0.265682
Test - acc:         0.860800 loss:        0.425312
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.954480 loss:        0.137525
Test - acc:         0.935700 loss:        0.191745
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.968320 loss:        0.095904
Test - acc:         0.938100 loss:        0.185870
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.973060 loss:        0.080323
Test - acc:         0.941000 loss:        0.179022
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.978120 loss:        0.066997
Test - acc:         0.942600 loss:        0.176679
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.980420 loss:        0.060552
Test - acc:         0.943600 loss:        0.176992
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.983640 loss:        0.052006
Test - acc:         0.945500 loss:        0.173402
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.985160 loss:        0.047266
Test - acc:         0.942300 loss:        0.185396
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.986740 loss:        0.042525
Test - acc:         0.943500 loss:        0.184664
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.987880 loss:        0.037820
Test - acc:         0.944800 loss:        0.187411
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.988980 loss:        0.034567
Test - acc:         0.944400 loss:        0.198344
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.989740 loss:        0.032750
Test - acc:         0.941400 loss:        0.202685
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.990400 loss:        0.031092
Test - acc:         0.944800 loss:        0.190570
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.991100 loss:        0.028027
Test - acc:         0.946500 loss:        0.188715
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.991620 loss:        0.026635
Test - acc:         0.943400 loss:        0.199113
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.991240 loss:        0.027136
Test - acc:         0.944100 loss:        0.205915
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.990600 loss:        0.028178
Test - acc:         0.942200 loss:        0.212038
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.992400 loss:        0.025027
Test - acc:         0.944800 loss:        0.211054
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.993200 loss:        0.023153
Test - acc:         0.942000 loss:        0.213433
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.992560 loss:        0.024511
Test - acc:         0.939700 loss:        0.229883
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.992580 loss:        0.023822
Test - acc:         0.939500 loss:        0.217876
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.992480 loss:        0.025170
Test - acc:         0.942100 loss:        0.209600
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.991840 loss:        0.027084
Test - acc:         0.937200 loss:        0.228333
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.992300 loss:        0.024475
Test - acc:         0.941300 loss:        0.214160
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.991880 loss:        0.025769
Test - acc:         0.941400 loss:        0.218097
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.991740 loss:        0.025553
Test - acc:         0.941000 loss:        0.220394
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.991260 loss:        0.028020
Test - acc:         0.937800 loss:        0.227642
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.990220 loss:        0.029623
Test - acc:         0.935600 loss:        0.233557
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.991100 loss:        0.028063
Test - acc:         0.933800 loss:        0.247193
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.991700 loss:        0.026886
Test - acc:         0.937200 loss:        0.244984
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.990620 loss:        0.029249
Test - acc:         0.938900 loss:        0.235713
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.990100 loss:        0.030923
Test - acc:         0.930500 loss:        0.258543
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.989240 loss:        0.032346
Test - acc:         0.936400 loss:        0.234128
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.990080 loss:        0.031028
Test - acc:         0.929400 loss:        0.256936
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.989320 loss:        0.033298
Test - acc:         0.934000 loss:        0.237929
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.989580 loss:        0.032193
Test - acc:         0.925500 loss:        0.285013
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.988660 loss:        0.033733
Test - acc:         0.931300 loss:        0.250771
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.989500 loss:        0.032757
Test - acc:         0.925200 loss:        0.275319
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.988120 loss:        0.036529
Test - acc:         0.932900 loss:        0.246463
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.989420 loss:        0.033453
Test - acc:         0.924300 loss:        0.285618
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.987560 loss:        0.036676
Test - acc:         0.935300 loss:        0.244596
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.988120 loss:        0.036702
Test - acc:         0.933700 loss:        0.241394
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.987940 loss:        0.036017
Test - acc:         0.930100 loss:        0.256476
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.986940 loss:        0.040205
Test - acc:         0.929500 loss:        0.262850
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.987060 loss:        0.038849
Test - acc:         0.934000 loss:        0.229585
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.987000 loss:        0.039821
Test - acc:         0.932100 loss:        0.240023
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.987060 loss:        0.040219
Test - acc:         0.925500 loss:        0.273857
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.983640 loss:        0.047408
Test - acc:         0.931100 loss:        0.243724
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.986680 loss:        0.040334
Test - acc:         0.936000 loss:        0.228028
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.987580 loss:        0.038826
Test - acc:         0.933900 loss:        0.243251
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.986240 loss:        0.042679
Test - acc:         0.929600 loss:        0.250213
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.986020 loss:        0.043527
Test - acc:         0.935300 loss:        0.231554
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.986180 loss:        0.042619
Test - acc:         0.925000 loss:        0.267432
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.986520 loss:        0.041556
Test - acc:         0.918100 loss:        0.302742
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.987120 loss:        0.040239
Test - acc:         0.933900 loss:        0.249077
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.986380 loss:        0.041686
Test - acc:         0.932200 loss:        0.239109
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.986740 loss:        0.040697
Test - acc:         0.932300 loss:        0.237545
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.985980 loss:        0.042651
Test - acc:         0.916600 loss:        0.295755
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.985220 loss:        0.044639
Test - acc:         0.931800 loss:        0.236382
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.986020 loss:        0.042036
Test - acc:         0.935300 loss:        0.235174
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.984760 loss:        0.045599
Test - acc:         0.926700 loss:        0.256247
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.985720 loss:        0.043945
Test - acc:         0.924000 loss:        0.265673
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.986120 loss:        0.043093
Test - acc:         0.932800 loss:        0.239793
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.987400 loss:        0.039806
Test - acc:         0.930100 loss:        0.258138
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.986480 loss:        0.041134
Test - acc:         0.929400 loss:        0.250991
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.984080 loss:        0.047014
Test - acc:         0.933600 loss:        0.230978
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.986300 loss:        0.041422
Test - acc:         0.929700 loss:        0.249694
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.987200 loss:        0.038843
Test - acc:         0.934100 loss:        0.237831
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.984040 loss:        0.047560
Test - acc:         0.927100 loss:        0.270741
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.986560 loss:        0.041462
Test - acc:         0.932200 loss:        0.241817
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.986320 loss:        0.041456
Test - acc:         0.921900 loss:        0.280766
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.987260 loss:        0.039777
Test - acc:         0.929600 loss:        0.253879
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.986460 loss:        0.042029
Test - acc:         0.927300 loss:        0.266687
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.985560 loss:        0.044489
Test - acc:         0.927200 loss:        0.268122
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.985020 loss:        0.045419
Test - acc:         0.922500 loss:        0.285500
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.987600 loss:        0.037626
Test - acc:         0.929900 loss:        0.265486
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.987400 loss:        0.039570
Test - acc:         0.920100 loss:        0.301015
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.985800 loss:        0.044139
Test - acc:         0.931100 loss:        0.247900
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.987120 loss:        0.039723
Test - acc:         0.932100 loss:        0.261451
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.988040 loss:        0.036703
Test - acc:         0.926000 loss:        0.268935
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.985700 loss:        0.043703
Test - acc:         0.934400 loss:        0.246003
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.986640 loss:        0.040388
Test - acc:         0.922300 loss:        0.288006
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.986100 loss:        0.042340
Test - acc:         0.927400 loss:        0.256501
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.986840 loss:        0.041686
Test - acc:         0.934800 loss:        0.243190
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.985800 loss:        0.041839
Test - acc:         0.926200 loss:        0.261160
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.991380 loss:        0.030251
Test - acc:         0.938900 loss:        0.220435
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.993660 loss:        0.023109
Test - acc:         0.934700 loss:        0.244886
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.992580 loss:        0.024816
Test - acc:         0.935700 loss:        0.246010
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.993580 loss:        0.022593
Test - acc:         0.933300 loss:        0.257409
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.993520 loss:        0.022455
Test - acc:         0.937200 loss:        0.241587
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.992840 loss:        0.024010
Test - acc:         0.932300 loss:        0.256534
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.991720 loss:        0.026772
Test - acc:         0.938200 loss:        0.235508
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.991220 loss:        0.027457
Test - acc:         0.937400 loss:        0.252779
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.991580 loss:        0.027841
Test - acc:         0.931600 loss:        0.261027
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.989760 loss:        0.031762
Test - acc:         0.934400 loss:        0.252136
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.990660 loss:        0.029062
Test - acc:         0.931100 loss:        0.259150
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.991120 loss:        0.028440
Test - acc:         0.930300 loss:        0.263842
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.990640 loss:        0.030216
Test - acc:         0.930700 loss:        0.263165
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.991020 loss:        0.028004
Test - acc:         0.931900 loss:        0.254566
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.991020 loss:        0.028994
Test - acc:         0.935500 loss:        0.243466
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.991040 loss:        0.028022
Test - acc:         0.935900 loss:        0.240197
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.996600 loss:        0.013283
Test - acc:         0.946900 loss:        0.202148
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.998280 loss:        0.008200
Test - acc:         0.948000 loss:        0.201901
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.998600 loss:        0.007100
Test - acc:         0.948500 loss:        0.199539
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.998940 loss:        0.005951
Test - acc:         0.948800 loss:        0.199478
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.004580
Test - acc:         0.949700 loss:        0.199022
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.004815
Test - acc:         0.949600 loss:        0.198039
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.999220 loss:        0.004602
Test - acc:         0.949300 loss:        0.196631
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.004172
Test - acc:         0.949200 loss:        0.197464
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.003830
Test - acc:         0.948500 loss:        0.198759
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.003913
Test - acc:         0.949300 loss:        0.197914
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.003777
Test - acc:         0.950500 loss:        0.195607
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.003518
Test - acc:         0.950100 loss:        0.197614
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.003260
Test - acc:         0.950900 loss:        0.195904
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.003268
Test - acc:         0.950700 loss:        0.196693
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.003130
Test - acc:         0.950900 loss:        0.194835
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.003192
Test - acc:         0.951500 loss:        0.194628
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003005
Test - acc:         0.950900 loss:        0.194352
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.003022
Test - acc:         0.951300 loss:        0.193241
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.002898
Test - acc:         0.952000 loss:        0.192066
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002670
Test - acc:         0.951100 loss:        0.195605
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002539
Test - acc:         0.951000 loss:        0.194220
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002506
Test - acc:         0.951500 loss:        0.193105
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002629
Test - acc:         0.951800 loss:        0.194244
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.002687
Test - acc:         0.951600 loss:        0.195231
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.002678
Test - acc:         0.951900 loss:        0.192165
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002581
Test - acc:         0.951800 loss:        0.192537
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002428
Test - acc:         0.951400 loss:        0.192859
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002657
Test - acc:         0.952300 loss:        0.191348
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002483
Test - acc:         0.951200 loss:        0.192105
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002506
Test - acc:         0.950400 loss:        0.194571
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002369
Test - acc:         0.952400 loss:        0.190633
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002333
Test - acc:         0.951300 loss:        0.192120
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002527
Test - acc:         0.952300 loss:        0.192992
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002412
Test - acc:         0.951600 loss:        0.193864
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002511
Test - acc:         0.952000 loss:        0.193083
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002218
Test - acc:         0.951500 loss:        0.193492
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002160
Test - acc:         0.952000 loss:        0.193614
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002157
Test - acc:         0.952600 loss:        0.190874
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002272
Test - acc:         0.952600 loss:        0.190968
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.002509
Test - acc:         0.951600 loss:        0.192469
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002205
Test - acc:         0.951600 loss:        0.191058
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002384
Test - acc:         0.952200 loss:        0.191057
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002314
Test - acc:         0.951700 loss:        0.190762
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002140
Test - acc:         0.952700 loss:        0.191263
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002151
Test - acc:         0.951700 loss:        0.191652
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002092
Test - acc:         0.951500 loss:        0.193064
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002345
Test - acc:         0.951000 loss:        0.191534
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002205
Test - acc:         0.952400 loss:        0.190271
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002069
Test - acc:         0.952200 loss:        0.190439
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002190
Test - acc:         0.951400 loss:        0.190586
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002244
Test - acc:         0.951400 loss:        0.191815
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002388
Test - acc:         0.952500 loss:        0.191130
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002146
Test - acc:         0.952900 loss:        0.190073
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002149
Test - acc:         0.952700 loss:        0.189119
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002215
Test - acc:         0.953200 loss:        0.190245
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002249
Test - acc:         0.952000 loss:        0.191183
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002098
Test - acc:         0.951700 loss:        0.190423
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002081
Test - acc:         0.951800 loss:        0.189870
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002162
Test - acc:         0.952800 loss:        0.188226
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002226
Test - acc:         0.951700 loss:        0.189702
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002246
Test - acc:         0.952900 loss:        0.188748
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002208
Test - acc:         0.952900 loss:        0.188378
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002178
Test - acc:         0.952500 loss:        0.189864
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002112
Test - acc:         0.952200 loss:        0.190298
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002126
Test - acc:         0.952100 loss:        0.189843
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002139
Test - acc:         0.952200 loss:        0.188906
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002185
Test - acc:         0.952200 loss:        0.190313
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002072
Test - acc:         0.951400 loss:        0.188741
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002103
Test - acc:         0.952000 loss:        0.189567
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002264
Test - acc:         0.952500 loss:        0.190676
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002167
Test - acc:         0.952000 loss:        0.188929
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002175
Test - acc:         0.952100 loss:        0.190403
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002209
Test - acc:         0.952600 loss:        0.188993
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002195
Test - acc:         0.952500 loss:        0.190257
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002199
Test - acc:         0.953300 loss:        0.189654
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002036
Test - acc:         0.951900 loss:        0.189070
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002150
Test - acc:         0.952400 loss:        0.188509
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002226
Test - acc:         0.953300 loss:        0.187824
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002135
Test - acc:         0.952800 loss:        0.186956
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002208
Test - acc:         0.952700 loss:        0.187763
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002116
Test - acc:         0.952100 loss:        0.188973
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002049
Test - acc:         0.952800 loss:        0.187951
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002109
Test - acc:         0.952700 loss:        0.188059
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002059
Test - acc:         0.952000 loss:        0.189332
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002181
Test - acc:         0.952700 loss:        0.188731
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002141
Test - acc:         0.952100 loss:        0.187594
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002116
Test - acc:         0.951200 loss:        0.189418
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002167
Test - acc:         0.952500 loss:        0.186378
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002024
Test - acc:         0.952100 loss:        0.189110
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002182
Test - acc:         0.952900 loss:        0.187600
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002196
Test - acc:         0.952400 loss:        0.189374
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002193
Test - acc:         0.953000 loss:        0.187891
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002109
Test - acc:         0.953900 loss:        0.186758
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002140
Test - acc:         0.952500 loss:        0.188964
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002187
Test - acc:         0.953500 loss:        0.187155
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002101
Test - acc:         0.952700 loss:        0.187898
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002180
Test - acc:         0.952300 loss:        0.187699
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002087
Test - acc:         0.952800 loss:        0.188509
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002056
Test - acc:         0.953000 loss:        0.186756
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002134
Test - acc:         0.952900 loss:        0.187987
Sparsity :          0.7500
Wdecay :        0.000500
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "magnitude",
    "prune_freq": 70,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf70_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.321120 loss:        1.854424
Test - acc:         0.411200 loss:        1.565847
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.503360 loss:        1.355529
Test - acc:         0.531400 loss:        1.293555
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.617780 loss:        1.072467
Test - acc:         0.591400 loss:        1.217174
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.680140 loss:        0.906641
Test - acc:         0.678000 loss:        0.939917
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.732700 loss:        0.765037
Test - acc:         0.736100 loss:        0.773973
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.771620 loss:        0.656548
Test - acc:         0.757200 loss:        0.711667
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.798220 loss:        0.586788
Test - acc:         0.761500 loss:        0.711917
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.810080 loss:        0.548399
Test - acc:         0.749500 loss:        0.790651
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.822320 loss:        0.513472
Test - acc:         0.744300 loss:        0.789349
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.831620 loss:        0.493238
Test - acc:         0.786600 loss:        0.631747
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.840040 loss:        0.468217
Test - acc:         0.771300 loss:        0.675180
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.843080 loss:        0.456581
Test - acc:         0.778400 loss:        0.638347
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.848780 loss:        0.444750
Test - acc:         0.790000 loss:        0.639025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.852020 loss:        0.428776
Test - acc:         0.774900 loss:        0.715545
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.856340 loss:        0.424426
Test - acc:         0.792900 loss:        0.618070
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.857340 loss:        0.412202
Test - acc:         0.822300 loss:        0.529257
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.861340 loss:        0.406203
Test - acc:         0.732400 loss:        0.895132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.864500 loss:        0.396459
Test - acc:         0.814100 loss:        0.557998
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.865940 loss:        0.393389
Test - acc:         0.792200 loss:        0.626124
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.868100 loss:        0.385620
Test - acc:         0.859600 loss:        0.433525
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.869140 loss:        0.378930
Test - acc:         0.775600 loss:        0.673397
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.872320 loss:        0.374244
Test - acc:         0.820300 loss:        0.542755
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.868560 loss:        0.377958
Test - acc:         0.832300 loss:        0.520596
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.874120 loss:        0.367059
Test - acc:         0.836200 loss:        0.490914
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.875740 loss:        0.365345
Test - acc:         0.793600 loss:        0.673231
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.877340 loss:        0.363791
Test - acc:         0.796600 loss:        0.618069
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.877120 loss:        0.358088
Test - acc:         0.835000 loss:        0.508686
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.877520 loss:        0.356427
Test - acc:         0.802200 loss:        0.692358
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.879100 loss:        0.354404
Test - acc:         0.821900 loss:        0.532333
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.880980 loss:        0.354309
Test - acc:         0.815300 loss:        0.532912
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.880820 loss:        0.349731
Test - acc:         0.829100 loss:        0.504460
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.880720 loss:        0.348386
Test - acc:         0.840200 loss:        0.468264
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.883940 loss:        0.344529
Test - acc:         0.749700 loss:        0.876263
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.883580 loss:        0.341230
Test - acc:         0.802100 loss:        0.621983
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.883180 loss:        0.345867
Test - acc:         0.813300 loss:        0.591781
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.885000 loss:        0.341380
Test - acc:         0.856500 loss:        0.435875
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.883020 loss:        0.344775
Test - acc:         0.851100 loss:        0.454024
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.884900 loss:        0.336330
Test - acc:         0.813600 loss:        0.574401
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.885520 loss:        0.338197
Test - acc:         0.852600 loss:        0.461663
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.882320 loss:        0.345838
Test - acc:         0.822000 loss:        0.520061
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.883080 loss:        0.342687
Test - acc:         0.787700 loss:        0.688354
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.885640 loss:        0.333763
Test - acc:         0.834800 loss:        0.509691
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.885820 loss:        0.333605
Test - acc:         0.836500 loss:        0.501145
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.887380 loss:        0.330377
Test - acc:         0.853400 loss:        0.438513
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.888040 loss:        0.330697
Test - acc:         0.851500 loss:        0.454154
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.889620 loss:        0.326160
Test - acc:         0.865700 loss:        0.417851
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.887000 loss:        0.331534
Test - acc:         0.872000 loss:        0.384061
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.887280 loss:        0.330618
Test - acc:         0.842200 loss:        0.476856
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.888100 loss:        0.330641
Test - acc:         0.814700 loss:        0.589198
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.886980 loss:        0.330561
Test - acc:         0.838200 loss:        0.506258
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.888860 loss:        0.326933
Test - acc:         0.843900 loss:        0.483417
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.889800 loss:        0.325366
Test - acc:         0.839700 loss:        0.473554
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.888920 loss:        0.324661
Test - acc:         0.813800 loss:        0.596887
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.889620 loss:        0.327166
Test - acc:         0.834800 loss:        0.496957
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.890800 loss:        0.323371
Test - acc:         0.773100 loss:        0.728922
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.887840 loss:        0.328113
Test - acc:         0.826600 loss:        0.537099
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.889540 loss:        0.322512
Test - acc:         0.824400 loss:        0.573904
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.888940 loss:        0.326143
Test - acc:         0.865400 loss:        0.410687
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.891040 loss:        0.326104
Test - acc:         0.857700 loss:        0.430683
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.890460 loss:        0.321243
Test - acc:         0.789500 loss:        0.701078
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.891420 loss:        0.322934
Test - acc:         0.875300 loss:        0.385217
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.888880 loss:        0.326754
Test - acc:         0.855600 loss:        0.427254
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.889160 loss:        0.322752
Test - acc:         0.842200 loss:        0.471284
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.891220 loss:        0.320879
Test - acc:         0.804200 loss:        0.580389
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.888680 loss:        0.319731
Test - acc:         0.823400 loss:        0.538242
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.892700 loss:        0.318083
Test - acc:         0.823000 loss:        0.548749
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.891480 loss:        0.320668
Test - acc:         0.850700 loss:        0.439127
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.891920 loss:        0.318147
Test - acc:         0.871900 loss:        0.381574
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.890080 loss:        0.324433
Test - acc:         0.781400 loss:        0.685586
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.896000 loss:        0.310720
Test - acc:         0.850400 loss:        0.456744
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.908060 loss:        0.270067
Test - acc:         0.810000 loss:        0.584250
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.905000 loss:        0.278809
Test - acc:         0.849200 loss:        0.461769
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.904760 loss:        0.279851
Test - acc:         0.852500 loss:        0.432833
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.905340 loss:        0.281067
Test - acc:         0.861000 loss:        0.414245
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.903080 loss:        0.280568
Test - acc:         0.859200 loss:        0.426506
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.904920 loss:        0.278919
Test - acc:         0.848100 loss:        0.467783
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.906000 loss:        0.274548
Test - acc:         0.792400 loss:        0.702633
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.903720 loss:        0.282165
Test - acc:         0.790600 loss:        0.648373
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.905760 loss:        0.276971
Test - acc:         0.724800 loss:        0.930054
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.905820 loss:        0.279055
Test - acc:         0.867900 loss:        0.405336
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.903660 loss:        0.281417
Test - acc:         0.844500 loss:        0.458962
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.904820 loss:        0.276902
Test - acc:         0.863400 loss:        0.427157
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.904780 loss:        0.275880
Test - acc:         0.841800 loss:        0.499260
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.904560 loss:        0.280250
Test - acc:         0.878700 loss:        0.372651
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.906460 loss:        0.277352
Test - acc:         0.824500 loss:        0.555727
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.905380 loss:        0.278539
Test - acc:         0.794400 loss:        0.668844
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.907220 loss:        0.274029
Test - acc:         0.825000 loss:        0.562381
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.904720 loss:        0.273696
Test - acc:         0.850200 loss:        0.455457
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.905700 loss:        0.276356
Test - acc:         0.860100 loss:        0.418767
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.907900 loss:        0.272438
Test - acc:         0.863900 loss:        0.424452
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.904860 loss:        0.277114
Test - acc:         0.830900 loss:        0.546952
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.908360 loss:        0.271254
Test - acc:         0.816500 loss:        0.595600
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.906480 loss:        0.272517
Test - acc:         0.855400 loss:        0.446563
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.905380 loss:        0.275893
Test - acc:         0.855700 loss:        0.445146
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.906500 loss:        0.274718
Test - acc:         0.852800 loss:        0.437573
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.906380 loss:        0.273783
Test - acc:         0.859000 loss:        0.435184
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.906680 loss:        0.273655
Test - acc:         0.846700 loss:        0.510246
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.908020 loss:        0.269775
Test - acc:         0.830700 loss:        0.524815
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.906400 loss:        0.274547
Test - acc:         0.846300 loss:        0.483672
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.907240 loss:        0.271153
Test - acc:         0.833500 loss:        0.521679
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.906440 loss:        0.275170
Test - acc:         0.863900 loss:        0.434643
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.906540 loss:        0.276411
Test - acc:         0.838200 loss:        0.495915
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.907180 loss:        0.269305
Test - acc:         0.853600 loss:        0.458093
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.906080 loss:        0.275887
Test - acc:         0.830600 loss:        0.539336
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.907100 loss:        0.269900
Test - acc:         0.864000 loss:        0.419652
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.905540 loss:        0.271069
Test - acc:         0.864300 loss:        0.430024
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.905040 loss:        0.277226
Test - acc:         0.829200 loss:        0.541670
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.904540 loss:        0.278905
Test - acc:         0.840400 loss:        0.494949
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.907580 loss:        0.271441
Test - acc:         0.856600 loss:        0.432267
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.905640 loss:        0.272761
Test - acc:         0.825000 loss:        0.560688
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.908360 loss:        0.268145
Test - acc:         0.860100 loss:        0.421970
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.908140 loss:        0.272177
Test - acc:         0.869000 loss:        0.387307
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.906140 loss:        0.277544
Test - acc:         0.803900 loss:        0.613756
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.906880 loss:        0.272529
Test - acc:         0.861500 loss:        0.448514
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.905020 loss:        0.276126
Test - acc:         0.851900 loss:        0.441600
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.906080 loss:        0.271263
Test - acc:         0.869400 loss:        0.391350
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.908600 loss:        0.267191
Test - acc:         0.805600 loss:        0.652174
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.904480 loss:        0.276788
Test - acc:         0.840400 loss:        0.486820
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.908740 loss:        0.267102
Test - acc:         0.848900 loss:        0.490692
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.904400 loss:        0.276169
Test - acc:         0.837900 loss:        0.531600
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.907300 loss:        0.274218
Test - acc:         0.885500 loss:        0.349905
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.906780 loss:        0.273687
Test - acc:         0.863800 loss:        0.410988
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.903580 loss:        0.278983
Test - acc:         0.855200 loss:        0.431023
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.908920 loss:        0.271257
Test - acc:         0.873000 loss:        0.384387
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.904940 loss:        0.274916
Test - acc:         0.837000 loss:        0.513026
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.905500 loss:        0.273322
Test - acc:         0.837400 loss:        0.504269
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.907500 loss:        0.269585
Test - acc:         0.862000 loss:        0.432839
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.905720 loss:        0.275907
Test - acc:         0.874200 loss:        0.377546
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.906700 loss:        0.271450
Test - acc:         0.823400 loss:        0.525162
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.906520 loss:        0.272797
Test - acc:         0.807400 loss:        0.592054
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.905300 loss:        0.273886
Test - acc:         0.829800 loss:        0.569319
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.906520 loss:        0.272210
Test - acc:         0.863400 loss:        0.409344
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.906480 loss:        0.275215
Test - acc:         0.876200 loss:        0.353158
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.907780 loss:        0.271228
Test - acc:         0.863100 loss:        0.419978
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.905980 loss:        0.272757
Test - acc:         0.845700 loss:        0.454154
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.908220 loss:        0.269259
Test - acc:         0.855800 loss:        0.445089
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.906120 loss:        0.271752
Test - acc:         0.833200 loss:        0.524154
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.906240 loss:        0.274881
Test - acc:         0.837600 loss:        0.512055
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.908000 loss:        0.274428
Test - acc:         0.870800 loss:        0.398759
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.906280 loss:        0.272026
Test - acc:         0.834700 loss:        0.515109
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.920680 loss:        0.230913
Test - acc:         0.872000 loss:        0.398131
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.916040 loss:        0.242448
Test - acc:         0.855300 loss:        0.451952
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.916340 loss:        0.244811
Test - acc:         0.846600 loss:        0.467242
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.917880 loss:        0.244798
Test - acc:         0.883800 loss:        0.361030
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.915120 loss:        0.250250
Test - acc:         0.831600 loss:        0.523255
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.914920 loss:        0.249657
Test - acc:         0.871900 loss:        0.402431
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.915340 loss:        0.248988
Test - acc:         0.865600 loss:        0.408498
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.916580 loss:        0.246075
Test - acc:         0.847600 loss:        0.474437
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.912160 loss:        0.253104
Test - acc:         0.859300 loss:        0.432628
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.916080 loss:        0.248266
Test - acc:         0.864500 loss:        0.425537
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.958980 loss:        0.124373
Test - acc:         0.937600 loss:        0.188376
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.971800 loss:        0.087838
Test - acc:         0.940600 loss:        0.182804
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.975520 loss:        0.075733
Test - acc:         0.941800 loss:        0.177903
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.978120 loss:        0.065784
Test - acc:         0.942300 loss:        0.178258
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.981380 loss:        0.057774
Test - acc:         0.943000 loss:        0.176420
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.983520 loss:        0.051836
Test - acc:         0.941000 loss:        0.179056
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.985280 loss:        0.046470
Test - acc:         0.942300 loss:        0.185503
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.986880 loss:        0.041431
Test - acc:         0.941200 loss:        0.187009
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.987640 loss:        0.038074
Test - acc:         0.941800 loss:        0.191196
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.988780 loss:        0.035810
Test - acc:         0.940300 loss:        0.193911
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.990140 loss:        0.032682
Test - acc:         0.942800 loss:        0.194977
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.990200 loss:        0.032307
Test - acc:         0.940700 loss:        0.191540
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.991480 loss:        0.029223
Test - acc:         0.939700 loss:        0.205506
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.990340 loss:        0.030545
Test - acc:         0.941600 loss:        0.209924
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.991180 loss:        0.029592
Test - acc:         0.942500 loss:        0.205054
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.990900 loss:        0.028864
Test - acc:         0.942400 loss:        0.203144
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.992200 loss:        0.026217
Test - acc:         0.939600 loss:        0.212764
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.993100 loss:        0.023900
Test - acc:         0.941400 loss:        0.213400
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.991640 loss:        0.026139
Test - acc:         0.941500 loss:        0.204568
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.992580 loss:        0.025510
Test - acc:         0.938100 loss:        0.219070
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.992420 loss:        0.025265
Test - acc:         0.940000 loss:        0.211673
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.992220 loss:        0.025646
Test - acc:         0.937800 loss:        0.216401
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.992500 loss:        0.024829
Test - acc:         0.936700 loss:        0.224039
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.993340 loss:        0.022569
Test - acc:         0.940900 loss:        0.217741
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.991980 loss:        0.025075
Test - acc:         0.937600 loss:        0.219108
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.991300 loss:        0.028072
Test - acc:         0.937300 loss:        0.218763
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.991520 loss:        0.027783
Test - acc:         0.939700 loss:        0.215101
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.991480 loss:        0.028061
Test - acc:         0.938600 loss:        0.214014
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.990260 loss:        0.030157
Test - acc:         0.935300 loss:        0.238359
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.991060 loss:        0.028740
Test - acc:         0.937100 loss:        0.224486
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.989980 loss:        0.030887
Test - acc:         0.935500 loss:        0.237552
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.987820 loss:        0.036898
Test - acc:         0.933100 loss:        0.239548
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.989680 loss:        0.032817
Test - acc:         0.934800 loss:        0.232000
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.989020 loss:        0.033070
Test - acc:         0.934800 loss:        0.235411
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.989600 loss:        0.032807
Test - acc:         0.931400 loss:        0.248396
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.987480 loss:        0.036864
Test - acc:         0.931100 loss:        0.234537
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.988780 loss:        0.034019
Test - acc:         0.936900 loss:        0.228076
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.988620 loss:        0.035104
Test - acc:         0.938300 loss:        0.225110
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.988060 loss:        0.036201
Test - acc:         0.932100 loss:        0.245894
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.987740 loss:        0.037764
Test - acc:         0.932900 loss:        0.241985
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.988800 loss:        0.036289
Test - acc:         0.931700 loss:        0.247263
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.988960 loss:        0.034529
Test - acc:         0.928100 loss:        0.266211
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.986620 loss:        0.040634
Test - acc:         0.934200 loss:        0.231993
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.987660 loss:        0.037563
Test - acc:         0.930800 loss:        0.252483
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.985960 loss:        0.042021
Test - acc:         0.926000 loss:        0.275935
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.986120 loss:        0.041756
Test - acc:         0.931000 loss:        0.238031
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.986560 loss:        0.040669
Test - acc:         0.930100 loss:        0.251672
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.987040 loss:        0.040577
Test - acc:         0.934400 loss:        0.234860
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.987160 loss:        0.041486
Test - acc:         0.931200 loss:        0.241415
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.986720 loss:        0.041079
Test - acc:         0.934400 loss:        0.233368
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.987360 loss:        0.040188
Test - acc:         0.927800 loss:        0.253029
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.985880 loss:        0.044119
Test - acc:         0.929800 loss:        0.246868
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.987260 loss:        0.040179
Test - acc:         0.932000 loss:        0.243125
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.987720 loss:        0.036976
Test - acc:         0.929300 loss:        0.261670
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.985840 loss:        0.043195
Test - acc:         0.926100 loss:        0.270754
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.986580 loss:        0.040499
Test - acc:         0.927800 loss:        0.263963
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.986640 loss:        0.040982
Test - acc:         0.929000 loss:        0.261319
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.987120 loss:        0.040261
Test - acc:         0.925200 loss:        0.278840
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.985980 loss:        0.043226
Test - acc:         0.925600 loss:        0.265517
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.987160 loss:        0.040280
Test - acc:         0.930000 loss:        0.245615
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.988520 loss:        0.039403
Test - acc:         0.936400 loss:        0.220934
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.990320 loss:        0.034573
Test - acc:         0.937000 loss:        0.225445
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.989740 loss:        0.032814
Test - acc:         0.932900 loss:        0.244253
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.989500 loss:        0.033048
Test - acc:         0.935400 loss:        0.234365
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.989500 loss:        0.032880
Test - acc:         0.925200 loss:        0.266769
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.990940 loss:        0.030002
Test - acc:         0.931900 loss:        0.248512
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.990420 loss:        0.031365
Test - acc:         0.933900 loss:        0.259550
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.990620 loss:        0.031514
Test - acc:         0.933900 loss:        0.242419
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.989760 loss:        0.033412
Test - acc:         0.927300 loss:        0.263198
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.990720 loss:        0.031013
Test - acc:         0.933500 loss:        0.244137
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.989620 loss:        0.033270
Test - acc:         0.938900 loss:        0.234500
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.988740 loss:        0.034539
Test - acc:         0.933100 loss:        0.252965
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.988740 loss:        0.035262
Test - acc:         0.936200 loss:        0.226748
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.989700 loss:        0.032673
Test - acc:         0.927800 loss:        0.253883
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.988860 loss:        0.033381
Test - acc:         0.933500 loss:        0.249658
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.988500 loss:        0.035804
Test - acc:         0.926500 loss:        0.271639
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.987140 loss:        0.038091
Test - acc:         0.933500 loss:        0.257964
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.988960 loss:        0.034815
Test - acc:         0.929200 loss:        0.265422
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.988740 loss:        0.034165
Test - acc:         0.933900 loss:        0.246310
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.987260 loss:        0.038711
Test - acc:         0.932400 loss:        0.247193
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.989420 loss:        0.033300
Test - acc:         0.929300 loss:        0.269372
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.988320 loss:        0.035331
Test - acc:         0.931000 loss:        0.239033
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.989520 loss:        0.033376
Test - acc:         0.931900 loss:        0.250537
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.988220 loss:        0.035563
Test - acc:         0.926800 loss:        0.269231
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.988320 loss:        0.035518
Test - acc:         0.933500 loss:        0.250155
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.989060 loss:        0.035159
Test - acc:         0.925300 loss:        0.277807
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.988280 loss:        0.037263
Test - acc:         0.927000 loss:        0.271635
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.989460 loss:        0.033549
Test - acc:         0.924100 loss:        0.298000
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.988380 loss:        0.035741
Test - acc:         0.930600 loss:        0.252232
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.989420 loss:        0.033962
Test - acc:         0.935000 loss:        0.235910
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.990560 loss:        0.030641
Test - acc:         0.925000 loss:        0.282269
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.986680 loss:        0.039733
Test - acc:         0.926800 loss:        0.274363
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.989280 loss:        0.033461
Test - acc:         0.934700 loss:        0.245118
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.988960 loss:        0.034141
Test - acc:         0.928400 loss:        0.266881
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.988080 loss:        0.036755
Test - acc:         0.927300 loss:        0.267121
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.987100 loss:        0.039206
Test - acc:         0.933000 loss:        0.246714
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.989420 loss:        0.034003
Test - acc:         0.927000 loss:        0.266746
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.987140 loss:        0.039109
Test - acc:         0.932600 loss:        0.256318
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.989320 loss:        0.033471
Test - acc:         0.936800 loss:        0.240196
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.989800 loss:        0.033189
Test - acc:         0.924900 loss:        0.285638
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.995180 loss:        0.017423
Test - acc:         0.944100 loss:        0.201065
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.997740 loss:        0.010418
Test - acc:         0.945100 loss:        0.197832
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.998000 loss:        0.009027
Test - acc:         0.946500 loss:        0.196249
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.998420 loss:        0.008126
Test - acc:         0.947500 loss:        0.194099
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.998860 loss:        0.006853
Test - acc:         0.947500 loss:        0.195500
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.998840 loss:        0.006791
Test - acc:         0.948300 loss:        0.192791
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.006028
Test - acc:         0.947900 loss:        0.192861
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.998960 loss:        0.005957
Test - acc:         0.948600 loss:        0.190987
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.004713
Test - acc:         0.949500 loss:        0.190205
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.004884
Test - acc:         0.948300 loss:        0.191513
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.004978
Test - acc:         0.948000 loss:        0.192562
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.999160 loss:        0.004762
Test - acc:         0.948600 loss:        0.192482
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.004448
Test - acc:         0.948900 loss:        0.192923
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.004210
Test - acc:         0.948100 loss:        0.192751
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.004108
Test - acc:         0.948900 loss:        0.190740
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.004202
Test - acc:         0.948700 loss:        0.189652
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003937
Test - acc:         0.948900 loss:        0.190228
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003629
Test - acc:         0.948800 loss:        0.188779
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.003871
Test - acc:         0.949300 loss:        0.188329
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.003565
Test - acc:         0.950300 loss:        0.188790
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.003378
Test - acc:         0.949600 loss:        0.188746
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.003598
Test - acc:         0.949500 loss:        0.189770
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.003841
Test - acc:         0.949300 loss:        0.188114
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.003332
Test - acc:         0.950000 loss:        0.187504
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.003376
Test - acc:         0.951300 loss:        0.186879
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003352
Test - acc:         0.950200 loss:        0.187702
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.003272
Test - acc:         0.949900 loss:        0.188566
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003253
Test - acc:         0.950100 loss:        0.187892
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003285
Test - acc:         0.950800 loss:        0.189248
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.003015
Test - acc:         0.951100 loss:        0.188103
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.991520 loss:        0.041887
Test - acc:         0.938700 loss:        0.206236
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.995600 loss:        0.027112
Test - acc:         0.941300 loss:        0.199281
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.996500 loss:        0.023344
Test - acc:         0.941200 loss:        0.199266
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.996700 loss:        0.020910
Test - acc:         0.941000 loss:        0.196672
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.997360 loss:        0.018812
Test - acc:         0.942200 loss:        0.194379
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.998060 loss:        0.016672
Test - acc:         0.943200 loss:        0.195555
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.997920 loss:        0.015342
Test - acc:         0.942900 loss:        0.194476
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.998160 loss:        0.014426
Test - acc:         0.943600 loss:        0.194109
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.998580 loss:        0.013402
Test - acc:         0.943500 loss:        0.194525
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.998380 loss:        0.012801
Test - acc:         0.943700 loss:        0.196763
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.998680 loss:        0.012188
Test - acc:         0.944600 loss:        0.195950
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.998480 loss:        0.011990
Test - acc:         0.944200 loss:        0.196648
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.998540 loss:        0.011247
Test - acc:         0.944900 loss:        0.197145
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.998740 loss:        0.010338
Test - acc:         0.942900 loss:        0.196985
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.010607
Test - acc:         0.944000 loss:        0.195781
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.998720 loss:        0.009845
Test - acc:         0.945500 loss:        0.194535
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.998540 loss:        0.010170
Test - acc:         0.946500 loss:        0.193884
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.998980 loss:        0.008998
Test - acc:         0.945700 loss:        0.195696
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.999040 loss:        0.008806
Test - acc:         0.945700 loss:        0.196656
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999000 loss:        0.008802
Test - acc:         0.945200 loss:        0.195605
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.998880 loss:        0.008708
Test - acc:         0.945400 loss:        0.197536
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.008385
Test - acc:         0.945200 loss:        0.195722
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.008107
Test - acc:         0.945400 loss:        0.195659
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.007786
Test - acc:         0.946300 loss:        0.196915
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.007151
Test - acc:         0.944800 loss:        0.197654
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.006763
Test - acc:         0.946300 loss:        0.196831
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999100 loss:        0.007473
Test - acc:         0.945600 loss:        0.198674
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.007004
Test - acc:         0.946300 loss:        0.196226
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.998960 loss:        0.007267
Test - acc:         0.946500 loss:        0.196776
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.006836
Test - acc:         0.945700 loss:        0.196958
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999280 loss:        0.006716
Test - acc:         0.946300 loss:        0.197298
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.006593
Test - acc:         0.946400 loss:        0.197949
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999160 loss:        0.006643
Test - acc:         0.946300 loss:        0.197291
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.006135
Test - acc:         0.945600 loss:        0.196944
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.005997
Test - acc:         0.946600 loss:        0.199050
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.006092
Test - acc:         0.946000 loss:        0.197878
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.999380 loss:        0.005862
Test - acc:         0.947300 loss:        0.198603
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.005927
Test - acc:         0.946600 loss:        0.198603
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.005620
Test - acc:         0.946300 loss:        0.199761
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.005723
Test - acc:         0.945800 loss:        0.200397
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.005394
Test - acc:         0.945600 loss:        0.202832
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.005534
Test - acc:         0.946200 loss:        0.200931
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.999180 loss:        0.006044
Test - acc:         0.946300 loss:        0.200919
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.005562
Test - acc:         0.946000 loss:        0.199303
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.005655
Test - acc:         0.946400 loss:        0.197827
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.005073
Test - acc:         0.946600 loss:        0.198989
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.005424
Test - acc:         0.945900 loss:        0.200907
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.005512
Test - acc:         0.946300 loss:        0.198396
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004873
Test - acc:         0.947000 loss:        0.197759
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.005429
Test - acc:         0.947000 loss:        0.198452
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.005105
Test - acc:         0.946300 loss:        0.199540
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.005140
Test - acc:         0.946000 loss:        0.200884
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.004612
Test - acc:         0.946900 loss:        0.201596
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.004735
Test - acc:         0.945900 loss:        0.201766
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.004775
Test - acc:         0.946100 loss:        0.200545
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.004942
Test - acc:         0.947100 loss:        0.200028
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.004532
Test - acc:         0.947100 loss:        0.200407
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.004487
Test - acc:         0.945900 loss:        0.198790
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.004752
Test - acc:         0.946900 loss:        0.201178
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.004668
Test - acc:         0.946800 loss:        0.200878
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.004608
Test - acc:         0.946800 loss:        0.199232
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004435
Test - acc:         0.947300 loss:        0.201519
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.004536
Test - acc:         0.946900 loss:        0.200111
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.004806
Test - acc:         0.946100 loss:        0.201538
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.004415
Test - acc:         0.946700 loss:        0.200866
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.004425
Test - acc:         0.946700 loss:        0.200950
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004291
Test - acc:         0.946300 loss:        0.204895
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.004356
Test - acc:         0.947200 loss:        0.199825
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.003936
Test - acc:         0.947200 loss:        0.200402
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.004141
Test - acc:         0.946800 loss:        0.201299
Sparsity :          0.9375
Wdecay :        0.000500
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "magnitude",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf50_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.321120 loss:        1.854424
Test - acc:         0.411200 loss:        1.565847
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.503360 loss:        1.355529
Test - acc:         0.531400 loss:        1.293555
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.617780 loss:        1.072467
Test - acc:         0.591400 loss:        1.217174
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.680140 loss:        0.906641
Test - acc:         0.678000 loss:        0.939917
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.732700 loss:        0.765037
Test - acc:         0.736100 loss:        0.773973
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.771620 loss:        0.656548
Test - acc:         0.757200 loss:        0.711667
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.798220 loss:        0.586788
Test - acc:         0.761500 loss:        0.711917
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.810080 loss:        0.548399
Test - acc:         0.749500 loss:        0.790651
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.822320 loss:        0.513472
Test - acc:         0.744300 loss:        0.789349
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.831620 loss:        0.493238
Test - acc:         0.786600 loss:        0.631747
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.840040 loss:        0.468217
Test - acc:         0.771300 loss:        0.675180
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.843080 loss:        0.456581
Test - acc:         0.778400 loss:        0.638347
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.848780 loss:        0.444750
Test - acc:         0.790000 loss:        0.639025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.852020 loss:        0.428776
Test - acc:         0.774900 loss:        0.715545
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.856340 loss:        0.424426
Test - acc:         0.792900 loss:        0.618070
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.857340 loss:        0.412202
Test - acc:         0.822300 loss:        0.529257
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.861340 loss:        0.406203
Test - acc:         0.732400 loss:        0.895132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.864500 loss:        0.396459
Test - acc:         0.814100 loss:        0.557998
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.865940 loss:        0.393389
Test - acc:         0.792200 loss:        0.626124
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.868100 loss:        0.385620
Test - acc:         0.859600 loss:        0.433525
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.869140 loss:        0.378930
Test - acc:         0.775600 loss:        0.673397
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.872320 loss:        0.374244
Test - acc:         0.820300 loss:        0.542755
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.868560 loss:        0.377958
Test - acc:         0.832300 loss:        0.520596
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.874120 loss:        0.367059
Test - acc:         0.836200 loss:        0.490914
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.875740 loss:        0.365345
Test - acc:         0.793600 loss:        0.673231
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.877340 loss:        0.363791
Test - acc:         0.796600 loss:        0.618069
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.877120 loss:        0.358088
Test - acc:         0.835000 loss:        0.508686
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.877520 loss:        0.356427
Test - acc:         0.802200 loss:        0.692358
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.879100 loss:        0.354404
Test - acc:         0.821900 loss:        0.532333
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.880980 loss:        0.354309
Test - acc:         0.815300 loss:        0.532912
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.880820 loss:        0.349731
Test - acc:         0.829100 loss:        0.504460
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.880720 loss:        0.348386
Test - acc:         0.840200 loss:        0.468264
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.883940 loss:        0.344529
Test - acc:         0.749700 loss:        0.876263
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.883580 loss:        0.341230
Test - acc:         0.802100 loss:        0.621983
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.883180 loss:        0.345867
Test - acc:         0.813300 loss:        0.591781
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.885000 loss:        0.341380
Test - acc:         0.856500 loss:        0.435875
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.883020 loss:        0.344775
Test - acc:         0.851100 loss:        0.454024
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.884900 loss:        0.336330
Test - acc:         0.813600 loss:        0.574401
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.885520 loss:        0.338197
Test - acc:         0.852600 loss:        0.461663
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.882320 loss:        0.345838
Test - acc:         0.822000 loss:        0.520061
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.883080 loss:        0.342687
Test - acc:         0.787700 loss:        0.688354
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.885640 loss:        0.333763
Test - acc:         0.834800 loss:        0.509691
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.885820 loss:        0.333605
Test - acc:         0.836500 loss:        0.501145
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.887380 loss:        0.330377
Test - acc:         0.853400 loss:        0.438513
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.888040 loss:        0.330697
Test - acc:         0.851500 loss:        0.454154
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.889620 loss:        0.326160
Test - acc:         0.865700 loss:        0.417851
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.887000 loss:        0.331534
Test - acc:         0.872000 loss:        0.384061
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.887280 loss:        0.330618
Test - acc:         0.842200 loss:        0.476856
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.888100 loss:        0.330641
Test - acc:         0.814700 loss:        0.589198
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.886980 loss:        0.330561
Test - acc:         0.838200 loss:        0.506258
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.905240 loss:        0.276352
Test - acc:         0.850000 loss:        0.448996
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.904180 loss:        0.280480
Test - acc:         0.846300 loss:        0.480529
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.901460 loss:        0.288821
Test - acc:         0.844600 loss:        0.464791
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.902720 loss:        0.287210
Test - acc:         0.842700 loss:        0.488863
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.902680 loss:        0.285980
Test - acc:         0.845500 loss:        0.491431
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.901900 loss:        0.290245
Test - acc:         0.783000 loss:        0.747070
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.901020 loss:        0.287667
Test - acc:         0.856600 loss:        0.453415
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.900900 loss:        0.286210
Test - acc:         0.859300 loss:        0.427274
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.901360 loss:        0.290095
Test - acc:         0.851200 loss:        0.433870
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.905120 loss:        0.279733
Test - acc:         0.868000 loss:        0.406118
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.902940 loss:        0.281754
Test - acc:         0.835900 loss:        0.503069
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.901520 loss:        0.289204
Test - acc:         0.864800 loss:        0.427267
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.901560 loss:        0.286967
Test - acc:         0.788000 loss:        0.669108
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.903560 loss:        0.284625
Test - acc:         0.834500 loss:        0.520381
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.903660 loss:        0.281891
Test - acc:         0.821300 loss:        0.582907
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.902940 loss:        0.280296
Test - acc:         0.859900 loss:        0.430822
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.904700 loss:        0.278950
Test - acc:         0.848000 loss:        0.455217
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.901960 loss:        0.287079
Test - acc:         0.870900 loss:        0.388501
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.901200 loss:        0.285361
Test - acc:         0.832800 loss:        0.509995
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.903540 loss:        0.279946
Test - acc:         0.866400 loss:        0.409794
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.904300 loss:        0.280775
Test - acc:         0.866700 loss:        0.386633
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.902740 loss:        0.282400
Test - acc:         0.857800 loss:        0.445103
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.904260 loss:        0.279051
Test - acc:         0.874500 loss:        0.360375
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.904560 loss:        0.279588
Test - acc:         0.845700 loss:        0.474271
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.901580 loss:        0.286410
Test - acc:         0.853100 loss:        0.441375
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.903080 loss:        0.283793
Test - acc:         0.809200 loss:        0.610099
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.905600 loss:        0.276496
Test - acc:         0.834700 loss:        0.501890
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.901840 loss:        0.286865
Test - acc:         0.826600 loss:        0.530112
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.904340 loss:        0.280258
Test - acc:         0.824000 loss:        0.531438
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.905600 loss:        0.279158
Test - acc:         0.834900 loss:        0.509644
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.902800 loss:        0.283331
Test - acc:         0.859900 loss:        0.420608
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.903460 loss:        0.280265
Test - acc:         0.880000 loss:        0.349228
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.904680 loss:        0.276836
Test - acc:         0.847300 loss:        0.479059
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.901040 loss:        0.286264
Test - acc:         0.872000 loss:        0.380829
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.904340 loss:        0.278755
Test - acc:         0.805100 loss:        0.586923
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.904460 loss:        0.278255
Test - acc:         0.876900 loss:        0.373995
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.904780 loss:        0.278050
Test - acc:         0.845400 loss:        0.452757
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.905820 loss:        0.275905
Test - acc:         0.861400 loss:        0.423336
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.905780 loss:        0.277812
Test - acc:         0.876200 loss:        0.396713
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.903920 loss:        0.279437
Test - acc:         0.868300 loss:        0.393869
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.903100 loss:        0.281316
Test - acc:         0.868500 loss:        0.403999
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.905160 loss:        0.278733
Test - acc:         0.835800 loss:        0.529675
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.904760 loss:        0.275750
Test - acc:         0.859200 loss:        0.429092
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.904100 loss:        0.280010
Test - acc:         0.879700 loss:        0.359748
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.904580 loss:        0.278783
Test - acc:         0.858200 loss:        0.435641
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.904720 loss:        0.280218
Test - acc:         0.869900 loss:        0.407023
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.904580 loss:        0.277435
Test - acc:         0.859700 loss:        0.426345
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.903420 loss:        0.278221
Test - acc:         0.811000 loss:        0.565858
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.902040 loss:        0.282952
Test - acc:         0.813400 loss:        0.607313
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.903060 loss:        0.281023
Test - acc:         0.868900 loss:        0.407685
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.917500 loss:        0.239565
Test - acc:         0.847100 loss:        0.508041
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.916140 loss:        0.244958
Test - acc:         0.803000 loss:        0.638996
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.916680 loss:        0.245930
Test - acc:         0.841100 loss:        0.516579
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.912800 loss:        0.256690
Test - acc:         0.827100 loss:        0.576509
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.915520 loss:        0.250914
Test - acc:         0.872200 loss:        0.400930
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.913540 loss:        0.251525
Test - acc:         0.823200 loss:        0.602696
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.912220 loss:        0.258263
Test - acc:         0.818300 loss:        0.569082
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.913180 loss:        0.252633
Test - acc:         0.834800 loss:        0.525300
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.912640 loss:        0.252826
Test - acc:         0.827100 loss:        0.544689
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.914400 loss:        0.248796
Test - acc:         0.872300 loss:        0.396491
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.916260 loss:        0.245363
Test - acc:         0.847000 loss:        0.452621
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.911300 loss:        0.257927
Test - acc:         0.862500 loss:        0.429748
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.913980 loss:        0.251476
Test - acc:         0.836200 loss:        0.522808
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.913360 loss:        0.252227
Test - acc:         0.841600 loss:        0.489149
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.914860 loss:        0.249475
Test - acc:         0.840200 loss:        0.495862
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.914140 loss:        0.250369
Test - acc:         0.848400 loss:        0.485940
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.913800 loss:        0.247725
Test - acc:         0.857300 loss:        0.458936
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.912340 loss:        0.255404
Test - acc:         0.869200 loss:        0.404670
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.912120 loss:        0.252272
Test - acc:         0.811000 loss:        0.620740
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.913220 loss:        0.249877
Test - acc:         0.866600 loss:        0.409245
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.914960 loss:        0.248781
Test - acc:         0.865300 loss:        0.427728
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.914020 loss:        0.252957
Test - acc:         0.864100 loss:        0.416035
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.911120 loss:        0.256967
Test - acc:         0.867400 loss:        0.413704
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.914640 loss:        0.249114
Test - acc:         0.862900 loss:        0.437223
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.913800 loss:        0.251451
Test - acc:         0.875100 loss:        0.389320
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.913240 loss:        0.251339
Test - acc:         0.863900 loss:        0.417394
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.914180 loss:        0.247898
Test - acc:         0.861800 loss:        0.429144
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.912560 loss:        0.254996
Test - acc:         0.852200 loss:        0.464084
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.911520 loss:        0.253057
Test - acc:         0.846200 loss:        0.480095
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.915600 loss:        0.248830
Test - acc:         0.811600 loss:        0.600916
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.913760 loss:        0.250692
Test - acc:         0.877400 loss:        0.373793
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.914520 loss:        0.247694
Test - acc:         0.842900 loss:        0.506374
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.911660 loss:        0.254746
Test - acc:         0.859400 loss:        0.425336
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.912240 loss:        0.256851
Test - acc:         0.813500 loss:        0.598275
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.917360 loss:        0.244086
Test - acc:         0.864900 loss:        0.423594
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.914240 loss:        0.249555
Test - acc:         0.869000 loss:        0.387217
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.914180 loss:        0.252487
Test - acc:         0.855700 loss:        0.433685
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.914260 loss:        0.253995
Test - acc:         0.835000 loss:        0.519608
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.912620 loss:        0.254037
Test - acc:         0.875800 loss:        0.380857
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.914920 loss:        0.248558
Test - acc:         0.822000 loss:        0.558614
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.914620 loss:        0.247032
Test - acc:         0.848600 loss:        0.487828
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.911360 loss:        0.252782
Test - acc:         0.875100 loss:        0.394890
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.913600 loss:        0.252833
Test - acc:         0.833100 loss:        0.515788
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.914520 loss:        0.251320
Test - acc:         0.847400 loss:        0.478339
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.915000 loss:        0.249124
Test - acc:         0.842000 loss:        0.528175
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.915520 loss:        0.246224
Test - acc:         0.866400 loss:        0.409798
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.912580 loss:        0.256197
Test - acc:         0.859100 loss:        0.425144
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.915620 loss:        0.244266
Test - acc:         0.854500 loss:        0.462486
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.912980 loss:        0.253008
Test - acc:         0.869100 loss:        0.408035
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.912000 loss:        0.255385
Test - acc:         0.862500 loss:        0.416784
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.949680 loss:        0.161614
Test - acc:         0.932900 loss:        0.197223
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.965720 loss:        0.110223
Test - acc:         0.936400 loss:        0.191137
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.970020 loss:        0.093078
Test - acc:         0.938800 loss:        0.185020
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.973960 loss:        0.082281
Test - acc:         0.940000 loss:        0.187190
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.975820 loss:        0.074815
Test - acc:         0.940100 loss:        0.187176
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.978640 loss:        0.066651
Test - acc:         0.939800 loss:        0.190931
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.979700 loss:        0.061628
Test - acc:         0.941800 loss:        0.186279
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.981720 loss:        0.056244
Test - acc:         0.939400 loss:        0.194031
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.982100 loss:        0.052955
Test - acc:         0.939700 loss:        0.194207
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.984680 loss:        0.048281
Test - acc:         0.941600 loss:        0.198885
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.984460 loss:        0.047597
Test - acc:         0.937100 loss:        0.211019
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.985740 loss:        0.043570
Test - acc:         0.940700 loss:        0.205861
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.986500 loss:        0.041133
Test - acc:         0.940800 loss:        0.205952
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.987080 loss:        0.040552
Test - acc:         0.940900 loss:        0.207749
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.987500 loss:        0.039037
Test - acc:         0.940600 loss:        0.204211
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.988220 loss:        0.037005
Test - acc:         0.938300 loss:        0.211816
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.989180 loss:        0.034642
Test - acc:         0.941000 loss:        0.200003
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.989700 loss:        0.032250
Test - acc:         0.939100 loss:        0.223330
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.989900 loss:        0.031945
Test - acc:         0.938600 loss:        0.220905
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.989540 loss:        0.032085
Test - acc:         0.934000 loss:        0.236330
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.989440 loss:        0.032497
Test - acc:         0.937600 loss:        0.215544
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.990520 loss:        0.030640
Test - acc:         0.937700 loss:        0.222689
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.990820 loss:        0.029426
Test - acc:         0.936700 loss:        0.227137
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.990740 loss:        0.029884
Test - acc:         0.934900 loss:        0.236248
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.989920 loss:        0.031688
Test - acc:         0.938200 loss:        0.218807
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.989960 loss:        0.031707
Test - acc:         0.936000 loss:        0.237180
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.990440 loss:        0.030444
Test - acc:         0.933200 loss:        0.237555
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.990280 loss:        0.031261
Test - acc:         0.939400 loss:        0.220553
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.989680 loss:        0.031818
Test - acc:         0.935600 loss:        0.233834
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.989260 loss:        0.032682
Test - acc:         0.936900 loss:        0.229062
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.989620 loss:        0.033451
Test - acc:         0.935600 loss:        0.228731
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.989960 loss:        0.031989
Test - acc:         0.933600 loss:        0.233049
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.990500 loss:        0.030434
Test - acc:         0.932400 loss:        0.253701
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.989400 loss:        0.032993
Test - acc:         0.932400 loss:        0.239929
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.988280 loss:        0.035795
Test - acc:         0.934500 loss:        0.228475
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.988960 loss:        0.035329
Test - acc:         0.935300 loss:        0.234459
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.988540 loss:        0.036447
Test - acc:         0.933100 loss:        0.238625
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.988220 loss:        0.036955
Test - acc:         0.930100 loss:        0.241410
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.988900 loss:        0.035852
Test - acc:         0.931900 loss:        0.241499
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.988720 loss:        0.035387
Test - acc:         0.931900 loss:        0.245636
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.987040 loss:        0.040499
Test - acc:         0.933700 loss:        0.247433
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.986680 loss:        0.040997
Test - acc:         0.932700 loss:        0.245428
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.986400 loss:        0.040737
Test - acc:         0.930800 loss:        0.240151
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.986940 loss:        0.040604
Test - acc:         0.934400 loss:        0.237312
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.986280 loss:        0.040979
Test - acc:         0.934900 loss:        0.235225
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.986220 loss:        0.042207
Test - acc:         0.930500 loss:        0.243431
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.987080 loss:        0.040153
Test - acc:         0.933000 loss:        0.250923
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.986600 loss:        0.042251
Test - acc:         0.937400 loss:        0.231846
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.985980 loss:        0.041343
Test - acc:         0.930800 loss:        0.253424
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.986240 loss:        0.042536
Test - acc:         0.929400 loss:        0.250358
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.974740 loss:        0.081660
Test - acc:         0.928700 loss:        0.243614
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.981680 loss:        0.057185
Test - acc:         0.924600 loss:        0.261828
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.982560 loss:        0.055288
Test - acc:         0.915600 loss:        0.311196
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.981880 loss:        0.055881
Test - acc:         0.925200 loss:        0.272755
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.983060 loss:        0.052331
Test - acc:         0.921600 loss:        0.276907
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.984720 loss:        0.046616
Test - acc:         0.927700 loss:        0.260775
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.984700 loss:        0.046372
Test - acc:         0.921700 loss:        0.283972
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.984620 loss:        0.048138
Test - acc:         0.929200 loss:        0.263248
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.983120 loss:        0.050695
Test - acc:         0.928800 loss:        0.259900
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.984700 loss:        0.046736
Test - acc:         0.929800 loss:        0.252553
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.984160 loss:        0.046146
Test - acc:         0.927600 loss:        0.267549
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.985240 loss:        0.045606
Test - acc:         0.928900 loss:        0.264606
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.983680 loss:        0.047560
Test - acc:         0.932200 loss:        0.248690
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.984760 loss:        0.046549
Test - acc:         0.925100 loss:        0.278679
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.984520 loss:        0.046398
Test - acc:         0.927600 loss:        0.264977
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.984980 loss:        0.045644
Test - acc:         0.926100 loss:        0.273458
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.985300 loss:        0.045065
Test - acc:         0.929600 loss:        0.258351
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.985640 loss:        0.044682
Test - acc:         0.925800 loss:        0.279234
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.984080 loss:        0.047656
Test - acc:         0.929700 loss:        0.256946
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.984940 loss:        0.044782
Test - acc:         0.930200 loss:        0.256028
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.986000 loss:        0.043077
Test - acc:         0.921500 loss:        0.285884
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.984680 loss:        0.046222
Test - acc:         0.928800 loss:        0.257098
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.985220 loss:        0.044757
Test - acc:         0.926300 loss:        0.277982
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.985180 loss:        0.045025
Test - acc:         0.928300 loss:        0.264663
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.985400 loss:        0.044826
Test - acc:         0.930100 loss:        0.255609
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.985740 loss:        0.045290
Test - acc:         0.920900 loss:        0.296605
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.984820 loss:        0.046695
Test - acc:         0.928300 loss:        0.269904
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.985340 loss:        0.043964
Test - acc:         0.925200 loss:        0.282009
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.984740 loss:        0.045824
Test - acc:         0.925700 loss:        0.271807
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.984740 loss:        0.048220
Test - acc:         0.925900 loss:        0.273475
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.985340 loss:        0.044959
Test - acc:         0.921100 loss:        0.291403
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.984800 loss:        0.045213
Test - acc:         0.917400 loss:        0.316677
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.984900 loss:        0.045900
Test - acc:         0.923900 loss:        0.292223
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.984900 loss:        0.045249
Test - acc:         0.927500 loss:        0.258622
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.985360 loss:        0.046113
Test - acc:         0.928100 loss:        0.272431
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.985940 loss:        0.043894
Test - acc:         0.925300 loss:        0.271960
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.984440 loss:        0.046429
Test - acc:         0.929400 loss:        0.261096
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.986580 loss:        0.041860
Test - acc:         0.926000 loss:        0.283183
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.986360 loss:        0.040991
Test - acc:         0.926500 loss:        0.276510
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.986060 loss:        0.042852
Test - acc:         0.924100 loss:        0.273686
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.986260 loss:        0.042424
Test - acc:         0.924800 loss:        0.290351
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.984620 loss:        0.046496
Test - acc:         0.920000 loss:        0.297283
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.984560 loss:        0.045595
Test - acc:         0.929800 loss:        0.267008
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.985280 loss:        0.043272
Test - acc:         0.927200 loss:        0.280048
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.985300 loss:        0.047026
Test - acc:         0.928900 loss:        0.266421
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.986840 loss:        0.040902
Test - acc:         0.926200 loss:        0.276465
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.985100 loss:        0.044061
Test - acc:         0.922900 loss:        0.289247
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.985480 loss:        0.043594
Test - acc:         0.924200 loss:        0.268931
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.987100 loss:        0.040026
Test - acc:         0.926800 loss:        0.262334
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.985940 loss:        0.043839
Test - acc:         0.914700 loss:        0.315956
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.961600 loss:        0.132636
Test - acc:         0.924500 loss:        0.232294
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.980020 loss:        0.078016
Test - acc:         0.930900 loss:        0.220360
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.983500 loss:        0.065633
Test - acc:         0.929900 loss:        0.214048
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.984840 loss:        0.057617
Test - acc:         0.932200 loss:        0.215017
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.988200 loss:        0.048488
Test - acc:         0.933800 loss:        0.214434
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.988740 loss:        0.045480
Test - acc:         0.934700 loss:        0.212338
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.990740 loss:        0.039733
Test - acc:         0.934800 loss:        0.211140
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.991320 loss:        0.037460
Test - acc:         0.935600 loss:        0.209536
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.991880 loss:        0.034478
Test - acc:         0.934700 loss:        0.212860
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.992560 loss:        0.032363
Test - acc:         0.936600 loss:        0.210814
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.993000 loss:        0.030996
Test - acc:         0.937000 loss:        0.210395
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.994140 loss:        0.027931
Test - acc:         0.938200 loss:        0.211725
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.992960 loss:        0.029398
Test - acc:         0.936500 loss:        0.212602
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.993740 loss:        0.026567
Test - acc:         0.937800 loss:        0.212325
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.994360 loss:        0.026088
Test - acc:         0.938400 loss:        0.213891
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.994560 loss:        0.024600
Test - acc:         0.937000 loss:        0.212964
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.995120 loss:        0.022723
Test - acc:         0.937300 loss:        0.213709
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.994820 loss:        0.022944
Test - acc:         0.937500 loss:        0.212430
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.995140 loss:        0.021987
Test - acc:         0.937400 loss:        0.214565
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.995780 loss:        0.020778
Test - acc:         0.935600 loss:        0.216465
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.995280 loss:        0.020694
Test - acc:         0.938900 loss:        0.215744
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.995360 loss:        0.020091
Test - acc:         0.938500 loss:        0.215955
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.995840 loss:        0.019598
Test - acc:         0.938400 loss:        0.216112
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.996180 loss:        0.018408
Test - acc:         0.939000 loss:        0.217418
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.995920 loss:        0.018658
Test - acc:         0.937700 loss:        0.218033
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.996120 loss:        0.018182
Test - acc:         0.937700 loss:        0.218757
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.996460 loss:        0.017987
Test - acc:         0.937400 loss:        0.221332
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.996380 loss:        0.017026
Test - acc:         0.938500 loss:        0.219160
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.996720 loss:        0.016166
Test - acc:         0.938200 loss:        0.220544
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.996880 loss:        0.015185
Test - acc:         0.939100 loss:        0.221204
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.996740 loss:        0.015388
Test - acc:         0.938200 loss:        0.218669
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.996720 loss:        0.015649
Test - acc:         0.937800 loss:        0.221801
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.996860 loss:        0.015371
Test - acc:         0.937700 loss:        0.223749
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.997260 loss:        0.014429
Test - acc:         0.939700 loss:        0.221564
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.996960 loss:        0.014651
Test - acc:         0.939300 loss:        0.222122
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.996320 loss:        0.015314
Test - acc:         0.938400 loss:        0.221571
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.997340 loss:        0.013570
Test - acc:         0.939800 loss:        0.221157
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.997140 loss:        0.013898
Test - acc:         0.939100 loss:        0.222938
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.997600 loss:        0.013299
Test - acc:         0.938100 loss:        0.226559
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.997340 loss:        0.012830
Test - acc:         0.939200 loss:        0.220163
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.997060 loss:        0.013779
Test - acc:         0.938700 loss:        0.221219
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.997700 loss:        0.012066
Test - acc:         0.939800 loss:        0.220488
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.997660 loss:        0.012487
Test - acc:         0.939700 loss:        0.219986
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.997900 loss:        0.011878
Test - acc:         0.939300 loss:        0.219907
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.997700 loss:        0.012162
Test - acc:         0.939800 loss:        0.221022
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.997420 loss:        0.012158
Test - acc:         0.938600 loss:        0.228406
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.997760 loss:        0.012107
Test - acc:         0.940200 loss:        0.223683
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.997680 loss:        0.012032
Test - acc:         0.939600 loss:        0.221269
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.997780 loss:        0.011264
Test - acc:         0.939200 loss:        0.220586
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.997180 loss:        0.012182
Test - acc:         0.939300 loss:        0.222086
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.884160 loss:        0.374216
Test - acc:         0.896900 loss:        0.339367
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.932860 loss:        0.216359
Test - acc:         0.905800 loss:        0.310449
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.945980 loss:        0.176335
Test - acc:         0.910100 loss:        0.293233
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.951820 loss:        0.156455
Test - acc:         0.912700 loss:        0.282899
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.955300 loss:        0.142227
Test - acc:         0.915200 loss:        0.284233
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.958760 loss:        0.130121
Test - acc:         0.916200 loss:        0.283465
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.960540 loss:        0.124033
Test - acc:         0.913500 loss:        0.279038
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.965480 loss:        0.111763
Test - acc:         0.914300 loss:        0.280276
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.965100 loss:        0.109642
Test - acc:         0.916100 loss:        0.278784
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.967020 loss:        0.103502
Test - acc:         0.915100 loss:        0.276560
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.967800 loss:        0.100244
Test - acc:         0.917800 loss:        0.277588
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.971380 loss:        0.093424
Test - acc:         0.918800 loss:        0.274681
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.970380 loss:        0.092817
Test - acc:         0.921000 loss:        0.273999
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.971920 loss:        0.089548
Test - acc:         0.919100 loss:        0.270035
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.972740 loss:        0.087441
Test - acc:         0.919400 loss:        0.271207
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.974360 loss:        0.081849
Test - acc:         0.917400 loss:        0.278374
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.974300 loss:        0.081612
Test - acc:         0.919700 loss:        0.274552
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.974920 loss:        0.080333
Test - acc:         0.918300 loss:        0.277048
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.975640 loss:        0.077462
Test - acc:         0.919800 loss:        0.272036
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.974880 loss:        0.078715
Test - acc:         0.919100 loss:        0.275003
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.977060 loss:        0.073345
Test - acc:         0.918300 loss:        0.279523
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.977280 loss:        0.071149
Test - acc:         0.921000 loss:        0.272602
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.976740 loss:        0.074163
Test - acc:         0.921000 loss:        0.274638
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.978080 loss:        0.070010
Test - acc:         0.920600 loss:        0.270947
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.977780 loss:        0.070630
Test - acc:         0.919600 loss:        0.279432
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.978860 loss:        0.066140
Test - acc:         0.923100 loss:        0.263394
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.979840 loss:        0.065031
Test - acc:         0.923100 loss:        0.267598
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.977700 loss:        0.069183
Test - acc:         0.922600 loss:        0.270668
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.979480 loss:        0.065202
Test - acc:         0.923800 loss:        0.271817
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.979540 loss:        0.064143
Test - acc:         0.919300 loss:        0.278982
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.980060 loss:        0.062117
Test - acc:         0.922900 loss:        0.277170
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.980300 loss:        0.061977
Test - acc:         0.920400 loss:        0.273192
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.980820 loss:        0.060652
Test - acc:         0.923100 loss:        0.274210
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.980860 loss:        0.059018
Test - acc:         0.921100 loss:        0.276777
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.980000 loss:        0.061396
Test - acc:         0.921800 loss:        0.282039
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.981200 loss:        0.061243
Test - acc:         0.922100 loss:        0.276836
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.982440 loss:        0.056093
Test - acc:         0.922900 loss:        0.272266
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.981880 loss:        0.057783
Test - acc:         0.924200 loss:        0.275155
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.981980 loss:        0.057160
Test - acc:         0.922200 loss:        0.282677
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.981820 loss:        0.055889
Test - acc:         0.923800 loss:        0.276301
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.982180 loss:        0.056275
Test - acc:         0.924000 loss:        0.275863
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.982500 loss:        0.055626
Test - acc:         0.922500 loss:        0.270213
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.982840 loss:        0.054829
Test - acc:         0.925000 loss:        0.270005
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.983460 loss:        0.052065
Test - acc:         0.924400 loss:        0.272475
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.982580 loss:        0.054681
Test - acc:         0.927900 loss:        0.266659
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.984520 loss:        0.049775
Test - acc:         0.924400 loss:        0.274305
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.982840 loss:        0.052819
Test - acc:         0.925100 loss:        0.277156
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.983020 loss:        0.052165
Test - acc:         0.924000 loss:        0.274583
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.983980 loss:        0.051391
Test - acc:         0.925100 loss:        0.271323
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.984360 loss:        0.050621
Test - acc:         0.923500 loss:        0.279799
Sparsity :          0.9844
Wdecay :        0.000500
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "magnitude",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf39_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.321120 loss:        1.854424
Test - acc:         0.411200 loss:        1.565847
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.503360 loss:        1.355529
Test - acc:         0.531400 loss:        1.293555
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.617780 loss:        1.072467
Test - acc:         0.591400 loss:        1.217174
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.680140 loss:        0.906641
Test - acc:         0.678000 loss:        0.939917
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.732700 loss:        0.765037
Test - acc:         0.736100 loss:        0.773973
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.771620 loss:        0.656548
Test - acc:         0.757200 loss:        0.711667
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.798220 loss:        0.586788
Test - acc:         0.761500 loss:        0.711917
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.810080 loss:        0.548399
Test - acc:         0.749500 loss:        0.790651
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.822320 loss:        0.513472
Test - acc:         0.744300 loss:        0.789349
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.831620 loss:        0.493238
Test - acc:         0.786600 loss:        0.631747
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.840040 loss:        0.468217
Test - acc:         0.771300 loss:        0.675180
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.843080 loss:        0.456581
Test - acc:         0.778400 loss:        0.638347
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.848780 loss:        0.444750
Test - acc:         0.790000 loss:        0.639025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.852020 loss:        0.428776
Test - acc:         0.774900 loss:        0.715545
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.856340 loss:        0.424426
Test - acc:         0.792900 loss:        0.618070
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.857340 loss:        0.412202
Test - acc:         0.822300 loss:        0.529257
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.861340 loss:        0.406203
Test - acc:         0.732400 loss:        0.895132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.864500 loss:        0.396459
Test - acc:         0.814100 loss:        0.557998
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.865940 loss:        0.393389
Test - acc:         0.792200 loss:        0.626124
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.868100 loss:        0.385620
Test - acc:         0.859600 loss:        0.433525
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.869140 loss:        0.378930
Test - acc:         0.775600 loss:        0.673397
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.872320 loss:        0.374244
Test - acc:         0.820300 loss:        0.542755
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.868560 loss:        0.377958
Test - acc:         0.832300 loss:        0.520596
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.874120 loss:        0.367059
Test - acc:         0.836200 loss:        0.490914
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.875740 loss:        0.365345
Test - acc:         0.793600 loss:        0.673231
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.877340 loss:        0.363791
Test - acc:         0.796600 loss:        0.618069
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.877120 loss:        0.358088
Test - acc:         0.835000 loss:        0.508686
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.877520 loss:        0.356427
Test - acc:         0.802200 loss:        0.692358
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.879100 loss:        0.354404
Test - acc:         0.821900 loss:        0.532333
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.880980 loss:        0.354309
Test - acc:         0.815300 loss:        0.532912
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.880820 loss:        0.349731
Test - acc:         0.829100 loss:        0.504460
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.880720 loss:        0.348386
Test - acc:         0.840200 loss:        0.468264
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.883940 loss:        0.344529
Test - acc:         0.749700 loss:        0.876263
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.883580 loss:        0.341230
Test - acc:         0.802100 loss:        0.621983
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.883180 loss:        0.345867
Test - acc:         0.813300 loss:        0.591781
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.885000 loss:        0.341380
Test - acc:         0.856500 loss:        0.435875
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.883020 loss:        0.344775
Test - acc:         0.851100 loss:        0.454024
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.884900 loss:        0.336330
Test - acc:         0.813600 loss:        0.574401
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.885520 loss:        0.338197
Test - acc:         0.852600 loss:        0.461663
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.901080 loss:        0.285618
Test - acc:         0.862200 loss:        0.419125
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.897700 loss:        0.294970
Test - acc:         0.858400 loss:        0.421899
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.900120 loss:        0.294184
Test - acc:         0.861600 loss:        0.426166
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.898140 loss:        0.296746
Test - acc:         0.819400 loss:        0.592152
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.898340 loss:        0.294374
Test - acc:         0.851100 loss:        0.460768
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.901460 loss:        0.287793
Test - acc:         0.855200 loss:        0.439118
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.902520 loss:        0.289511
Test - acc:         0.833400 loss:        0.535498
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.901000 loss:        0.293213
Test - acc:         0.868200 loss:        0.408791
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.898760 loss:        0.294480
Test - acc:         0.858900 loss:        0.434579
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.899500 loss:        0.292275
Test - acc:         0.871800 loss:        0.384691
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.899760 loss:        0.291845
Test - acc:         0.869200 loss:        0.396352
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.900700 loss:        0.284936
Test - acc:         0.814000 loss:        0.588174
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.899360 loss:        0.293729
Test - acc:         0.862800 loss:        0.411606
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.902460 loss:        0.289262
Test - acc:         0.821100 loss:        0.535822
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.905760 loss:        0.280562
Test - acc:         0.861600 loss:        0.420996
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.902000 loss:        0.287561
Test - acc:         0.818400 loss:        0.579613
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.899700 loss:        0.293287
Test - acc:         0.783700 loss:        0.673474
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.902480 loss:        0.285486
Test - acc:         0.825400 loss:        0.557787
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.903740 loss:        0.282808
Test - acc:         0.869700 loss:        0.388511
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.902140 loss:        0.289113
Test - acc:         0.841500 loss:        0.485952
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.902400 loss:        0.286381
Test - acc:         0.864500 loss:        0.412993
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.901580 loss:        0.287138
Test - acc:         0.882800 loss:        0.358707
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.899120 loss:        0.291309
Test - acc:         0.776700 loss:        0.786150
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.901760 loss:        0.287231
Test - acc:         0.835900 loss:        0.491168
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.903380 loss:        0.285739
Test - acc:         0.870400 loss:        0.383237
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.901980 loss:        0.287452
Test - acc:         0.852800 loss:        0.439169
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.903740 loss:        0.286516
Test - acc:         0.849400 loss:        0.460444
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.902900 loss:        0.283395
Test - acc:         0.795900 loss:        0.672854
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.903380 loss:        0.287557
Test - acc:         0.859100 loss:        0.419957
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.902440 loss:        0.285678
Test - acc:         0.839400 loss:        0.504210
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.904200 loss:        0.280371
Test - acc:         0.835600 loss:        0.529485
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.902380 loss:        0.286953
Test - acc:         0.854900 loss:        0.436580
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.900440 loss:        0.291209
Test - acc:         0.831400 loss:        0.520237
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.903600 loss:        0.281077
Test - acc:         0.880500 loss:        0.360457
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.904180 loss:        0.278857
Test - acc:         0.842800 loss:        0.472091
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.901880 loss:        0.288317
Test - acc:         0.828300 loss:        0.520345
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.904300 loss:        0.282495
Test - acc:         0.849800 loss:        0.454282
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.905180 loss:        0.281356
Test - acc:         0.834500 loss:        0.516074
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.901960 loss:        0.287136
Test - acc:         0.730000 loss:        0.934050
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.916740 loss:        0.246203
Test - acc:         0.862100 loss:        0.425587
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.914960 loss:        0.249372
Test - acc:         0.846200 loss:        0.518005
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.911900 loss:        0.257007
Test - acc:         0.832200 loss:        0.522802
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.910820 loss:        0.257427
Test - acc:         0.880300 loss:        0.375841
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.913560 loss:        0.255002
Test - acc:         0.877700 loss:        0.366692
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.911700 loss:        0.257176
Test - acc:         0.879900 loss:        0.356450
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.913400 loss:        0.255993
Test - acc:         0.771700 loss:        0.740511
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.912660 loss:        0.255639
Test - acc:         0.870100 loss:        0.383194
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.912460 loss:        0.254626
Test - acc:         0.833700 loss:        0.517788
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.914520 loss:        0.251311
Test - acc:         0.859200 loss:        0.428045
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.910720 loss:        0.258883
Test - acc:         0.860600 loss:        0.429020
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.911960 loss:        0.255981
Test - acc:         0.873500 loss:        0.396881
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.912360 loss:        0.256035
Test - acc:         0.856100 loss:        0.440207
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.913060 loss:        0.253502
Test - acc:         0.854700 loss:        0.456327
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.912820 loss:        0.254857
Test - acc:         0.872800 loss:        0.386334
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.914060 loss:        0.249445
Test - acc:         0.886900 loss:        0.337192
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.912080 loss:        0.256226
Test - acc:         0.841300 loss:        0.506819
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.913180 loss:        0.254821
Test - acc:         0.865100 loss:        0.431981
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.912440 loss:        0.256615
Test - acc:         0.860700 loss:        0.424005
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.914380 loss:        0.249626
Test - acc:         0.860400 loss:        0.449302
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.915300 loss:        0.250903
Test - acc:         0.852700 loss:        0.471169
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.911560 loss:        0.258769
Test - acc:         0.830300 loss:        0.556583
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.912760 loss:        0.251606
Test - acc:         0.834100 loss:        0.522037
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.911480 loss:        0.257735
Test - acc:         0.871100 loss:        0.399552
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.912980 loss:        0.253140
Test - acc:         0.866700 loss:        0.411665
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.911420 loss:        0.255995
Test - acc:         0.837900 loss:        0.503690
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.914880 loss:        0.248878
Test - acc:         0.854600 loss:        0.451550
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.911240 loss:        0.256123
Test - acc:         0.870500 loss:        0.394928
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.912160 loss:        0.255906
Test - acc:         0.874500 loss:        0.370417
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.912040 loss:        0.254227
Test - acc:         0.813400 loss:        0.596838
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.912080 loss:        0.253999
Test - acc:         0.863300 loss:        0.413145
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.913500 loss:        0.252000
Test - acc:         0.863000 loss:        0.437982
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.914480 loss:        0.252104
Test - acc:         0.844900 loss:        0.483623
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.911500 loss:        0.254589
Test - acc:         0.863000 loss:        0.412656
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.913280 loss:        0.253408
Test - acc:         0.876400 loss:        0.371870
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.912380 loss:        0.256043
Test - acc:         0.881500 loss:        0.363717
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.915300 loss:        0.249675
Test - acc:         0.818400 loss:        0.565702
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.911880 loss:        0.254491
Test - acc:         0.887200 loss:        0.338050
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.910680 loss:        0.256433
Test - acc:         0.821300 loss:        0.598847
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.922060 loss:        0.229189
Test - acc:         0.862400 loss:        0.442105
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.919620 loss:        0.228652
Test - acc:         0.876300 loss:        0.376794
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.918280 loss:        0.236528
Test - acc:         0.869000 loss:        0.408327
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.917200 loss:        0.239028
Test - acc:         0.883200 loss:        0.365809
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.917960 loss:        0.239187
Test - acc:         0.865000 loss:        0.410555
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.916400 loss:        0.243565
Test - acc:         0.859900 loss:        0.448765
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.918420 loss:        0.238522
Test - acc:         0.867200 loss:        0.415776
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.917000 loss:        0.240028
Test - acc:         0.862200 loss:        0.445832
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.919240 loss:        0.237299
Test - acc:         0.865300 loss:        0.417591
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.920140 loss:        0.236232
Test - acc:         0.878000 loss:        0.370340
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.918940 loss:        0.238013
Test - acc:         0.876900 loss:        0.389870
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.917940 loss:        0.241953
Test - acc:         0.813800 loss:        0.633408
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.920600 loss:        0.232956
Test - acc:         0.802700 loss:        0.644344
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.917260 loss:        0.240258
Test - acc:         0.874200 loss:        0.389031
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.919140 loss:        0.234246
Test - acc:         0.866600 loss:        0.427756
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.917280 loss:        0.239281
Test - acc:         0.877500 loss:        0.367994
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.916580 loss:        0.241922
Test - acc:         0.837100 loss:        0.520360
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.919240 loss:        0.233129
Test - acc:         0.858500 loss:        0.458665
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.918780 loss:        0.236462
Test - acc:         0.880500 loss:        0.368485
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.917400 loss:        0.238911
Test - acc:         0.870600 loss:        0.399036
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.918360 loss:        0.237018
Test - acc:         0.863600 loss:        0.410925
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.917120 loss:        0.236129
Test - acc:         0.894500 loss:        0.326706
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.916640 loss:        0.241459
Test - acc:         0.863300 loss:        0.428420
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.919140 loss:        0.232689
Test - acc:         0.863300 loss:        0.422919
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.918800 loss:        0.236580
Test - acc:         0.860800 loss:        0.448861
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.918360 loss:        0.235986
Test - acc:         0.861700 loss:        0.419237
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.917400 loss:        0.242164
Test - acc:         0.884400 loss:        0.347931
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.919720 loss:        0.237225
Test - acc:         0.883600 loss:        0.348948
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.919540 loss:        0.232358
Test - acc:         0.843200 loss:        0.471890
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.916180 loss:        0.243566
Test - acc:         0.882600 loss:        0.351702
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.918740 loss:        0.235580
Test - acc:         0.878600 loss:        0.360131
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.917060 loss:        0.242001
Test - acc:         0.866100 loss:        0.404434
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.919340 loss:        0.238347
Test - acc:         0.842700 loss:        0.479084
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.957740 loss:        0.126190
Test - acc:         0.933600 loss:        0.191636
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.970160 loss:        0.091238
Test - acc:         0.940300 loss:        0.182999
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.974600 loss:        0.078117
Test - acc:         0.938300 loss:        0.181653
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.977260 loss:        0.068726
Test - acc:         0.940600 loss:        0.180746
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.980100 loss:        0.063053
Test - acc:         0.940600 loss:        0.184776
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.982680 loss:        0.055799
Test - acc:         0.941500 loss:        0.185123
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.972880 loss:        0.087311
Test - acc:         0.935600 loss:        0.195239
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.978600 loss:        0.068107
Test - acc:         0.938400 loss:        0.193441
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.059402
Test - acc:         0.940600 loss:        0.195850
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.982800 loss:        0.054598
Test - acc:         0.937600 loss:        0.199498
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.983140 loss:        0.052014
Test - acc:         0.933400 loss:        0.210783
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.985140 loss:        0.048273
Test - acc:         0.939600 loss:        0.201226
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.985220 loss:        0.046866
Test - acc:         0.938500 loss:        0.205362
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.984400 loss:        0.046683
Test - acc:         0.937700 loss:        0.208898
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.985900 loss:        0.043699
Test - acc:         0.938000 loss:        0.211409
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.985480 loss:        0.044021
Test - acc:         0.935700 loss:        0.210366
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.987060 loss:        0.041356
Test - acc:         0.936400 loss:        0.209696
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.988240 loss:        0.038613
Test - acc:         0.935400 loss:        0.218631
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.987500 loss:        0.039395
Test - acc:         0.934300 loss:        0.225814
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.987260 loss:        0.039287
Test - acc:         0.934700 loss:        0.223113
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.987400 loss:        0.039824
Test - acc:         0.937800 loss:        0.214542
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.987760 loss:        0.038350
Test - acc:         0.937100 loss:        0.220846
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.987840 loss:        0.037485
Test - acc:         0.938600 loss:        0.220849
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.987500 loss:        0.038333
Test - acc:         0.936200 loss:        0.222080
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.988280 loss:        0.036756
Test - acc:         0.935800 loss:        0.223674
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.988160 loss:        0.036850
Test - acc:         0.934800 loss:        0.232387
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.987720 loss:        0.038549
Test - acc:         0.937400 loss:        0.225277
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.988340 loss:        0.037263
Test - acc:         0.935100 loss:        0.229961
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.986920 loss:        0.040050
Test - acc:         0.927400 loss:        0.264221
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.987560 loss:        0.038644
Test - acc:         0.933900 loss:        0.236036
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.986900 loss:        0.040227
Test - acc:         0.931000 loss:        0.248294
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.987260 loss:        0.040660
Test - acc:         0.931900 loss:        0.243807
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.987100 loss:        0.038938
Test - acc:         0.932000 loss:        0.238437
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.985500 loss:        0.042798
Test - acc:         0.929000 loss:        0.259528
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.986400 loss:        0.042035
Test - acc:         0.931500 loss:        0.241557
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.985720 loss:        0.042776
Test - acc:         0.927500 loss:        0.258279
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.985520 loss:        0.043730
Test - acc:         0.927400 loss:        0.250591
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.984420 loss:        0.046471
Test - acc:         0.931400 loss:        0.242359
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.986060 loss:        0.044012
Test - acc:         0.930000 loss:        0.257070
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.984860 loss:        0.045689
Test - acc:         0.929700 loss:        0.240375
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.984800 loss:        0.045438
Test - acc:         0.928100 loss:        0.252331
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.984320 loss:        0.048716
Test - acc:         0.928600 loss:        0.254717
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.983080 loss:        0.049562
Test - acc:         0.928100 loss:        0.252732
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.985160 loss:        0.046977
Test - acc:         0.931400 loss:        0.235768
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.984880 loss:        0.046838
Test - acc:         0.929100 loss:        0.253212
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.959740 loss:        0.123959
Test - acc:         0.913200 loss:        0.274686
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.970060 loss:        0.090837
Test - acc:         0.905100 loss:        0.320979
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.972040 loss:        0.084174
Test - acc:         0.912900 loss:        0.301232
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.971940 loss:        0.085210
Test - acc:         0.916800 loss:        0.277445
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.972340 loss:        0.080260
Test - acc:         0.916800 loss:        0.286532
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.974580 loss:        0.077099
Test - acc:         0.920200 loss:        0.272730
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.973480 loss:        0.076623
Test - acc:         0.918600 loss:        0.267952
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.974400 loss:        0.075780
Test - acc:         0.916000 loss:        0.288920
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.977160 loss:        0.068072
Test - acc:         0.918900 loss:        0.275943
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.975680 loss:        0.072617
Test - acc:         0.918300 loss:        0.279082
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.974340 loss:        0.072846
Test - acc:         0.918500 loss:        0.286633
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.975380 loss:        0.071942
Test - acc:         0.912000 loss:        0.300407
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.975640 loss:        0.071308
Test - acc:         0.917500 loss:        0.300809
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.976580 loss:        0.069694
Test - acc:         0.918200 loss:        0.279190
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.977220 loss:        0.068464
Test - acc:         0.919300 loss:        0.266504
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.977240 loss:        0.065957
Test - acc:         0.915700 loss:        0.296734
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.977780 loss:        0.067857
Test - acc:         0.916100 loss:        0.294135
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.977020 loss:        0.068749
Test - acc:         0.922000 loss:        0.276812
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.977160 loss:        0.067807
Test - acc:         0.912500 loss:        0.301468
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.977360 loss:        0.066666
Test - acc:         0.916100 loss:        0.302808
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.977040 loss:        0.068208
Test - acc:         0.913900 loss:        0.300807
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.977840 loss:        0.066594
Test - acc:         0.921300 loss:        0.268162
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.977960 loss:        0.065078
Test - acc:         0.919300 loss:        0.294178
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.977540 loss:        0.066520
Test - acc:         0.922300 loss:        0.281689
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.978280 loss:        0.065478
Test - acc:         0.919200 loss:        0.278988
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.978600 loss:        0.062234
Test - acc:         0.911600 loss:        0.307580
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.977320 loss:        0.066206
Test - acc:         0.919300 loss:        0.287666
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.978000 loss:        0.065802
Test - acc:         0.918500 loss:        0.278461
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.977520 loss:        0.065874
Test - acc:         0.923900 loss:        0.269838
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.979500 loss:        0.061914
Test - acc:         0.916000 loss:        0.305659
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.978020 loss:        0.066047
Test - acc:         0.921200 loss:        0.268126
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.978580 loss:        0.063505
Test - acc:         0.919600 loss:        0.270098
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.978520 loss:        0.063847
Test - acc:         0.924300 loss:        0.268796
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.979500 loss:        0.061626
Test - acc:         0.924400 loss:        0.277073
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.977560 loss:        0.066477
Test - acc:         0.919800 loss:        0.283132
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.978420 loss:        0.062861
Test - acc:         0.920000 loss:        0.283358
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.979780 loss:        0.060237
Test - acc:         0.918400 loss:        0.282759
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.978100 loss:        0.062701
Test - acc:         0.910300 loss:        0.326452
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.978660 loss:        0.064937
Test - acc:         0.914300 loss:        0.290583
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.930160 loss:        0.210274
Test - acc:         0.893900 loss:        0.331487
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.945980 loss:        0.157981
Test - acc:         0.892500 loss:        0.354178
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.948740 loss:        0.147317
Test - acc:         0.893400 loss:        0.344903
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.951140 loss:        0.142577
Test - acc:         0.898800 loss:        0.318481
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.952440 loss:        0.135483
Test - acc:         0.901000 loss:        0.327182
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.953740 loss:        0.133811
Test - acc:         0.903700 loss:        0.315749
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.954340 loss:        0.131952
Test - acc:         0.912900 loss:        0.287010
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.954700 loss:        0.131653
Test - acc:         0.903000 loss:        0.311918
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.956600 loss:        0.125311
Test - acc:         0.907600 loss:        0.309194
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.956420 loss:        0.125439
Test - acc:         0.905500 loss:        0.314944
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.956120 loss:        0.125623
Test - acc:         0.906400 loss:        0.311759
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.957020 loss:        0.121593
Test - acc:         0.907000 loss:        0.312950
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.960860 loss:        0.115427
Test - acc:         0.905700 loss:        0.312800
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.959500 loss:        0.117739
Test - acc:         0.899900 loss:        0.353414
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.958760 loss:        0.117767
Test - acc:         0.910800 loss:        0.305359
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.961780 loss:        0.109834
Test - acc:         0.906600 loss:        0.304601
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.974100 loss:        0.076628
Test - acc:         0.925900 loss:        0.242138
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.979780 loss:        0.063806
Test - acc:         0.927900 loss:        0.240957
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.981320 loss:        0.059757
Test - acc:         0.927400 loss:        0.239527
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.982900 loss:        0.055917
Test - acc:         0.928300 loss:        0.238732
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.983380 loss:        0.052338
Test - acc:         0.929200 loss:        0.239601
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.984920 loss:        0.049593
Test - acc:         0.929100 loss:        0.239057
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.985500 loss:        0.049106
Test - acc:         0.929300 loss:        0.240353
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.985640 loss:        0.046889
Test - acc:         0.929600 loss:        0.240805
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.986020 loss:        0.046163
Test - acc:         0.928600 loss:        0.241533
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.986360 loss:        0.044062
Test - acc:         0.929500 loss:        0.240851
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.987560 loss:        0.042987
Test - acc:         0.929500 loss:        0.241203
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.987300 loss:        0.043007
Test - acc:         0.930500 loss:        0.240300
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.988180 loss:        0.041149
Test - acc:         0.929400 loss:        0.239953
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.988000 loss:        0.041662
Test - acc:         0.930200 loss:        0.241741
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.988000 loss:        0.040560
Test - acc:         0.931800 loss:        0.240596
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.988660 loss:        0.039101
Test - acc:         0.931800 loss:        0.238598
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.988160 loss:        0.038970
Test - acc:         0.930600 loss:        0.241429
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.988840 loss:        0.038298
Test - acc:         0.929600 loss:        0.241526
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.989860 loss:        0.036153
Test - acc:         0.930400 loss:        0.244456
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.989480 loss:        0.036612
Test - acc:         0.929300 loss:        0.242216
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.988760 loss:        0.036318
Test - acc:         0.929800 loss:        0.243897
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.989540 loss:        0.035081
Test - acc:         0.930000 loss:        0.245124
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.990060 loss:        0.034774
Test - acc:         0.931700 loss:        0.244647
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.877520 loss:        0.393556
Test - acc:         0.884100 loss:        0.351001
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.921540 loss:        0.247759
Test - acc:         0.892900 loss:        0.326180
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.928360 loss:        0.221040
Test - acc:         0.896900 loss:        0.315140
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.935100 loss:        0.199706
Test - acc:         0.896800 loss:        0.310549
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.935980 loss:        0.193660
Test - acc:         0.902000 loss:        0.302089
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.940740 loss:        0.182894
Test - acc:         0.901000 loss:        0.301934
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.942940 loss:        0.174499
Test - acc:         0.902300 loss:        0.296376
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.944500 loss:        0.168485
Test - acc:         0.903300 loss:        0.292657
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.945120 loss:        0.163856
Test - acc:         0.904000 loss:        0.291472
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.946380 loss:        0.160080
Test - acc:         0.903600 loss:        0.290388
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.947140 loss:        0.157990
Test - acc:         0.907400 loss:        0.286967
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.949680 loss:        0.151932
Test - acc:         0.904300 loss:        0.293764
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.950180 loss:        0.148740
Test - acc:         0.907300 loss:        0.292828
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.950340 loss:        0.148817
Test - acc:         0.906800 loss:        0.288720
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.950940 loss:        0.144785
Test - acc:         0.907600 loss:        0.288731
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.952180 loss:        0.142938
Test - acc:         0.908000 loss:        0.284289
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.955720 loss:        0.135997
Test - acc:         0.909200 loss:        0.286238
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.954000 loss:        0.139021
Test - acc:         0.911000 loss:        0.283506
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.954640 loss:        0.134597
Test - acc:         0.910100 loss:        0.286439
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.955660 loss:        0.132337
Test - acc:         0.910500 loss:        0.288459
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.953100 loss:        0.136981
Test - acc:         0.910400 loss:        0.289751
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.955900 loss:        0.131613
Test - acc:         0.909400 loss:        0.294236
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.955960 loss:        0.130977
Test - acc:         0.911500 loss:        0.285829
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.955020 loss:        0.131041
Test - acc:         0.910900 loss:        0.283484
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.957220 loss:        0.128164
Test - acc:         0.909600 loss:        0.286899
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.955780 loss:        0.129008
Test - acc:         0.909400 loss:        0.288858
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.957300 loss:        0.125509
Test - acc:         0.910300 loss:        0.284983
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.955660 loss:        0.127541
Test - acc:         0.910000 loss:        0.286589
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.956340 loss:        0.127491
Test - acc:         0.911900 loss:        0.283567
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.960220 loss:        0.119953
Test - acc:         0.910300 loss:        0.282592
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.957580 loss:        0.124071
Test - acc:         0.911600 loss:        0.283042
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.958940 loss:        0.121206
Test - acc:         0.911000 loss:        0.288031
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.958720 loss:        0.119207
Test - acc:         0.911500 loss:        0.285088
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.960620 loss:        0.117852
Test - acc:         0.911600 loss:        0.286886
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.959420 loss:        0.119035
Test - acc:         0.908500 loss:        0.289579
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.959760 loss:        0.117344
Test - acc:         0.912600 loss:        0.288288
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.960460 loss:        0.116329
Test - acc:         0.912000 loss:        0.285593
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.961900 loss:        0.113878
Test - acc:         0.911800 loss:        0.290280
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.961300 loss:        0.113179
Test - acc:         0.913800 loss:        0.283996
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.589040 loss:        1.274588
Test - acc:         0.710100 loss:        0.876377
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.719620 loss:        0.840536
Test - acc:         0.745900 loss:        0.765208
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.749620 loss:        0.740150
Test - acc:         0.747900 loss:        0.742068
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.769140 loss:        0.688334
Test - acc:         0.778700 loss:        0.657294
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.780680 loss:        0.645771
Test - acc:         0.791300 loss:        0.618691
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.788360 loss:        0.619969
Test - acc:         0.780900 loss:        0.637097
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.796120 loss:        0.600405
Test - acc:         0.796600 loss:        0.598471
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.801800 loss:        0.579967
Test - acc:         0.784100 loss:        0.648214
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.808520 loss:        0.565363
Test - acc:         0.788700 loss:        0.640191
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.808360 loss:        0.558590
Test - acc:         0.804100 loss:        0.567649
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.814500 loss:        0.543611
Test - acc:         0.811700 loss:        0.560286
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.817120 loss:        0.534689
Test - acc:         0.810700 loss:        0.562372
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.820440 loss:        0.524481
Test - acc:         0.790300 loss:        0.629458
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.820500 loss:        0.519178
Test - acc:         0.820100 loss:        0.528925
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.825960 loss:        0.509126
Test - acc:         0.818800 loss:        0.531766
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.826660 loss:        0.503042
Test - acc:         0.823500 loss:        0.524653
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.827160 loss:        0.504521
Test - acc:         0.825500 loss:        0.514671
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.830000 loss:        0.492516
Test - acc:         0.819300 loss:        0.533653
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.831140 loss:        0.489800
Test - acc:         0.818400 loss:        0.520023
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.832180 loss:        0.484438
Test - acc:         0.815900 loss:        0.554786
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.832240 loss:        0.479589
Test - acc:         0.820800 loss:        0.529569
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.835720 loss:        0.473248
Test - acc:         0.814100 loss:        0.544773
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.838920 loss:        0.470439
Test - acc:         0.831300 loss:        0.497330
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.838240 loss:        0.469977
Test - acc:         0.825800 loss:        0.509273
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.840740 loss:        0.462436
Test - acc:         0.829100 loss:        0.511948
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.840080 loss:        0.460056
Test - acc:         0.835100 loss:        0.489697
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.840920 loss:        0.460487
Test - acc:         0.834500 loss:        0.492318
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.842160 loss:        0.456496
Test - acc:         0.833800 loss:        0.489475
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.843860 loss:        0.454465
Test - acc:         0.832000 loss:        0.482860
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.846060 loss:        0.447589
Test - acc:         0.839000 loss:        0.483124
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.844480 loss:        0.451716
Test - acc:         0.836900 loss:        0.478225
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.845040 loss:        0.448029
Test - acc:         0.836900 loss:        0.477806
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.846420 loss:        0.444422
Test - acc:         0.839100 loss:        0.475410
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.847060 loss:        0.439419
Test - acc:         0.840300 loss:        0.471167
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.847180 loss:        0.443177
Test - acc:         0.839500 loss:        0.473462
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.848800 loss:        0.439739
Test - acc:         0.840700 loss:        0.477037
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.850280 loss:        0.437151
Test - acc:         0.831100 loss:        0.490168
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.848740 loss:        0.436877
Test - acc:         0.839100 loss:        0.478035
Sparsity :          0.9961
Wdecay :        0.000500
