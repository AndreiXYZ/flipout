Running --prune_criterion topflip --seed 42 --prune_freq 70 --prune_rate 0.5 --noise --comment=vgg19_crit=topflip_pf=70_seed=42 --save_model=pre-finetune/vgg19_topflip_pf70_s42
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "topflip",
    "prune_freq": 70,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_topflip_pf70_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.112220 loss:        2.707889
Test - acc:         0.115500 loss:        2.285625
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.171100 loss:        2.119530
Test - acc:         0.247800 loss:        1.889029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.260180 loss:        1.845003
Test - acc:         0.293800 loss:        1.793629
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.351700 loss:        1.666860
Test - acc:         0.365700 loss:        1.606198
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.442140 loss:        1.475611
Test - acc:         0.498800 loss:        1.346169
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.541180 loss:        1.260178
Test - acc:         0.453900 loss:        1.738315
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.619740 loss:        1.083854
Test - acc:         0.532000 loss:        1.515536
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.675980 loss:        0.929632
Test - acc:         0.621200 loss:        1.227844
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.710540 loss:        0.855181
Test - acc:         0.638900 loss:        1.146975
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.730440 loss:        0.803610
Test - acc:         0.680300 loss:        1.070751
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.752360 loss:        0.752724
Test - acc:         0.695200 loss:        0.963042
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.761480 loss:        0.725837
Test - acc:         0.731800 loss:        0.838521
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.771840 loss:        0.701123
Test - acc:         0.694700 loss:        0.991394
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.778780 loss:        0.676362
Test - acc:         0.667800 loss:        1.115059
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.781940 loss:        0.664950
Test - acc:         0.744500 loss:        0.805362
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.790840 loss:        0.641081
Test - acc:         0.779400 loss:        0.675063
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.795240 loss:        0.630031
Test - acc:         0.697300 loss:        0.917811
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.800100 loss:        0.615935
Test - acc:         0.719800 loss:        0.926714
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.800380 loss:        0.612391
Test - acc:         0.752900 loss:        0.813506
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.802400 loss:        0.604146
Test - acc:         0.770800 loss:        0.743486
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.807380 loss:        0.594119
Test - acc:         0.665200 loss:        1.083300
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.810160 loss:        0.585650
Test - acc:         0.778600 loss:        0.685493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.809940 loss:        0.585200
Test - acc:         0.780900 loss:        0.664840
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.811000 loss:        0.585315
Test - acc:         0.780700 loss:        0.692555
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.812760 loss:        0.576658
Test - acc:         0.723800 loss:        0.945873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.819780 loss:        0.555273
Test - acc:         0.759000 loss:        0.754278
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.816960 loss:        0.567006
Test - acc:         0.759200 loss:        0.769223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.818100 loss:        0.561920
Test - acc:         0.784400 loss:        0.689784
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.818680 loss:        0.560318
Test - acc:         0.781500 loss:        0.680860
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.818160 loss:        0.564143
Test - acc:         0.704200 loss:        0.952644
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.824040 loss:        0.548989
Test - acc:         0.768300 loss:        0.734637
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.822460 loss:        0.552491
Test - acc:         0.795300 loss:        0.651316
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.822260 loss:        0.549665
Test - acc:         0.776000 loss:        0.726616
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.824040 loss:        0.545305
Test - acc:         0.696300 loss:        1.066222
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.828700 loss:        0.532606
Test - acc:         0.763700 loss:        0.736834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.825980 loss:        0.536146
Test - acc:         0.797400 loss:        0.639977
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.825080 loss:        0.541546
Test - acc:         0.783400 loss:        0.693384
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.827300 loss:        0.528451
Test - acc:         0.736300 loss:        0.862552
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.824660 loss:        0.537154
Test - acc:         0.745800 loss:        0.838598
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.830840 loss:        0.527442
Test - acc:         0.776500 loss:        0.700459
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.827020 loss:        0.530133
Test - acc:         0.761700 loss:        0.742538
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.827200 loss:        0.537913
Test - acc:         0.782600 loss:        0.694255
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.831900 loss:        0.523550
Test - acc:         0.741600 loss:        0.814439
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.834320 loss:        0.514167
Test - acc:         0.719800 loss:        0.969886
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.830520 loss:        0.522136
Test - acc:         0.717100 loss:        1.180856
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.833200 loss:        0.516630
Test - acc:         0.786700 loss:        0.678516
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.830840 loss:        0.519828
Test - acc:         0.772900 loss:        0.756524
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.831600 loss:        0.518631
Test - acc:         0.815600 loss:        0.572250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.835020 loss:        0.510291
Test - acc:         0.822800 loss:        0.550194
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.833200 loss:        0.518434
Test - acc:         0.786100 loss:        0.652066
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.836580 loss:        0.505090
Test - acc:         0.735300 loss:        0.861792
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.832680 loss:        0.512519
Test - acc:         0.788800 loss:        0.647510
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.836100 loss:        0.501027
Test - acc:         0.728100 loss:        0.885563
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.834200 loss:        0.505712
Test - acc:         0.745700 loss:        0.868215
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.834340 loss:        0.505220
Test - acc:         0.784600 loss:        0.665123
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.839360 loss:        0.494393
Test - acc:         0.812100 loss:        0.580466
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.835600 loss:        0.502950
Test - acc:         0.764100 loss:        0.750372
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.838180 loss:        0.500562
Test - acc:         0.776200 loss:        0.756491
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.836300 loss:        0.502078
Test - acc:         0.789500 loss:        0.655714
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.836660 loss:        0.502930
Test - acc:         0.707700 loss:        0.958034
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.839560 loss:        0.493270
Test - acc:         0.778800 loss:        0.711446
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.839140 loss:        0.498796
Test - acc:         0.793600 loss:        0.662148
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.840560 loss:        0.484859
Test - acc:         0.794200 loss:        0.628181
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.838240 loss:        0.494592
Test - acc:         0.789800 loss:        0.661584
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.833880 loss:        0.504412
Test - acc:         0.755700 loss:        0.800765
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.842280 loss:        0.487378
Test - acc:         0.716700 loss:        0.987596
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.841560 loss:        0.488262
Test - acc:         0.737600 loss:        0.838389
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.837080 loss:        0.505145
Test - acc:         0.810900 loss:        0.588247
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.841920 loss:        0.489849
Test - acc:         0.831600 loss:        0.527978
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.841560 loss:        0.486142
Test - acc:         0.818400 loss:        0.568968
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.846980 loss:        0.469407
Test - acc:         0.803400 loss:        0.614609
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.851960 loss:        0.451807
Test - acc:         0.817800 loss:        0.569774
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.853400 loss:        0.446446
Test - acc:         0.790500 loss:        0.625445
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.854020 loss:        0.442666
Test - acc:         0.771000 loss:        0.725056
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.855380 loss:        0.437798
Test - acc:         0.792400 loss:        0.636124
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.855360 loss:        0.433753
Test - acc:         0.843600 loss:        0.461984
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.857440 loss:        0.431622
Test - acc:         0.752700 loss:        0.802356
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.857600 loss:        0.428914
Test - acc:         0.779900 loss:        0.681671
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.856100 loss:        0.434866
Test - acc:         0.814900 loss:        0.569497
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.858240 loss:        0.431016
Test - acc:         0.803500 loss:        0.626531
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.858840 loss:        0.424513
Test - acc:         0.718300 loss:        0.983324
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.859180 loss:        0.426980
Test - acc:         0.809300 loss:        0.571960
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.857920 loss:        0.428731
Test - acc:         0.796200 loss:        0.614699
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.861560 loss:        0.414922
Test - acc:         0.769000 loss:        0.759911
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.860400 loss:        0.422913
Test - acc:         0.824300 loss:        0.540238
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.859480 loss:        0.420878
Test - acc:         0.820300 loss:        0.583606
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.859360 loss:        0.423091
Test - acc:         0.835600 loss:        0.486420
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.860940 loss:        0.418511
Test - acc:         0.803800 loss:        0.643608
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.860040 loss:        0.421310
Test - acc:         0.761500 loss:        0.731642
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.861900 loss:        0.416911
Test - acc:         0.792200 loss:        0.664585
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.859840 loss:        0.418905
Test - acc:         0.792100 loss:        0.638871
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.861120 loss:        0.417651
Test - acc:         0.802900 loss:        0.629598
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.859980 loss:        0.417703
Test - acc:         0.771600 loss:        0.749917
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.861680 loss:        0.414462
Test - acc:         0.835200 loss:        0.501361
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.862980 loss:        0.418849
Test - acc:         0.803200 loss:        0.640039
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.862580 loss:        0.413624
Test - acc:         0.833100 loss:        0.494487
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.861560 loss:        0.414460
Test - acc:         0.802100 loss:        0.622710
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.866280 loss:        0.406022
Test - acc:         0.825400 loss:        0.517438
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.860260 loss:        0.417252
Test - acc:         0.773600 loss:        0.737572
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.862160 loss:        0.409069
Test - acc:         0.831700 loss:        0.537868
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.864200 loss:        0.408326
Test - acc:         0.770300 loss:        0.755828
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.861160 loss:        0.415990
Test - acc:         0.765200 loss:        0.790669
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.859940 loss:        0.417468
Test - acc:         0.777100 loss:        0.695216
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.862360 loss:        0.410495
Test - acc:         0.831000 loss:        0.527790
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.863200 loss:        0.407812
Test - acc:         0.790000 loss:        0.678226
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.862380 loss:        0.414633
Test - acc:         0.801000 loss:        0.612883
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.864100 loss:        0.407637
Test - acc:         0.830700 loss:        0.509945
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.866860 loss:        0.405894
Test - acc:         0.828900 loss:        0.541666
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.861940 loss:        0.413865
Test - acc:         0.784300 loss:        0.694070
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.862620 loss:        0.412037
Test - acc:         0.811200 loss:        0.576582
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.861660 loss:        0.410025
Test - acc:         0.705600 loss:        1.063471
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.862760 loss:        0.410988
Test - acc:         0.818200 loss:        0.580038
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.862340 loss:        0.411222
Test - acc:         0.803500 loss:        0.593078
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.862200 loss:        0.416919
Test - acc:         0.802300 loss:        0.635300
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.865600 loss:        0.412607
Test - acc:         0.771700 loss:        0.719445
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.865960 loss:        0.407197
Test - acc:         0.795500 loss:        0.650893
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.862440 loss:        0.414669
Test - acc:         0.842100 loss:        0.482362
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.866700 loss:        0.403127
Test - acc:         0.783900 loss:        0.665886
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.863660 loss:        0.411940
Test - acc:         0.834200 loss:        0.496801
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.862840 loss:        0.408829
Test - acc:         0.805400 loss:        0.584951
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.863920 loss:        0.407637
Test - acc:         0.837000 loss:        0.505051
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.863660 loss:        0.410640
Test - acc:         0.851700 loss:        0.454741
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.861280 loss:        0.416809
Test - acc:         0.805700 loss:        0.586401
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.862520 loss:        0.412501
Test - acc:         0.781600 loss:        0.689223
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.864220 loss:        0.410743
Test - acc:         0.781800 loss:        0.686576
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.864240 loss:        0.407394
Test - acc:         0.733300 loss:        0.868181
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.863640 loss:        0.407183
Test - acc:         0.828200 loss:        0.530562
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.863960 loss:        0.405249
Test - acc:         0.735600 loss:        0.945475
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.862780 loss:        0.413222
Test - acc:         0.830500 loss:        0.518249
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.863900 loss:        0.408715
Test - acc:         0.778200 loss:        0.693597
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.866580 loss:        0.406338
Test - acc:         0.746700 loss:        0.809735
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.864120 loss:        0.410038
Test - acc:         0.813100 loss:        0.569955
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.863420 loss:        0.409433
Test - acc:         0.707800 loss:        1.064652
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.862540 loss:        0.413597
Test - acc:         0.785500 loss:        0.661229
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.862040 loss:        0.410682
Test - acc:         0.813500 loss:        0.551845
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.860220 loss:        0.416017
Test - acc:         0.771200 loss:        0.679111
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.862780 loss:        0.409803
Test - acc:         0.808500 loss:        0.602278
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.864140 loss:        0.408498
Test - acc:         0.741600 loss:        0.854773
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.865120 loss:        0.405226
Test - acc:         0.825800 loss:        0.527276
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.865380 loss:        0.406743
Test - acc:         0.731800 loss:        0.934645
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.877600 loss:        0.366693
Test - acc:         0.843000 loss:        0.486982
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.876480 loss:        0.369909
Test - acc:         0.834400 loss:        0.514551
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.874980 loss:        0.371772
Test - acc:         0.843200 loss:        0.479484
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.872980 loss:        0.374197
Test - acc:         0.815800 loss:        0.548909
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.876660 loss:        0.367272
Test - acc:         0.840000 loss:        0.481031
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.879200 loss:        0.361894
Test - acc:         0.835700 loss:        0.509518
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.877980 loss:        0.365814
Test - acc:         0.789600 loss:        0.696770
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.876180 loss:        0.365701
Test - acc:         0.802900 loss:        0.642547
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.878040 loss:        0.361907
Test - acc:         0.830100 loss:        0.513612
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.878820 loss:        0.363673
Test - acc:         0.824200 loss:        0.553278
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.922940 loss:        0.231659
Test - acc:         0.908300 loss:        0.283728
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.935500 loss:        0.195196
Test - acc:         0.910700 loss:        0.276074
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.940460 loss:        0.178467
Test - acc:         0.913900 loss:        0.272672
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.944320 loss:        0.166556
Test - acc:         0.915200 loss:        0.268703
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.947040 loss:        0.153829
Test - acc:         0.915500 loss:        0.273923
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.950040 loss:        0.147731
Test - acc:         0.914900 loss:        0.278526
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.951180 loss:        0.143898
Test - acc:         0.915000 loss:        0.279476
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.951340 loss:        0.141358
Test - acc:         0.911100 loss:        0.288749
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.954320 loss:        0.136457
Test - acc:         0.913300 loss:        0.280015
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.955880 loss:        0.129737
Test - acc:         0.914100 loss:        0.273355
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.957000 loss:        0.127779
Test - acc:         0.918300 loss:        0.273699
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.958140 loss:        0.123084
Test - acc:         0.917400 loss:        0.281637
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.957920 loss:        0.120634
Test - acc:         0.916200 loss:        0.277920
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.959940 loss:        0.115775
Test - acc:         0.912600 loss:        0.296606
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.958340 loss:        0.121270
Test - acc:         0.911800 loss:        0.290114
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.958660 loss:        0.117383
Test - acc:         0.913500 loss:        0.296433
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.960900 loss:        0.112851
Test - acc:         0.910400 loss:        0.306885
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.959300 loss:        0.115869
Test - acc:         0.908100 loss:        0.309741
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.961120 loss:        0.111604
Test - acc:         0.907900 loss:        0.311035
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.960800 loss:        0.109867
Test - acc:         0.904800 loss:        0.316715
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.960780 loss:        0.112578
Test - acc:         0.906300 loss:        0.313183
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.961900 loss:        0.108751
Test - acc:         0.910700 loss:        0.308647
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.961760 loss:        0.108855
Test - acc:         0.903000 loss:        0.332567
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.961380 loss:        0.112077
Test - acc:         0.905300 loss:        0.327447
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.961660 loss:        0.109438
Test - acc:         0.910000 loss:        0.316885
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.962180 loss:        0.109858
Test - acc:         0.907400 loss:        0.319619
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.961040 loss:        0.111709
Test - acc:         0.910600 loss:        0.301346
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.962140 loss:        0.110722
Test - acc:         0.901200 loss:        0.347147
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.960340 loss:        0.112200
Test - acc:         0.901300 loss:        0.341146
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.963140 loss:        0.105884
Test - acc:         0.899300 loss:        0.360967
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.963220 loss:        0.106825
Test - acc:         0.907000 loss:        0.328419
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.961320 loss:        0.113252
Test - acc:         0.898800 loss:        0.339622
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.961800 loss:        0.110555
Test - acc:         0.901200 loss:        0.343149
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.962980 loss:        0.109861
Test - acc:         0.906300 loss:        0.323929
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.962360 loss:        0.108402
Test - acc:         0.897000 loss:        0.360561
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.959720 loss:        0.116193
Test - acc:         0.892900 loss:        0.367734
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.962220 loss:        0.110435
Test - acc:         0.896400 loss:        0.356823
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.961300 loss:        0.110860
Test - acc:         0.908500 loss:        0.328184
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.961860 loss:        0.111207
Test - acc:         0.895300 loss:        0.366961
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.962740 loss:        0.108466
Test - acc:         0.898200 loss:        0.362669
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.960680 loss:        0.113072
Test - acc:         0.907900 loss:        0.323793
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.961280 loss:        0.112837
Test - acc:         0.896100 loss:        0.370669
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.961460 loss:        0.112046
Test - acc:         0.894400 loss:        0.364099
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.962600 loss:        0.108134
Test - acc:         0.910400 loss:        0.315556
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.961240 loss:        0.113291
Test - acc:         0.903300 loss:        0.338058
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.962680 loss:        0.109568
Test - acc:         0.894500 loss:        0.375407
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.962080 loss:        0.108182
Test - acc:         0.909200 loss:        0.332036
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.961180 loss:        0.111901
Test - acc:         0.904700 loss:        0.339973
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.961660 loss:        0.111118
Test - acc:         0.907200 loss:        0.312172
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.962040 loss:        0.109138
Test - acc:         0.904300 loss:        0.319912
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.963480 loss:        0.106677
Test - acc:         0.900800 loss:        0.349430
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.962440 loss:        0.109985
Test - acc:         0.908300 loss:        0.322485
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.961520 loss:        0.109746
Test - acc:         0.897000 loss:        0.363189
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.961840 loss:        0.108509
Test - acc:         0.899600 loss:        0.360576
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.961700 loss:        0.111352
Test - acc:         0.902400 loss:        0.333666
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.961260 loss:        0.110149
Test - acc:         0.897000 loss:        0.356901
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.962000 loss:        0.110544
Test - acc:         0.900500 loss:        0.351626
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.961300 loss:        0.109624
Test - acc:         0.891300 loss:        0.385318
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.963060 loss:        0.106585
Test - acc:         0.900000 loss:        0.350351
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.962800 loss:        0.106808
Test - acc:         0.899500 loss:        0.352403
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.951540 loss:        0.140964
Test - acc:         0.900800 loss:        0.331708
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.955480 loss:        0.126869
Test - acc:         0.896200 loss:        0.344575
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.957380 loss:        0.124419
Test - acc:         0.904200 loss:        0.326494
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.958260 loss:        0.121677
Test - acc:         0.900200 loss:        0.338997
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.957140 loss:        0.123340
Test - acc:         0.906400 loss:        0.307040
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.957780 loss:        0.120849
Test - acc:         0.903000 loss:        0.331030
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.956460 loss:        0.126296
Test - acc:         0.897000 loss:        0.351945
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.959440 loss:        0.117327
Test - acc:         0.898600 loss:        0.339944
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.957320 loss:        0.123384
Test - acc:         0.891500 loss:        0.365806
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.959160 loss:        0.118888
Test - acc:         0.908600 loss:        0.336759
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.958600 loss:        0.120393
Test - acc:         0.908400 loss:        0.325158
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.960320 loss:        0.115400
Test - acc:         0.899800 loss:        0.360655
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.959500 loss:        0.114944
Test - acc:         0.902400 loss:        0.353062
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.958280 loss:        0.120673
Test - acc:         0.899200 loss:        0.353211
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.959500 loss:        0.117932
Test - acc:         0.898300 loss:        0.348270
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.959020 loss:        0.115520
Test - acc:         0.901700 loss:        0.350346
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.958600 loss:        0.118184
Test - acc:         0.894600 loss:        0.368051
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.959120 loss:        0.116936
Test - acc:         0.904500 loss:        0.327567
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.958900 loss:        0.118531
Test - acc:         0.901500 loss:        0.343472
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.960360 loss:        0.116656
Test - acc:         0.895500 loss:        0.369067
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.958780 loss:        0.118982
Test - acc:         0.898900 loss:        0.357188
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.958600 loss:        0.118518
Test - acc:         0.905800 loss:        0.322723
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.959860 loss:        0.116009
Test - acc:         0.907000 loss:        0.317224
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.958560 loss:        0.118933
Test - acc:         0.897600 loss:        0.362760
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.959680 loss:        0.116420
Test - acc:         0.887100 loss:        0.405850
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.960880 loss:        0.111906
Test - acc:         0.901700 loss:        0.360247
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.958760 loss:        0.119328
Test - acc:         0.905000 loss:        0.330096
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.959860 loss:        0.115625
Test - acc:         0.908500 loss:        0.323074
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.959760 loss:        0.114098
Test - acc:         0.891900 loss:        0.379153
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.961120 loss:        0.111361
Test - acc:         0.910700 loss:        0.318993
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.961040 loss:        0.112120
Test - acc:         0.896400 loss:        0.367240
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.960400 loss:        0.109834
Test - acc:         0.903200 loss:        0.345645
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.961000 loss:        0.111145
Test - acc:         0.882300 loss:        0.458098
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.960060 loss:        0.114812
Test - acc:         0.899500 loss:        0.359890
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.960960 loss:        0.113106
Test - acc:         0.905600 loss:        0.329253
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.962060 loss:        0.106969
Test - acc:         0.898500 loss:        0.374409
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.960060 loss:        0.116391
Test - acc:         0.901000 loss:        0.351814
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.962040 loss:        0.108343
Test - acc:         0.894000 loss:        0.370789
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.960260 loss:        0.112294
Test - acc:         0.892800 loss:        0.388304
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.960300 loss:        0.113012
Test - acc:         0.900400 loss:        0.342592
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.974880 loss:        0.076050
Test - acc:         0.917900 loss:        0.286355
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.979820 loss:        0.060338
Test - acc:         0.919000 loss:        0.285509
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.982900 loss:        0.051809
Test - acc:         0.919800 loss:        0.290322
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.983780 loss:        0.048513
Test - acc:         0.921300 loss:        0.293598
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.985160 loss:        0.045302
Test - acc:         0.922100 loss:        0.296636
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.985700 loss:        0.043441
Test - acc:         0.919400 loss:        0.300508
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.986360 loss:        0.041357
Test - acc:         0.921700 loss:        0.303422
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.987240 loss:        0.039776
Test - acc:         0.923200 loss:        0.307893
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.988880 loss:        0.034848
Test - acc:         0.919600 loss:        0.314106
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.988340 loss:        0.036248
Test - acc:         0.920700 loss:        0.314459
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.989720 loss:        0.031553
Test - acc:         0.922400 loss:        0.313469
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.990180 loss:        0.031800
Test - acc:         0.921400 loss:        0.317610
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.990060 loss:        0.031074
Test - acc:         0.921200 loss:        0.317715
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.990300 loss:        0.030439
Test - acc:         0.921800 loss:        0.325413
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.990660 loss:        0.028229
Test - acc:         0.922300 loss:        0.322306
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.991340 loss:        0.026665
Test - acc:         0.922000 loss:        0.325141
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.991140 loss:        0.027593
Test - acc:         0.922800 loss:        0.325044
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.991720 loss:        0.026939
Test - acc:         0.920500 loss:        0.334970
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.991700 loss:        0.026427
Test - acc:         0.921000 loss:        0.331715
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.991500 loss:        0.025883
Test - acc:         0.919900 loss:        0.332212
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.992040 loss:        0.025065
Test - acc:         0.921300 loss:        0.336318
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.991600 loss:        0.026268
Test - acc:         0.921300 loss:        0.336697
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.992180 loss:        0.024299
Test - acc:         0.921700 loss:        0.336984
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.992540 loss:        0.023339
Test - acc:         0.922100 loss:        0.336341
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.992620 loss:        0.022458
Test - acc:         0.923500 loss:        0.338467
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.993160 loss:        0.021890
Test - acc:         0.921200 loss:        0.342157
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.992380 loss:        0.024209
Test - acc:         0.921800 loss:        0.341691
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.993120 loss:        0.021872
Test - acc:         0.921400 loss:        0.342085
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.993300 loss:        0.022129
Test - acc:         0.919600 loss:        0.345445
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.993220 loss:        0.021553
Test - acc:         0.921800 loss:        0.347505
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.973220 loss:        0.077649
Test - acc:         0.917300 loss:        0.323586
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.978160 loss:        0.063506
Test - acc:         0.916800 loss:        0.323144
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.978180 loss:        0.063062
Test - acc:         0.918300 loss:        0.322678
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.980040 loss:        0.056712
Test - acc:         0.919900 loss:        0.310336
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.981320 loss:        0.054341
Test - acc:         0.917900 loss:        0.319695
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.981620 loss:        0.053130
Test - acc:         0.917400 loss:        0.315482
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.982880 loss:        0.050927
Test - acc:         0.919100 loss:        0.318320
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.983840 loss:        0.046916
Test - acc:         0.919200 loss:        0.324535
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.985160 loss:        0.044923
Test - acc:         0.918200 loss:        0.327430
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.983960 loss:        0.046296
Test - acc:         0.917300 loss:        0.329655
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.983860 loss:        0.046270
Test - acc:         0.917700 loss:        0.330991
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.985040 loss:        0.043960
Test - acc:         0.917000 loss:        0.339731
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.985920 loss:        0.040670
Test - acc:         0.916400 loss:        0.340698
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.985920 loss:        0.042025
Test - acc:         0.917300 loss:        0.336058
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.985940 loss:        0.041494
Test - acc:         0.917100 loss:        0.339034
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.986340 loss:        0.039146
Test - acc:         0.918600 loss:        0.338522
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.987000 loss:        0.038293
Test - acc:         0.917800 loss:        0.345244
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.986860 loss:        0.038886
Test - acc:         0.919700 loss:        0.335765
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.987780 loss:        0.037035
Test - acc:         0.919400 loss:        0.340414
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.986980 loss:        0.038355
Test - acc:         0.917800 loss:        0.341151
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.988740 loss:        0.034381
Test - acc:         0.916900 loss:        0.341882
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.987960 loss:        0.035183
Test - acc:         0.918000 loss:        0.347132
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.987960 loss:        0.035149
Test - acc:         0.919800 loss:        0.340614
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.988180 loss:        0.035293
Test - acc:         0.918200 loss:        0.345112
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.987220 loss:        0.037574
Test - acc:         0.917100 loss:        0.348241
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.986280 loss:        0.038452
Test - acc:         0.916100 loss:        0.346663
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.988200 loss:        0.035359
Test - acc:         0.918800 loss:        0.350220
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.987820 loss:        0.034392
Test - acc:         0.919500 loss:        0.352977
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.988060 loss:        0.034949
Test - acc:         0.918000 loss:        0.351371
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.988360 loss:        0.033144
Test - acc:         0.919600 loss:        0.347687
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.989480 loss:        0.030966
Test - acc:         0.918100 loss:        0.347117
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.989680 loss:        0.030521
Test - acc:         0.919200 loss:        0.349888
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.988740 loss:        0.031980
Test - acc:         0.918100 loss:        0.358360
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.989820 loss:        0.031952
Test - acc:         0.918900 loss:        0.355758
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.989960 loss:        0.030589
Test - acc:         0.920500 loss:        0.357732
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.990240 loss:        0.029714
Test - acc:         0.919200 loss:        0.356098
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.989680 loss:        0.031372
Test - acc:         0.917800 loss:        0.359566
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.990100 loss:        0.030189
Test - acc:         0.918200 loss:        0.365747
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.989220 loss:        0.031917
Test - acc:         0.919800 loss:        0.359185
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.990760 loss:        0.028372
Test - acc:         0.916600 loss:        0.364234
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.990180 loss:        0.030476
Test - acc:         0.918400 loss:        0.357613
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.990100 loss:        0.029422
Test - acc:         0.918100 loss:        0.369596
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.989920 loss:        0.030074
Test - acc:         0.918600 loss:        0.362164
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.990940 loss:        0.027223
Test - acc:         0.920700 loss:        0.355186
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.990820 loss:        0.027090
Test - acc:         0.918900 loss:        0.365844
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.990940 loss:        0.027028
Test - acc:         0.918600 loss:        0.366665
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.990260 loss:        0.027977
Test - acc:         0.919800 loss:        0.361352
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.990540 loss:        0.028029
Test - acc:         0.918600 loss:        0.368447
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.990880 loss:        0.028439
Test - acc:         0.920400 loss:        0.360285
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.990620 loss:        0.028450
Test - acc:         0.917900 loss:        0.362318
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.991280 loss:        0.026341
Test - acc:         0.917200 loss:        0.362348
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.990840 loss:        0.027330
Test - acc:         0.919400 loss:        0.364357
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.990940 loss:        0.027216
Test - acc:         0.916900 loss:        0.374043
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.991020 loss:        0.026684
Test - acc:         0.919200 loss:        0.372689
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.990440 loss:        0.028169
Test - acc:         0.919400 loss:        0.371923
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.991000 loss:        0.026312
Test - acc:         0.920100 loss:        0.369589
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.990620 loss:        0.027345
Test - acc:         0.920700 loss:        0.371694
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.992160 loss:        0.023943
Test - acc:         0.921400 loss:        0.368427
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.991700 loss:        0.025068
Test - acc:         0.921000 loss:        0.367874
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.992120 loss:        0.024217
Test - acc:         0.919200 loss:        0.376365
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.991240 loss:        0.026078
Test - acc:         0.919200 loss:        0.369361
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.990780 loss:        0.028020
Test - acc:         0.920400 loss:        0.362029
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.991960 loss:        0.023521
Test - acc:         0.921400 loss:        0.357481
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.991920 loss:        0.023722
Test - acc:         0.920500 loss:        0.367799
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.992180 loss:        0.024224
Test - acc:         0.921000 loss:        0.368528
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.992020 loss:        0.023509
Test - acc:         0.917800 loss:        0.368250
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.991860 loss:        0.025151
Test - acc:         0.920400 loss:        0.368568
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.992220 loss:        0.024055
Test - acc:         0.920500 loss:        0.369806
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.992620 loss:        0.022223
Test - acc:         0.917100 loss:        0.379615
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.992420 loss:        0.023653
Test - acc:         0.919500 loss:        0.377390
Sparsity :          0.9375
Wdecay :        0.000500
