Running --prune_criterion snip --seed 42 --snip_sparsity 0.9375 --comment=vgg19_crit=snip_sparsity=0.9375_seed=42 --save_model=pre-finetune/vgg19_snip_sp0.9375_s42
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "snip",
    "prune_freq": 1,
    "prune_rate": 0.2,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.9375,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_snip_sp0.9375_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.246540 loss:        2.166157
Test - acc:         0.332400 loss:        1.746605
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.399400 loss:        1.600166
Test - acc:         0.450100 loss:        1.480101
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.529180 loss:        1.296814
Test - acc:         0.554700 loss:        1.254790
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.632560 loss:        1.043169
Test - acc:         0.672300 loss:        0.951994
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.703040 loss:        0.858834
Test - acc:         0.670100 loss:        1.054204
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.744160 loss:        0.748349
Test - acc:         0.655400 loss:        1.018838
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.771120 loss:        0.678943
Test - acc:         0.725800 loss:        0.792300
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.783340 loss:        0.638615
Test - acc:         0.737800 loss:        0.777075
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.794620 loss:        0.608238
Test - acc:         0.769800 loss:        0.688242
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.803600 loss:        0.580223
Test - acc:         0.746400 loss:        0.754585
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.814180 loss:        0.551591
Test - acc:         0.734800 loss:        0.776458
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.817540 loss:        0.542730
Test - acc:         0.776800 loss:        0.635772
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.823200 loss:        0.527836
Test - acc:         0.765100 loss:        0.740023
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.826720 loss:        0.514254
Test - acc:         0.786500 loss:        0.636766
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.830580 loss:        0.503124
Test - acc:         0.809100 loss:        0.585313
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.833160 loss:        0.493074
Test - acc:         0.802000 loss:        0.590038
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.835220 loss:        0.485191
Test - acc:         0.821500 loss:        0.541767
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.839120 loss:        0.474205
Test - acc:         0.796100 loss:        0.631818
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.840580 loss:        0.471374
Test - acc:         0.739000 loss:        0.803040
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.843420 loss:        0.468592
Test - acc:         0.785200 loss:        0.664229
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.844400 loss:        0.454551
Test - acc:         0.811000 loss:        0.579459
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.845300 loss:        0.458296
Test - acc:         0.807200 loss:        0.576018
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.847000 loss:        0.453643
Test - acc:         0.772000 loss:        0.725385
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.850580 loss:        0.445420
Test - acc:         0.810000 loss:        0.566362
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.851400 loss:        0.442685
Test - acc:         0.800600 loss:        0.610842
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.849260 loss:        0.443960
Test - acc:         0.771100 loss:        0.691621
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.851780 loss:        0.436791
Test - acc:         0.796500 loss:        0.638494
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.853280 loss:        0.436390
Test - acc:         0.829000 loss:        0.512859
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.852420 loss:        0.431998
Test - acc:         0.801200 loss:        0.599708
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.856440 loss:        0.428252
Test - acc:         0.712600 loss:        0.977667
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.857880 loss:        0.422186
Test - acc:         0.647400 loss:        1.405024
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.853980 loss:        0.428253
Test - acc:         0.770800 loss:        0.730774
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.857960 loss:        0.416196
Test - acc:         0.786300 loss:        0.677605
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.857080 loss:        0.421448
Test - acc:         0.758400 loss:        0.757859
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.856760 loss:        0.424867
Test - acc:         0.820200 loss:        0.533058
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.858580 loss:        0.416303
Test - acc:         0.811400 loss:        0.585421
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.859660 loss:        0.416942
Test - acc:         0.745700 loss:        0.832609
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.855520 loss:        0.429442
Test - acc:         0.754900 loss:        0.731851
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.859040 loss:        0.418152
Test - acc:         0.801900 loss:        0.620171
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.862020 loss:        0.410452
Test - acc:         0.758100 loss:        0.791974
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.863740 loss:        0.405222
Test - acc:         0.799700 loss:        0.602800
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.858580 loss:        0.415319
Test - acc:         0.791400 loss:        0.662156
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.862400 loss:        0.407853
Test - acc:         0.824300 loss:        0.532266
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.863280 loss:        0.405976
Test - acc:         0.803600 loss:        0.591691
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.861100 loss:        0.409405
Test - acc:         0.781000 loss:        0.694952
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.862820 loss:        0.408342
Test - acc:         0.827700 loss:        0.531600
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.863740 loss:        0.403180
Test - acc:         0.766500 loss:        0.749930
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.864760 loss:        0.399971
Test - acc:         0.793500 loss:        0.611399
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.863420 loss:        0.406621
Test - acc:         0.790800 loss:        0.672148
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.862860 loss:        0.401250
Test - acc:         0.835100 loss:        0.487032
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.862700 loss:        0.405683
Test - acc:         0.820000 loss:        0.532310
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.863480 loss:        0.405293
Test - acc:         0.748800 loss:        0.850785
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.865760 loss:        0.398503
Test - acc:         0.768300 loss:        0.720684
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.864060 loss:        0.398784
Test - acc:         0.822800 loss:        0.552787
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.867080 loss:        0.395300
Test - acc:         0.821000 loss:        0.545623
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.865380 loss:        0.397627
Test - acc:         0.766700 loss:        0.759558
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.866360 loss:        0.394365
Test - acc:         0.803300 loss:        0.607283
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.866920 loss:        0.396869
Test - acc:         0.812000 loss:        0.587131
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.866700 loss:        0.392134
Test - acc:         0.807100 loss:        0.593429
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.864960 loss:        0.396144
Test - acc:         0.841300 loss:        0.484585
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.868760 loss:        0.396722
Test - acc:         0.819800 loss:        0.580951
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.867240 loss:        0.391788
Test - acc:         0.782000 loss:        0.666431
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.869360 loss:        0.390041
Test - acc:         0.807600 loss:        0.594897
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.866340 loss:        0.395464
Test - acc:         0.787700 loss:        0.631083
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.864880 loss:        0.397589
Test - acc:         0.821100 loss:        0.539260
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.869040 loss:        0.388458
Test - acc:         0.825100 loss:        0.539197
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.867800 loss:        0.393086
Test - acc:         0.834000 loss:        0.496809
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.866600 loss:        0.395354
Test - acc:         0.839700 loss:        0.482427
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.867400 loss:        0.394331
Test - acc:         0.782100 loss:        0.664421
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.869220 loss:        0.389687
Test - acc:         0.827600 loss:        0.525815
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.868380 loss:        0.390386
Test - acc:         0.776000 loss:        0.759821
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.868960 loss:        0.389214
Test - acc:         0.799400 loss:        0.597715
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.869380 loss:        0.389246
Test - acc:         0.824700 loss:        0.534455
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.867300 loss:        0.391925
Test - acc:         0.825000 loss:        0.520873
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.867360 loss:        0.392227
Test - acc:         0.829500 loss:        0.510987
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.870540 loss:        0.383831
Test - acc:         0.839600 loss:        0.476886
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.867720 loss:        0.392015
Test - acc:         0.822000 loss:        0.550003
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.870100 loss:        0.389335
Test - acc:         0.794600 loss:        0.624989
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.864740 loss:        0.399064
Test - acc:         0.785000 loss:        0.689508
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.867800 loss:        0.390337
Test - acc:         0.839700 loss:        0.486194
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.865800 loss:        0.397386
Test - acc:         0.812700 loss:        0.592705
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.866900 loss:        0.393652
Test - acc:         0.799900 loss:        0.590803
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.870440 loss:        0.385351
Test - acc:         0.818400 loss:        0.546185
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.869200 loss:        0.391313
Test - acc:         0.819200 loss:        0.554134
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.871240 loss:        0.386851
Test - acc:         0.810500 loss:        0.600700
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.870560 loss:        0.384987
Test - acc:         0.758100 loss:        0.755406
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.865880 loss:        0.394261
Test - acc:         0.781500 loss:        0.671581
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.868600 loss:        0.388357
Test - acc:         0.782700 loss:        0.669195
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.868680 loss:        0.386824
Test - acc:         0.819700 loss:        0.542262
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.868260 loss:        0.389365
Test - acc:         0.841100 loss:        0.466008
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.868360 loss:        0.387151
Test - acc:         0.771200 loss:        0.734042
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.866980 loss:        0.393323
Test - acc:         0.766300 loss:        0.719822
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.866980 loss:        0.395007
Test - acc:         0.817600 loss:        0.567948
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.870900 loss:        0.382461
Test - acc:         0.773400 loss:        0.694468
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.869200 loss:        0.388023
Test - acc:         0.799900 loss:        0.633568
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.871140 loss:        0.383584
Test - acc:         0.792000 loss:        0.626594
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.872120 loss:        0.382024
Test - acc:         0.848400 loss:        0.469200
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.869640 loss:        0.385541
Test - acc:         0.825600 loss:        0.520252
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.390814
Test - acc:         0.814900 loss:        0.568101
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.869440 loss:        0.387558
Test - acc:         0.738000 loss:        0.856355
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.872960 loss:        0.378142
Test - acc:         0.805800 loss:        0.591823
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.871240 loss:        0.379668
Test - acc:         0.766100 loss:        0.777005
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.869520 loss:        0.386348
Test - acc:         0.813000 loss:        0.583126
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.869780 loss:        0.386555
Test - acc:         0.829200 loss:        0.520771
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.868560 loss:        0.388603
Test - acc:         0.807800 loss:        0.592406
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.870540 loss:        0.381878
Test - acc:         0.842100 loss:        0.479948
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.868660 loss:        0.390252
Test - acc:         0.826600 loss:        0.537363
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.871400 loss:        0.383231
Test - acc:         0.795900 loss:        0.634727
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.872160 loss:        0.379681
Test - acc:         0.799000 loss:        0.620458
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.868640 loss:        0.386582
Test - acc:         0.785900 loss:        0.659652
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.870200 loss:        0.383739
Test - acc:         0.812600 loss:        0.589284
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.867660 loss:        0.388508
Test - acc:         0.799700 loss:        0.601599
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.871280 loss:        0.386331
Test - acc:         0.841100 loss:        0.483160
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.869580 loss:        0.384765
Test - acc:         0.808800 loss:        0.570650
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.868440 loss:        0.388812
Test - acc:         0.830500 loss:        0.517603
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.869200 loss:        0.385774
Test - acc:         0.802300 loss:        0.589597
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.873580 loss:        0.376052
Test - acc:         0.829800 loss:        0.545456
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.870380 loss:        0.385746
Test - acc:         0.826700 loss:        0.513117
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.871240 loss:        0.381392
Test - acc:         0.789800 loss:        0.686683
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.872640 loss:        0.379002
Test - acc:         0.822000 loss:        0.559300
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.867240 loss:        0.389583
Test - acc:         0.820400 loss:        0.556357
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.869640 loss:        0.382625
Test - acc:         0.812700 loss:        0.570209
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.870680 loss:        0.383360
Test - acc:         0.779600 loss:        0.700186
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.870900 loss:        0.383687
Test - acc:         0.822400 loss:        0.558479
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.871660 loss:        0.378004
Test - acc:         0.808900 loss:        0.582713
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.872040 loss:        0.378693
Test - acc:         0.849900 loss:        0.455808
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.870740 loss:        0.380572
Test - acc:         0.793600 loss:        0.588225
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.872060 loss:        0.382922
Test - acc:         0.781500 loss:        0.685855
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.873000 loss:        0.381011
Test - acc:         0.795900 loss:        0.638980
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.871960 loss:        0.376880
Test - acc:         0.835500 loss:        0.487386
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.871760 loss:        0.382134
Test - acc:         0.781300 loss:        0.696081
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.871840 loss:        0.381533
Test - acc:         0.809200 loss:        0.572477
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.872840 loss:        0.377714
Test - acc:         0.829600 loss:        0.533913
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.867740 loss:        0.389968
Test - acc:         0.818500 loss:        0.597705
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.871440 loss:        0.379087
Test - acc:         0.751300 loss:        0.786661
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.868500 loss:        0.387394
Test - acc:         0.814100 loss:        0.580165
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.872060 loss:        0.377182
Test - acc:         0.837500 loss:        0.500334
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.869680 loss:        0.382918
Test - acc:         0.711500 loss:        0.950723
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.873040 loss:        0.376138
Test - acc:         0.843000 loss:        0.473008
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.872600 loss:        0.379412
Test - acc:         0.826900 loss:        0.516259
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.875840 loss:        0.369932
Test - acc:         0.833000 loss:        0.494059
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.870900 loss:        0.384779
Test - acc:         0.823800 loss:        0.525988
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.871800 loss:        0.379295
Test - acc:         0.836500 loss:        0.514344
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.872340 loss:        0.377016
Test - acc:         0.791700 loss:        0.650043
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.872360 loss:        0.377495
Test - acc:         0.802000 loss:        0.606839
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.872460 loss:        0.382578
Test - acc:         0.802200 loss:        0.626837
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.871980 loss:        0.380338
Test - acc:         0.833800 loss:        0.502779
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.871300 loss:        0.382723
Test - acc:         0.821700 loss:        0.550279
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.875340 loss:        0.373915
Test - acc:         0.765500 loss:        0.731806
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.873740 loss:        0.375385
Test - acc:         0.786500 loss:        0.704007
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.928200 loss:        0.217511
Test - acc:         0.915900 loss:        0.252318
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.945480 loss:        0.161246
Test - acc:         0.920200 loss:        0.246834
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.951540 loss:        0.142531
Test - acc:         0.923200 loss:        0.239106
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.956880 loss:        0.129570
Test - acc:         0.925000 loss:        0.239785
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.960920 loss:        0.115815
Test - acc:         0.924000 loss:        0.240968
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.963860 loss:        0.106865
Test - acc:         0.921600 loss:        0.249360
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.965980 loss:        0.099096
Test - acc:         0.922400 loss:        0.254273
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.968380 loss:        0.092794
Test - acc:         0.923800 loss:        0.247673
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.969620 loss:        0.088588
Test - acc:         0.924300 loss:        0.250711
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.973280 loss:        0.077737
Test - acc:         0.924200 loss:        0.268810
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.974620 loss:        0.074673
Test - acc:         0.922600 loss:        0.268202
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.974880 loss:        0.073472
Test - acc:         0.920900 loss:        0.274324
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.975640 loss:        0.069357
Test - acc:         0.921200 loss:        0.276413
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.976260 loss:        0.069647
Test - acc:         0.922300 loss:        0.279214
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.976920 loss:        0.065066
Test - acc:         0.920200 loss:        0.287301
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.977500 loss:        0.064416
Test - acc:         0.920200 loss:        0.288302
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.977980 loss:        0.062986
Test - acc:         0.916100 loss:        0.304131
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.978000 loss:        0.063447
Test - acc:         0.917400 loss:        0.299536
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.978920 loss:        0.061440
Test - acc:         0.916300 loss:        0.300517
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.979140 loss:        0.062975
Test - acc:         0.917900 loss:        0.295659
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.978300 loss:        0.063132
Test - acc:         0.914400 loss:        0.323840
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.978140 loss:        0.062208
Test - acc:         0.918300 loss:        0.297420
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.979580 loss:        0.059626
Test - acc:         0.914200 loss:        0.307863
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.979860 loss:        0.060528
Test - acc:         0.906900 loss:        0.342373
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.978360 loss:        0.060934
Test - acc:         0.915200 loss:        0.310551
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.977160 loss:        0.064291
Test - acc:         0.918900 loss:        0.300447
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.976920 loss:        0.064275
Test - acc:         0.914400 loss:        0.305075
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.977200 loss:        0.065223
Test - acc:         0.912000 loss:        0.316256
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.976940 loss:        0.067917
Test - acc:         0.906600 loss:        0.327299
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.977220 loss:        0.065043
Test - acc:         0.912900 loss:        0.316830
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.976980 loss:        0.065756
Test - acc:         0.916000 loss:        0.291840
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.976980 loss:        0.066664
Test - acc:         0.904700 loss:        0.326995
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.976840 loss:        0.068562
Test - acc:         0.904900 loss:        0.331617
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.977080 loss:        0.068592
Test - acc:         0.914500 loss:        0.316794
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.976720 loss:        0.065671
Test - acc:         0.904600 loss:        0.352150
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.976840 loss:        0.067940
Test - acc:         0.910500 loss:        0.322704
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.975260 loss:        0.070426
Test - acc:         0.910800 loss:        0.309345
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.975900 loss:        0.069278
Test - acc:         0.903800 loss:        0.350021
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.974720 loss:        0.072651
Test - acc:         0.912300 loss:        0.315138
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.974020 loss:        0.073864
Test - acc:         0.912400 loss:        0.309461
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.976320 loss:        0.069602
Test - acc:         0.903100 loss:        0.335949
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.974920 loss:        0.071995
Test - acc:         0.910600 loss:        0.325856
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.976020 loss:        0.070091
Test - acc:         0.906300 loss:        0.334849
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.974740 loss:        0.073123
Test - acc:         0.910000 loss:        0.321077
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.974400 loss:        0.074281
Test - acc:         0.915000 loss:        0.302594
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.976300 loss:        0.070478
Test - acc:         0.906400 loss:        0.343070
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.975780 loss:        0.071531
Test - acc:         0.895000 loss:        0.392274
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.975680 loss:        0.070925
Test - acc:         0.913400 loss:        0.321707
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.972860 loss:        0.076291
Test - acc:         0.905400 loss:        0.331995
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.976900 loss:        0.066467
Test - acc:         0.891900 loss:        0.409475
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.976100 loss:        0.069035
Test - acc:         0.906500 loss:        0.335630
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.974800 loss:        0.073236
Test - acc:         0.900000 loss:        0.370526
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.974500 loss:        0.071679
Test - acc:         0.905700 loss:        0.333678
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.974840 loss:        0.072374
Test - acc:         0.899600 loss:        0.361538
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.975380 loss:        0.068449
Test - acc:         0.909500 loss:        0.336158
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.973140 loss:        0.078474
Test - acc:         0.900300 loss:        0.355163
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.975780 loss:        0.071635
Test - acc:         0.905200 loss:        0.338443
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.975980 loss:        0.069193
Test - acc:         0.901100 loss:        0.347771
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.973360 loss:        0.077154
Test - acc:         0.908500 loss:        0.326427
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.978220 loss:        0.064720
Test - acc:         0.908300 loss:        0.316176
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.975740 loss:        0.071068
Test - acc:         0.907700 loss:        0.330307
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.974680 loss:        0.074527
Test - acc:         0.908500 loss:        0.319918
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.974560 loss:        0.075314
Test - acc:         0.903500 loss:        0.337849
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.975300 loss:        0.071308
Test - acc:         0.909600 loss:        0.316750
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.975640 loss:        0.072914
Test - acc:         0.909700 loss:        0.332120
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.974720 loss:        0.073417
Test - acc:         0.896700 loss:        0.366889
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.975680 loss:        0.071253
Test - acc:         0.908500 loss:        0.323718
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.977080 loss:        0.066666
Test - acc:         0.899400 loss:        0.355852
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.976800 loss:        0.067226
Test - acc:         0.901300 loss:        0.362988
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.976240 loss:        0.070779
Test - acc:         0.912700 loss:        0.308141
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.975260 loss:        0.071600
Test - acc:         0.895600 loss:        0.393958
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.974380 loss:        0.073474
Test - acc:         0.899900 loss:        0.359448
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.974440 loss:        0.073823
Test - acc:         0.902100 loss:        0.353174
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.977300 loss:        0.066750
Test - acc:         0.902300 loss:        0.347206
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.974840 loss:        0.070671
Test - acc:         0.903100 loss:        0.358725
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.974780 loss:        0.074492
Test - acc:         0.895100 loss:        0.370586
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.976160 loss:        0.070199
Test - acc:         0.904800 loss:        0.349563
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.974440 loss:        0.072284
Test - acc:         0.899100 loss:        0.355482
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.976320 loss:        0.068644
Test - acc:         0.901300 loss:        0.360291
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.976340 loss:        0.070273
Test - acc:         0.900200 loss:        0.363703
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.975360 loss:        0.072445
Test - acc:         0.906800 loss:        0.345853
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.975680 loss:        0.071865
Test - acc:         0.906100 loss:        0.335757
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.975720 loss:        0.071318
Test - acc:         0.899900 loss:        0.359475
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.975320 loss:        0.070690
Test - acc:         0.891500 loss:        0.396050
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.976320 loss:        0.070589
Test - acc:         0.876600 loss:        0.490411
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.976200 loss:        0.069226
Test - acc:         0.906600 loss:        0.335586
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.975620 loss:        0.071801
Test - acc:         0.893600 loss:        0.369077
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.976960 loss:        0.068678
Test - acc:         0.908700 loss:        0.328532
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.975420 loss:        0.072009
Test - acc:         0.889700 loss:        0.393038
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.974300 loss:        0.074834
Test - acc:         0.903700 loss:        0.343267
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.976740 loss:        0.068651
Test - acc:         0.908900 loss:        0.324970
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.975600 loss:        0.070184
Test - acc:         0.913600 loss:        0.311212
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.978640 loss:        0.063904
Test - acc:         0.904100 loss:        0.353305
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.975680 loss:        0.071114
Test - acc:         0.906600 loss:        0.341535
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.977460 loss:        0.067437
Test - acc:         0.907800 loss:        0.337394
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.976100 loss:        0.069518
Test - acc:         0.906300 loss:        0.336590
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.975260 loss:        0.072663
Test - acc:         0.909200 loss:        0.324281
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.976680 loss:        0.067294
Test - acc:         0.904300 loss:        0.347477
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.977300 loss:        0.064909
Test - acc:         0.912100 loss:        0.321320
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.976480 loss:        0.068406
Test - acc:         0.903100 loss:        0.348750
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.989300 loss:        0.035015
Test - acc:         0.926900 loss:        0.263746
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.993920 loss:        0.021643
Test - acc:         0.929200 loss:        0.262159
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.995800 loss:        0.016284
Test - acc:         0.929500 loss:        0.259710
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.996060 loss:        0.014655
Test - acc:         0.932000 loss:        0.264841
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.996440 loss:        0.013184
Test - acc:         0.932000 loss:        0.263714
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.996800 loss:        0.011882
Test - acc:         0.932300 loss:        0.267684
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.997040 loss:        0.010554
Test - acc:         0.932200 loss:        0.268003
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.997620 loss:        0.009448
Test - acc:         0.931100 loss:        0.268935
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.997960 loss:        0.008532
Test - acc:         0.933700 loss:        0.268652
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.998000 loss:        0.007793
Test - acc:         0.933900 loss:        0.270822
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.998200 loss:        0.007626
Test - acc:         0.935500 loss:        0.272097
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.998420 loss:        0.007099
Test - acc:         0.934400 loss:        0.271312
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.998140 loss:        0.007442
Test - acc:         0.933300 loss:        0.276354
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.998580 loss:        0.006386
Test - acc:         0.934800 loss:        0.275669
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.998560 loss:        0.005685
Test - acc:         0.935000 loss:        0.278692
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.998760 loss:        0.005901
Test - acc:         0.935700 loss:        0.275414
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.998820 loss:        0.005531
Test - acc:         0.934500 loss:        0.275792
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.998600 loss:        0.006023
Test - acc:         0.934600 loss:        0.279693
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999040 loss:        0.004761
Test - acc:         0.936200 loss:        0.278008
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.004926
Test - acc:         0.935300 loss:        0.279598
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.004580
Test - acc:         0.936600 loss:        0.280392
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.998980 loss:        0.004933
Test - acc:         0.935200 loss:        0.282118
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.998820 loss:        0.004899
Test - acc:         0.935400 loss:        0.282285
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.998900 loss:        0.004894
Test - acc:         0.935600 loss:        0.280421
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.004274
Test - acc:         0.934800 loss:        0.282862
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.004490
Test - acc:         0.936900 loss:        0.283287
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999160 loss:        0.003868
Test - acc:         0.937400 loss:        0.281805
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999080 loss:        0.004147
Test - acc:         0.936200 loss:        0.284744
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.998940 loss:        0.004460
Test - acc:         0.935300 loss:        0.284203
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999260 loss:        0.003998
Test - acc:         0.934300 loss:        0.283678
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.003690
Test - acc:         0.934800 loss:        0.283428
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.003341
Test - acc:         0.936100 loss:        0.283533
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.003287
Test - acc:         0.935300 loss:        0.285264
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.003455
Test - acc:         0.935000 loss:        0.283224
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.003573
Test - acc:         0.935100 loss:        0.287721
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.003375
Test - acc:         0.935400 loss:        0.287549
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.003218
Test - acc:         0.936600 loss:        0.285453
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.002729
Test - acc:         0.936400 loss:        0.286012
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.002945
Test - acc:         0.936300 loss:        0.287809
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.003038
Test - acc:         0.936700 loss:        0.286983
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.003200
Test - acc:         0.934900 loss:        0.285229
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.002868
Test - acc:         0.934900 loss:        0.285742
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.003076
Test - acc:         0.934800 loss:        0.288852
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.002484
Test - acc:         0.935600 loss:        0.286267
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.003063
Test - acc:         0.935900 loss:        0.287284
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.002754
Test - acc:         0.935300 loss:        0.286535
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.002736
Test - acc:         0.935400 loss:        0.288403
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.002361
Test - acc:         0.936600 loss:        0.292374
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.002403
Test - acc:         0.935300 loss:        0.291503
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.002693
Test - acc:         0.935700 loss:        0.292193
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.002491
Test - acc:         0.935900 loss:        0.291851
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.002865
Test - acc:         0.935200 loss:        0.292175
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.002404
Test - acc:         0.936400 loss:        0.287866
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.002171
Test - acc:         0.934800 loss:        0.289067
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.002519
Test - acc:         0.936700 loss:        0.291028
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.002491
Test - acc:         0.936000 loss:        0.288992
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.002058
Test - acc:         0.936100 loss:        0.290273
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.002239
Test - acc:         0.935100 loss:        0.293472
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.002218
Test - acc:         0.935900 loss:        0.292149
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.002290
Test - acc:         0.936400 loss:        0.293060
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.002244
Test - acc:         0.937400 loss:        0.292130
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.002294
Test - acc:         0.936500 loss:        0.295551
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.002151
Test - acc:         0.934700 loss:        0.291154
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.002172
Test - acc:         0.936300 loss:        0.291001
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.002013
Test - acc:         0.936600 loss:        0.292211
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.002032
Test - acc:         0.935700 loss:        0.287645
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002047
Test - acc:         0.935700 loss:        0.290853
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.002062
Test - acc:         0.935300 loss:        0.292225
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.002059
Test - acc:         0.935300 loss:        0.293311
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.002002
Test - acc:         0.934700 loss:        0.294162
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.002184
Test - acc:         0.935700 loss:        0.291720
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.002216
Test - acc:         0.936400 loss:        0.290167
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.002171
Test - acc:         0.936100 loss:        0.290429
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.001964
Test - acc:         0.936500 loss:        0.287651
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.002292
Test - acc:         0.936800 loss:        0.287326
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.001843
Test - acc:         0.936900 loss:        0.287210
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.001860
Test - acc:         0.936200 loss:        0.287836
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.001864
Test - acc:         0.936400 loss:        0.287761
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001784
Test - acc:         0.936600 loss:        0.289498
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.001928
Test - acc:         0.934700 loss:        0.288204
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.001809
Test - acc:         0.936500 loss:        0.292710
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.001832
Test - acc:         0.936300 loss:        0.290556
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001702
Test - acc:         0.936600 loss:        0.288573
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.001928
Test - acc:         0.935000 loss:        0.290154
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.001958
Test - acc:         0.936600 loss:        0.288151
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.001851
Test - acc:         0.936600 loss:        0.289984
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001663
Test - acc:         0.935700 loss:        0.290495
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001828
Test - acc:         0.936500 loss:        0.289876
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001686
Test - acc:         0.936100 loss:        0.289431
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.001843
Test - acc:         0.935700 loss:        0.291392
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.001583
Test - acc:         0.935700 loss:        0.289651
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.001568
Test - acc:         0.936400 loss:        0.289241
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.001728
Test - acc:         0.936400 loss:        0.288298
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001783
Test - acc:         0.934400 loss:        0.289856
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001695
Test - acc:         0.937600 loss:        0.291270
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.001783
Test - acc:         0.936500 loss:        0.289303
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001578
Test - acc:         0.936900 loss:        0.289320
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.001804
Test - acc:         0.935800 loss:        0.296049
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.001683
Test - acc:         0.935700 loss:        0.291680
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.001827
Test - acc:         0.934100 loss:        0.289770
Sparsity :          0.9375
Wdecay :        0.000500
