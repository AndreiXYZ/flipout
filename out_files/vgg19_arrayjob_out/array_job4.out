Running --prune_criterion topflip --seed 42 --prune_freq 117 --prune_rate 0.5 --noise --comment=vgg19_crit=topflip_pf=117_seed=42 --save_model=pre-finetune/vgg19_topflip_pf117_s42
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "topflip",
    "prune_freq": 117,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_topflip_pf117_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.112220 loss:        2.707889
Test - acc:         0.115500 loss:        2.285625
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.171100 loss:        2.119530
Test - acc:         0.247800 loss:        1.889029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.260180 loss:        1.845003
Test - acc:         0.293800 loss:        1.793629
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.351700 loss:        1.666860
Test - acc:         0.365700 loss:        1.606198
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.442140 loss:        1.475611
Test - acc:         0.498800 loss:        1.346169
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.541180 loss:        1.260178
Test - acc:         0.453900 loss:        1.738315
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.619740 loss:        1.083854
Test - acc:         0.532000 loss:        1.515536
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.675980 loss:        0.929632
Test - acc:         0.621200 loss:        1.227844
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.710540 loss:        0.855181
Test - acc:         0.638900 loss:        1.146975
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.730440 loss:        0.803610
Test - acc:         0.680300 loss:        1.070751
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.752360 loss:        0.752724
Test - acc:         0.695200 loss:        0.963042
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.761480 loss:        0.725837
Test - acc:         0.731800 loss:        0.838521
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.771840 loss:        0.701123
Test - acc:         0.694700 loss:        0.991394
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.778780 loss:        0.676362
Test - acc:         0.667800 loss:        1.115059
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.781940 loss:        0.664950
Test - acc:         0.744500 loss:        0.805362
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.790840 loss:        0.641081
Test - acc:         0.779400 loss:        0.675063
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.795240 loss:        0.630031
Test - acc:         0.697300 loss:        0.917811
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.800100 loss:        0.615935
Test - acc:         0.719800 loss:        0.926714
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.800380 loss:        0.612391
Test - acc:         0.752900 loss:        0.813506
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.802400 loss:        0.604146
Test - acc:         0.770800 loss:        0.743486
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.807380 loss:        0.594119
Test - acc:         0.665200 loss:        1.083300
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.810160 loss:        0.585650
Test - acc:         0.778600 loss:        0.685493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.809940 loss:        0.585200
Test - acc:         0.780900 loss:        0.664840
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.811000 loss:        0.585315
Test - acc:         0.780700 loss:        0.692555
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.812760 loss:        0.576658
Test - acc:         0.723800 loss:        0.945873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.819780 loss:        0.555273
Test - acc:         0.759000 loss:        0.754278
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.816960 loss:        0.567006
Test - acc:         0.759200 loss:        0.769223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.818100 loss:        0.561920
Test - acc:         0.784400 loss:        0.689784
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.818680 loss:        0.560318
Test - acc:         0.781500 loss:        0.680860
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.818160 loss:        0.564143
Test - acc:         0.704200 loss:        0.952644
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.824040 loss:        0.548989
Test - acc:         0.768300 loss:        0.734637
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.822460 loss:        0.552491
Test - acc:         0.795300 loss:        0.651316
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.822260 loss:        0.549665
Test - acc:         0.776000 loss:        0.726616
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.824040 loss:        0.545305
Test - acc:         0.696300 loss:        1.066222
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.828700 loss:        0.532606
Test - acc:         0.763700 loss:        0.736834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.825980 loss:        0.536146
Test - acc:         0.797400 loss:        0.639977
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.825080 loss:        0.541546
Test - acc:         0.783400 loss:        0.693384
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.827300 loss:        0.528451
Test - acc:         0.736300 loss:        0.862552
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.824660 loss:        0.537154
Test - acc:         0.745800 loss:        0.838598
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.830840 loss:        0.527442
Test - acc:         0.776500 loss:        0.700459
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.827020 loss:        0.530133
Test - acc:         0.761700 loss:        0.742538
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.827200 loss:        0.537913
Test - acc:         0.782600 loss:        0.694255
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.831900 loss:        0.523550
Test - acc:         0.741600 loss:        0.814439
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.834320 loss:        0.514167
Test - acc:         0.719800 loss:        0.969886
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.830520 loss:        0.522136
Test - acc:         0.717100 loss:        1.180856
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.833200 loss:        0.516630
Test - acc:         0.786700 loss:        0.678516
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.830840 loss:        0.519828
Test - acc:         0.772900 loss:        0.756524
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.831600 loss:        0.518631
Test - acc:         0.815600 loss:        0.572250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.835020 loss:        0.510291
Test - acc:         0.822800 loss:        0.550194
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.833200 loss:        0.518434
Test - acc:         0.786100 loss:        0.652066
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.836580 loss:        0.505090
Test - acc:         0.735300 loss:        0.861792
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.832680 loss:        0.512519
Test - acc:         0.788800 loss:        0.647510
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.836100 loss:        0.501027
Test - acc:         0.728100 loss:        0.885563
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.834200 loss:        0.505712
Test - acc:         0.745700 loss:        0.868215
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.834340 loss:        0.505220
Test - acc:         0.784600 loss:        0.665123
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.839360 loss:        0.494393
Test - acc:         0.812100 loss:        0.580466
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.835600 loss:        0.502950
Test - acc:         0.764100 loss:        0.750372
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.838180 loss:        0.500562
Test - acc:         0.776200 loss:        0.756491
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.836300 loss:        0.502078
Test - acc:         0.789500 loss:        0.655714
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.836660 loss:        0.502930
Test - acc:         0.707700 loss:        0.958034
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.839560 loss:        0.493270
Test - acc:         0.778800 loss:        0.711446
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.839140 loss:        0.498796
Test - acc:         0.793600 loss:        0.662148
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.840560 loss:        0.484859
Test - acc:         0.794200 loss:        0.628181
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.838240 loss:        0.494592
Test - acc:         0.789800 loss:        0.661584
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.833880 loss:        0.504412
Test - acc:         0.755700 loss:        0.800765
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.842280 loss:        0.487378
Test - acc:         0.716700 loss:        0.987596
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.841560 loss:        0.488262
Test - acc:         0.737600 loss:        0.838389
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.837080 loss:        0.505145
Test - acc:         0.810900 loss:        0.588247
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.841920 loss:        0.489849
Test - acc:         0.831600 loss:        0.527978
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.841560 loss:        0.486142
Test - acc:         0.818400 loss:        0.568968
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.839180 loss:        0.491500
Test - acc:         0.795500 loss:        0.644582
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.839340 loss:        0.492808
Test - acc:         0.825600 loss:        0.568776
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.841020 loss:        0.486094
Test - acc:         0.775700 loss:        0.710300
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.843200 loss:        0.489516
Test - acc:         0.747900 loss:        0.798557
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.842280 loss:        0.484698
Test - acc:         0.791400 loss:        0.662652
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.840240 loss:        0.480997
Test - acc:         0.777600 loss:        0.713044
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.842100 loss:        0.485714
Test - acc:         0.758300 loss:        0.787995
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.840980 loss:        0.484376
Test - acc:         0.769000 loss:        0.759977
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.841100 loss:        0.488905
Test - acc:         0.790000 loss:        0.651230
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.838860 loss:        0.492596
Test - acc:         0.785500 loss:        0.703516
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.842020 loss:        0.484098
Test - acc:         0.750600 loss:        0.815833
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.841720 loss:        0.484183
Test - acc:         0.749000 loss:        0.832634
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.840160 loss:        0.486216
Test - acc:         0.720700 loss:        0.900586
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.844980 loss:        0.473192
Test - acc:         0.794400 loss:        0.684174
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.841460 loss:        0.480126
Test - acc:         0.727900 loss:        0.973362
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.843980 loss:        0.478059
Test - acc:         0.815300 loss:        0.584632
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.843320 loss:        0.484048
Test - acc:         0.753100 loss:        0.806388
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.845080 loss:        0.479732
Test - acc:         0.740500 loss:        0.808637
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.844820 loss:        0.475006
Test - acc:         0.812300 loss:        0.564672
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.843340 loss:        0.479455
Test - acc:         0.774600 loss:        0.786840
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.842260 loss:        0.481259
Test - acc:         0.802500 loss:        0.626435
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.842980 loss:        0.481080
Test - acc:         0.778600 loss:        0.741176
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.842420 loss:        0.480092
Test - acc:         0.730300 loss:        0.831974
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.846800 loss:        0.475593
Test - acc:         0.819800 loss:        0.561958
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.846060 loss:        0.477569
Test - acc:         0.792800 loss:        0.652379
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.844480 loss:        0.480377
Test - acc:         0.719800 loss:        0.982706
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.842540 loss:        0.481628
Test - acc:         0.801200 loss:        0.637462
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.851460 loss:        0.461784
Test - acc:         0.820200 loss:        0.546325
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.843900 loss:        0.474563
Test - acc:         0.726100 loss:        0.883175
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.842520 loss:        0.474886
Test - acc:         0.770100 loss:        0.743878
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.844180 loss:        0.476916
Test - acc:         0.749000 loss:        0.789361
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.845920 loss:        0.468375
Test - acc:         0.767800 loss:        0.778573
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.844240 loss:        0.472211
Test - acc:         0.815400 loss:        0.572983
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.845520 loss:        0.473794
Test - acc:         0.728600 loss:        0.915502
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.844160 loss:        0.474979
Test - acc:         0.747400 loss:        0.784316
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.847320 loss:        0.467965
Test - acc:         0.787700 loss:        0.672218
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.846100 loss:        0.475936
Test - acc:         0.815800 loss:        0.555780
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.850680 loss:        0.462272
Test - acc:         0.789900 loss:        0.667694
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.844100 loss:        0.480861
Test - acc:         0.753900 loss:        0.796539
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.845760 loss:        0.473753
Test - acc:         0.784700 loss:        0.669074
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.845000 loss:        0.474215
Test - acc:         0.800200 loss:        0.647863
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.843660 loss:        0.478080
Test - acc:         0.821600 loss:        0.575370
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.845380 loss:        0.471158
Test - acc:         0.774900 loss:        0.707722
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.843720 loss:        0.473856
Test - acc:         0.778400 loss:        0.688682
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.844140 loss:        0.477203
Test - acc:         0.785700 loss:        0.657524
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.842520 loss:        0.475525
Test - acc:         0.803700 loss:        0.607505
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.846660 loss:        0.468667
Test - acc:         0.833600 loss:        0.512082
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.857180 loss:        0.432672
Test - acc:         0.798300 loss:        0.658114
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.861700 loss:        0.417909
Test - acc:         0.817700 loss:        0.547451
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.862900 loss:        0.410912
Test - acc:         0.819500 loss:        0.537418
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.863740 loss:        0.407759
Test - acc:         0.849100 loss:        0.451199
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.865400 loss:        0.405866
Test - acc:         0.822200 loss:        0.543916
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.863680 loss:        0.408799
Test - acc:         0.825700 loss:        0.575012
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.864840 loss:        0.407520
Test - acc:         0.823100 loss:        0.542691
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.866600 loss:        0.403180
Test - acc:         0.797300 loss:        0.624802
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.866000 loss:        0.402382
Test - acc:         0.813500 loss:        0.593218
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.867900 loss:        0.397642
Test - acc:         0.812500 loss:        0.603133
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.866680 loss:        0.403429
Test - acc:         0.827300 loss:        0.533884
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.867240 loss:        0.398817
Test - acc:         0.790000 loss:        0.643304
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.866820 loss:        0.400181
Test - acc:         0.800100 loss:        0.641183
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.866540 loss:        0.400892
Test - acc:         0.773200 loss:        0.683318
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.868460 loss:        0.396890
Test - acc:         0.837900 loss:        0.515775
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.867540 loss:        0.397357
Test - acc:         0.803300 loss:        0.620661
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.867240 loss:        0.400387
Test - acc:         0.798100 loss:        0.629257
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.866800 loss:        0.396698
Test - acc:         0.833700 loss:        0.497259
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.868520 loss:        0.396282
Test - acc:         0.813600 loss:        0.563263
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.866420 loss:        0.397066
Test - acc:         0.838500 loss:        0.500955
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.870400 loss:        0.393661
Test - acc:         0.829100 loss:        0.549509
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.870560 loss:        0.389681
Test - acc:         0.846600 loss:        0.459195
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.869860 loss:        0.393041
Test - acc:         0.803800 loss:        0.630768
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.869780 loss:        0.393226
Test - acc:         0.811100 loss:        0.597406
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.868520 loss:        0.395608
Test - acc:         0.797500 loss:        0.656101
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.866440 loss:        0.394547
Test - acc:         0.818200 loss:        0.567088
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.865180 loss:        0.400387
Test - acc:         0.835900 loss:        0.501748
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.868740 loss:        0.396462
Test - acc:         0.800600 loss:        0.639747
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.869480 loss:        0.392128
Test - acc:         0.818000 loss:        0.549265
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.869520 loss:        0.391502
Test - acc:         0.763300 loss:        0.769057
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.870720 loss:        0.389626
Test - acc:         0.840800 loss:        0.500819
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.873700 loss:        0.383961
Test - acc:         0.841000 loss:        0.474456
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.870140 loss:        0.391205
Test - acc:         0.802500 loss:        0.636560
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.921120 loss:        0.238638
Test - acc:         0.903500 loss:        0.302000
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.933840 loss:        0.198890
Test - acc:         0.908800 loss:        0.281132
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.939540 loss:        0.179876
Test - acc:         0.915200 loss:        0.271034
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.943840 loss:        0.166356
Test - acc:         0.914800 loss:        0.270220
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.948020 loss:        0.153314
Test - acc:         0.913200 loss:        0.276297
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.950680 loss:        0.146435
Test - acc:         0.914800 loss:        0.277960
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.952540 loss:        0.140101
Test - acc:         0.915900 loss:        0.271565
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.954600 loss:        0.136256
Test - acc:         0.915200 loss:        0.280766
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.956940 loss:        0.128354
Test - acc:         0.914700 loss:        0.277136
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.958700 loss:        0.120509
Test - acc:         0.911400 loss:        0.289081
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.960300 loss:        0.119031
Test - acc:         0.914400 loss:        0.283377
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.960020 loss:        0.117678
Test - acc:         0.913000 loss:        0.288416
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.959600 loss:        0.114254
Test - acc:         0.916200 loss:        0.280687
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.961280 loss:        0.109784
Test - acc:         0.912900 loss:        0.295160
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.961980 loss:        0.111334
Test - acc:         0.915200 loss:        0.289254
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.964400 loss:        0.103744
Test - acc:         0.912000 loss:        0.313576
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.963220 loss:        0.104835
Test - acc:         0.912700 loss:        0.306412
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.966100 loss:        0.100997
Test - acc:         0.912700 loss:        0.297657
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.965940 loss:        0.098730
Test - acc:         0.902500 loss:        0.349318
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.965060 loss:        0.098247
Test - acc:         0.905400 loss:        0.329413
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.964280 loss:        0.101729
Test - acc:         0.912300 loss:        0.307854
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.965980 loss:        0.098620
Test - acc:         0.912500 loss:        0.312318
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.964620 loss:        0.101889
Test - acc:         0.899400 loss:        0.351950
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.964420 loss:        0.101000
Test - acc:         0.907800 loss:        0.323251
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.966160 loss:        0.098051
Test - acc:         0.907800 loss:        0.331748
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.964880 loss:        0.099987
Test - acc:         0.913300 loss:        0.304667
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.965840 loss:        0.100461
Test - acc:         0.914500 loss:        0.298685
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.965820 loss:        0.099679
Test - acc:         0.914500 loss:        0.308180
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.965380 loss:        0.098775
Test - acc:         0.904600 loss:        0.341517
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.964640 loss:        0.100514
Test - acc:         0.910500 loss:        0.328618
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.967020 loss:        0.098033
Test - acc:         0.902800 loss:        0.366241
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.966220 loss:        0.095682
Test - acc:         0.907300 loss:        0.335026
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.965320 loss:        0.100885
Test - acc:         0.904900 loss:        0.345706
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.964980 loss:        0.101249
Test - acc:         0.904900 loss:        0.343532
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.965500 loss:        0.098581
Test - acc:         0.900000 loss:        0.362113
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.964880 loss:        0.102397
Test - acc:         0.905200 loss:        0.335571
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.966660 loss:        0.097106
Test - acc:         0.909500 loss:        0.318638
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.965460 loss:        0.100605
Test - acc:         0.901000 loss:        0.333941
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.965540 loss:        0.101143
Test - acc:         0.905900 loss:        0.320439
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.966640 loss:        0.099876
Test - acc:         0.896500 loss:        0.364960
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.963980 loss:        0.104017
Test - acc:         0.899200 loss:        0.376589
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.964100 loss:        0.105508
Test - acc:         0.894600 loss:        0.383138
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.966500 loss:        0.097642
Test - acc:         0.894800 loss:        0.389788
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.965380 loss:        0.101277
Test - acc:         0.896500 loss:        0.375641
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.965420 loss:        0.102277
Test - acc:         0.895300 loss:        0.384879
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.966580 loss:        0.099837
Test - acc:         0.901800 loss:        0.344608
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.965000 loss:        0.100585
Test - acc:         0.902000 loss:        0.346088
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.966440 loss:        0.099277
Test - acc:         0.905200 loss:        0.335909
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.965440 loss:        0.099549
Test - acc:         0.903700 loss:        0.353846
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.965960 loss:        0.099443
Test - acc:         0.902900 loss:        0.346568
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.965300 loss:        0.098029
Test - acc:         0.890600 loss:        0.384224
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.964340 loss:        0.103576
Test - acc:         0.903200 loss:        0.346817
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.964720 loss:        0.100281
Test - acc:         0.897700 loss:        0.361500
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.964060 loss:        0.104331
Test - acc:         0.896200 loss:        0.377324
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.965800 loss:        0.101048
Test - acc:         0.897700 loss:        0.367464
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.965160 loss:        0.100391
Test - acc:         0.902600 loss:        0.350372
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.966620 loss:        0.096209
Test - acc:         0.907800 loss:        0.331907
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.964300 loss:        0.103934
Test - acc:         0.895600 loss:        0.375886
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.965120 loss:        0.102826
Test - acc:         0.896300 loss:        0.373773
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.965920 loss:        0.099597
Test - acc:         0.901100 loss:        0.347292
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.964620 loss:        0.101663
Test - acc:         0.899800 loss:        0.372251
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.965240 loss:        0.100231
Test - acc:         0.905300 loss:        0.328772
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.967980 loss:        0.095125
Test - acc:         0.896000 loss:        0.359373
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.965840 loss:        0.098164
Test - acc:         0.900600 loss:        0.370644
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.967020 loss:        0.095534
Test - acc:         0.898000 loss:        0.365985
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.967260 loss:        0.096493
Test - acc:         0.902800 loss:        0.338374
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.966740 loss:        0.097890
Test - acc:         0.883500 loss:        0.428308
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.967400 loss:        0.095551
Test - acc:         0.903500 loss:        0.350923
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.966240 loss:        0.099523
Test - acc:         0.905200 loss:        0.343757
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.968300 loss:        0.092299
Test - acc:         0.906900 loss:        0.330202
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.967360 loss:        0.095654
Test - acc:         0.903400 loss:        0.352754
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.967980 loss:        0.093781
Test - acc:         0.905900 loss:        0.342369
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.968680 loss:        0.092636
Test - acc:         0.900100 loss:        0.349625
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.966980 loss:        0.097784
Test - acc:         0.904500 loss:        0.347873
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.967900 loss:        0.095326
Test - acc:         0.906600 loss:        0.325839
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.969680 loss:        0.089505
Test - acc:         0.894000 loss:        0.388838
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.967040 loss:        0.095132
Test - acc:         0.903600 loss:        0.344192
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.969100 loss:        0.092606
Test - acc:         0.897300 loss:        0.365529
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.966840 loss:        0.096744
Test - acc:         0.900000 loss:        0.350767
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.968900 loss:        0.093320
Test - acc:         0.901100 loss:        0.355008
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.968260 loss:        0.093525
Test - acc:         0.893100 loss:        0.384734
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.968000 loss:        0.094125
Test - acc:         0.898700 loss:        0.371430
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.968280 loss:        0.092077
Test - acc:         0.899200 loss:        0.376311
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.966860 loss:        0.096924
Test - acc:         0.898900 loss:        0.344656
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.955920 loss:        0.130259
Test - acc:         0.900600 loss:        0.324210
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.960680 loss:        0.115929
Test - acc:         0.899000 loss:        0.357563
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.961200 loss:        0.113661
Test - acc:         0.905600 loss:        0.322299
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.961820 loss:        0.110779
Test - acc:         0.907000 loss:        0.324094
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.964700 loss:        0.104255
Test - acc:         0.903500 loss:        0.352733
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.963500 loss:        0.107781
Test - acc:         0.902100 loss:        0.346457
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.964980 loss:        0.101299
Test - acc:         0.903300 loss:        0.338860
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.967240 loss:        0.096066
Test - acc:         0.904500 loss:        0.345233
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.964380 loss:        0.103796
Test - acc:         0.894400 loss:        0.381413
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.963640 loss:        0.105246
Test - acc:         0.903900 loss:        0.332347
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.963880 loss:        0.102544
Test - acc:         0.907800 loss:        0.335045
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.966220 loss:        0.098161
Test - acc:         0.900000 loss:        0.355348
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.966260 loss:        0.097541
Test - acc:         0.901400 loss:        0.333327
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.966300 loss:        0.098389
Test - acc:         0.907700 loss:        0.326891
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.966800 loss:        0.095924
Test - acc:         0.908500 loss:        0.327675
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.965240 loss:        0.099939
Test - acc:         0.900700 loss:        0.352665
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.978760 loss:        0.064801
Test - acc:         0.920800 loss:        0.272886
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.985960 loss:        0.045304
Test - acc:         0.921800 loss:        0.277110
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.988120 loss:        0.039887
Test - acc:         0.923500 loss:        0.281308
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.987840 loss:        0.037183
Test - acc:         0.923700 loss:        0.286841
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.990120 loss:        0.031841
Test - acc:         0.924700 loss:        0.287758
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.989740 loss:        0.031358
Test - acc:         0.921600 loss:        0.295655
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.990200 loss:        0.030458
Test - acc:         0.922800 loss:        0.298087
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.991940 loss:        0.026883
Test - acc:         0.923600 loss:        0.299586
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.991000 loss:        0.027004
Test - acc:         0.922300 loss:        0.303501
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.992320 loss:        0.023940
Test - acc:         0.923500 loss:        0.301982
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.993000 loss:        0.023614
Test - acc:         0.924800 loss:        0.304829
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.992180 loss:        0.023816
Test - acc:         0.926100 loss:        0.307530
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.993460 loss:        0.021383
Test - acc:         0.925200 loss:        0.309814
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.993500 loss:        0.021085
Test - acc:         0.924700 loss:        0.314548
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.993460 loss:        0.020329
Test - acc:         0.926500 loss:        0.310254
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.993940 loss:        0.019228
Test - acc:         0.925300 loss:        0.317473
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.994300 loss:        0.018555
Test - acc:         0.925800 loss:        0.317138
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.993420 loss:        0.020318
Test - acc:         0.924300 loss:        0.325610
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.994340 loss:        0.018342
Test - acc:         0.926000 loss:        0.322096
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.994840 loss:        0.016954
Test - acc:         0.925300 loss:        0.323568
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.995320 loss:        0.016148
Test - acc:         0.924400 loss:        0.328890
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.995040 loss:        0.016362
Test - acc:         0.925000 loss:        0.326453
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.995020 loss:        0.015888
Test - acc:         0.924100 loss:        0.328242
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.995600 loss:        0.014237
Test - acc:         0.925000 loss:        0.331232
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.995660 loss:        0.013996
Test - acc:         0.924600 loss:        0.334986
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.995140 loss:        0.015185
Test - acc:         0.924600 loss:        0.338965
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.995720 loss:        0.014290
Test - acc:         0.923000 loss:        0.338253
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.995360 loss:        0.014599
Test - acc:         0.925700 loss:        0.336181
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.995480 loss:        0.014300
Test - acc:         0.924100 loss:        0.336028
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.995020 loss:        0.015078
Test - acc:         0.923400 loss:        0.342694
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.996280 loss:        0.012775
Test - acc:         0.923800 loss:        0.336159
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.995540 loss:        0.013735
Test - acc:         0.924700 loss:        0.339390
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.996180 loss:        0.012940
Test - acc:         0.923800 loss:        0.341526
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.995880 loss:        0.013181
Test - acc:         0.924600 loss:        0.338370
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.996860 loss:        0.011714
Test - acc:         0.926200 loss:        0.340699
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.996660 loss:        0.011444
Test - acc:         0.924900 loss:        0.344536
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.995880 loss:        0.012698
Test - acc:         0.923800 loss:        0.345178
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.996300 loss:        0.012260
Test - acc:         0.925700 loss:        0.339462
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.995980 loss:        0.012568
Test - acc:         0.924900 loss:        0.343634
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.996280 loss:        0.012318
Test - acc:         0.925200 loss:        0.344193
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.997100 loss:        0.010530
Test - acc:         0.925000 loss:        0.350396
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.996320 loss:        0.011182
Test - acc:         0.923700 loss:        0.349175
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.996800 loss:        0.010588
Test - acc:         0.924800 loss:        0.349538
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.996760 loss:        0.010645
Test - acc:         0.924100 loss:        0.350478
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.996360 loss:        0.011658
Test - acc:         0.924900 loss:        0.349532
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.996620 loss:        0.011419
Test - acc:         0.926200 loss:        0.344578
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.997000 loss:        0.009736
Test - acc:         0.925200 loss:        0.356618
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.997300 loss:        0.009451
Test - acc:         0.923700 loss:        0.355783
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.997220 loss:        0.009558
Test - acc:         0.924900 loss:        0.351295
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.997020 loss:        0.010024
Test - acc:         0.925800 loss:        0.348614
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.997600 loss:        0.008654
Test - acc:         0.923800 loss:        0.356971
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.997080 loss:        0.009802
Test - acc:         0.925200 loss:        0.351295
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.997200 loss:        0.009243
Test - acc:         0.925400 loss:        0.353811
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.997200 loss:        0.009540
Test - acc:         0.925500 loss:        0.355346
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.997340 loss:        0.009242
Test - acc:         0.926000 loss:        0.357779
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.996480 loss:        0.011056
Test - acc:         0.925100 loss:        0.355408
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.997500 loss:        0.009393
Test - acc:         0.926100 loss:        0.353585
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.997680 loss:        0.007883
Test - acc:         0.927400 loss:        0.352918
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.997240 loss:        0.009204
Test - acc:         0.923900 loss:        0.360314
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.997380 loss:        0.008989
Test - acc:         0.926700 loss:        0.359536
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.997100 loss:        0.010148
Test - acc:         0.925400 loss:        0.359138
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.997160 loss:        0.009722
Test - acc:         0.925100 loss:        0.353822
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.997120 loss:        0.009498
Test - acc:         0.923500 loss:        0.357789
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.996960 loss:        0.009920
Test - acc:         0.924700 loss:        0.358562
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.997380 loss:        0.009109
Test - acc:         0.925500 loss:        0.362652
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.997560 loss:        0.008540
Test - acc:         0.923200 loss:        0.356116
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.997920 loss:        0.007850
Test - acc:         0.926900 loss:        0.357732
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.997260 loss:        0.009030
Test - acc:         0.922900 loss:        0.365245
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.997380 loss:        0.009252
Test - acc:         0.924800 loss:        0.361503
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.997460 loss:        0.008935
Test - acc:         0.924200 loss:        0.364335
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.997160 loss:        0.009564
Test - acc:         0.924400 loss:        0.359420
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.997820 loss:        0.008081
Test - acc:         0.924000 loss:        0.363417
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.997540 loss:        0.008314
Test - acc:         0.925100 loss:        0.362932
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.997500 loss:        0.008324
Test - acc:         0.922700 loss:        0.354930
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.997680 loss:        0.007904
Test - acc:         0.925400 loss:        0.361912
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.997540 loss:        0.008368
Test - acc:         0.924500 loss:        0.363611
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.997900 loss:        0.007329
Test - acc:         0.924400 loss:        0.369695
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.997280 loss:        0.009345
Test - acc:         0.923900 loss:        0.370302
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.997880 loss:        0.007115
Test - acc:         0.924700 loss:        0.365187
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.997500 loss:        0.007934
Test - acc:         0.926200 loss:        0.363857
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.997460 loss:        0.008523
Test - acc:         0.925100 loss:        0.364614
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.997700 loss:        0.007664
Test - acc:         0.923600 loss:        0.366564
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.997520 loss:        0.008299
Test - acc:         0.926700 loss:        0.366461
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.997840 loss:        0.006736
Test - acc:         0.923400 loss:        0.370290
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.997860 loss:        0.007257
Test - acc:         0.924300 loss:        0.370214
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.997800 loss:        0.007471
Test - acc:         0.925400 loss:        0.362189
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.997760 loss:        0.008169
Test - acc:         0.924600 loss:        0.370504
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.998000 loss:        0.007233
Test - acc:         0.922900 loss:        0.371713
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.998100 loss:        0.006655
Test - acc:         0.924400 loss:        0.370542
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.998160 loss:        0.006266
Test - acc:         0.925400 loss:        0.374242
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.998220 loss:        0.006417
Test - acc:         0.925200 loss:        0.373214
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.997760 loss:        0.007555
Test - acc:         0.922900 loss:        0.380006
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.997940 loss:        0.007205
Test - acc:         0.924600 loss:        0.373645
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.997300 loss:        0.008409
Test - acc:         0.924500 loss:        0.372434
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.997660 loss:        0.007900
Test - acc:         0.924900 loss:        0.364955
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.997940 loss:        0.007220
Test - acc:         0.925800 loss:        0.367610
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.997420 loss:        0.008322
Test - acc:         0.923800 loss:        0.365577
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.998160 loss:        0.006670
Test - acc:         0.924100 loss:        0.364099
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.997740 loss:        0.007899
Test - acc:         0.924600 loss:        0.363288
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.997680 loss:        0.007747
Test - acc:         0.924900 loss:        0.361949
Sparsity :          0.7500
Wdecay :        0.000500
