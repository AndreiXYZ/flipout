Running --prune_criterion random --seed 42 --prune_freq 39 --prune_rate 0.5 --comment=vgg19_crit=random_pf=39_seed=42 --save_model=pre-finetune/vgg19_random_pf39_s42
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "random",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_random_pf39_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.106580 loss:        2.520409
Test - acc:         0.107600 loss:        2.294274
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.121620 loss:        2.275786
Test - acc:         0.128800 loss:        2.258387
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.167360 loss:        2.134492
Test - acc:         0.208100 loss:        1.905877
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.258380 loss:        1.859667
Test - acc:         0.287600 loss:        1.767105
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.322920 loss:        1.699367
Test - acc:         0.410600 loss:        1.501470
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.445160 loss:        1.456379
Test - acc:         0.508200 loss:        1.356367
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.546300 loss:        1.241191
Test - acc:         0.498200 loss:        1.477734
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.604540 loss:        1.101290
Test - acc:         0.483800 loss:        1.743679
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.650880 loss:        0.993529
Test - acc:         0.619700 loss:        1.166982
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.703160 loss:        0.882477
Test - acc:         0.659900 loss:        1.012752
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.733140 loss:        0.808711
Test - acc:         0.648500 loss:        1.121658
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.752060 loss:        0.763825
Test - acc:         0.697100 loss:        0.950123
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.763380 loss:        0.735478
Test - acc:         0.647600 loss:        1.210369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.773560 loss:        0.706368
Test - acc:         0.717500 loss:        0.901679
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.780940 loss:        0.681272
Test - acc:         0.682800 loss:        1.025874
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.791420 loss:        0.649883
Test - acc:         0.736800 loss:        0.872370
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.796700 loss:        0.634338
Test - acc:         0.700800 loss:        0.932917
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.801840 loss:        0.615529
Test - acc:         0.797400 loss:        0.635734
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.804520 loss:        0.605495
Test - acc:         0.786700 loss:        0.672975
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.811300 loss:        0.586166
Test - acc:         0.692500 loss:        1.006768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.818600 loss:        0.574597
Test - acc:         0.764600 loss:        0.752744
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.819720 loss:        0.564470
Test - acc:         0.732800 loss:        0.858983
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.822020 loss:        0.561122
Test - acc:         0.767600 loss:        0.719709
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.821520 loss:        0.558114
Test - acc:         0.744500 loss:        0.842082
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.827620 loss:        0.542572
Test - acc:         0.761300 loss:        0.789428
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.830100 loss:        0.535151
Test - acc:         0.741300 loss:        0.871493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.828600 loss:        0.532575
Test - acc:         0.787500 loss:        0.687972
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.833340 loss:        0.523183
Test - acc:         0.781500 loss:        0.714158
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.832740 loss:        0.525514
Test - acc:         0.761200 loss:        0.775278
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.831200 loss:        0.525716
Test - acc:         0.754700 loss:        0.881011
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.838100 loss:        0.512746
Test - acc:         0.776500 loss:        0.724635
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.839420 loss:        0.504651
Test - acc:         0.760400 loss:        0.863967
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.838440 loss:        0.504414
Test - acc:         0.785700 loss:        0.718698
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.838460 loss:        0.503714
Test - acc:         0.815300 loss:        0.567434
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.840220 loss:        0.497685
Test - acc:         0.662800 loss:        1.328712
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.842920 loss:        0.493377
Test - acc:         0.732000 loss:        0.900541
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.841740 loss:        0.493601
Test - acc:         0.813600 loss:        0.592560
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.841380 loss:        0.496394
Test - acc:         0.789100 loss:        0.683082
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.846480 loss:        0.485149
Test - acc:         0.747100 loss:        0.824019
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.766740 loss:        0.730888
Test - acc:         0.752700 loss:        0.841157
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.808020 loss:        0.593585
Test - acc:         0.693000 loss:        1.063748
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.819540 loss:        0.563267
Test - acc:         0.681300 loss:        1.105433
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.825580 loss:        0.543118
Test - acc:         0.752200 loss:        0.830031
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.830760 loss:        0.522893
Test - acc:         0.728100 loss:        0.917086
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.831660 loss:        0.523849
Test - acc:         0.796300 loss:        0.667117
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.833680 loss:        0.518917
Test - acc:         0.734700 loss:        0.928820
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.832440 loss:        0.515524
Test - acc:         0.758800 loss:        0.806983
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.834420 loss:        0.511763
Test - acc:         0.792700 loss:        0.665359
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.839020 loss:        0.502637
Test - acc:         0.806300 loss:        0.600914
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.839680 loss:        0.501367
Test - acc:         0.730800 loss:        0.988034
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.841820 loss:        0.498687
Test - acc:         0.746600 loss:        0.803424
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.840660 loss:        0.496368
Test - acc:         0.661300 loss:        1.159839
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.837600 loss:        0.501433
Test - acc:         0.784900 loss:        0.703525
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.839720 loss:        0.493203
Test - acc:         0.825000 loss:        0.548598
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.842560 loss:        0.490992
Test - acc:         0.815200 loss:        0.557557
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.841520 loss:        0.488997
Test - acc:         0.804700 loss:        0.600361
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.845700 loss:        0.479067
Test - acc:         0.766100 loss:        0.766293
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.841600 loss:        0.497027
Test - acc:         0.803800 loss:        0.630100
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.846600 loss:        0.475348
Test - acc:         0.803200 loss:        0.628526
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.845340 loss:        0.489168
Test - acc:         0.797100 loss:        0.648633
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.846280 loss:        0.475221
Test - acc:         0.768500 loss:        0.722809
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.844680 loss:        0.485986
Test - acc:         0.805900 loss:        0.635847
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.845780 loss:        0.478192
Test - acc:         0.678300 loss:        1.176454
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.846480 loss:        0.478383
Test - acc:         0.769400 loss:        0.750527
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.845020 loss:        0.481235
Test - acc:         0.779700 loss:        0.712145
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.848720 loss:        0.472412
Test - acc:         0.755300 loss:        0.797907
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.848320 loss:        0.474306
Test - acc:         0.756100 loss:        0.767968
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.848540 loss:        0.476111
Test - acc:         0.785800 loss:        0.702607
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.847960 loss:        0.475452
Test - acc:         0.784100 loss:        0.697043
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.850520 loss:        0.465827
Test - acc:         0.714100 loss:        0.941669
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.848960 loss:        0.474420
Test - acc:         0.790600 loss:        0.685648
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.853000 loss:        0.461545
Test - acc:         0.820100 loss:        0.566946
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.853940 loss:        0.457322
Test - acc:         0.792900 loss:        0.679706
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.851460 loss:        0.463536
Test - acc:         0.792100 loss:        0.700771
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.852400 loss:        0.455721
Test - acc:         0.826100 loss:        0.556708
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.850420 loss:        0.460248
Test - acc:         0.740200 loss:        0.827950
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.852020 loss:        0.459367
Test - acc:         0.765400 loss:        0.748074
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.852020 loss:        0.455908
Test - acc:         0.784500 loss:        0.688721
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.745560 loss:        0.774861
Test - acc:         0.752400 loss:        0.803328
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.803740 loss:        0.601901
Test - acc:         0.748900 loss:        0.821574
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.816860 loss:        0.554382
Test - acc:         0.766500 loss:        0.751808
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.825540 loss:        0.533678
Test - acc:         0.778400 loss:        0.698257
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.827780 loss:        0.524291
Test - acc:         0.750000 loss:        0.753998
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.829280 loss:        0.512266
Test - acc:         0.627700 loss:        1.383911
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.834940 loss:        0.504574
Test - acc:         0.810600 loss:        0.601137
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.834220 loss:        0.496122
Test - acc:         0.791300 loss:        0.674221
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.835760 loss:        0.496972
Test - acc:         0.788900 loss:        0.645935
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.836640 loss:        0.488351
Test - acc:         0.696300 loss:        1.069981
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.838880 loss:        0.482304
Test - acc:         0.805500 loss:        0.602233
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.844080 loss:        0.474855
Test - acc:         0.747200 loss:        0.793756
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.842380 loss:        0.479764
Test - acc:         0.784800 loss:        0.665448
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.842240 loss:        0.473067
Test - acc:         0.759300 loss:        0.798814
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.844200 loss:        0.469464
Test - acc:         0.782000 loss:        0.692549
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.840760 loss:        0.474669
Test - acc:         0.810300 loss:        0.589375
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.843040 loss:        0.468400
Test - acc:         0.749100 loss:        0.779888
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.845940 loss:        0.463183
Test - acc:         0.687300 loss:        1.147274
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.846200 loss:        0.464662
Test - acc:         0.749700 loss:        0.761808
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.849460 loss:        0.453317
Test - acc:         0.793000 loss:        0.628262
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.847540 loss:        0.459203
Test - acc:         0.817300 loss:        0.555257
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.847580 loss:        0.454123
Test - acc:         0.774100 loss:        0.737548
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.848820 loss:        0.451450
Test - acc:         0.801400 loss:        0.599729
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.846880 loss:        0.456181
Test - acc:         0.803900 loss:        0.584731
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.848260 loss:        0.456186
Test - acc:         0.808400 loss:        0.601954
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.847520 loss:        0.452899
Test - acc:         0.796000 loss:        0.643223
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.850100 loss:        0.443990
Test - acc:         0.768700 loss:        0.746190
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.848040 loss:        0.451264
Test - acc:         0.784600 loss:        0.663428
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.848980 loss:        0.447290
Test - acc:         0.791400 loss:        0.676681
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.853300 loss:        0.443455
Test - acc:         0.819100 loss:        0.535256
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.851240 loss:        0.447309
Test - acc:         0.795100 loss:        0.644733
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.851840 loss:        0.440879
Test - acc:         0.791900 loss:        0.659479
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.850000 loss:        0.447603
Test - acc:         0.752400 loss:        0.836847
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.849300 loss:        0.444323
Test - acc:         0.771300 loss:        0.776977
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.853940 loss:        0.435568
Test - acc:         0.807600 loss:        0.594293
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.850340 loss:        0.442747
Test - acc:         0.799200 loss:        0.622638
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.852800 loss:        0.437616
Test - acc:         0.832700 loss:        0.518687
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.852140 loss:        0.442266
Test - acc:         0.804600 loss:        0.605248
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.853300 loss:        0.436518
Test - acc:         0.824300 loss:        0.548105
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.700580 loss:        0.878756
Test - acc:         0.720800 loss:        0.846248
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.782700 loss:        0.639505
Test - acc:         0.710700 loss:        0.933536
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.798520 loss:        0.595540
Test - acc:         0.735500 loss:        0.770507
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.805040 loss:        0.571615
Test - acc:         0.788800 loss:        0.630873
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.811400 loss:        0.558090
Test - acc:         0.726100 loss:        0.831005
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.815880 loss:        0.543731
Test - acc:         0.751200 loss:        0.823556
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.818320 loss:        0.537285
Test - acc:         0.761800 loss:        0.720059
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.818760 loss:        0.531826
Test - acc:         0.744300 loss:        0.747403
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.822640 loss:        0.525012
Test - acc:         0.741900 loss:        0.808312
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.823780 loss:        0.521252
Test - acc:         0.788200 loss:        0.655022
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.824840 loss:        0.520155
Test - acc:         0.730100 loss:        0.839404
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.828280 loss:        0.506445
Test - acc:         0.815500 loss:        0.571819
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.825640 loss:        0.515871
Test - acc:         0.738800 loss:        0.826748
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.826900 loss:        0.514750
Test - acc:         0.681700 loss:        0.971070
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.828420 loss:        0.505513
Test - acc:         0.774100 loss:        0.711890
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.828640 loss:        0.508020
Test - acc:         0.606000 loss:        1.472908
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.825040 loss:        0.511286
Test - acc:         0.706600 loss:        0.910648
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.830700 loss:        0.500930
Test - acc:         0.803800 loss:        0.571188
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.830640 loss:        0.495844
Test - acc:         0.739800 loss:        0.802164
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.830400 loss:        0.498360
Test - acc:         0.782900 loss:        0.673594
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.832180 loss:        0.495606
Test - acc:         0.767400 loss:        0.728635
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.831760 loss:        0.495513
Test - acc:         0.780500 loss:        0.629211
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.830060 loss:        0.499427
Test - acc:         0.748200 loss:        0.770019
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.832620 loss:        0.493309
Test - acc:         0.767200 loss:        0.705225
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.834460 loss:        0.493283
Test - acc:         0.767200 loss:        0.753452
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.831360 loss:        0.495122
Test - acc:         0.805600 loss:        0.581727
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.833540 loss:        0.494352
Test - acc:         0.791600 loss:        0.632227
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.834320 loss:        0.489876
Test - acc:         0.733700 loss:        0.857405
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.835320 loss:        0.485462
Test - acc:         0.812900 loss:        0.564287
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.838120 loss:        0.483284
Test - acc:         0.770200 loss:        0.693855
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.833700 loss:        0.491736
Test - acc:         0.791000 loss:        0.631786
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.834980 loss:        0.488196
Test - acc:         0.784400 loss:        0.639827
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.836800 loss:        0.482667
Test - acc:         0.743900 loss:        0.791739
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.891500 loss:        0.318771
Test - acc:         0.885700 loss:        0.338081
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.910000 loss:        0.263838
Test - acc:         0.890400 loss:        0.322239
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.915880 loss:        0.244198
Test - acc:         0.893100 loss:        0.319073
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.922160 loss:        0.226490
Test - acc:         0.896200 loss:        0.303074
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.927160 loss:        0.211175
Test - acc:         0.897400 loss:        0.307890
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.931880 loss:        0.201663
Test - acc:         0.897100 loss:        0.310983
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.477480 loss:        1.433628
Test - acc:         0.588700 loss:        1.147631
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.678120 loss:        0.916192
Test - acc:         0.731800 loss:        0.767881
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.750680 loss:        0.720593
Test - acc:         0.761300 loss:        0.709586
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.783880 loss:        0.625708
Test - acc:         0.792600 loss:        0.600735
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.805500 loss:        0.567582
Test - acc:         0.810300 loss:        0.557110
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.818040 loss:        0.530550
Test - acc:         0.803700 loss:        0.585299
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.828080 loss:        0.497599
Test - acc:         0.812800 loss:        0.566176
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.834440 loss:        0.479289
Test - acc:         0.814100 loss:        0.538028
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.843360 loss:        0.454448
Test - acc:         0.824800 loss:        0.516591
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.844880 loss:        0.445982
Test - acc:         0.834800 loss:        0.484015
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.853340 loss:        0.426707
Test - acc:         0.827600 loss:        0.514433
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.856840 loss:        0.415686
Test - acc:         0.811900 loss:        0.539346
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.860880 loss:        0.405188
Test - acc:         0.835100 loss:        0.489254
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.861660 loss:        0.397078
Test - acc:         0.839300 loss:        0.478877
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.864840 loss:        0.385994
Test - acc:         0.835300 loss:        0.494625
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.864460 loss:        0.387809
Test - acc:         0.842700 loss:        0.469282
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.871000 loss:        0.377198
Test - acc:         0.831900 loss:        0.499704
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.871020 loss:        0.369909
Test - acc:         0.829600 loss:        0.509626
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.871260 loss:        0.372490
Test - acc:         0.833500 loss:        0.492941
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.873460 loss:        0.363713
Test - acc:         0.848000 loss:        0.457979
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.876800 loss:        0.353932
Test - acc:         0.843200 loss:        0.466516
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.878200 loss:        0.352067
Test - acc:         0.834400 loss:        0.495085
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.879520 loss:        0.347817
Test - acc:         0.839700 loss:        0.478217
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.879100 loss:        0.347033
Test - acc:         0.843200 loss:        0.484005
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.881240 loss:        0.338211
Test - acc:         0.831200 loss:        0.503363
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.881240 loss:        0.337648
Test - acc:         0.849100 loss:        0.461950
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.885740 loss:        0.329062
Test - acc:         0.844300 loss:        0.474709
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.886140 loss:        0.326439
Test - acc:         0.842500 loss:        0.478639
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.884520 loss:        0.330242
Test - acc:         0.839900 loss:        0.469928
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.887360 loss:        0.322803
Test - acc:         0.847900 loss:        0.466632
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.887100 loss:        0.324726
Test - acc:         0.846000 loss:        0.450667
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.890860 loss:        0.314960
Test - acc:         0.845900 loss:        0.458637
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.889340 loss:        0.320519
Test - acc:         0.851600 loss:        0.454714
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.891220 loss:        0.315939
Test - acc:         0.851300 loss:        0.452701
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.892000 loss:        0.308546
Test - acc:         0.844200 loss:        0.486579
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.893940 loss:        0.307891
Test - acc:         0.840100 loss:        0.483206
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.892340 loss:        0.309424
Test - acc:         0.847800 loss:        0.462166
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.893780 loss:        0.306591
Test - acc:         0.841200 loss:        0.488798
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.893660 loss:        0.304416
Test - acc:         0.852500 loss:        0.450060
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.383560 loss:        1.649534
Test - acc:         0.515700 loss:        1.365688
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.579540 loss:        1.177014
Test - acc:         0.596500 loss:        1.146129
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.641740 loss:        1.009918
Test - acc:         0.654500 loss:        0.981382
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.676040 loss:        0.917713
Test - acc:         0.666100 loss:        0.952548
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.699220 loss:        0.852991
Test - acc:         0.713200 loss:        0.815862
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.719140 loss:        0.798640
Test - acc:         0.722900 loss:        0.789021
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.731540 loss:        0.765678
Test - acc:         0.703700 loss:        0.849792
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.743700 loss:        0.731766
Test - acc:         0.736600 loss:        0.757840
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.752460 loss:        0.706709
Test - acc:         0.739200 loss:        0.760069
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.761600 loss:        0.687412
Test - acc:         0.726200 loss:        0.789563
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.770460 loss:        0.664607
Test - acc:         0.768500 loss:        0.684605
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.772240 loss:        0.655333
Test - acc:         0.738500 loss:        0.777368
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.778780 loss:        0.633019
Test - acc:         0.774100 loss:        0.660435
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.781920 loss:        0.623572
Test - acc:         0.761400 loss:        0.692965
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.789280 loss:        0.608976
Test - acc:         0.769000 loss:        0.678716
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.790480 loss:        0.602986
Test - acc:         0.775700 loss:        0.654860
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.795920 loss:        0.586426
Test - acc:         0.760200 loss:        0.681242
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.796080 loss:        0.585594
Test - acc:         0.763500 loss:        0.686455
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.797700 loss:        0.577514
Test - acc:         0.774200 loss:        0.652372
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.802700 loss:        0.566590
Test - acc:         0.780400 loss:        0.641198
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.804620 loss:        0.561696
Test - acc:         0.756000 loss:        0.708922
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.807740 loss:        0.555368
Test - acc:         0.735100 loss:        0.795594
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.805760 loss:        0.551951
Test - acc:         0.775700 loss:        0.672859
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.810180 loss:        0.543175
Test - acc:         0.767200 loss:        0.696027
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.814000 loss:        0.537966
Test - acc:         0.795500 loss:        0.600788
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.812500 loss:        0.535841
Test - acc:         0.787100 loss:        0.631271
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.819700 loss:        0.523932
Test - acc:         0.782000 loss:        0.648003
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.818200 loss:        0.521438
Test - acc:         0.784300 loss:        0.643561
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.818120 loss:        0.522462
Test - acc:         0.793900 loss:        0.606232
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.822880 loss:        0.511638
Test - acc:         0.798200 loss:        0.595020
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.823700 loss:        0.505226
Test - acc:         0.792700 loss:        0.605106
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.823660 loss:        0.504733
Test - acc:         0.790900 loss:        0.629716
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.823080 loss:        0.508909
Test - acc:         0.792100 loss:        0.607320
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.827080 loss:        0.497462
Test - acc:         0.790500 loss:        0.615934
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.828220 loss:        0.495669
Test - acc:         0.799800 loss:        0.578230
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.828520 loss:        0.492377
Test - acc:         0.786500 loss:        0.627099
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.829260 loss:        0.491316
Test - acc:         0.801100 loss:        0.580354
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.831520 loss:        0.484999
Test - acc:         0.783800 loss:        0.645784
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.830560 loss:        0.486788
Test - acc:         0.785300 loss:        0.629956
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.350180 loss:        1.768581
Test - acc:         0.430800 loss:        1.561577
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.489760 loss:        1.400952
Test - acc:         0.523700 loss:        1.328219
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.545920 loss:        1.257576
Test - acc:         0.510100 loss:        1.401359
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.582640 loss:        1.164708
Test - acc:         0.594100 loss:        1.119524
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.610040 loss:        1.097581
Test - acc:         0.566500 loss:        1.200406
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.622580 loss:        1.057684
Test - acc:         0.643500 loss:        1.002504
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.644880 loss:        1.008261
Test - acc:         0.574100 loss:        1.190218
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.653920 loss:        0.973290
Test - acc:         0.641700 loss:        1.020635
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.664240 loss:        0.949652
Test - acc:         0.658400 loss:        0.972897
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.675080 loss:        0.923169
Test - acc:         0.669200 loss:        0.947229
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.682580 loss:        0.900720
Test - acc:         0.613000 loss:        1.119125
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.689500 loss:        0.879937
Test - acc:         0.642900 loss:        1.042944
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.697260 loss:        0.865670
Test - acc:         0.689200 loss:        0.890558
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.701960 loss:        0.847213
Test - acc:         0.676900 loss:        0.930485
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.707620 loss:        0.836279
Test - acc:         0.625500 loss:        1.062636
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.711360 loss:        0.821115
Test - acc:         0.711400 loss:        0.839621
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.738780 loss:        0.747276
Test - acc:         0.750800 loss:        0.718155
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.750900 loss:        0.715777
Test - acc:         0.750900 loss:        0.713297
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.755160 loss:        0.702609
Test - acc:         0.754600 loss:        0.700236
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.757000 loss:        0.695730
Test - acc:         0.755100 loss:        0.698743
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.758040 loss:        0.692382
Test - acc:         0.755900 loss:        0.697889
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.759860 loss:        0.688529
Test - acc:         0.758400 loss:        0.690415
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.762140 loss:        0.682002
Test - acc:         0.757900 loss:        0.688368
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.764680 loss:        0.675749
Test - acc:         0.754000 loss:        0.697637
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.763340 loss:        0.672480
Test - acc:         0.757800 loss:        0.692101
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.763140 loss:        0.673602
Test - acc:         0.762500 loss:        0.676661
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.768980 loss:        0.664548
Test - acc:         0.761800 loss:        0.678279
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.768320 loss:        0.662496
Test - acc:         0.763600 loss:        0.679886
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.770040 loss:        0.660314
Test - acc:         0.766200 loss:        0.674194
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.768940 loss:        0.660709
Test - acc:         0.766100 loss:        0.673093
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.771740 loss:        0.651974
Test - acc:         0.767800 loss:        0.667127
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.773020 loss:        0.649111
Test - acc:         0.766700 loss:        0.674297
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.774900 loss:        0.645714
Test - acc:         0.770400 loss:        0.667410
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.774580 loss:        0.646018
Test - acc:         0.771500 loss:        0.666776
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.777960 loss:        0.640334
Test - acc:         0.768800 loss:        0.664086
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.777600 loss:        0.637733
Test - acc:         0.767400 loss:        0.664177
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.777080 loss:        0.635845
Test - acc:         0.771200 loss:        0.671298
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.775560 loss:        0.639853
Test - acc:         0.769600 loss:        0.667218
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.781940 loss:        0.628971
Test - acc:         0.770200 loss:        0.667575
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.157060 loss:        2.322018
Test - acc:         0.193300 loss:        2.115794
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.256680 loss:        1.953317
Test - acc:         0.277100 loss:        1.915488
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.299740 loss:        1.852484
Test - acc:         0.282000 loss:        1.875626
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.331080 loss:        1.794701
Test - acc:         0.240600 loss:        2.081426
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.343160 loss:        1.755547
Test - acc:         0.301300 loss:        1.926369
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.356060 loss:        1.727583
Test - acc:         0.290200 loss:        1.956589
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.368060 loss:        1.698351
Test - acc:         0.364000 loss:        1.739801
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.373660 loss:        1.683149
Test - acc:         0.343100 loss:        1.798637
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.384540 loss:        1.666691
Test - acc:         0.338400 loss:        1.812029
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.398020 loss:        1.642773
Test - acc:         0.403200 loss:        1.660080
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.408560 loss:        1.617613
Test - acc:         0.419900 loss:        1.622276
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.420760 loss:        1.591674
Test - acc:         0.437200 loss:        1.539430
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.428880 loss:        1.572942
Test - acc:         0.362300 loss:        1.773118
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.438000 loss:        1.553068
Test - acc:         0.433600 loss:        1.571824
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.441400 loss:        1.537361
Test - acc:         0.379100 loss:        1.753684
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.445540 loss:        1.524576
Test - acc:         0.455100 loss:        1.502015
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.452200 loss:        1.510106
Test - acc:         0.469300 loss:        1.467103
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.452500 loss:        1.503084
Test - acc:         0.464600 loss:        1.478285
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.459800 loss:        1.492314
Test - acc:         0.477000 loss:        1.460210
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.463920 loss:        1.480034
Test - acc:         0.431500 loss:        1.558259
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.465900 loss:        1.473032
Test - acc:         0.344000 loss:        1.837054
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.471040 loss:        1.457645
Test - acc:         0.473500 loss:        1.463910
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.469820 loss:        1.453613
Test - acc:         0.392100 loss:        1.662608
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.472760 loss:        1.445539
Test - acc:         0.468300 loss:        1.477594
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.479440 loss:        1.436443
Test - acc:         0.410200 loss:        1.661999
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.491260 loss:        1.423877
Test - acc:         0.500300 loss:        1.400892
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.496300 loss:        1.413290
Test - acc:         0.502700 loss:        1.410090
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.506500 loss:        1.390041
Test - acc:         0.518400 loss:        1.348006
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.506420 loss:        1.382392
Test - acc:         0.500300 loss:        1.396053
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.513840 loss:        1.369136
Test - acc:         0.494400 loss:        1.422233
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.516880 loss:        1.356310
Test - acc:         0.463800 loss:        1.493582
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.521480 loss:        1.350231
Test - acc:         0.542300 loss:        1.299319
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.526580 loss:        1.339727
Test - acc:         0.539900 loss:        1.297227
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.524360 loss:        1.334752
Test - acc:         0.533700 loss:        1.308399
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.527300 loss:        1.332192
Test - acc:         0.543900 loss:        1.292353
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.527960 loss:        1.321876
Test - acc:         0.524900 loss:        1.327432
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.534420 loss:        1.310298
Test - acc:         0.533400 loss:        1.298219
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.536580 loss:        1.302787
Test - acc:         0.482900 loss:        1.440429
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.535300 loss:        1.300696
Test - acc:         0.520100 loss:        1.355006
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.620730
Test - acc:         0.100000 loss:        2.388980
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.348393
Test - acc:         0.100000 loss:        2.322150
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.315907
Test - acc:         0.100000 loss:        2.309966
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.099400 loss:        2.308639
Test - acc:         0.100000 loss:        2.320651
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.098880 loss:        2.305710
Test - acc:         0.100000 loss:        2.347834
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.098920 loss:        2.304979
Test - acc:         0.100000 loss:        2.309271
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.099180 loss:        2.304412
Test - acc:         0.100000 loss:        2.303202
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.098300 loss:        2.304642
Test - acc:         0.100000 loss:        2.302789
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.097860 loss:        2.304241
Test - acc:         0.100000 loss:        2.304139
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.101160 loss:        2.303736
Test - acc:         0.100000 loss:        2.303759
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.100820 loss:        2.303750
Test - acc:         0.100000 loss:        2.304827
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.097560 loss:        2.303791
Test - acc:         0.100000 loss:        2.305435
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.097800 loss:        2.303399
Test - acc:         0.100000 loss:        2.306421
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.099840 loss:        2.303291
Test - acc:         0.100000 loss:        2.303071
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.100100 loss:        2.303119
Test - acc:         0.100000 loss:        2.313126
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.098660 loss:        2.303350
Test - acc:         0.100000 loss:        2.317170
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.100120 loss:        2.303267
Test - acc:         0.100000 loss:        2.303458
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.099200 loss:        2.303274
Test - acc:         0.100000 loss:        2.303551
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.100200 loss:        2.303079
Test - acc:         0.100000 loss:        2.310184
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.097660 loss:        2.303248
Test - acc:         0.100000 loss:        2.303620
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.096860 loss:        2.303235
Test - acc:         0.100000 loss:        2.303638
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.100020 loss:        2.303208
Test - acc:         0.100000 loss:        2.306446
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.099640 loss:        2.303183
Test - acc:         0.100000 loss:        2.303998
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.101360 loss:        2.303231
Test - acc:         0.100000 loss:        2.302705
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.099640 loss:        2.303041
Test - acc:         0.100000 loss:        2.307130
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.100500 loss:        2.303098
Test - acc:         0.100000 loss:        2.302831
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.098320 loss:        2.303220
Test - acc:         0.100000 loss:        2.303270
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.100020 loss:        2.302975
Test - acc:         0.100000 loss:        2.303266
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.098980 loss:        2.303204
Test - acc:         0.100000 loss:        2.302814
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.098640 loss:        2.303210
Test - acc:         0.100000 loss:        2.302818
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.098180 loss:        2.303043
Test - acc:         0.100000 loss:        2.302721
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.100480 loss:        2.303106
Test - acc:         0.100000 loss:        2.303295
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.098720 loss:        2.303039
Test - acc:         0.100000 loss:        2.303121
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.101020 loss:        2.302993
Test - acc:         0.100000 loss:        2.304749
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.100240 loss:        2.303123
Test - acc:         0.100000 loss:        2.302853
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.100100 loss:        2.302947
Test - acc:         0.100000 loss:        2.302991
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.098820 loss:        2.303013
Test - acc:         0.100000 loss:        2.304257
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.099480 loss:        2.302942
Test - acc:         0.100000 loss:        2.302955
Sparsity :          0.9961
Wdecay :        0.000500
