Running --prune_criterion global_magnitude --seed 43 --prune_freq 70 --prune_rate 0.5 --comment=vgg19_crit=global_magnitude_pf=70_seed=43 --save_model=pre-finetune/vgg19_global_magnitude_pf70_s43
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "global_magnitude",
    "prune_freq": 70,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_global_magnitude_pf70_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.105300 loss:        2.601138
Test - acc:         0.109400 loss:        2.297950
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.109500 loss:        2.296811
Test - acc:         0.121000 loss:        2.290626
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.135060 loss:        2.237199
Test - acc:         0.195300 loss:        2.051474
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.236160 loss:        1.912667
Test - acc:         0.262600 loss:        1.785715
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.333360 loss:        1.685028
Test - acc:         0.349500 loss:        1.665372
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.444340 loss:        1.475077
Test - acc:         0.474900 loss:        1.444693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.547480 loss:        1.252955
Test - acc:         0.515300 loss:        1.357213
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.628240 loss:        1.060720
Test - acc:         0.621900 loss:        1.115540
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.682960 loss:        0.919136
Test - acc:         0.642600 loss:        1.043907
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.722260 loss:        0.830122
Test - acc:         0.668000 loss:        1.003053
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.744180 loss:        0.775274
Test - acc:         0.625800 loss:        1.237646
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.763340 loss:        0.715959
Test - acc:         0.698600 loss:        0.897794
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.772800 loss:        0.692598
Test - acc:         0.643300 loss:        1.246807
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.783280 loss:        0.663185
Test - acc:         0.740300 loss:        0.836798
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.791500 loss:        0.640620
Test - acc:         0.727400 loss:        0.895406
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.793680 loss:        0.635948
Test - acc:         0.722900 loss:        0.857718
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.801200 loss:        0.611050
Test - acc:         0.686800 loss:        1.000325
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.803660 loss:        0.604088
Test - acc:         0.787200 loss:        0.699029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.808180 loss:        0.589044
Test - acc:         0.729800 loss:        0.866465
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.818440 loss:        0.569596
Test - acc:         0.746700 loss:        0.787903
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.817640 loss:        0.565120
Test - acc:         0.740400 loss:        0.878687
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.818520 loss:        0.559278
Test - acc:         0.729600 loss:        0.940122
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.825000 loss:        0.546039
Test - acc:         0.771700 loss:        0.706058
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.829120 loss:        0.535224
Test - acc:         0.712800 loss:        0.952716
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.826080 loss:        0.540991
Test - acc:         0.765300 loss:        0.804311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.829180 loss:        0.530004
Test - acc:         0.777800 loss:        0.723334
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.834860 loss:        0.515444
Test - acc:         0.759500 loss:        0.789311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.833080 loss:        0.519963
Test - acc:         0.705800 loss:        1.057780
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.838420 loss:        0.500873
Test - acc:         0.757500 loss:        0.824629
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.839660 loss:        0.498996
Test - acc:         0.768500 loss:        0.803411
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.840520 loss:        0.494644
Test - acc:         0.676400 loss:        1.210812
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.840660 loss:        0.494944
Test - acc:         0.712200 loss:        1.064473
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.839980 loss:        0.493296
Test - acc:         0.793100 loss:        0.669570
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.843080 loss:        0.486499
Test - acc:         0.807000 loss:        0.617336
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.843280 loss:        0.483888
Test - acc:         0.794200 loss:        0.667158
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.843480 loss:        0.484887
Test - acc:         0.765100 loss:        0.799214
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.843420 loss:        0.488633
Test - acc:         0.765000 loss:        0.769964
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.845880 loss:        0.479952
Test - acc:         0.704900 loss:        1.180704
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.847740 loss:        0.470945
Test - acc:         0.763200 loss:        0.836014
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.848960 loss:        0.468243
Test - acc:         0.767700 loss:        0.790092
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.849860 loss:        0.472595
Test - acc:         0.799800 loss:        0.640374
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.851620 loss:        0.464365
Test - acc:         0.737900 loss:        1.000664
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.849000 loss:        0.469655
Test - acc:         0.794700 loss:        0.648971
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.852740 loss:        0.458308
Test - acc:         0.772600 loss:        0.740477
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.851760 loss:        0.460629
Test - acc:         0.764200 loss:        0.768356
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.852440 loss:        0.459701
Test - acc:         0.824600 loss:        0.551497
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.853960 loss:        0.455432
Test - acc:         0.770000 loss:        0.753559
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.853760 loss:        0.450759
Test - acc:         0.765000 loss:        0.778837
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.852020 loss:        0.453369
Test - acc:         0.763100 loss:        0.781117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.853780 loss:        0.454691
Test - acc:         0.786800 loss:        0.697041
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.855860 loss:        0.449234
Test - acc:         0.706300 loss:        1.038955
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.854740 loss:        0.450947
Test - acc:         0.760000 loss:        0.800164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.854380 loss:        0.448492
Test - acc:         0.745700 loss:        0.904669
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.857000 loss:        0.446824
Test - acc:         0.818900 loss:        0.570430
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.857600 loss:        0.441885
Test - acc:         0.677200 loss:        1.257951
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.856520 loss:        0.442883
Test - acc:         0.835100 loss:        0.516913
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.858440 loss:        0.436299
Test - acc:         0.766300 loss:        0.785000
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.855940 loss:        0.443078
Test - acc:         0.779200 loss:        0.680428
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.858600 loss:        0.436619
Test - acc:         0.802300 loss:        0.653361
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.858300 loss:        0.439553
Test - acc:         0.789100 loss:        0.669027
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.862160 loss:        0.422611
Test - acc:         0.781400 loss:        0.743858
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.859620 loss:        0.434089
Test - acc:         0.755400 loss:        0.795991
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.863180 loss:        0.430666
Test - acc:         0.843900 loss:        0.484440
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.862740 loss:        0.430180
Test - acc:         0.817900 loss:        0.579326
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.859260 loss:        0.435166
Test - acc:         0.797700 loss:        0.670136
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.862320 loss:        0.427011
Test - acc:         0.777300 loss:        0.777830
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.862340 loss:        0.430751
Test - acc:         0.779300 loss:        0.719918
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.864000 loss:        0.420051
Test - acc:         0.836300 loss:        0.506845
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.862700 loss:        0.424257
Test - acc:         0.764600 loss:        0.743010
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.863880 loss:        0.421460
Test - acc:         0.812900 loss:        0.626851
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.862960 loss:        0.423621
Test - acc:         0.774800 loss:        0.682367
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.866400 loss:        0.415846
Test - acc:         0.801500 loss:        0.649183
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.863900 loss:        0.418152
Test - acc:         0.769700 loss:        0.740128
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.865540 loss:        0.410350
Test - acc:         0.822800 loss:        0.534460
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.865980 loss:        0.412649
Test - acc:         0.818300 loss:        0.566983
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.865460 loss:        0.415791
Test - acc:         0.821500 loss:        0.558972
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.866020 loss:        0.415981
Test - acc:         0.820100 loss:        0.547698
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.868240 loss:        0.405846
Test - acc:         0.796800 loss:        0.668037
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.865880 loss:        0.409657
Test - acc:         0.785100 loss:        0.695907
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.867380 loss:        0.410862
Test - acc:         0.784300 loss:        0.653139
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.865740 loss:        0.407342
Test - acc:         0.804400 loss:        0.622229
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.868300 loss:        0.403418
Test - acc:         0.827500 loss:        0.557900
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.870280 loss:        0.401138
Test - acc:         0.790100 loss:        0.663927
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.869400 loss:        0.401179
Test - acc:         0.827900 loss:        0.538546
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.866460 loss:        0.402548
Test - acc:         0.730700 loss:        0.891631
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.871400 loss:        0.392912
Test - acc:         0.805300 loss:        0.599658
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.873400 loss:        0.386518
Test - acc:         0.798300 loss:        0.655564
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.869860 loss:        0.395724
Test - acc:         0.828800 loss:        0.525337
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.872340 loss:        0.392049
Test - acc:         0.828100 loss:        0.550041
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.870960 loss:        0.393924
Test - acc:         0.849300 loss:        0.449579
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.870260 loss:        0.391714
Test - acc:         0.822600 loss:        0.540145
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.872740 loss:        0.387887
Test - acc:         0.803100 loss:        0.611475
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.871160 loss:        0.386340
Test - acc:         0.795500 loss:        0.723539
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.871160 loss:        0.387199
Test - acc:         0.788500 loss:        0.696513
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.870420 loss:        0.393609
Test - acc:         0.813700 loss:        0.562070
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.873640 loss:        0.383573
Test - acc:         0.839600 loss:        0.497126
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.873180 loss:        0.385196
Test - acc:         0.820700 loss:        0.547787
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.872400 loss:        0.383993
Test - acc:         0.811600 loss:        0.606183
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.872300 loss:        0.382004
Test - acc:         0.795600 loss:        0.679940
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.874160 loss:        0.377345
Test - acc:         0.804700 loss:        0.594561
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.874720 loss:        0.377024
Test - acc:         0.799100 loss:        0.640151
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.875600 loss:        0.374945
Test - acc:         0.840400 loss:        0.494700
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.873860 loss:        0.382761
Test - acc:         0.792000 loss:        0.648925
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.874480 loss:        0.380591
Test - acc:         0.817400 loss:        0.558511
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.874540 loss:        0.374409
Test - acc:         0.851300 loss:        0.456518
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.875940 loss:        0.374537
Test - acc:         0.775100 loss:        0.712093
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.874280 loss:        0.377426
Test - acc:         0.802900 loss:        0.626789
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.872180 loss:        0.384311
Test - acc:         0.836500 loss:        0.503212
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.875260 loss:        0.378469
Test - acc:         0.806800 loss:        0.597980
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.877200 loss:        0.369773
Test - acc:         0.841300 loss:        0.494923
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.875500 loss:        0.371511
Test - acc:         0.827800 loss:        0.523347
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.876780 loss:        0.371166
Test - acc:         0.825700 loss:        0.529306
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.875700 loss:        0.376113
Test - acc:         0.807400 loss:        0.623907
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.877320 loss:        0.373082
Test - acc:         0.809700 loss:        0.572914
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.876560 loss:        0.370905
Test - acc:         0.818400 loss:        0.531273
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.876660 loss:        0.371798
Test - acc:         0.814500 loss:        0.605918
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.878960 loss:        0.368618
Test - acc:         0.848800 loss:        0.458882
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.874600 loss:        0.376389
Test - acc:         0.845600 loss:        0.488860
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.877140 loss:        0.370744
Test - acc:         0.832300 loss:        0.509330
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.874700 loss:        0.376481
Test - acc:         0.724500 loss:        1.040577
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.875220 loss:        0.373510
Test - acc:         0.808300 loss:        0.594116
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.878140 loss:        0.368128
Test - acc:         0.840800 loss:        0.478890
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.875500 loss:        0.372070
Test - acc:         0.827200 loss:        0.544513
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.877340 loss:        0.370321
Test - acc:         0.797000 loss:        0.666195
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.877220 loss:        0.367324
Test - acc:         0.808600 loss:        0.594816
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.875680 loss:        0.369837
Test - acc:         0.748800 loss:        0.866797
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.876760 loss:        0.371894
Test - acc:         0.846700 loss:        0.492515
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.880080 loss:        0.362697
Test - acc:         0.814800 loss:        0.647964
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.875240 loss:        0.374547
Test - acc:         0.761900 loss:        0.827053
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.880740 loss:        0.361126
Test - acc:         0.832000 loss:        0.511222
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.876940 loss:        0.370038
Test - acc:         0.798600 loss:        0.643893
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.876140 loss:        0.373079
Test - acc:         0.779200 loss:        0.702572
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.873280 loss:        0.380524
Test - acc:         0.824600 loss:        0.555229
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.878620 loss:        0.364330
Test - acc:         0.787300 loss:        0.664278
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.877140 loss:        0.368947
Test - acc:         0.796000 loss:        0.630448
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.877300 loss:        0.367557
Test - acc:         0.793800 loss:        0.632338
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.878200 loss:        0.367743
Test - acc:         0.791000 loss:        0.685856
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.876660 loss:        0.371473
Test - acc:         0.800400 loss:        0.624176
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.878360 loss:        0.364961
Test - acc:         0.813400 loss:        0.595644
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.877560 loss:        0.366425
Test - acc:         0.826700 loss:        0.563219
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.878380 loss:        0.366821
Test - acc:         0.100000 loss:       26.783725
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.876480 loss:        0.374008
Test - acc:         0.816700 loss:        0.570611
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.878300 loss:        0.368906
Test - acc:         0.820900 loss:        0.558698
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.876440 loss:        0.364549
Test - acc:         0.802700 loss:        0.624142
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.877440 loss:        0.370460
Test - acc:         0.829100 loss:        0.536127
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.878520 loss:        0.367367
Test - acc:         0.817800 loss:        0.533228
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.878620 loss:        0.366510
Test - acc:         0.825600 loss:        0.551520
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.879780 loss:        0.366249
Test - acc:         0.778100 loss:        0.714570
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.878260 loss:        0.366456
Test - acc:         0.793700 loss:        0.605645
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.876940 loss:        0.367795
Test - acc:         0.851500 loss:        0.458080
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.935980 loss:        0.192247
Test - acc:         0.919300 loss:        0.249521
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.953420 loss:        0.141327
Test - acc:         0.924500 loss:        0.236238
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.959640 loss:        0.120731
Test - acc:         0.927900 loss:        0.230850
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.964120 loss:        0.105924
Test - acc:         0.930400 loss:        0.228642
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.968020 loss:        0.096383
Test - acc:         0.928600 loss:        0.236023
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.971480 loss:        0.084401
Test - acc:         0.929200 loss:        0.232570
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.974320 loss:        0.077909
Test - acc:         0.923400 loss:        0.245215
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.976840 loss:        0.069339
Test - acc:         0.929000 loss:        0.238677
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.977840 loss:        0.064987
Test - acc:         0.927500 loss:        0.246706
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.979480 loss:        0.061700
Test - acc:         0.926600 loss:        0.261105
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.980600 loss:        0.058275
Test - acc:         0.927600 loss:        0.256886
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.982020 loss:        0.054437
Test - acc:         0.928200 loss:        0.259182
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.981660 loss:        0.053949
Test - acc:         0.927300 loss:        0.265440
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.982540 loss:        0.052127
Test - acc:         0.927700 loss:        0.263159
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.984060 loss:        0.048331
Test - acc:         0.921900 loss:        0.277166
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.984420 loss:        0.047106
Test - acc:         0.919900 loss:        0.304124
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.983880 loss:        0.047184
Test - acc:         0.922300 loss:        0.305348
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.983440 loss:        0.047938
Test - acc:         0.920000 loss:        0.294312
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.983640 loss:        0.047910
Test - acc:         0.922100 loss:        0.285061
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.984240 loss:        0.046265
Test - acc:         0.920900 loss:        0.295241
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.985180 loss:        0.045442
Test - acc:         0.919000 loss:        0.303258
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.984480 loss:        0.047921
Test - acc:         0.923600 loss:        0.287088
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.984720 loss:        0.045580
Test - acc:         0.919700 loss:        0.301886
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.983660 loss:        0.046940
Test - acc:         0.924800 loss:        0.289963
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.983260 loss:        0.049640
Test - acc:         0.909200 loss:        0.362495
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.983760 loss:        0.050422
Test - acc:         0.919100 loss:        0.307327
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.983760 loss:        0.049450
Test - acc:         0.920800 loss:        0.290801
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.983140 loss:        0.049469
Test - acc:         0.917100 loss:        0.313667
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.983060 loss:        0.050213
Test - acc:         0.917700 loss:        0.313179
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.982800 loss:        0.051688
Test - acc:         0.918100 loss:        0.312011
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.982620 loss:        0.050726
Test - acc:         0.908300 loss:        0.365680
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.981780 loss:        0.054481
Test - acc:         0.912400 loss:        0.343727
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.980900 loss:        0.057557
Test - acc:         0.910500 loss:        0.333350
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.981120 loss:        0.053955
Test - acc:         0.915600 loss:        0.329806
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.981860 loss:        0.053203
Test - acc:         0.913200 loss:        0.322703
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.980940 loss:        0.054311
Test - acc:         0.912700 loss:        0.337236
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.980960 loss:        0.056312
Test - acc:         0.910400 loss:        0.335201
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.981420 loss:        0.055044
Test - acc:         0.914100 loss:        0.319641
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.980500 loss:        0.058053
Test - acc:         0.911000 loss:        0.325701
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.980560 loss:        0.056755
Test - acc:         0.910500 loss:        0.337879
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.982220 loss:        0.051763
Test - acc:         0.918000 loss:        0.306368
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.979200 loss:        0.061847
Test - acc:         0.913000 loss:        0.321621
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.980820 loss:        0.055503
Test - acc:         0.914000 loss:        0.315662
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.981040 loss:        0.056766
Test - acc:         0.905000 loss:        0.352270
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.979220 loss:        0.061713
Test - acc:         0.912300 loss:        0.325277
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.980900 loss:        0.056653
Test - acc:         0.909300 loss:        0.351930
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.980540 loss:        0.058547
Test - acc:         0.909900 loss:        0.349481
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.980220 loss:        0.060732
Test - acc:         0.910000 loss:        0.356422
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.978700 loss:        0.061487
Test - acc:         0.904700 loss:        0.342230
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.980620 loss:        0.057443
Test - acc:         0.910700 loss:        0.322760
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.980720 loss:        0.057351
Test - acc:         0.914700 loss:        0.314548
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.059948
Test - acc:         0.914000 loss:        0.327459
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.979320 loss:        0.061988
Test - acc:         0.909100 loss:        0.347628
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.978900 loss:        0.063642
Test - acc:         0.913700 loss:        0.337620
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.979800 loss:        0.057656
Test - acc:         0.918300 loss:        0.303108
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.979080 loss:        0.061352
Test - acc:         0.902400 loss:        0.371736
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.979660 loss:        0.061795
Test - acc:         0.899900 loss:        0.357107
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.978840 loss:        0.061596
Test - acc:         0.905800 loss:        0.343016
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.980940 loss:        0.056445
Test - acc:         0.910000 loss:        0.331781
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.979880 loss:        0.060084
Test - acc:         0.915400 loss:        0.324436
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.982080 loss:        0.053564
Test - acc:         0.900800 loss:        0.390031
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.983760 loss:        0.048889
Test - acc:         0.910200 loss:        0.348597
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.981400 loss:        0.054296
Test - acc:         0.916000 loss:        0.331791
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.982940 loss:        0.050607
Test - acc:         0.915600 loss:        0.320272
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.983440 loss:        0.050648
Test - acc:         0.891200 loss:        0.405978
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.983340 loss:        0.051669
Test - acc:         0.914800 loss:        0.325644
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.983420 loss:        0.050050
Test - acc:         0.911500 loss:        0.325777
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.981320 loss:        0.055410
Test - acc:         0.908100 loss:        0.354746
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.983320 loss:        0.050303
Test - acc:         0.908100 loss:        0.352848
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.982720 loss:        0.052506
Test - acc:         0.900700 loss:        0.394982
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.980960 loss:        0.056430
Test - acc:         0.908600 loss:        0.344572
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.983060 loss:        0.051912
Test - acc:         0.904500 loss:        0.362121
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.982820 loss:        0.051895
Test - acc:         0.908500 loss:        0.344405
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.981500 loss:        0.054079
Test - acc:         0.907800 loss:        0.347467
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.981940 loss:        0.055527
Test - acc:         0.905500 loss:        0.370682
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.982320 loss:        0.053942
Test - acc:         0.902700 loss:        0.357385
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.981160 loss:        0.055685
Test - acc:         0.916200 loss:        0.316465
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.981320 loss:        0.054036
Test - acc:         0.899100 loss:        0.391390
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.982840 loss:        0.051811
Test - acc:         0.910600 loss:        0.339840
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.982940 loss:        0.052564
Test - acc:         0.906900 loss:        0.376627
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.981820 loss:        0.054446
Test - acc:         0.912400 loss:        0.343513
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.982860 loss:        0.050189
Test - acc:         0.916300 loss:        0.327845
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.981200 loss:        0.055121
Test - acc:         0.904400 loss:        0.378203
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.981920 loss:        0.054181
Test - acc:         0.908900 loss:        0.328924
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.981480 loss:        0.055012
Test - acc:         0.904900 loss:        0.379527
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.982420 loss:        0.053364
Test - acc:         0.906100 loss:        0.363871
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.983960 loss:        0.048807
Test - acc:         0.903000 loss:        0.375176
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.981220 loss:        0.056889
Test - acc:         0.913500 loss:        0.313030
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.982660 loss:        0.053163
Test - acc:         0.905600 loss:        0.366007
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.979940 loss:        0.058670
Test - acc:         0.907600 loss:        0.352255
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.980820 loss:        0.056029
Test - acc:         0.905300 loss:        0.378511
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.981640 loss:        0.054178
Test - acc:         0.911700 loss:        0.340480
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.982700 loss:        0.052967
Test - acc:         0.902700 loss:        0.372659
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.981740 loss:        0.053715
Test - acc:         0.909900 loss:        0.337055
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.984000 loss:        0.050744
Test - acc:         0.911400 loss:        0.332089
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.982500 loss:        0.053044
Test - acc:         0.916500 loss:        0.322345
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.982940 loss:        0.051797
Test - acc:         0.909400 loss:        0.345622
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.056353
Test - acc:         0.905900 loss:        0.347057
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.983340 loss:        0.050345
Test - acc:         0.904800 loss:        0.357649
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.981240 loss:        0.056133
Test - acc:         0.897400 loss:        0.395636
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.991820 loss:        0.025998
Test - acc:         0.931200 loss:        0.259643
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.996500 loss:        0.013302
Test - acc:         0.931600 loss:        0.259650
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.997140 loss:        0.010589
Test - acc:         0.933400 loss:        0.260319
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.998080 loss:        0.008374
Test - acc:         0.933800 loss:        0.258149
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.998580 loss:        0.007050
Test - acc:         0.934900 loss:        0.257739
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.998280 loss:        0.006459
Test - acc:         0.935400 loss:        0.261467
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.999100 loss:        0.005194
Test - acc:         0.936000 loss:        0.264716
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.999080 loss:        0.004979
Test - acc:         0.935900 loss:        0.266506
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.999040 loss:        0.004954
Test - acc:         0.935400 loss:        0.266091
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.998980 loss:        0.004695
Test - acc:         0.937200 loss:        0.266180
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.003903
Test - acc:         0.935600 loss:        0.266976
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.004061
Test - acc:         0.936000 loss:        0.267704
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.003554
Test - acc:         0.935900 loss:        0.267017
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.999260 loss:        0.003933
Test - acc:         0.935700 loss:        0.271925
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.003168
Test - acc:         0.935400 loss:        0.275857
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.003037
Test - acc:         0.935500 loss:        0.273478
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.003342
Test - acc:         0.936300 loss:        0.274061
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999380 loss:        0.003176
Test - acc:         0.936700 loss:        0.275537
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.002729
Test - acc:         0.936000 loss:        0.274546
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.002583
Test - acc:         0.938000 loss:        0.275577
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.003070
Test - acc:         0.936900 loss:        0.275188
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.002518
Test - acc:         0.936600 loss:        0.276160
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.002605
Test - acc:         0.936600 loss:        0.277148
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.002462
Test - acc:         0.936500 loss:        0.276408
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.002448
Test - acc:         0.936000 loss:        0.278173
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.002431
Test - acc:         0.936600 loss:        0.276105
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002220
Test - acc:         0.937400 loss:        0.278321
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.002301
Test - acc:         0.937000 loss:        0.281172
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.002368
Test - acc:         0.938400 loss:        0.276501
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.002188
Test - acc:         0.939000 loss:        0.277123
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.003795
Test - acc:         0.935800 loss:        0.283793
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.002941
Test - acc:         0.937200 loss:        0.283277
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.002584
Test - acc:         0.937100 loss:        0.282541
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.002461
Test - acc:         0.936500 loss:        0.283980
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.002339
Test - acc:         0.936600 loss:        0.285229
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.002592
Test - acc:         0.936200 loss:        0.283392
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.002388
Test - acc:         0.937100 loss:        0.284300
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.002419
Test - acc:         0.937800 loss:        0.283170
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.002602
Test - acc:         0.936800 loss:        0.282824
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002120
Test - acc:         0.936100 loss:        0.285026
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.002362
Test - acc:         0.936200 loss:        0.288765
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.002206
Test - acc:         0.936600 loss:        0.287322
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.002041
Test - acc:         0.936600 loss:        0.286057
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001865
Test - acc:         0.936500 loss:        0.287081
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.001913
Test - acc:         0.936900 loss:        0.284501
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.002132
Test - acc:         0.936800 loss:        0.285587
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.001678
Test - acc:         0.937000 loss:        0.287302
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.001913
Test - acc:         0.936400 loss:        0.287622
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.001977
Test - acc:         0.938100 loss:        0.287807
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001750
Test - acc:         0.936800 loss:        0.290077
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.001797
Test - acc:         0.937500 loss:        0.288738
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.001693
Test - acc:         0.937000 loss:        0.287316
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.001744
Test - acc:         0.938600 loss:        0.286139
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001625
Test - acc:         0.938100 loss:        0.284361
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001616
Test - acc:         0.938500 loss:        0.284682
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001614
Test - acc:         0.938500 loss:        0.284998
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001683
Test - acc:         0.938500 loss:        0.286495
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001687
Test - acc:         0.937700 loss:        0.286785
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.001632
Test - acc:         0.937000 loss:        0.289757
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001541
Test - acc:         0.937200 loss:        0.287874
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.001348
Test - acc:         0.937000 loss:        0.289327
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001529
Test - acc:         0.936800 loss:        0.287909
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.001384
Test - acc:         0.937000 loss:        0.288206
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001520
Test - acc:         0.936600 loss:        0.288510
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001520
Test - acc:         0.936600 loss:        0.289217
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001474
Test - acc:         0.937100 loss:        0.290559
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.001722
Test - acc:         0.935400 loss:        0.292549
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.001562
Test - acc:         0.937800 loss:        0.289046
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.001681
Test - acc:         0.936800 loss:        0.289930
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.001673
Test - acc:         0.935900 loss:        0.290967
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001638
Test - acc:         0.936700 loss:        0.290180
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.001618
Test - acc:         0.936900 loss:        0.290972
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001550
Test - acc:         0.937200 loss:        0.289460
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.001458
Test - acc:         0.937400 loss:        0.289759
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.001504
Test - acc:         0.938200 loss:        0.287645
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.001570
Test - acc:         0.938900 loss:        0.289105
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.001571
Test - acc:         0.937900 loss:        0.286122
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001376
Test - acc:         0.937800 loss:        0.289371
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001392
Test - acc:         0.936500 loss:        0.289681
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001429
Test - acc:         0.936700 loss:        0.288480
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.001324
Test - acc:         0.937700 loss:        0.289409
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.001329
Test - acc:         0.937200 loss:        0.291755
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001388
Test - acc:         0.936900 loss:        0.290424
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001383
Test - acc:         0.937000 loss:        0.290439
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001328
Test - acc:         0.936400 loss:        0.288919
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001400
Test - acc:         0.936700 loss:        0.289631
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.001520
Test - acc:         0.936700 loss:        0.291521
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001390
Test - acc:         0.936900 loss:        0.290988
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001608
Test - acc:         0.936900 loss:        0.288899
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001321
Test - acc:         0.937500 loss:        0.288719
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001444
Test - acc:         0.937200 loss:        0.291443
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.001344
Test - acc:         0.936800 loss:        0.289078
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.001344
Test - acc:         0.936200 loss:        0.290149
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001199
Test - acc:         0.937500 loss:        0.290471
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001251
Test - acc:         0.936500 loss:        0.290389
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.001414
Test - acc:         0.936100 loss:        0.290510
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.001454
Test - acc:         0.937100 loss:        0.289097
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.001240
Test - acc:         0.937700 loss:        0.289754
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001234
Test - acc:         0.936100 loss:        0.290504
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001440
Test - acc:         0.937100 loss:        0.291007
Sparsity :          0.9375
Wdecay :        0.000500
