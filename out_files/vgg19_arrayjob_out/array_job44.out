Running --prune_criterion topflip --seed 44 --prune_freq 117 --prune_rate 0.5 --noise --comment=vgg19_crit=topflip_pf=117_seed=44 --save_model=pre-finetune/vgg19_topflip_pf117_s44
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "topflip",
    "prune_freq": 117,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_topflip_pf117_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.137260 loss:        2.667569
Test - acc:         0.211500 loss:        2.023356
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.256020 loss:        1.887445
Test - acc:         0.280600 loss:        1.794826
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.345700 loss:        1.656821
Test - acc:         0.337900 loss:        1.769398
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.455800 loss:        1.446620
Test - acc:         0.454200 loss:        1.529671
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.543560 loss:        1.267446
Test - acc:         0.505100 loss:        1.568276
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.624560 loss:        1.079909
Test - acc:         0.550900 loss:        1.411508
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.676180 loss:        0.942810
Test - acc:         0.652000 loss:        1.007693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.717300 loss:        0.841669
Test - acc:         0.624700 loss:        1.098031
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.741160 loss:        0.773259
Test - acc:         0.690500 loss:        1.019832
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.753000 loss:        0.747996
Test - acc:         0.693300 loss:        1.008944
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.772420 loss:        0.699642
Test - acc:         0.740000 loss:        0.769609
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.776320 loss:        0.679105
Test - acc:         0.753200 loss:        0.734386
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.781680 loss:        0.660457
Test - acc:         0.755400 loss:        0.744925
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.789220 loss:        0.647650
Test - acc:         0.701500 loss:        0.918221
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.793860 loss:        0.632406
Test - acc:         0.759700 loss:        0.735980
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.799420 loss:        0.622444
Test - acc:         0.736800 loss:        0.817756
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.798760 loss:        0.615050
Test - acc:         0.761500 loss:        0.751957
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.805560 loss:        0.596833
Test - acc:         0.735900 loss:        0.823823
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.810440 loss:        0.585183
Test - acc:         0.802200 loss:        0.601473
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.813080 loss:        0.577158
Test - acc:         0.778100 loss:        0.716816
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.814880 loss:        0.570712
Test - acc:         0.750100 loss:        0.804307
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.814300 loss:        0.566982
Test - acc:         0.700000 loss:        1.039362
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.817060 loss:        0.560618
Test - acc:         0.772300 loss:        0.745864
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.818640 loss:        0.559881
Test - acc:         0.786100 loss:        0.664755
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.819240 loss:        0.554396
Test - acc:         0.695200 loss:        0.987231
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.820840 loss:        0.550342
Test - acc:         0.748100 loss:        0.816494
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.825460 loss:        0.535709
Test - acc:         0.685600 loss:        1.034444
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.824200 loss:        0.540730
Test - acc:         0.699700 loss:        1.061181
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.825100 loss:        0.535450
Test - acc:         0.791300 loss:        0.636695
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.827000 loss:        0.532373
Test - acc:         0.727800 loss:        0.886447
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.827040 loss:        0.529372
Test - acc:         0.780800 loss:        0.687328
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.829920 loss:        0.524042
Test - acc:         0.781400 loss:        0.670314
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.830160 loss:        0.519633
Test - acc:         0.766400 loss:        0.734418
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.827920 loss:        0.526946
Test - acc:         0.780000 loss:        0.703194
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.830200 loss:        0.519099
Test - acc:         0.729700 loss:        0.954806
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.830820 loss:        0.519329
Test - acc:         0.758700 loss:        0.762857
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.829660 loss:        0.521598
Test - acc:         0.740800 loss:        0.792245
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.830660 loss:        0.519712
Test - acc:         0.807200 loss:        0.587582
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.833600 loss:        0.508954
Test - acc:         0.769300 loss:        0.710903
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.835140 loss:        0.506883
Test - acc:         0.765200 loss:        0.737310
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.834200 loss:        0.507816
Test - acc:         0.662300 loss:        1.130078
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.834140 loss:        0.509828
Test - acc:         0.693100 loss:        1.114416
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.831540 loss:        0.515874
Test - acc:         0.766900 loss:        0.765682
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.836300 loss:        0.504681
Test - acc:         0.767600 loss:        0.705592
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.838760 loss:        0.502383
Test - acc:         0.791700 loss:        0.679111
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.835820 loss:        0.506036
Test - acc:         0.804200 loss:        0.625337
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.837620 loss:        0.501661
Test - acc:         0.819100 loss:        0.573511
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.839100 loss:        0.492399
Test - acc:         0.797300 loss:        0.675725
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.836240 loss:        0.499422
Test - acc:         0.785500 loss:        0.682400
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.835300 loss:        0.506438
Test - acc:         0.778600 loss:        0.732806
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.836540 loss:        0.499564
Test - acc:         0.718700 loss:        0.920249
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.838780 loss:        0.490605
Test - acc:         0.791900 loss:        0.640846
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.840240 loss:        0.490185
Test - acc:         0.827600 loss:        0.553310
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.841240 loss:        0.487837
Test - acc:         0.753600 loss:        0.811424
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.842580 loss:        0.483812
Test - acc:         0.772800 loss:        0.702913
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.839780 loss:        0.493299
Test - acc:         0.746300 loss:        0.782941
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.837380 loss:        0.498162
Test - acc:         0.740600 loss:        0.854342
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.837480 loss:        0.499759
Test - acc:         0.811400 loss:        0.592640
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.838680 loss:        0.495455
Test - acc:         0.747900 loss:        0.821449
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.838060 loss:        0.495820
Test - acc:         0.680100 loss:        1.143265
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.838280 loss:        0.495435
Test - acc:         0.774800 loss:        0.742549
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.840880 loss:        0.488794
Test - acc:         0.748900 loss:        0.845163
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.840180 loss:        0.487498
Test - acc:         0.719400 loss:        0.915089
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.838960 loss:        0.493926
Test - acc:         0.795300 loss:        0.664978
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.837480 loss:        0.498365
Test - acc:         0.756800 loss:        0.783656
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.841160 loss:        0.487489
Test - acc:         0.695600 loss:        1.014308
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.840800 loss:        0.491330
Test - acc:         0.819400 loss:        0.561720
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.840780 loss:        0.485777
Test - acc:         0.774400 loss:        0.732269
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.842740 loss:        0.484895
Test - acc:         0.771100 loss:        0.745542
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.841800 loss:        0.486207
Test - acc:         0.763900 loss:        0.746533
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.838240 loss:        0.493512
Test - acc:         0.815700 loss:        0.550526
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.840220 loss:        0.487635
Test - acc:         0.794700 loss:        0.640872
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.841540 loss:        0.482542
Test - acc:         0.758400 loss:        0.801865
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.841220 loss:        0.491589
Test - acc:         0.773900 loss:        0.705224
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.843820 loss:        0.479670
Test - acc:         0.798300 loss:        0.637588
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.843400 loss:        0.478286
Test - acc:         0.796900 loss:        0.635059
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.840320 loss:        0.486780
Test - acc:         0.777700 loss:        0.719779
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.842860 loss:        0.483448
Test - acc:         0.776100 loss:        0.702622
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.841900 loss:        0.484574
Test - acc:         0.804800 loss:        0.587541
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.838740 loss:        0.489365
Test - acc:         0.758100 loss:        0.851574
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.844700 loss:        0.473950
Test - acc:         0.706900 loss:        1.201757
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.840640 loss:        0.486589
Test - acc:         0.776900 loss:        0.711953
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.842900 loss:        0.480255
Test - acc:         0.813100 loss:        0.559725
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.843160 loss:        0.477786
Test - acc:         0.718300 loss:        0.946239
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.842420 loss:        0.483630
Test - acc:         0.796400 loss:        0.653664
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.843140 loss:        0.480670
Test - acc:         0.821300 loss:        0.588553
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.841880 loss:        0.485313
Test - acc:         0.753800 loss:        0.777169
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.845920 loss:        0.470506
Test - acc:         0.799400 loss:        0.634476
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.843980 loss:        0.479996
Test - acc:         0.805900 loss:        0.596356
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.843440 loss:        0.472649
Test - acc:         0.801300 loss:        0.658643
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.844280 loss:        0.475004
Test - acc:         0.752700 loss:        0.885641
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.844880 loss:        0.476238
Test - acc:         0.798700 loss:        0.613647
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.843460 loss:        0.477121
Test - acc:         0.811500 loss:        0.603601
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.843560 loss:        0.478016
Test - acc:         0.799400 loss:        0.647328
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.844640 loss:        0.474763
Test - acc:         0.757400 loss:        0.848416
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.844340 loss:        0.480133
Test - acc:         0.808200 loss:        0.615081
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.844700 loss:        0.476699
Test - acc:         0.781700 loss:        0.702032
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.842480 loss:        0.487110
Test - acc:         0.823200 loss:        0.554278
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.845760 loss:        0.472133
Test - acc:         0.748900 loss:        0.857286
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.846940 loss:        0.469545
Test - acc:         0.828100 loss:        0.558874
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.844940 loss:        0.474973
Test - acc:         0.802400 loss:        0.644642
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.844960 loss:        0.477110
Test - acc:         0.814100 loss:        0.580400
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.844200 loss:        0.477748
Test - acc:         0.830600 loss:        0.524503
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.844780 loss:        0.477571
Test - acc:         0.784000 loss:        0.673137
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.844800 loss:        0.474161
Test - acc:         0.787700 loss:        0.671402
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.845620 loss:        0.470053
Test - acc:         0.720600 loss:        0.966951
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.842880 loss:        0.484557
Test - acc:         0.725000 loss:        0.937293
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.841500 loss:        0.485101
Test - acc:         0.763400 loss:        0.732651
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.846040 loss:        0.468499
Test - acc:         0.805200 loss:        0.582719
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.846240 loss:        0.471259
Test - acc:         0.810900 loss:        0.612851
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.845700 loss:        0.472613
Test - acc:         0.786500 loss:        0.638850
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.846300 loss:        0.471363
Test - acc:         0.750800 loss:        0.830925
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.842000 loss:        0.480515
Test - acc:         0.770200 loss:        0.836128
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.841740 loss:        0.482603
Test - acc:         0.740300 loss:        0.962048
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.847900 loss:        0.466425
Test - acc:         0.807800 loss:        0.583146
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.845660 loss:        0.473471
Test - acc:         0.763200 loss:        0.843948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.848100 loss:        0.460518
Test - acc:         0.782900 loss:        0.760986
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.854980 loss:        0.440129
Test - acc:         0.832100 loss:        0.523521
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.860420 loss:        0.417024
Test - acc:         0.821300 loss:        0.544782
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.864760 loss:        0.409706
Test - acc:         0.747300 loss:        0.858733
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.864280 loss:        0.410055
Test - acc:         0.796500 loss:        0.628550
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.863420 loss:        0.411046
Test - acc:         0.805600 loss:        0.609965
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.863180 loss:        0.409567
Test - acc:         0.813900 loss:        0.589103
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.867380 loss:        0.401935
Test - acc:         0.801500 loss:        0.639084
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.865480 loss:        0.405287
Test - acc:         0.726500 loss:        0.956132
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.869020 loss:        0.397383
Test - acc:         0.787300 loss:        0.666510
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.869600 loss:        0.390397
Test - acc:         0.830500 loss:        0.536385
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.866420 loss:        0.402470
Test - acc:         0.813100 loss:        0.599097
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.869280 loss:        0.395441
Test - acc:         0.763300 loss:        0.780094
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.868060 loss:        0.401705
Test - acc:         0.821400 loss:        0.557018
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.866180 loss:        0.398537
Test - acc:         0.818800 loss:        0.578937
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.868220 loss:        0.400108
Test - acc:         0.830300 loss:        0.529587
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.867480 loss:        0.400512
Test - acc:         0.814700 loss:        0.555145
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.866620 loss:        0.396104
Test - acc:         0.813400 loss:        0.616731
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.866180 loss:        0.397000
Test - acc:         0.780000 loss:        0.737060
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.866920 loss:        0.402382
Test - acc:         0.848200 loss:        0.459796
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.396928
Test - acc:         0.802400 loss:        0.630220
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.869660 loss:        0.395773
Test - acc:         0.797200 loss:        0.626233
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.868100 loss:        0.401350
Test - acc:         0.829700 loss:        0.527732
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.869280 loss:        0.393576
Test - acc:         0.812200 loss:        0.606519
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.872580 loss:        0.385267
Test - acc:         0.833300 loss:        0.503926
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.868440 loss:        0.395898
Test - acc:         0.804300 loss:        0.610761
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.867960 loss:        0.392768
Test - acc:         0.828700 loss:        0.562561
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.867700 loss:        0.395243
Test - acc:         0.750100 loss:        0.928975
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.869100 loss:        0.397140
Test - acc:         0.769800 loss:        0.786472
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.868160 loss:        0.398305
Test - acc:         0.774500 loss:        0.743282
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.870500 loss:        0.394162
Test - acc:         0.766500 loss:        0.741714
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.870220 loss:        0.385778
Test - acc:         0.833000 loss:        0.516189
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.867940 loss:        0.395883
Test - acc:         0.758300 loss:        0.839910
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.869340 loss:        0.391423
Test - acc:         0.805100 loss:        0.602416
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.919640 loss:        0.240061
Test - acc:         0.905400 loss:        0.285922
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.933880 loss:        0.196012
Test - acc:         0.912100 loss:        0.262783
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.939460 loss:        0.178168
Test - acc:         0.914700 loss:        0.258899
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.944700 loss:        0.164650
Test - acc:         0.914400 loss:        0.260316
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.947400 loss:        0.155573
Test - acc:         0.914800 loss:        0.257449
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.950740 loss:        0.145778
Test - acc:         0.913600 loss:        0.272460
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.951060 loss:        0.141420
Test - acc:         0.913900 loss:        0.271689
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.954640 loss:        0.132871
Test - acc:         0.914800 loss:        0.270908
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.957140 loss:        0.126359
Test - acc:         0.915800 loss:        0.270376
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.957140 loss:        0.124098
Test - acc:         0.915200 loss:        0.277198
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.958900 loss:        0.120486
Test - acc:         0.911300 loss:        0.291094
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.960700 loss:        0.115297
Test - acc:         0.911200 loss:        0.281247
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.961300 loss:        0.111968
Test - acc:         0.910300 loss:        0.307594
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.963600 loss:        0.106659
Test - acc:         0.911200 loss:        0.306115
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.965740 loss:        0.099707
Test - acc:         0.912800 loss:        0.300734
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.963240 loss:        0.105017
Test - acc:         0.908400 loss:        0.313688
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.963420 loss:        0.104842
Test - acc:         0.910500 loss:        0.299475
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.965140 loss:        0.101657
Test - acc:         0.908700 loss:        0.311674
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.966720 loss:        0.099532
Test - acc:         0.909400 loss:        0.314065
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.965260 loss:        0.101033
Test - acc:         0.913000 loss:        0.312859
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.967600 loss:        0.094843
Test - acc:         0.911000 loss:        0.312124
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.965240 loss:        0.099798
Test - acc:         0.905200 loss:        0.327763
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.966460 loss:        0.097102
Test - acc:         0.908300 loss:        0.322380
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.966880 loss:        0.096899
Test - acc:         0.892800 loss:        0.368229
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.966140 loss:        0.096992
Test - acc:         0.909400 loss:        0.309069
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.966280 loss:        0.097131
Test - acc:         0.908900 loss:        0.321877
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.966320 loss:        0.097108
Test - acc:         0.901900 loss:        0.349025
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.966580 loss:        0.097638
Test - acc:         0.903000 loss:        0.351715
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.965120 loss:        0.101006
Test - acc:         0.902200 loss:        0.340230
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.966840 loss:        0.094972
Test - acc:         0.902200 loss:        0.345759
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.966080 loss:        0.097289
Test - acc:         0.902000 loss:        0.359317
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.965520 loss:        0.097852
Test - acc:         0.909900 loss:        0.320652
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.965540 loss:        0.099587
Test - acc:         0.900100 loss:        0.350379
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.966500 loss:        0.097195
Test - acc:         0.906500 loss:        0.341740
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.965840 loss:        0.098318
Test - acc:         0.904400 loss:        0.337223
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.968120 loss:        0.094157
Test - acc:         0.893800 loss:        0.380420
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.964980 loss:        0.102827
Test - acc:         0.904000 loss:        0.329299
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.967020 loss:        0.095927
Test - acc:         0.905800 loss:        0.336832
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.967460 loss:        0.095900
Test - acc:         0.907300 loss:        0.335334
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.967680 loss:        0.095713
Test - acc:         0.899200 loss:        0.364187
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.966660 loss:        0.095046
Test - acc:         0.901700 loss:        0.348158
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.963120 loss:        0.105758
Test - acc:         0.905100 loss:        0.339062
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.964480 loss:        0.100180
Test - acc:         0.896900 loss:        0.362842
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.965600 loss:        0.099029
Test - acc:         0.905400 loss:        0.335635
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.967420 loss:        0.097191
Test - acc:         0.899200 loss:        0.363129
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.966880 loss:        0.095174
Test - acc:         0.896500 loss:        0.373030
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.967000 loss:        0.095088
Test - acc:         0.891400 loss:        0.401264
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.965560 loss:        0.100635
Test - acc:         0.898600 loss:        0.363582
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.966160 loss:        0.099212
Test - acc:         0.898700 loss:        0.350473
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.965120 loss:        0.100504
Test - acc:         0.901500 loss:        0.349610
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.966660 loss:        0.097390
Test - acc:         0.904400 loss:        0.337006
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.964680 loss:        0.102930
Test - acc:         0.898300 loss:        0.356441
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.966180 loss:        0.100292
Test - acc:         0.892900 loss:        0.382439
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.966000 loss:        0.099025
Test - acc:         0.898800 loss:        0.353716
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.966880 loss:        0.094819
Test - acc:         0.904900 loss:        0.349500
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.966260 loss:        0.097826
Test - acc:         0.900200 loss:        0.375271
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.965140 loss:        0.102049
Test - acc:         0.899000 loss:        0.368499
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.965620 loss:        0.100160
Test - acc:         0.904700 loss:        0.330589
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.966860 loss:        0.097091
Test - acc:         0.897100 loss:        0.367872
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.965900 loss:        0.099668
Test - acc:         0.906900 loss:        0.328090
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.967740 loss:        0.094560
Test - acc:         0.883300 loss:        0.418633
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.965700 loss:        0.099931
Test - acc:         0.900500 loss:        0.366680
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.968260 loss:        0.093865
Test - acc:         0.903200 loss:        0.346005
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.965120 loss:        0.101295
Test - acc:         0.906100 loss:        0.341422
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.967240 loss:        0.097057
Test - acc:         0.904200 loss:        0.348503
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.969840 loss:        0.089645
Test - acc:         0.900000 loss:        0.365435
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.966600 loss:        0.096707
Test - acc:         0.901500 loss:        0.356323
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.967880 loss:        0.094126
Test - acc:         0.906600 loss:        0.346093
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.966580 loss:        0.097059
Test - acc:         0.894700 loss:        0.371249
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.964860 loss:        0.101796
Test - acc:         0.898700 loss:        0.365830
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.969100 loss:        0.091037
Test - acc:         0.907600 loss:        0.335834
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.966200 loss:        0.099367
Test - acc:         0.903500 loss:        0.345717
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.967120 loss:        0.094793
Test - acc:         0.902300 loss:        0.343722
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.968420 loss:        0.093786
Test - acc:         0.898200 loss:        0.372391
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.967220 loss:        0.094971
Test - acc:         0.905700 loss:        0.327361
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.967360 loss:        0.094334
Test - acc:         0.903700 loss:        0.337501
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.968560 loss:        0.094188
Test - acc:         0.905000 loss:        0.337516
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.968920 loss:        0.093900
Test - acc:         0.908500 loss:        0.325661
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.968800 loss:        0.090590
Test - acc:         0.903700 loss:        0.332823
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.966700 loss:        0.096764
Test - acc:         0.894400 loss:        0.363708
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.969100 loss:        0.090638
Test - acc:         0.898000 loss:        0.368857
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.968160 loss:        0.091205
Test - acc:         0.905000 loss:        0.351833
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.966660 loss:        0.096683
Test - acc:         0.900500 loss:        0.355068
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.968340 loss:        0.092812
Test - acc:         0.900800 loss:        0.369488
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.955940 loss:        0.125578
Test - acc:         0.907500 loss:        0.308892
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.961100 loss:        0.113102
Test - acc:         0.893400 loss:        0.359686
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.963400 loss:        0.107676
Test - acc:         0.905700 loss:        0.322930
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.963460 loss:        0.106538
Test - acc:         0.906100 loss:        0.324092
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.965180 loss:        0.100872
Test - acc:         0.902100 loss:        0.359343
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.966220 loss:        0.100910
Test - acc:         0.889900 loss:        0.394682
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.964740 loss:        0.101642
Test - acc:         0.904600 loss:        0.327818
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.966120 loss:        0.097357
Test - acc:         0.895700 loss:        0.363208
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.966520 loss:        0.098568
Test - acc:         0.901600 loss:        0.358394
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.965120 loss:        0.100727
Test - acc:         0.903400 loss:        0.349977
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.965580 loss:        0.099249
Test - acc:         0.899800 loss:        0.358112
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.966820 loss:        0.098069
Test - acc:         0.896900 loss:        0.368692
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.965320 loss:        0.101123
Test - acc:         0.899600 loss:        0.371774
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.968980 loss:        0.091618
Test - acc:         0.891900 loss:        0.387880
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.967460 loss:        0.096419
Test - acc:         0.905400 loss:        0.333308
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.967460 loss:        0.092349
Test - acc:         0.891600 loss:        0.398883
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.980100 loss:        0.059770
Test - acc:         0.919900 loss:        0.289635
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.985640 loss:        0.044935
Test - acc:         0.920200 loss:        0.291392
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.988520 loss:        0.036668
Test - acc:         0.921400 loss:        0.294540
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.989720 loss:        0.032614
Test - acc:         0.921300 loss:        0.300155
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.990360 loss:        0.029818
Test - acc:         0.922400 loss:        0.302058
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.990760 loss:        0.029183
Test - acc:         0.923900 loss:        0.304482
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.991560 loss:        0.026635
Test - acc:         0.921500 loss:        0.316564
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.991700 loss:        0.026081
Test - acc:         0.921400 loss:        0.314709
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.992680 loss:        0.023342
Test - acc:         0.923900 loss:        0.312558
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.993640 loss:        0.021231
Test - acc:         0.923400 loss:        0.318847
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.993380 loss:        0.021073
Test - acc:         0.923400 loss:        0.320633
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.993700 loss:        0.020466
Test - acc:         0.923800 loss:        0.320635
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.994240 loss:        0.019012
Test - acc:         0.921700 loss:        0.329169
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.993260 loss:        0.020996
Test - acc:         0.922900 loss:        0.330203
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.994520 loss:        0.018168
Test - acc:         0.922200 loss:        0.332410
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.994500 loss:        0.017606
Test - acc:         0.920800 loss:        0.335587
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.994900 loss:        0.017078
Test - acc:         0.922400 loss:        0.337669
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.995060 loss:        0.015919
Test - acc:         0.923200 loss:        0.334932
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.995160 loss:        0.016200
Test - acc:         0.923000 loss:        0.341620
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.995760 loss:        0.014795
Test - acc:         0.922800 loss:        0.336897
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.995380 loss:        0.014750
Test - acc:         0.923600 loss:        0.336740
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.995620 loss:        0.015197
Test - acc:         0.922700 loss:        0.341343
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.995100 loss:        0.015620
Test - acc:         0.921600 loss:        0.341649
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.995720 loss:        0.014078
Test - acc:         0.923200 loss:        0.341525
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.996080 loss:        0.013173
Test - acc:         0.924200 loss:        0.343391
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.995980 loss:        0.012795
Test - acc:         0.924600 loss:        0.346365
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.995840 loss:        0.013155
Test - acc:         0.924100 loss:        0.346415
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.995940 loss:        0.013286
Test - acc:         0.924900 loss:        0.349887
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.996460 loss:        0.012727
Test - acc:         0.923400 loss:        0.350543
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.996260 loss:        0.012296
Test - acc:         0.925500 loss:        0.349090
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.996360 loss:        0.012450
Test - acc:         0.924500 loss:        0.348524
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.997040 loss:        0.010508
Test - acc:         0.925800 loss:        0.349610
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.996900 loss:        0.010606
Test - acc:         0.924700 loss:        0.354569
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.996860 loss:        0.010104
Test - acc:         0.923300 loss:        0.359494
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.996280 loss:        0.011744
Test - acc:         0.923600 loss:        0.357170
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.996980 loss:        0.010303
Test - acc:         0.923300 loss:        0.354862
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.997240 loss:        0.010188
Test - acc:         0.923100 loss:        0.357766
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.996780 loss:        0.010705
Test - acc:         0.924500 loss:        0.357558
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.997140 loss:        0.009973
Test - acc:         0.921200 loss:        0.362952
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.997260 loss:        0.009291
Test - acc:         0.924400 loss:        0.353464
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.997220 loss:        0.009553
Test - acc:         0.924700 loss:        0.362638
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.996560 loss:        0.010821
Test - acc:         0.922300 loss:        0.362885
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.997180 loss:        0.009615
Test - acc:         0.922200 loss:        0.366144
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.997360 loss:        0.008616
Test - acc:         0.924300 loss:        0.364251
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.997120 loss:        0.009337
Test - acc:         0.925400 loss:        0.364151
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.997220 loss:        0.009055
Test - acc:         0.924300 loss:        0.366225
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.997620 loss:        0.008978
Test - acc:         0.924000 loss:        0.364615
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.997140 loss:        0.009128
Test - acc:         0.923400 loss:        0.372333
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.997320 loss:        0.009104
Test - acc:         0.924200 loss:        0.369099
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.997400 loss:        0.009206
Test - acc:         0.924500 loss:        0.368923
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.997300 loss:        0.008804
Test - acc:         0.924200 loss:        0.370774
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.997200 loss:        0.008994
Test - acc:         0.925500 loss:        0.367618
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.997420 loss:        0.009104
Test - acc:         0.924300 loss:        0.367670
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.997280 loss:        0.008505
Test - acc:         0.926100 loss:        0.364977
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.997620 loss:        0.008283
Test - acc:         0.923700 loss:        0.375314
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.997000 loss:        0.008970
Test - acc:         0.924700 loss:        0.367390
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.997860 loss:        0.007998
Test - acc:         0.923400 loss:        0.371538
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.997700 loss:        0.008275
Test - acc:         0.922800 loss:        0.372891
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.997720 loss:        0.007887
Test - acc:         0.922200 loss:        0.375583
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.997520 loss:        0.008430
Test - acc:         0.923200 loss:        0.374388
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.997420 loss:        0.008426
Test - acc:         0.922100 loss:        0.373659
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.997700 loss:        0.007884
Test - acc:         0.921700 loss:        0.374742
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.998060 loss:        0.007017
Test - acc:         0.924400 loss:        0.371057
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.997460 loss:        0.008794
Test - acc:         0.922400 loss:        0.373677
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.997580 loss:        0.007596
Test - acc:         0.924100 loss:        0.375904
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.997540 loss:        0.008000
Test - acc:         0.923300 loss:        0.373341
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.997780 loss:        0.007696
Test - acc:         0.923800 loss:        0.378156
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.997980 loss:        0.007408
Test - acc:         0.924100 loss:        0.374670
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.997740 loss:        0.007196
Test - acc:         0.922200 loss:        0.379499
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.997940 loss:        0.007956
Test - acc:         0.922500 loss:        0.378548
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.997800 loss:        0.007189
Test - acc:         0.924200 loss:        0.377814
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.997560 loss:        0.008431
Test - acc:         0.923500 loss:        0.378332
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.997720 loss:        0.007696
Test - acc:         0.925700 loss:        0.370636
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.997720 loss:        0.007712
Test - acc:         0.924800 loss:        0.374838
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.997740 loss:        0.007386
Test - acc:         0.924700 loss:        0.377600
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.997980 loss:        0.007001
Test - acc:         0.922800 loss:        0.380068
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.997620 loss:        0.007798
Test - acc:         0.923300 loss:        0.382250
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.998220 loss:        0.006212
Test - acc:         0.923200 loss:        0.382525
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.997980 loss:        0.006737
Test - acc:         0.922300 loss:        0.382909
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.998080 loss:        0.007092
Test - acc:         0.923700 loss:        0.387637
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.997400 loss:        0.008038
Test - acc:         0.922800 loss:        0.384756
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.997800 loss:        0.007408
Test - acc:         0.924400 loss:        0.377481
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.997860 loss:        0.006857
Test - acc:         0.921000 loss:        0.381826
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.997740 loss:        0.006850
Test - acc:         0.924500 loss:        0.379485
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.998260 loss:        0.006364
Test - acc:         0.925400 loss:        0.379683
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.997940 loss:        0.007144
Test - acc:         0.922400 loss:        0.384925
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.998180 loss:        0.006057
Test - acc:         0.922200 loss:        0.383258
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.997940 loss:        0.006798
Test - acc:         0.922400 loss:        0.386928
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.998020 loss:        0.007149
Test - acc:         0.924200 loss:        0.381502
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.997980 loss:        0.006466
Test - acc:         0.921900 loss:        0.390071
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.998220 loss:        0.006502
Test - acc:         0.925000 loss:        0.384598
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.997760 loss:        0.007595
Test - acc:         0.923400 loss:        0.382938
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.998260 loss:        0.006252
Test - acc:         0.924900 loss:        0.386798
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.997800 loss:        0.007317
Test - acc:         0.922900 loss:        0.391858
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.998140 loss:        0.006595
Test - acc:         0.923600 loss:        0.390755
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.998260 loss:        0.006067
Test - acc:         0.924000 loss:        0.387384
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.998040 loss:        0.006515
Test - acc:         0.922100 loss:        0.390395
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.997760 loss:        0.007243
Test - acc:         0.925800 loss:        0.383240
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.998040 loss:        0.006699
Test - acc:         0.924900 loss:        0.389661
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.998180 loss:        0.007063
Test - acc:         0.922900 loss:        0.388625
Sparsity :          0.7500
Wdecay :        0.000500
