Running --prune_criterion magnitude --seed 43 --prune_freq 70 --prune_rate 0.5 --comment=vgg19_crit=magnitude_pf=70_seed=43 --save_model=pre-finetune/vgg19_magnitude_pf70_s43
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "magnitude",
    "prune_freq": 70,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_magnitude_pf70_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.105300 loss:        2.601138
Test - acc:         0.109400 loss:        2.297950
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.109500 loss:        2.296811
Test - acc:         0.121000 loss:        2.290626
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.135060 loss:        2.237199
Test - acc:         0.195300 loss:        2.051474
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.236160 loss:        1.912667
Test - acc:         0.262600 loss:        1.785715
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.333360 loss:        1.685028
Test - acc:         0.349500 loss:        1.665372
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.444340 loss:        1.475077
Test - acc:         0.474900 loss:        1.444693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.547480 loss:        1.252955
Test - acc:         0.515300 loss:        1.357213
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.628240 loss:        1.060720
Test - acc:         0.621900 loss:        1.115540
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.682960 loss:        0.919136
Test - acc:         0.642600 loss:        1.043907
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.722260 loss:        0.830122
Test - acc:         0.668000 loss:        1.003053
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.744180 loss:        0.775274
Test - acc:         0.625800 loss:        1.237646
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.763340 loss:        0.715959
Test - acc:         0.698600 loss:        0.897794
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.772800 loss:        0.692598
Test - acc:         0.643300 loss:        1.246807
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.783280 loss:        0.663185
Test - acc:         0.740300 loss:        0.836798
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.791500 loss:        0.640620
Test - acc:         0.727400 loss:        0.895406
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.793680 loss:        0.635948
Test - acc:         0.722900 loss:        0.857718
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.801200 loss:        0.611050
Test - acc:         0.686800 loss:        1.000325
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.803660 loss:        0.604088
Test - acc:         0.787200 loss:        0.699029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.808180 loss:        0.589044
Test - acc:         0.729800 loss:        0.866465
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.818440 loss:        0.569596
Test - acc:         0.746700 loss:        0.787903
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.817640 loss:        0.565120
Test - acc:         0.740400 loss:        0.878687
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.818520 loss:        0.559278
Test - acc:         0.729600 loss:        0.940122
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.825000 loss:        0.546039
Test - acc:         0.771700 loss:        0.706058
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.829120 loss:        0.535224
Test - acc:         0.712800 loss:        0.952716
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.826080 loss:        0.540991
Test - acc:         0.765300 loss:        0.804311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.829180 loss:        0.530004
Test - acc:         0.777800 loss:        0.723334
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.834860 loss:        0.515444
Test - acc:         0.759500 loss:        0.789311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.833080 loss:        0.519963
Test - acc:         0.705800 loss:        1.057780
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.838420 loss:        0.500873
Test - acc:         0.757500 loss:        0.824629
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.839660 loss:        0.498996
Test - acc:         0.768500 loss:        0.803411
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.840520 loss:        0.494644
Test - acc:         0.676400 loss:        1.210812
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.840660 loss:        0.494944
Test - acc:         0.712200 loss:        1.064473
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.839980 loss:        0.493296
Test - acc:         0.793100 loss:        0.669570
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.843080 loss:        0.486499
Test - acc:         0.807000 loss:        0.617336
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.843280 loss:        0.483888
Test - acc:         0.794200 loss:        0.667158
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.843480 loss:        0.484887
Test - acc:         0.765100 loss:        0.799214
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.843420 loss:        0.488633
Test - acc:         0.765000 loss:        0.769964
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.845880 loss:        0.479952
Test - acc:         0.704900 loss:        1.180704
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.847740 loss:        0.470945
Test - acc:         0.763200 loss:        0.836014
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.848960 loss:        0.468243
Test - acc:         0.767700 loss:        0.790092
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.849860 loss:        0.472595
Test - acc:         0.799800 loss:        0.640374
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.851620 loss:        0.464365
Test - acc:         0.737900 loss:        1.000664
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.849000 loss:        0.469655
Test - acc:         0.794700 loss:        0.648971
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.852740 loss:        0.458308
Test - acc:         0.772600 loss:        0.740477
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.851760 loss:        0.460629
Test - acc:         0.764200 loss:        0.768356
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.852440 loss:        0.459701
Test - acc:         0.824600 loss:        0.551497
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.853960 loss:        0.455432
Test - acc:         0.770000 loss:        0.753559
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.853760 loss:        0.450759
Test - acc:         0.765000 loss:        0.778837
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.852020 loss:        0.453369
Test - acc:         0.763100 loss:        0.781117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.853780 loss:        0.454691
Test - acc:         0.786800 loss:        0.697041
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.855860 loss:        0.449234
Test - acc:         0.706300 loss:        1.038955
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.854740 loss:        0.450947
Test - acc:         0.760000 loss:        0.800164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.854380 loss:        0.448492
Test - acc:         0.745700 loss:        0.904669
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.857000 loss:        0.446824
Test - acc:         0.818900 loss:        0.570430
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.857600 loss:        0.441885
Test - acc:         0.677200 loss:        1.257951
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.856520 loss:        0.442883
Test - acc:         0.835100 loss:        0.516913
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.858440 loss:        0.436299
Test - acc:         0.766300 loss:        0.785000
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.855940 loss:        0.443078
Test - acc:         0.779200 loss:        0.680428
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.858600 loss:        0.436619
Test - acc:         0.802300 loss:        0.653361
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.858300 loss:        0.439553
Test - acc:         0.789100 loss:        0.669027
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.862160 loss:        0.422611
Test - acc:         0.781400 loss:        0.743858
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.859620 loss:        0.434089
Test - acc:         0.755400 loss:        0.795991
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.863180 loss:        0.430666
Test - acc:         0.843900 loss:        0.484440
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.862740 loss:        0.430180
Test - acc:         0.817900 loss:        0.579326
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.859260 loss:        0.435166
Test - acc:         0.797700 loss:        0.670136
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.862320 loss:        0.427011
Test - acc:         0.777300 loss:        0.777830
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.862340 loss:        0.430751
Test - acc:         0.779300 loss:        0.719918
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.864000 loss:        0.420051
Test - acc:         0.836300 loss:        0.506845
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.862700 loss:        0.424257
Test - acc:         0.764600 loss:        0.743010
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.863880 loss:        0.421460
Test - acc:         0.812900 loss:        0.626851
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.877940 loss:        0.376901
Test - acc:         0.817500 loss:        0.575529
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.880900 loss:        0.369834
Test - acc:         0.843800 loss:        0.500946
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.877960 loss:        0.376646
Test - acc:         0.803700 loss:        0.664730
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.877480 loss:        0.376769
Test - acc:         0.795800 loss:        0.638702
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.875040 loss:        0.385232
Test - acc:         0.770000 loss:        0.819598
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.878640 loss:        0.373963
Test - acc:         0.828100 loss:        0.522709
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.874960 loss:        0.380555
Test - acc:         0.844700 loss:        0.485903
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.875380 loss:        0.380284
Test - acc:         0.783700 loss:        0.671306
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.879620 loss:        0.369115
Test - acc:         0.779100 loss:        0.772598
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.875820 loss:        0.380112
Test - acc:         0.792400 loss:        0.645608
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.877100 loss:        0.378936
Test - acc:         0.767400 loss:        0.800107
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.880800 loss:        0.367881
Test - acc:         0.753000 loss:        0.866503
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.877340 loss:        0.375189
Test - acc:         0.839900 loss:        0.521185
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.878900 loss:        0.370639
Test - acc:         0.782200 loss:        0.702968
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.879000 loss:        0.367564
Test - acc:         0.806000 loss:        0.644689
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.880320 loss:        0.367092
Test - acc:         0.836000 loss:        0.494090
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.881860 loss:        0.359208
Test - acc:         0.848300 loss:        0.483073
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.880680 loss:        0.363714
Test - acc:         0.831900 loss:        0.538636
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.879760 loss:        0.363541
Test - acc:         0.828800 loss:        0.555017
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.881360 loss:        0.359405
Test - acc:         0.847600 loss:        0.472962
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.879740 loss:        0.364128
Test - acc:         0.789600 loss:        0.715139
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.882860 loss:        0.357848
Test - acc:         0.808200 loss:        0.596366
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.883720 loss:        0.350984
Test - acc:         0.804900 loss:        0.602874
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.879760 loss:        0.357967
Test - acc:         0.828800 loss:        0.551947
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.882200 loss:        0.356004
Test - acc:         0.762600 loss:        0.729929
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.884000 loss:        0.357029
Test - acc:         0.828200 loss:        0.526408
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.882580 loss:        0.356377
Test - acc:         0.804800 loss:        0.610441
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.882140 loss:        0.356416
Test - acc:         0.830300 loss:        0.530111
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.881620 loss:        0.353784
Test - acc:         0.795100 loss:        0.667102
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.884220 loss:        0.350410
Test - acc:         0.720100 loss:        0.953731
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.878380 loss:        0.367115
Test - acc:         0.809400 loss:        0.679848
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.884000 loss:        0.347469
Test - acc:         0.848600 loss:        0.480636
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.883080 loss:        0.355751
Test - acc:         0.838600 loss:        0.496227
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.883740 loss:        0.349891
Test - acc:         0.856500 loss:        0.437355
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.883960 loss:        0.351452
Test - acc:         0.798400 loss:        0.666794
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.883340 loss:        0.352550
Test - acc:         0.794500 loss:        0.629688
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.883640 loss:        0.349359
Test - acc:         0.840900 loss:        0.497359
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.883240 loss:        0.351966
Test - acc:         0.816700 loss:        0.570762
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.883260 loss:        0.349232
Test - acc:         0.829200 loss:        0.535161
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.887540 loss:        0.342387
Test - acc:         0.811600 loss:        0.609206
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.886140 loss:        0.344165
Test - acc:         0.821000 loss:        0.546373
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.885000 loss:        0.346619
Test - acc:         0.795600 loss:        0.663546
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.883580 loss:        0.348517
Test - acc:         0.831600 loss:        0.525110
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.885080 loss:        0.344121
Test - acc:         0.825600 loss:        0.554195
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.885960 loss:        0.344674
Test - acc:         0.798900 loss:        0.627683
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.884840 loss:        0.346451
Test - acc:         0.849000 loss:        0.481088
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.884920 loss:        0.344177
Test - acc:         0.831000 loss:        0.518279
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.885460 loss:        0.345446
Test - acc:         0.827400 loss:        0.548346
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.884460 loss:        0.341735
Test - acc:         0.829300 loss:        0.535306
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.882880 loss:        0.351602
Test - acc:         0.726100 loss:        0.965875
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.885620 loss:        0.346991
Test - acc:         0.807300 loss:        0.645333
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.884980 loss:        0.344336
Test - acc:         0.856400 loss:        0.432292
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.886600 loss:        0.343305
Test - acc:         0.865600 loss:        0.427664
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.884520 loss:        0.344097
Test - acc:         0.818600 loss:        0.574670
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.885760 loss:        0.344516
Test - acc:         0.860300 loss:        0.448435
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.884860 loss:        0.341586
Test - acc:         0.715900 loss:        0.931556
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.884540 loss:        0.349114
Test - acc:         0.811300 loss:        0.612303
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.885900 loss:        0.342879
Test - acc:         0.838200 loss:        0.519142
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.883240 loss:        0.348762
Test - acc:         0.788600 loss:        0.735194
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.886840 loss:        0.340090
Test - acc:         0.828100 loss:        0.519086
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.888240 loss:        0.338653
Test - acc:         0.840600 loss:        0.496814
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.882860 loss:        0.352536
Test - acc:         0.792600 loss:        0.698562
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.884320 loss:        0.347216
Test - acc:         0.776000 loss:        0.740987
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.885840 loss:        0.343489
Test - acc:         0.800400 loss:        0.657373
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.888920 loss:        0.335589
Test - acc:         0.826000 loss:        0.556882
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.885680 loss:        0.344308
Test - acc:         0.784900 loss:        0.648997
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.883560 loss:        0.347040
Test - acc:         0.768500 loss:        0.752001
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.884020 loss:        0.347367
Test - acc:         0.843200 loss:        0.509509
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.886540 loss:        0.342521
Test - acc:         0.728800 loss:        0.930521
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.885760 loss:        0.344620
Test - acc:         0.824800 loss:        0.570254
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.899320 loss:        0.302329
Test - acc:         0.866000 loss:        0.410831
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.896580 loss:        0.308161
Test - acc:         0.820400 loss:        0.578787
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.896700 loss:        0.312282
Test - acc:         0.791700 loss:        0.674435
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.893800 loss:        0.314569
Test - acc:         0.804200 loss:        0.656375
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.895180 loss:        0.310709
Test - acc:         0.793400 loss:        0.719879
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.892620 loss:        0.321005
Test - acc:         0.829200 loss:        0.537124
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.896460 loss:        0.313297
Test - acc:         0.820900 loss:        0.610195
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.894340 loss:        0.319470
Test - acc:         0.842700 loss:        0.475507
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.892880 loss:        0.316784
Test - acc:         0.850500 loss:        0.471175
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.894880 loss:        0.314695
Test - acc:         0.821800 loss:        0.593553
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.942620 loss:        0.174508
Test - acc:         0.919000 loss:        0.246320
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.957120 loss:        0.129746
Test - acc:         0.922300 loss:        0.238565
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.961740 loss:        0.112065
Test - acc:         0.924300 loss:        0.235795
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.966320 loss:        0.100354
Test - acc:         0.924000 loss:        0.239314
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.968760 loss:        0.092597
Test - acc:         0.925000 loss:        0.247164
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.970500 loss:        0.085685
Test - acc:         0.923700 loss:        0.249345
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.973540 loss:        0.078859
Test - acc:         0.924500 loss:        0.251173
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.976520 loss:        0.070526
Test - acc:         0.923000 loss:        0.257622
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.975900 loss:        0.070511
Test - acc:         0.922800 loss:        0.263343
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.978180 loss:        0.064915
Test - acc:         0.924900 loss:        0.267797
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.979100 loss:        0.060828
Test - acc:         0.921700 loss:        0.274868
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.980700 loss:        0.057927
Test - acc:         0.922000 loss:        0.269971
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.980560 loss:        0.056300
Test - acc:         0.924100 loss:        0.284211
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.982180 loss:        0.052472
Test - acc:         0.921300 loss:        0.287344
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.981940 loss:        0.052764
Test - acc:         0.921800 loss:        0.290542
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.981880 loss:        0.053192
Test - acc:         0.920600 loss:        0.281989
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.983240 loss:        0.049401
Test - acc:         0.923000 loss:        0.285044
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.982400 loss:        0.051031
Test - acc:         0.920300 loss:        0.296663
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.982540 loss:        0.050231
Test - acc:         0.922300 loss:        0.301545
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.982140 loss:        0.052726
Test - acc:         0.922700 loss:        0.285077
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.983260 loss:        0.049857
Test - acc:         0.915600 loss:        0.319262
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.981780 loss:        0.053463
Test - acc:         0.922900 loss:        0.290572
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.982740 loss:        0.051860
Test - acc:         0.919900 loss:        0.290343
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.983080 loss:        0.049978
Test - acc:         0.923300 loss:        0.278025
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.981160 loss:        0.056449
Test - acc:         0.918700 loss:        0.298688
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.982460 loss:        0.052588
Test - acc:         0.917800 loss:        0.321045
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.980640 loss:        0.056361
Test - acc:         0.922100 loss:        0.288572
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.980900 loss:        0.054872
Test - acc:         0.914300 loss:        0.316948
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.981660 loss:        0.053632
Test - acc:         0.913800 loss:        0.325559
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.057558
Test - acc:         0.912200 loss:        0.321118
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.981720 loss:        0.053866
Test - acc:         0.921500 loss:        0.296955
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.980620 loss:        0.056662
Test - acc:         0.917100 loss:        0.308378
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.979620 loss:        0.058445
Test - acc:         0.918500 loss:        0.300262
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.978800 loss:        0.062979
Test - acc:         0.918200 loss:        0.305109
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.979580 loss:        0.059065
Test - acc:         0.919700 loss:        0.298464
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.980680 loss:        0.058713
Test - acc:         0.909500 loss:        0.342207
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.980860 loss:        0.057056
Test - acc:         0.918700 loss:        0.307290
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.058786
Test - acc:         0.914200 loss:        0.321943
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.979900 loss:        0.060544
Test - acc:         0.907800 loss:        0.350961
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.978900 loss:        0.062919
Test - acc:         0.910300 loss:        0.326697
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.979260 loss:        0.060144
Test - acc:         0.908400 loss:        0.324480
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.978620 loss:        0.063263
Test - acc:         0.908500 loss:        0.337887
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.978360 loss:        0.062879
Test - acc:         0.907900 loss:        0.333700
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.978060 loss:        0.063513
Test - acc:         0.915100 loss:        0.321963
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.979140 loss:        0.060041
Test - acc:         0.912500 loss:        0.332513
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.977760 loss:        0.065217
Test - acc:         0.919400 loss:        0.298891
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.980000 loss:        0.057759
Test - acc:         0.915400 loss:        0.312200
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.977500 loss:        0.066279
Test - acc:         0.907200 loss:        0.337175
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.979880 loss:        0.059183
Test - acc:         0.910700 loss:        0.334280
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.978220 loss:        0.063379
Test - acc:         0.913900 loss:        0.311192
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.976900 loss:        0.064606
Test - acc:         0.915300 loss:        0.297620
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.977940 loss:        0.064336
Test - acc:         0.912800 loss:        0.327948
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.977760 loss:        0.063826
Test - acc:         0.903800 loss:        0.349980
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.976860 loss:        0.068362
Test - acc:         0.909500 loss:        0.346816
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.978680 loss:        0.064117
Test - acc:         0.911900 loss:        0.332833
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.977020 loss:        0.066955
Test - acc:         0.912400 loss:        0.321934
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.979280 loss:        0.060547
Test - acc:         0.915600 loss:        0.315948
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.978520 loss:        0.063540
Test - acc:         0.909500 loss:        0.336789
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.977160 loss:        0.067592
Test - acc:         0.913200 loss:        0.331285
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.978060 loss:        0.065237
Test - acc:         0.913800 loss:        0.316435
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.974400 loss:        0.075830
Test - acc:         0.909000 loss:        0.326956
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.979060 loss:        0.062943
Test - acc:         0.911300 loss:        0.330793
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.060158
Test - acc:         0.913800 loss:        0.334500
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.980400 loss:        0.058049
Test - acc:         0.914500 loss:        0.323607
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.979640 loss:        0.058897
Test - acc:         0.914500 loss:        0.317979
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.980660 loss:        0.057687
Test - acc:         0.921000 loss:        0.318509
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.980680 loss:        0.057629
Test - acc:         0.908000 loss:        0.357741
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.981240 loss:        0.056290
Test - acc:         0.912700 loss:        0.331934
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.981900 loss:        0.053501
Test - acc:         0.906000 loss:        0.370003
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.980080 loss:        0.058723
Test - acc:         0.911700 loss:        0.330567
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.981600 loss:        0.054606
Test - acc:         0.912600 loss:        0.334514
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.981040 loss:        0.055701
Test - acc:         0.910000 loss:        0.364424
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.980280 loss:        0.057678
Test - acc:         0.919800 loss:        0.314193
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.980660 loss:        0.056873
Test - acc:         0.914400 loss:        0.326572
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.980700 loss:        0.056080
Test - acc:         0.898800 loss:        0.401303
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.980240 loss:        0.059402
Test - acc:         0.904200 loss:        0.356333
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.981400 loss:        0.055639
Test - acc:         0.914200 loss:        0.323481
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.981060 loss:        0.057281
Test - acc:         0.907300 loss:        0.385021
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.979700 loss:        0.059772
Test - acc:         0.910700 loss:        0.337285
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.981040 loss:        0.057958
Test - acc:         0.915000 loss:        0.324739
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.979780 loss:        0.060236
Test - acc:         0.913800 loss:        0.317862
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.980860 loss:        0.056810
Test - acc:         0.906900 loss:        0.353768
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.979560 loss:        0.060133
Test - acc:         0.908700 loss:        0.354363
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.981500 loss:        0.054358
Test - acc:         0.909100 loss:        0.342901
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.981040 loss:        0.055979
Test - acc:         0.905400 loss:        0.375685
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.980440 loss:        0.057434
Test - acc:         0.901100 loss:        0.379592
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.980100 loss:        0.060898
Test - acc:         0.912600 loss:        0.325829
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.981520 loss:        0.055867
Test - acc:         0.911400 loss:        0.343088
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.980560 loss:        0.055781
Test - acc:         0.911600 loss:        0.354152
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.978820 loss:        0.063132
Test - acc:         0.910500 loss:        0.341155
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.980160 loss:        0.058558
Test - acc:         0.911900 loss:        0.336467
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.980160 loss:        0.059055
Test - acc:         0.914400 loss:        0.314747
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.980280 loss:        0.056582
Test - acc:         0.912000 loss:        0.346839
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.979820 loss:        0.056585
Test - acc:         0.904500 loss:        0.379119
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.979340 loss:        0.060116
Test - acc:         0.915100 loss:        0.325901
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.982000 loss:        0.053160
Test - acc:         0.906600 loss:        0.385784
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.980740 loss:        0.058530
Test - acc:         0.908100 loss:        0.374837
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.980020 loss:        0.057463
Test - acc:         0.909500 loss:        0.339964
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.980900 loss:        0.055858
Test - acc:         0.909500 loss:        0.350166
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.979380 loss:        0.059553
Test - acc:         0.908000 loss:        0.337051
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.990440 loss:        0.029887
Test - acc:         0.927200 loss:        0.272410
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.993840 loss:        0.020303
Test - acc:         0.929500 loss:        0.274511
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.995240 loss:        0.016073
Test - acc:         0.929500 loss:        0.276562
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.996380 loss:        0.013259
Test - acc:         0.929500 loss:        0.279469
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.996660 loss:        0.012509
Test - acc:         0.930100 loss:        0.278158
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.996820 loss:        0.011899
Test - acc:         0.931300 loss:        0.282623
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.997380 loss:        0.009986
Test - acc:         0.931700 loss:        0.284552
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.997620 loss:        0.009686
Test - acc:         0.931300 loss:        0.287136
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.997640 loss:        0.008965
Test - acc:         0.931600 loss:        0.290957
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.997800 loss:        0.008108
Test - acc:         0.930700 loss:        0.289546
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.998040 loss:        0.007668
Test - acc:         0.932400 loss:        0.293413
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.997480 loss:        0.008869
Test - acc:         0.931400 loss:        0.295493
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.998680 loss:        0.006421
Test - acc:         0.931500 loss:        0.295753
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.998260 loss:        0.007479
Test - acc:         0.931300 loss:        0.298155
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.998060 loss:        0.006710
Test - acc:         0.932900 loss:        0.300360
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.998200 loss:        0.006241
Test - acc:         0.932200 loss:        0.296381
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.998600 loss:        0.005962
Test - acc:         0.931900 loss:        0.299256
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.998440 loss:        0.006304
Test - acc:         0.932100 loss:        0.306040
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.998440 loss:        0.006173
Test - acc:         0.932900 loss:        0.301413
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.998600 loss:        0.005921
Test - acc:         0.932000 loss:        0.303281
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.998440 loss:        0.006057
Test - acc:         0.932800 loss:        0.302217
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.998580 loss:        0.005928
Test - acc:         0.934000 loss:        0.306899
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.998840 loss:        0.005444
Test - acc:         0.932100 loss:        0.307613
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.998480 loss:        0.005887
Test - acc:         0.932200 loss:        0.307369
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.998700 loss:        0.005392
Test - acc:         0.931900 loss:        0.311370
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.998740 loss:        0.005149
Test - acc:         0.933700 loss:        0.305876
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.998660 loss:        0.004932
Test - acc:         0.933300 loss:        0.307609
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.998760 loss:        0.004511
Test - acc:         0.933500 loss:        0.311604
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.004731
Test - acc:         0.933100 loss:        0.308493
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.004776
Test - acc:         0.934000 loss:        0.308562
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.964800 loss:        0.105999
Test - acc:         0.912300 loss:        0.319679
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.983300 loss:        0.051933
Test - acc:         0.915400 loss:        0.310304
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.986140 loss:        0.042853
Test - acc:         0.919400 loss:        0.308126
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.987440 loss:        0.038660
Test - acc:         0.919100 loss:        0.311180
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.989040 loss:        0.032666
Test - acc:         0.920200 loss:        0.311467
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.990400 loss:        0.032010
Test - acc:         0.919800 loss:        0.313561
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.991180 loss:        0.028519
Test - acc:         0.921200 loss:        0.312321
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.990880 loss:        0.028631
Test - acc:         0.920900 loss:        0.312240
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.991480 loss:        0.027190
Test - acc:         0.923400 loss:        0.310771
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.992560 loss:        0.024050
Test - acc:         0.922700 loss:        0.317802
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.992300 loss:        0.024219
Test - acc:         0.922900 loss:        0.313873
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.992560 loss:        0.023349
Test - acc:         0.921000 loss:        0.322350
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.993600 loss:        0.020357
Test - acc:         0.922300 loss:        0.320216
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.994240 loss:        0.019712
Test - acc:         0.924900 loss:        0.322154
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.994340 loss:        0.018842
Test - acc:         0.923800 loss:        0.327265
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.993780 loss:        0.020456
Test - acc:         0.922700 loss:        0.328577
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.994480 loss:        0.017769
Test - acc:         0.923300 loss:        0.321117
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.993960 loss:        0.018878
Test - acc:         0.924900 loss:        0.321753
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.994060 loss:        0.019369
Test - acc:         0.924500 loss:        0.322874
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.994500 loss:        0.017288
Test - acc:         0.923800 loss:        0.325374
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.994700 loss:        0.016305
Test - acc:         0.923700 loss:        0.331769
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.994600 loss:        0.016269
Test - acc:         0.924200 loss:        0.329958
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.995380 loss:        0.016313
Test - acc:         0.923600 loss:        0.326139
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.995120 loss:        0.015719
Test - acc:         0.925800 loss:        0.327212
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.995000 loss:        0.016433
Test - acc:         0.924300 loss:        0.328857
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.995840 loss:        0.014405
Test - acc:         0.922900 loss:        0.339107
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.995580 loss:        0.013880
Test - acc:         0.922500 loss:        0.345123
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.995240 loss:        0.015338
Test - acc:         0.922900 loss:        0.341054
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.995700 loss:        0.013923
Test - acc:         0.924500 loss:        0.337493
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.995700 loss:        0.013466
Test - acc:         0.923800 loss:        0.335756
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.996020 loss:        0.013219
Test - acc:         0.924100 loss:        0.337621
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.996320 loss:        0.013159
Test - acc:         0.923600 loss:        0.336478
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.996160 loss:        0.012697
Test - acc:         0.923000 loss:        0.340690
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.995920 loss:        0.012798
Test - acc:         0.923900 loss:        0.345212
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.996440 loss:        0.011480
Test - acc:         0.924000 loss:        0.344226
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.996200 loss:        0.012564
Test - acc:         0.923400 loss:        0.345915
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.996020 loss:        0.013133
Test - acc:         0.924300 loss:        0.349777
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.996060 loss:        0.012235
Test - acc:         0.922700 loss:        0.345242
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.996500 loss:        0.011927
Test - acc:         0.924100 loss:        0.349995
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.996360 loss:        0.012103
Test - acc:         0.922000 loss:        0.346415
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.996180 loss:        0.011855
Test - acc:         0.923700 loss:        0.343415
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.996780 loss:        0.010790
Test - acc:         0.925500 loss:        0.348689
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.997160 loss:        0.009561
Test - acc:         0.924300 loss:        0.347514
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.996540 loss:        0.011227
Test - acc:         0.926100 loss:        0.345035
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.996600 loss:        0.011057
Test - acc:         0.922800 loss:        0.349286
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.996760 loss:        0.010585
Test - acc:         0.926200 loss:        0.343218
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.996400 loss:        0.010917
Test - acc:         0.927700 loss:        0.343117
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.996520 loss:        0.010991
Test - acc:         0.925600 loss:        0.345564
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.997260 loss:        0.009425
Test - acc:         0.926000 loss:        0.346316
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.997100 loss:        0.009468
Test - acc:         0.925300 loss:        0.346313
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.996760 loss:        0.010141
Test - acc:         0.924100 loss:        0.350801
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.997100 loss:        0.009797
Test - acc:         0.925400 loss:        0.344993
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.997200 loss:        0.009006
Test - acc:         0.924900 loss:        0.349485
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.997400 loss:        0.009436
Test - acc:         0.923900 loss:        0.355431
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.997580 loss:        0.008610
Test - acc:         0.922700 loss:        0.352804
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.997020 loss:        0.010370
Test - acc:         0.923100 loss:        0.362152
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.997060 loss:        0.009224
Test - acc:         0.924400 loss:        0.353644
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.997020 loss:        0.009619
Test - acc:         0.925000 loss:        0.354229
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.997020 loss:        0.009915
Test - acc:         0.926200 loss:        0.345867
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.997360 loss:        0.008566
Test - acc:         0.924600 loss:        0.354095
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.997340 loss:        0.009488
Test - acc:         0.926000 loss:        0.349939
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.997020 loss:        0.009376
Test - acc:         0.925100 loss:        0.356443
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.996860 loss:        0.010419
Test - acc:         0.926100 loss:        0.351433
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.997460 loss:        0.008648
Test - acc:         0.925700 loss:        0.352312
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.997340 loss:        0.009582
Test - acc:         0.925300 loss:        0.352049
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.997600 loss:        0.008499
Test - acc:         0.926800 loss:        0.348789
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.997540 loss:        0.008016
Test - acc:         0.927000 loss:        0.348916
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.997220 loss:        0.009065
Test - acc:         0.926700 loss:        0.345379
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.997260 loss:        0.009222
Test - acc:         0.925900 loss:        0.349753
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.996940 loss:        0.009530
Test - acc:         0.925200 loss:        0.356709
Sparsity :          0.9375
Wdecay :        0.000500
