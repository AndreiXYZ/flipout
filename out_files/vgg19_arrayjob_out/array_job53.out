Running --prune_criterion global_magnitude --seed 44 --prune_freq 39 --prune_rate 0.5 --comment=vgg19_crit=global_magnitude_pf=39_seed=44 --save_model=pre-finetune/vgg19_global_magnitude_pf39_s44
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "global_magnitude",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_global_magnitude_pf39_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.108300 loss:        2.474734
Test - acc:         0.119300 loss:        2.267837
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.150540 loss:        2.171051
Test - acc:         0.194700 loss:        1.934882
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.241500 loss:        1.878880
Test - acc:         0.265700 loss:        1.874334
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.306100 loss:        1.759676
Test - acc:         0.321600 loss:        1.772883
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.438900 loss:        1.470889
Test - acc:         0.480900 loss:        1.401025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.559400 loss:        1.215320
Test - acc:         0.570900 loss:        1.236438
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.640540 loss:        1.022796
Test - acc:         0.562900 loss:        1.313802
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.692820 loss:        0.908134
Test - acc:         0.458400 loss:        1.942289
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.726620 loss:        0.830650
Test - acc:         0.698100 loss:        0.963013
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.750900 loss:        0.760590
Test - acc:         0.690900 loss:        0.970304
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.770860 loss:        0.699995
Test - acc:         0.733400 loss:        0.833592
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.780940 loss:        0.667889
Test - acc:         0.743900 loss:        0.787249
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.789600 loss:        0.649976
Test - acc:         0.738800 loss:        0.849763
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.796420 loss:        0.625437
Test - acc:         0.722200 loss:        0.895129
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.802720 loss:        0.608729
Test - acc:         0.740500 loss:        0.775364
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.806240 loss:        0.596568
Test - acc:         0.700700 loss:        1.022337
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.812280 loss:        0.587819
Test - acc:         0.763300 loss:        0.771137
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.812580 loss:        0.580530
Test - acc:         0.772300 loss:        0.710877
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.818180 loss:        0.560021
Test - acc:         0.742400 loss:        0.803548
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.821800 loss:        0.557051
Test - acc:         0.728400 loss:        0.884478
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.827740 loss:        0.535048
Test - acc:         0.737700 loss:        0.793409
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.828660 loss:        0.530681
Test - acc:         0.763400 loss:        0.765218
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.825700 loss:        0.538257
Test - acc:         0.761600 loss:        0.788449
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.826500 loss:        0.539644
Test - acc:         0.788700 loss:        0.688115
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.833200 loss:        0.521488
Test - acc:         0.765500 loss:        0.769116
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.835460 loss:        0.515609
Test - acc:         0.707000 loss:        0.956001
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.836300 loss:        0.512242
Test - acc:         0.744700 loss:        0.809497
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.839600 loss:        0.497366
Test - acc:         0.704800 loss:        1.055230
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.837900 loss:        0.502614
Test - acc:         0.815400 loss:        0.592117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.838720 loss:        0.504216
Test - acc:         0.766300 loss:        0.782455
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.846120 loss:        0.485764
Test - acc:         0.739600 loss:        0.860019
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.840980 loss:        0.499632
Test - acc:         0.767800 loss:        0.798063
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.844020 loss:        0.487687
Test - acc:         0.785900 loss:        0.677178
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.840220 loss:        0.499990
Test - acc:         0.769100 loss:        0.729718
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.844640 loss:        0.483766
Test - acc:         0.789700 loss:        0.670113
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.847620 loss:        0.479940
Test - acc:         0.780700 loss:        0.708346
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.847820 loss:        0.476032
Test - acc:         0.694800 loss:        0.955153
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.846740 loss:        0.480284
Test - acc:         0.744600 loss:        0.948408
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.849640 loss:        0.473404
Test - acc:         0.783500 loss:        0.690285
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.850420 loss:        0.466312
Test - acc:         0.789000 loss:        0.697928
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.848820 loss:        0.472106
Test - acc:         0.764900 loss:        0.803698
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.848060 loss:        0.478576
Test - acc:         0.775400 loss:        0.746601
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.853160 loss:        0.456609
Test - acc:         0.802000 loss:        0.612041
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.851780 loss:        0.460553
Test - acc:         0.772700 loss:        0.762347
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.854860 loss:        0.455677
Test - acc:         0.779400 loss:        0.689403
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.854680 loss:        0.454273
Test - acc:         0.815200 loss:        0.572956
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.856800 loss:        0.447120
Test - acc:         0.788400 loss:        0.683813
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.853360 loss:        0.445710
Test - acc:         0.772100 loss:        0.840051
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.857960 loss:        0.437551
Test - acc:         0.821700 loss:        0.603698
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.855580 loss:        0.446339
Test - acc:         0.796100 loss:        0.643442
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.859560 loss:        0.436508
Test - acc:         0.757000 loss:        0.815190
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.858980 loss:        0.435915
Test - acc:         0.799800 loss:        0.596154
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.862000 loss:        0.427408
Test - acc:         0.788700 loss:        0.703771
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.862240 loss:        0.425377
Test - acc:         0.811800 loss:        0.595629
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.862680 loss:        0.424117
Test - acc:         0.780400 loss:        0.732642
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.863480 loss:        0.427425
Test - acc:         0.760000 loss:        0.901071
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.859200 loss:        0.440572
Test - acc:         0.815600 loss:        0.580871
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.860980 loss:        0.427356
Test - acc:         0.795200 loss:        0.636816
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.861620 loss:        0.424168
Test - acc:         0.826500 loss:        0.528265
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.864820 loss:        0.416293
Test - acc:         0.802500 loss:        0.625578
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.860960 loss:        0.427995
Test - acc:         0.835900 loss:        0.519816
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.863180 loss:        0.419209
Test - acc:         0.833100 loss:        0.518547
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.864580 loss:        0.412295
Test - acc:         0.771700 loss:        0.702624
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.863940 loss:        0.414102
Test - acc:         0.812400 loss:        0.602459
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.866620 loss:        0.412408
Test - acc:         0.795200 loss:        0.672739
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.867620 loss:        0.413057
Test - acc:         0.800600 loss:        0.650941
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.866280 loss:        0.414635
Test - acc:         0.808300 loss:        0.645848
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.866580 loss:        0.411683
Test - acc:         0.815700 loss:        0.583580
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.867500 loss:        0.409101
Test - acc:         0.830600 loss:        0.511765
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.864920 loss:        0.409605
Test - acc:         0.829900 loss:        0.534724
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.865960 loss:        0.405068
Test - acc:         0.799100 loss:        0.647315
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.868780 loss:        0.402543
Test - acc:         0.832300 loss:        0.535883
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.867600 loss:        0.405769
Test - acc:         0.799000 loss:        0.636794
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.866960 loss:        0.408343
Test - acc:         0.738300 loss:        0.887062
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.869820 loss:        0.396255
Test - acc:         0.836600 loss:        0.504415
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.870500 loss:        0.398357
Test - acc:         0.813600 loss:        0.583775
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.867860 loss:        0.398270
Test - acc:         0.842500 loss:        0.511068
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.871840 loss:        0.391791
Test - acc:         0.830600 loss:        0.543826
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.870680 loss:        0.393167
Test - acc:         0.803700 loss:        0.598882
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.869440 loss:        0.396450
Test - acc:         0.783400 loss:        0.696332
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.874840 loss:        0.383046
Test - acc:         0.816700 loss:        0.620625
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.868680 loss:        0.397987
Test - acc:         0.799700 loss:        0.644664
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.873860 loss:        0.384848
Test - acc:         0.853500 loss:        0.450926
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.872500 loss:        0.384294
Test - acc:         0.791800 loss:        0.660223
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.872640 loss:        0.386783
Test - acc:         0.774900 loss:        0.777104
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.871400 loss:        0.389231
Test - acc:         0.836500 loss:        0.503120
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.871880 loss:        0.383932
Test - acc:         0.823800 loss:        0.554446
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.874240 loss:        0.384270
Test - acc:         0.811600 loss:        0.582543
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.871560 loss:        0.386867
Test - acc:         0.800100 loss:        0.669692
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.875080 loss:        0.384200
Test - acc:         0.825500 loss:        0.580183
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.871840 loss:        0.388014
Test - acc:         0.819200 loss:        0.569437
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.873000 loss:        0.385273
Test - acc:         0.802000 loss:        0.616113
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.875100 loss:        0.380088
Test - acc:         0.812900 loss:        0.627737
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.872820 loss:        0.382836
Test - acc:         0.819000 loss:        0.541716
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.874440 loss:        0.379498
Test - acc:         0.784000 loss:        0.692728
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.872160 loss:        0.385309
Test - acc:         0.811800 loss:        0.600785
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.874860 loss:        0.380135
Test - acc:         0.826000 loss:        0.534449
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.876300 loss:        0.372815
Test - acc:         0.775300 loss:        0.759581
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.876640 loss:        0.374949
Test - acc:         0.830300 loss:        0.525410
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.874700 loss:        0.378089
Test - acc:         0.833100 loss:        0.503845
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.878480 loss:        0.370917
Test - acc:         0.836500 loss:        0.509540
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.875680 loss:        0.382127
Test - acc:         0.835600 loss:        0.491977
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.875680 loss:        0.377970
Test - acc:         0.840900 loss:        0.496003
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.874580 loss:        0.379785
Test - acc:         0.834800 loss:        0.508299
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.875700 loss:        0.380223
Test - acc:         0.833600 loss:        0.510814
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.875920 loss:        0.374639
Test - acc:         0.804200 loss:        0.618247
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.879300 loss:        0.367698
Test - acc:         0.812800 loss:        0.586354
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.875060 loss:        0.373948
Test - acc:         0.827200 loss:        0.530006
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.874480 loss:        0.381791
Test - acc:         0.833100 loss:        0.519327
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.873480 loss:        0.377568
Test - acc:         0.800300 loss:        0.649648
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.877580 loss:        0.369666
Test - acc:         0.849800 loss:        0.468828
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.873440 loss:        0.376873
Test - acc:         0.831100 loss:        0.521577
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.874800 loss:        0.374778
Test - acc:         0.697100 loss:        1.106795
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.876220 loss:        0.370745
Test - acc:         0.802700 loss:        0.618389
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.877960 loss:        0.368574
Test - acc:         0.811800 loss:        0.578836
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.874140 loss:        0.377806
Test - acc:         0.824700 loss:        0.555585
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.879180 loss:        0.364411
Test - acc:         0.838600 loss:        0.522977
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.882820 loss:        0.354756
Test - acc:         0.840600 loss:        0.495485
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.880260 loss:        0.357454
Test - acc:         0.840600 loss:        0.511014
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.880560 loss:        0.358725
Test - acc:         0.846900 loss:        0.476490
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.883480 loss:        0.354853
Test - acc:         0.780800 loss:        0.693791
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.883400 loss:        0.357308
Test - acc:         0.821100 loss:        0.564584
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.881620 loss:        0.359356
Test - acc:         0.832300 loss:        0.537128
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.879340 loss:        0.364063
Test - acc:         0.828100 loss:        0.533053
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.881260 loss:        0.356673
Test - acc:         0.841700 loss:        0.496962
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.879620 loss:        0.360744
Test - acc:         0.821400 loss:        0.562719
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.882280 loss:        0.351164
Test - acc:         0.847900 loss:        0.459466
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.883040 loss:        0.351262
Test - acc:         0.754900 loss:        0.802328
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.880480 loss:        0.359369
Test - acc:         0.784800 loss:        0.684023
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.880040 loss:        0.362497
Test - acc:         0.831800 loss:        0.538837
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.881840 loss:        0.355576
Test - acc:         0.779000 loss:        0.674992
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.879840 loss:        0.357735
Test - acc:         0.842800 loss:        0.506671
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.877920 loss:        0.362264
Test - acc:         0.837700 loss:        0.512793
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.884380 loss:        0.347942
Test - acc:         0.747600 loss:        0.910417
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.879460 loss:        0.363179
Test - acc:         0.826400 loss:        0.550961
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.880400 loss:        0.362767
Test - acc:         0.818500 loss:        0.562719
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.883060 loss:        0.353770
Test - acc:         0.820700 loss:        0.553163
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.878380 loss:        0.360194
Test - acc:         0.828300 loss:        0.538156
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.882000 loss:        0.356766
Test - acc:         0.807200 loss:        0.622159
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.881440 loss:        0.355430
Test - acc:         0.777900 loss:        0.729140
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.883140 loss:        0.352612
Test - acc:         0.821200 loss:        0.548761
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.882400 loss:        0.353467
Test - acc:         0.798300 loss:        0.670226
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.881800 loss:        0.353286
Test - acc:         0.778700 loss:        0.687419
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.882200 loss:        0.352166
Test - acc:         0.846300 loss:        0.481661
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.883060 loss:        0.351942
Test - acc:         0.805700 loss:        0.599059
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.880600 loss:        0.356645
Test - acc:         0.802900 loss:        0.614555
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.880960 loss:        0.352416
Test - acc:         0.853600 loss:        0.460797
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.883400 loss:        0.350413
Test - acc:         0.841500 loss:        0.498053
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.881820 loss:        0.356620
Test - acc:         0.840400 loss:        0.494541
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.880980 loss:        0.351684
Test - acc:         0.828900 loss:        0.531475
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
slurmstepd: error: _is_a_lwp: open() /proc/168534/status failed: No such file or directory
LR =  0.010000000000000002
Train - acc:        0.938460 loss:        0.185061
Test - acc:         0.919100 loss:        0.249424
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.954620 loss:        0.136071
Test - acc:         0.924100 loss:        0.236682
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.960940 loss:        0.117181
Test - acc:         0.926800 loss:        0.231176
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.965440 loss:        0.102764
Test - acc:         0.926400 loss:        0.228108
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.969900 loss:        0.090661
Test - acc:         0.925500 loss:        0.239792
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.971820 loss:        0.081820
Test - acc:         0.926400 loss:        0.251254
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.975340 loss:        0.074797
Test - acc:         0.927500 loss:        0.245761
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.978260 loss:        0.064995
Test - acc:         0.927900 loss:        0.244904
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.980240 loss:        0.059758
Test - acc:         0.922300 loss:        0.267566
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.981360 loss:        0.056330
Test - acc:         0.924400 loss:        0.268745
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.982360 loss:        0.052668
Test - acc:         0.928900 loss:        0.262375
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.982900 loss:        0.051934
Test - acc:         0.924800 loss:        0.271545
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.984560 loss:        0.046057
Test - acc:         0.924900 loss:        0.276838
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.984540 loss:        0.045128
Test - acc:         0.926000 loss:        0.280246
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.985700 loss:        0.042107
Test - acc:         0.923900 loss:        0.292035
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.985480 loss:        0.043035
Test - acc:         0.924500 loss:        0.286758
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.985740 loss:        0.042601
Test - acc:         0.927600 loss:        0.274015
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.985260 loss:        0.043298
Test - acc:         0.921400 loss:        0.290729
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.986500 loss:        0.039361
Test - acc:         0.923900 loss:        0.297954
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.986780 loss:        0.039606
Test - acc:         0.924400 loss:        0.285699
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.987440 loss:        0.036909
Test - acc:         0.919400 loss:        0.302666
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.985100 loss:        0.043603
Test - acc:         0.920900 loss:        0.293667
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.985780 loss:        0.043434
Test - acc:         0.924700 loss:        0.277984
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.987620 loss:        0.037476
Test - acc:         0.922100 loss:        0.286969
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.985220 loss:        0.042091
Test - acc:         0.924400 loss:        0.285108
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.986200 loss:        0.041369
Test - acc:         0.920200 loss:        0.295175
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.985980 loss:        0.042785
Test - acc:         0.916800 loss:        0.308670
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.985880 loss:        0.042147
Test - acc:         0.919400 loss:        0.326004
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.984180 loss:        0.045576
Test - acc:         0.915300 loss:        0.314050
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.983760 loss:        0.047574
Test - acc:         0.922400 loss:        0.286125
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.984060 loss:        0.048108
Test - acc:         0.915100 loss:        0.308557
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.984800 loss:        0.044614
Test - acc:         0.919000 loss:        0.309561
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.983400 loss:        0.048850
Test - acc:         0.919700 loss:        0.305357
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.984280 loss:        0.046924
Test - acc:         0.916700 loss:        0.319495
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.983900 loss:        0.046394
Test - acc:         0.914100 loss:        0.325222
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.982580 loss:        0.050138
Test - acc:         0.914200 loss:        0.326911
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.983900 loss:        0.047851
Test - acc:         0.919600 loss:        0.301026
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.983780 loss:        0.048292
Test - acc:         0.915700 loss:        0.319425
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.983940 loss:        0.046798
Test - acc:         0.902700 loss:        0.379312
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.982220 loss:        0.051865
Test - acc:         0.917500 loss:        0.314840
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.981880 loss:        0.053001
Test - acc:         0.914200 loss:        0.323788
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.982760 loss:        0.051700
Test - acc:         0.919500 loss:        0.305734
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.982380 loss:        0.050407
Test - acc:         0.914500 loss:        0.312564
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.981180 loss:        0.057107
Test - acc:         0.915700 loss:        0.316523
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.982220 loss:        0.051610
Test - acc:         0.909800 loss:        0.326469
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.980440 loss:        0.058667
Test - acc:         0.915100 loss:        0.312388
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.986080 loss:        0.043053
Test - acc:         0.921200 loss:        0.294636
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.985820 loss:        0.043128
Test - acc:         0.920300 loss:        0.301349
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.985260 loss:        0.044058
Test - acc:         0.917600 loss:        0.318928
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.986460 loss:        0.040625
Test - acc:         0.914400 loss:        0.340069
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.985940 loss:        0.041875
Test - acc:         0.916700 loss:        0.316062
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.985520 loss:        0.044487
Test - acc:         0.917800 loss:        0.320983
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.985840 loss:        0.042863
Test - acc:         0.917500 loss:        0.322801
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.985820 loss:        0.042428
Test - acc:         0.915500 loss:        0.352068
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.985720 loss:        0.041964
Test - acc:         0.911100 loss:        0.338568
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.984580 loss:        0.046179
Test - acc:         0.916400 loss:        0.335582
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.983680 loss:        0.049314
Test - acc:         0.915400 loss:        0.322890
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.986520 loss:        0.042220
Test - acc:         0.915000 loss:        0.327808
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.984280 loss:        0.046572
Test - acc:         0.920300 loss:        0.302616
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.983220 loss:        0.049711
Test - acc:         0.911300 loss:        0.331611
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.984880 loss:        0.045229
Test - acc:         0.915200 loss:        0.314623
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.983140 loss:        0.049606
Test - acc:         0.916800 loss:        0.318635
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.983540 loss:        0.048643
Test - acc:         0.910900 loss:        0.342366
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.983920 loss:        0.047461
Test - acc:         0.911700 loss:        0.342916
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.985940 loss:        0.043765
Test - acc:         0.916700 loss:        0.314463
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.984000 loss:        0.046772
Test - acc:         0.910500 loss:        0.342268
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.983860 loss:        0.048510
Test - acc:         0.914000 loss:        0.332521
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.984780 loss:        0.047152
Test - acc:         0.912400 loss:        0.331005
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.983460 loss:        0.048429
Test - acc:         0.919600 loss:        0.296609
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.982560 loss:        0.051724
Test - acc:         0.921200 loss:        0.297347
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.984400 loss:        0.047914
Test - acc:         0.910000 loss:        0.350720
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.983820 loss:        0.048622
Test - acc:         0.917300 loss:        0.299432
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.984640 loss:        0.046347
Test - acc:         0.907100 loss:        0.357024
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.983920 loss:        0.047474
Test - acc:         0.913200 loss:        0.330232
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.982520 loss:        0.051286
Test - acc:         0.913600 loss:        0.318422
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.983080 loss:        0.049707
Test - acc:         0.916000 loss:        0.311020
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.981820 loss:        0.054963
Test - acc:         0.904400 loss:        0.362276
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.984920 loss:        0.046904
Test - acc:         0.920200 loss:        0.311549
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.985580 loss:        0.043568
Test - acc:         0.913600 loss:        0.322670
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.984620 loss:        0.045953
Test - acc:         0.918500 loss:        0.310934
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.983940 loss:        0.047606
Test - acc:         0.915800 loss:        0.301032
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.982720 loss:        0.051530
Test - acc:         0.911700 loss:        0.326548
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.983200 loss:        0.050748
Test - acc:         0.902600 loss:        0.362451
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.984040 loss:        0.048726
Test - acc:         0.912400 loss:        0.337269
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.974940 loss:        0.074374
Test - acc:         0.900700 loss:        0.354089
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.982300 loss:        0.054292
Test - acc:         0.905500 loss:        0.358292
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.982820 loss:        0.049907
Test - acc:         0.916200 loss:        0.309206
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.983360 loss:        0.050152
Test - acc:         0.911600 loss:        0.329913
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.983060 loss:        0.050363
Test - acc:         0.911600 loss:        0.334817
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.982040 loss:        0.052258
Test - acc:         0.914800 loss:        0.312036
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.983220 loss:        0.050345
Test - acc:         0.914000 loss:        0.337543
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.984060 loss:        0.048559
Test - acc:         0.899900 loss:        0.376565
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.982960 loss:        0.049658
Test - acc:         0.917500 loss:        0.304325
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.984200 loss:        0.048191
Test - acc:         0.917700 loss:        0.300184
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.983700 loss:        0.048424
Test - acc:         0.912400 loss:        0.325996
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.982880 loss:        0.049938
Test - acc:         0.912200 loss:        0.330112
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.983740 loss:        0.047726
Test - acc:         0.916000 loss:        0.308078
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.984940 loss:        0.045022
Test - acc:         0.906200 loss:        0.369427
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.982640 loss:        0.051347
Test - acc:         0.916500 loss:        0.321437
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.982400 loss:        0.051832
Test - acc:         0.918300 loss:        0.323242
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.992620 loss:        0.025394
Test - acc:         0.929900 loss:        0.267911
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.994940 loss:        0.017858
Test - acc:         0.931700 loss:        0.261576
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.996640 loss:        0.012788
Test - acc:         0.932800 loss:        0.261710
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.997140 loss:        0.011009
Test - acc:         0.932400 loss:        0.262803
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.997780 loss:        0.009728
Test - acc:         0.932700 loss:        0.263091
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.997040 loss:        0.010721
Test - acc:         0.933200 loss:        0.262535
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.997660 loss:        0.008794
Test - acc:         0.932500 loss:        0.262470
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.998220 loss:        0.008142
Test - acc:         0.933400 loss:        0.265182
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.998120 loss:        0.008011
Test - acc:         0.931800 loss:        0.264507
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.997980 loss:        0.007608
Test - acc:         0.932800 loss:        0.267266
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.998440 loss:        0.006857
Test - acc:         0.933800 loss:        0.267614
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.998480 loss:        0.006925
Test - acc:         0.933700 loss:        0.267264
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.998740 loss:        0.005976
Test - acc:         0.933000 loss:        0.268980
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.005965
Test - acc:         0.933400 loss:        0.267215
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.998840 loss:        0.005707
Test - acc:         0.933300 loss:        0.266406
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.998660 loss:        0.005695
Test - acc:         0.933500 loss:        0.269138
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999000 loss:        0.005058
Test - acc:         0.933700 loss:        0.270983
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999140 loss:        0.004954
Test - acc:         0.935000 loss:        0.269449
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999080 loss:        0.005034
Test - acc:         0.934300 loss:        0.268900
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.998960 loss:        0.004842
Test - acc:         0.935800 loss:        0.268137
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999160 loss:        0.004593
Test - acc:         0.934500 loss:        0.266915
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.998840 loss:        0.004903
Test - acc:         0.935200 loss:        0.265545
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999180 loss:        0.004522
Test - acc:         0.934200 loss:        0.266900
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.980300 loss:        0.060185
Test - acc:         0.917400 loss:        0.320693
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.984580 loss:        0.046069
Test - acc:         0.919700 loss:        0.308003
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.986420 loss:        0.040475
Test - acc:         0.920600 loss:        0.301059
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.988740 loss:        0.034412
Test - acc:         0.922600 loss:        0.302167
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.989840 loss:        0.031305
Test - acc:         0.922700 loss:        0.300034
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.990840 loss:        0.028984
Test - acc:         0.925200 loss:        0.293789
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.991980 loss:        0.027208
Test - acc:         0.923800 loss:        0.300777
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.992840 loss:        0.023684
Test - acc:         0.924900 loss:        0.297887
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.992600 loss:        0.024136
Test - acc:         0.924500 loss:        0.302629
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.992740 loss:        0.023357
Test - acc:         0.924200 loss:        0.297678
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.993380 loss:        0.021466
Test - acc:         0.926500 loss:        0.295322
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.993860 loss:        0.020209
Test - acc:         0.926300 loss:        0.303186
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.994460 loss:        0.019087
Test - acc:         0.925800 loss:        0.303886
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.994520 loss:        0.018639
Test - acc:         0.928400 loss:        0.297832
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.995560 loss:        0.016602
Test - acc:         0.928300 loss:        0.302134
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.994480 loss:        0.017553
Test - acc:         0.925500 loss:        0.308501
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.995680 loss:        0.015711
Test - acc:         0.926900 loss:        0.301312
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.995360 loss:        0.015389
Test - acc:         0.927700 loss:        0.301706
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.995040 loss:        0.016091
Test - acc:         0.926200 loss:        0.300100
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.995700 loss:        0.015796
Test - acc:         0.928700 loss:        0.300919
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.995940 loss:        0.014150
Test - acc:         0.927100 loss:        0.306165
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.995280 loss:        0.016142
Test - acc:         0.927100 loss:        0.301518
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.996300 loss:        0.013807
Test - acc:         0.925300 loss:        0.307376
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.996480 loss:        0.012990
Test - acc:         0.927400 loss:        0.303358
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.996020 loss:        0.013198
Test - acc:         0.926400 loss:        0.308433
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.996180 loss:        0.012920
Test - acc:         0.925900 loss:        0.310453
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.996820 loss:        0.011742
Test - acc:         0.928000 loss:        0.303747
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.996340 loss:        0.013139
Test - acc:         0.927700 loss:        0.305519
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.996780 loss:        0.011974
Test - acc:         0.930000 loss:        0.301008
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.996740 loss:        0.011383
Test - acc:         0.929800 loss:        0.306577
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.996760 loss:        0.011401
Test - acc:         0.928300 loss:        0.306742
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.997000 loss:        0.011052
Test - acc:         0.929300 loss:        0.307577
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.996940 loss:        0.010784
Test - acc:         0.928500 loss:        0.305790
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.997280 loss:        0.010163
Test - acc:         0.928700 loss:        0.309093
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.996980 loss:        0.011026
Test - acc:         0.927900 loss:        0.309075
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.997480 loss:        0.009293
Test - acc:         0.928900 loss:        0.307490
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.997060 loss:        0.009869
Test - acc:         0.930500 loss:        0.310471
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.997540 loss:        0.010048
Test - acc:         0.928000 loss:        0.315643
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.997060 loss:        0.009938
Test - acc:         0.930000 loss:        0.310415
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.915400 loss:        0.261538
Test - acc:         0.893500 loss:        0.376449
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.942640 loss:        0.168426
Test - acc:         0.899400 loss:        0.344407
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.952760 loss:        0.139312
Test - acc:         0.903600 loss:        0.339362
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.955780 loss:        0.129835
Test - acc:         0.904500 loss:        0.334092
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.959960 loss:        0.115549
Test - acc:         0.904800 loss:        0.341293
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.962580 loss:        0.109935
Test - acc:         0.906900 loss:        0.322981
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.964820 loss:        0.102645
Test - acc:         0.906100 loss:        0.323991
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.967160 loss:        0.096876
Test - acc:         0.908200 loss:        0.329375
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.967500 loss:        0.094159
Test - acc:         0.910100 loss:        0.320509
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.969580 loss:        0.089353
Test - acc:         0.908600 loss:        0.335736
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.969200 loss:        0.087436
Test - acc:         0.911700 loss:        0.321014
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.971900 loss:        0.083626
Test - acc:         0.910000 loss:        0.329017
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.972980 loss:        0.079900
Test - acc:         0.909800 loss:        0.323187
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.973560 loss:        0.078405
Test - acc:         0.911000 loss:        0.328015
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.973840 loss:        0.075039
Test - acc:         0.911000 loss:        0.329478
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.975280 loss:        0.071126
Test - acc:         0.912100 loss:        0.330456
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.975920 loss:        0.071113
Test - acc:         0.914500 loss:        0.322330
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.975560 loss:        0.071172
Test - acc:         0.914400 loss:        0.331354
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.977080 loss:        0.069753
Test - acc:         0.911900 loss:        0.330579
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.975500 loss:        0.069339
Test - acc:         0.915100 loss:        0.326180
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.977660 loss:        0.067479
Test - acc:         0.916100 loss:        0.320747
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.978620 loss:        0.064355
Test - acc:         0.915500 loss:        0.322664
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.978360 loss:        0.063371
Test - acc:         0.914700 loss:        0.326887
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.979380 loss:        0.062293
Test - acc:         0.912600 loss:        0.338290
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.979080 loss:        0.060917
Test - acc:         0.911800 loss:        0.339660
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.979600 loss:        0.060964
Test - acc:         0.916900 loss:        0.326108
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.978840 loss:        0.061865
Test - acc:         0.911600 loss:        0.344780
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.980120 loss:        0.060308
Test - acc:         0.910300 loss:        0.339612
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.980880 loss:        0.056068
Test - acc:         0.913800 loss:        0.328792
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.980340 loss:        0.057666
Test - acc:         0.914000 loss:        0.332145
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.980840 loss:        0.056095
Test - acc:         0.915100 loss:        0.328105
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.980280 loss:        0.055934
Test - acc:         0.913100 loss:        0.336557
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.981260 loss:        0.056364
Test - acc:         0.914200 loss:        0.339281
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.982020 loss:        0.054732
Test - acc:         0.911800 loss:        0.343481
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.982180 loss:        0.052877
Test - acc:         0.917200 loss:        0.327580
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.982740 loss:        0.051834
Test - acc:         0.918100 loss:        0.330886
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.981940 loss:        0.053911
Test - acc:         0.913000 loss:        0.346947
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.982080 loss:        0.052677
Test - acc:         0.914200 loss:        0.338313
Sparsity :          0.9961
Wdecay :        0.000500
