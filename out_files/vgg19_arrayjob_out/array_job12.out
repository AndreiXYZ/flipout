Running --prune_criterion topflip --seed 42 --prune_freq 50 --prune_rate 0.5 --noise --comment=vgg19_crit=topflip_pf=50_seed=42 --save_model=pre-finetune/vgg19_topflip_pf50_s42
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "topflip",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_topflip_pf50_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.112220 loss:        2.707889
Test - acc:         0.115500 loss:        2.285625
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.171100 loss:        2.119530
Test - acc:         0.247800 loss:        1.889029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.260180 loss:        1.845003
Test - acc:         0.293800 loss:        1.793629
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.351700 loss:        1.666860
Test - acc:         0.365700 loss:        1.606198
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.442140 loss:        1.475611
Test - acc:         0.498800 loss:        1.346169
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.541180 loss:        1.260178
Test - acc:         0.453900 loss:        1.738315
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.619740 loss:        1.083854
Test - acc:         0.532000 loss:        1.515536
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.675980 loss:        0.929632
Test - acc:         0.621200 loss:        1.227844
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.710540 loss:        0.855181
Test - acc:         0.638900 loss:        1.146975
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.730440 loss:        0.803610
Test - acc:         0.680300 loss:        1.070751
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.752360 loss:        0.752724
Test - acc:         0.695200 loss:        0.963042
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.761480 loss:        0.725837
Test - acc:         0.731800 loss:        0.838521
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.771840 loss:        0.701123
Test - acc:         0.694700 loss:        0.991394
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.778780 loss:        0.676362
Test - acc:         0.667800 loss:        1.115059
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.781940 loss:        0.664950
Test - acc:         0.744500 loss:        0.805362
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.790840 loss:        0.641081
Test - acc:         0.779400 loss:        0.675063
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.795240 loss:        0.630031
Test - acc:         0.697300 loss:        0.917811
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.800100 loss:        0.615935
Test - acc:         0.719800 loss:        0.926714
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.800380 loss:        0.612391
Test - acc:         0.752900 loss:        0.813506
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.802400 loss:        0.604146
Test - acc:         0.770800 loss:        0.743486
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.807380 loss:        0.594119
Test - acc:         0.665200 loss:        1.083300
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.810160 loss:        0.585650
Test - acc:         0.778600 loss:        0.685493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.809940 loss:        0.585200
Test - acc:         0.780900 loss:        0.664840
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.811000 loss:        0.585315
Test - acc:         0.780700 loss:        0.692555
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.812760 loss:        0.576658
Test - acc:         0.723800 loss:        0.945873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.819780 loss:        0.555273
Test - acc:         0.759000 loss:        0.754278
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.816960 loss:        0.567006
Test - acc:         0.759200 loss:        0.769223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.818100 loss:        0.561920
Test - acc:         0.784400 loss:        0.689784
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.818680 loss:        0.560318
Test - acc:         0.781500 loss:        0.680860
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.818160 loss:        0.564143
Test - acc:         0.704200 loss:        0.952644
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.824040 loss:        0.548989
Test - acc:         0.768300 loss:        0.734637
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.822460 loss:        0.552491
Test - acc:         0.795300 loss:        0.651316
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.822260 loss:        0.549665
Test - acc:         0.776000 loss:        0.726616
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.824040 loss:        0.545305
Test - acc:         0.696300 loss:        1.066222
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.828700 loss:        0.532606
Test - acc:         0.763700 loss:        0.736834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.825980 loss:        0.536146
Test - acc:         0.797400 loss:        0.639977
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.825080 loss:        0.541546
Test - acc:         0.783400 loss:        0.693384
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.827300 loss:        0.528451
Test - acc:         0.736300 loss:        0.862552
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.824660 loss:        0.537154
Test - acc:         0.745800 loss:        0.838598
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.830840 loss:        0.527442
Test - acc:         0.776500 loss:        0.700459
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.827020 loss:        0.530133
Test - acc:         0.761700 loss:        0.742538
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.827200 loss:        0.537913
Test - acc:         0.782600 loss:        0.694255
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.831900 loss:        0.523550
Test - acc:         0.741600 loss:        0.814439
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.834320 loss:        0.514167
Test - acc:         0.719800 loss:        0.969886
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.830520 loss:        0.522136
Test - acc:         0.717100 loss:        1.180856
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.833200 loss:        0.516630
Test - acc:         0.786700 loss:        0.678516
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.830840 loss:        0.519828
Test - acc:         0.772900 loss:        0.756524
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.831600 loss:        0.518631
Test - acc:         0.815600 loss:        0.572250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.835020 loss:        0.510291
Test - acc:         0.822800 loss:        0.550194
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.833200 loss:        0.518434
Test - acc:         0.786100 loss:        0.652066
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.840640 loss:        0.494160
Test - acc:         0.757700 loss:        0.758376
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.841780 loss:        0.478672
Test - acc:         0.806500 loss:        0.596971
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.847600 loss:        0.465140
Test - acc:         0.733200 loss:        0.960177
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.850700 loss:        0.454097
Test - acc:         0.785400 loss:        0.673340
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.847340 loss:        0.459572
Test - acc:         0.791700 loss:        0.629834
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.849980 loss:        0.453432
Test - acc:         0.825400 loss:        0.544458
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.851300 loss:        0.449773
Test - acc:         0.766100 loss:        0.739585
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.851420 loss:        0.452640
Test - acc:         0.816900 loss:        0.573025
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.850380 loss:        0.453178
Test - acc:         0.781900 loss:        0.733908
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.852120 loss:        0.452779
Test - acc:         0.735900 loss:        0.859042
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.851760 loss:        0.447057
Test - acc:         0.829300 loss:        0.530595
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.851360 loss:        0.450695
Test - acc:         0.810200 loss:        0.589924
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.853740 loss:        0.441669
Test - acc:         0.797400 loss:        0.665956
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.855520 loss:        0.443234
Test - acc:         0.800700 loss:        0.612500
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.850720 loss:        0.450253
Test - acc:         0.794300 loss:        0.658588
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.853980 loss:        0.438406
Test - acc:         0.765100 loss:        0.726544
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.857740 loss:        0.432708
Test - acc:         0.809600 loss:        0.568271
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.853160 loss:        0.443929
Test - acc:         0.819000 loss:        0.570187
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.853820 loss:        0.440373
Test - acc:         0.835500 loss:        0.512609
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.852000 loss:        0.442472
Test - acc:         0.803200 loss:        0.586710
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.854040 loss:        0.440586
Test - acc:         0.808600 loss:        0.610576
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.855340 loss:        0.439182
Test - acc:         0.788800 loss:        0.693403
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.855120 loss:        0.438019
Test - acc:         0.808000 loss:        0.588085
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.853720 loss:        0.441914
Test - acc:         0.827300 loss:        0.514463
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.854280 loss:        0.434392
Test - acc:         0.790400 loss:        0.629426
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.853940 loss:        0.436710
Test - acc:         0.795000 loss:        0.632438
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.854360 loss:        0.437870
Test - acc:         0.809800 loss:        0.601032
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.855620 loss:        0.432885
Test - acc:         0.752600 loss:        0.795545
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.852900 loss:        0.438956
Test - acc:         0.807400 loss:        0.606819
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.854360 loss:        0.435185
Test - acc:         0.822700 loss:        0.555650
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.853420 loss:        0.438431
Test - acc:         0.731700 loss:        0.972806
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.855060 loss:        0.436298
Test - acc:         0.806000 loss:        0.597535
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.852640 loss:        0.440456
Test - acc:         0.705000 loss:        0.902857
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.856760 loss:        0.427278
Test - acc:         0.796100 loss:        0.671322
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.855780 loss:        0.430456
Test - acc:         0.728000 loss:        0.900546
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.855440 loss:        0.431811
Test - acc:         0.768100 loss:        0.742966
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.852960 loss:        0.437649
Test - acc:         0.804300 loss:        0.607858
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.860520 loss:        0.426260
Test - acc:         0.764900 loss:        0.838630
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.853600 loss:        0.432727
Test - acc:         0.829300 loss:        0.523547
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.853460 loss:        0.435335
Test - acc:         0.783800 loss:        0.718951
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.855280 loss:        0.432596
Test - acc:         0.798900 loss:        0.641326
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.857400 loss:        0.425507
Test - acc:         0.825600 loss:        0.555516
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.855400 loss:        0.434917
Test - acc:         0.786700 loss:        0.686220
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.856980 loss:        0.427020
Test - acc:         0.788500 loss:        0.673891
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.855580 loss:        0.431116
Test - acc:         0.797100 loss:        0.624179
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.856560 loss:        0.431553
Test - acc:         0.813600 loss:        0.597327
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.856120 loss:        0.432063
Test - acc:         0.795100 loss:        0.670063
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.857360 loss:        0.426200
Test - acc:         0.807800 loss:        0.594574
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.854340 loss:        0.442590
Test - acc:         0.840200 loss:        0.481514
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.856940 loss:        0.428940
Test - acc:         0.834400 loss:        0.494309
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.870740 loss:        0.383996
Test - acc:         0.796700 loss:        0.652324
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.866920 loss:        0.391159
Test - acc:         0.796500 loss:        0.625210
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.871160 loss:        0.385114
Test - acc:         0.825500 loss:        0.518747
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.869480 loss:        0.387721
Test - acc:         0.827700 loss:        0.501607
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.873220 loss:        0.379875
Test - acc:         0.822900 loss:        0.539619
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.869680 loss:        0.385458
Test - acc:         0.779900 loss:        0.660017
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.871220 loss:        0.383394
Test - acc:         0.833600 loss:        0.515926
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.875480 loss:        0.373805
Test - acc:         0.829600 loss:        0.526889
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.869740 loss:        0.386063
Test - acc:         0.833400 loss:        0.521371
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.870440 loss:        0.383022
Test - acc:         0.807000 loss:        0.608842
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.869600 loss:        0.388513
Test - acc:         0.808300 loss:        0.590473
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.872660 loss:        0.379374
Test - acc:         0.821200 loss:        0.554772
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.871260 loss:        0.383130
Test - acc:         0.830700 loss:        0.514836
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.872740 loss:        0.381881
Test - acc:         0.835900 loss:        0.504566
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.869880 loss:        0.384640
Test - acc:         0.841700 loss:        0.483235
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.872580 loss:        0.378264
Test - acc:         0.845600 loss:        0.461816
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.872120 loss:        0.379818
Test - acc:         0.822900 loss:        0.567629
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.873540 loss:        0.377178
Test - acc:         0.691600 loss:        1.139483
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.872540 loss:        0.376903
Test - acc:         0.820200 loss:        0.534532
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.871320 loss:        0.380672
Test - acc:         0.826700 loss:        0.520213
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.871080 loss:        0.380165
Test - acc:         0.828200 loss:        0.533748
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.872800 loss:        0.377301
Test - acc:         0.792500 loss:        0.653938
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.870260 loss:        0.385253
Test - acc:         0.824600 loss:        0.569952
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.872280 loss:        0.379204
Test - acc:         0.802600 loss:        0.610685
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.871860 loss:        0.378067
Test - acc:         0.780100 loss:        0.658854
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.871980 loss:        0.377279
Test - acc:         0.802800 loss:        0.628558
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.870720 loss:        0.381533
Test - acc:         0.825000 loss:        0.561137
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.872320 loss:        0.377713
Test - acc:         0.761300 loss:        0.788272
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.873180 loss:        0.375889
Test - acc:         0.851800 loss:        0.448355
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.871800 loss:        0.381336
Test - acc:         0.811100 loss:        0.586121
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.870580 loss:        0.383095
Test - acc:         0.803200 loss:        0.604802
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.872620 loss:        0.379985
Test - acc:         0.797400 loss:        0.661505
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.871160 loss:        0.380536
Test - acc:         0.782400 loss:        0.737693
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.869480 loss:        0.384433
Test - acc:         0.838900 loss:        0.486019
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.871780 loss:        0.379177
Test - acc:         0.831300 loss:        0.495957
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.869680 loss:        0.380700
Test - acc:         0.839300 loss:        0.469199
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.871900 loss:        0.379290
Test - acc:         0.800500 loss:        0.605271
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.872740 loss:        0.379200
Test - acc:         0.752200 loss:        0.910500
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.870820 loss:        0.381181
Test - acc:         0.838700 loss:        0.486106
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.872900 loss:        0.378319
Test - acc:         0.763100 loss:        0.790531
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.872000 loss:        0.379092
Test - acc:         0.810900 loss:        0.590171
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.872160 loss:        0.381567
Test - acc:         0.812700 loss:        0.602203
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.873300 loss:        0.379449
Test - acc:         0.848600 loss:        0.443839
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.870920 loss:        0.380275
Test - acc:         0.807800 loss:        0.603528
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.871560 loss:        0.381983
Test - acc:         0.824200 loss:        0.567453
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.872720 loss:        0.377328
Test - acc:         0.821600 loss:        0.558898
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.873660 loss:        0.376596
Test - acc:         0.814100 loss:        0.596119
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.871180 loss:        0.382258
Test - acc:         0.834400 loss:        0.545462
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.872960 loss:        0.377197
Test - acc:         0.833600 loss:        0.505273
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.872660 loss:        0.377885
Test - acc:         0.789800 loss:        0.616296
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.917640 loss:        0.243767
Test - acc:         0.899700 loss:        0.297038
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.930080 loss:        0.209251
Test - acc:         0.907200 loss:        0.282545
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.933380 loss:        0.196635
Test - acc:         0.906700 loss:        0.277637
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.937440 loss:        0.184088
Test - acc:         0.908600 loss:        0.277974
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.942160 loss:        0.172283
Test - acc:         0.909700 loss:        0.284266
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.942420 loss:        0.167017
Test - acc:         0.908500 loss:        0.286807
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.945500 loss:        0.161648
Test - acc:         0.910500 loss:        0.282436
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.945580 loss:        0.160208
Test - acc:         0.908200 loss:        0.286674
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.947300 loss:        0.153362
Test - acc:         0.908100 loss:        0.285638
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.949240 loss:        0.148383
Test - acc:         0.907800 loss:        0.292457
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.950040 loss:        0.147117
Test - acc:         0.908900 loss:        0.289520
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.950380 loss:        0.143017
Test - acc:         0.908900 loss:        0.297656
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.950280 loss:        0.142522
Test - acc:         0.904900 loss:        0.307663
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.953140 loss:        0.138121
Test - acc:         0.908600 loss:        0.296621
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.951780 loss:        0.139287
Test - acc:         0.908300 loss:        0.291816
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.952200 loss:        0.137427
Test - acc:         0.906200 loss:        0.310392
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.953500 loss:        0.133798
Test - acc:         0.905200 loss:        0.306077
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.955260 loss:        0.131920
Test - acc:         0.902900 loss:        0.326468
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.955260 loss:        0.129385
Test - acc:         0.904200 loss:        0.317766
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.954280 loss:        0.131538
Test - acc:         0.901100 loss:        0.311006
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.954700 loss:        0.130050
Test - acc:         0.903300 loss:        0.316747
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.956240 loss:        0.126647
Test - acc:         0.906500 loss:        0.317086
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.955360 loss:        0.127368
Test - acc:         0.901600 loss:        0.325161
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.955180 loss:        0.130647
Test - acc:         0.898800 loss:        0.329542
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.955780 loss:        0.126589
Test - acc:         0.893900 loss:        0.348625
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.953100 loss:        0.132498
Test - acc:         0.900700 loss:        0.329744
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.954680 loss:        0.130606
Test - acc:         0.902900 loss:        0.324369
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.952980 loss:        0.132868
Test - acc:         0.906700 loss:        0.319128
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.955520 loss:        0.127055
Test - acc:         0.892300 loss:        0.355087
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.955440 loss:        0.128973
Test - acc:         0.903200 loss:        0.318392
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.956780 loss:        0.126039
Test - acc:         0.904500 loss:        0.322518
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.954260 loss:        0.131584
Test - acc:         0.895800 loss:        0.346230
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.955140 loss:        0.127732
Test - acc:         0.898500 loss:        0.332084
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.954620 loss:        0.129636
Test - acc:         0.897400 loss:        0.338275
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.953460 loss:        0.132083
Test - acc:         0.898300 loss:        0.342880
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.953220 loss:        0.131812
Test - acc:         0.894500 loss:        0.342981
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.955300 loss:        0.128274
Test - acc:         0.894100 loss:        0.358562
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.954700 loss:        0.129867
Test - acc:         0.891400 loss:        0.362293
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.956180 loss:        0.127647
Test - acc:         0.901800 loss:        0.326785
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.954280 loss:        0.131808
Test - acc:         0.898800 loss:        0.334514
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.954300 loss:        0.128185
Test - acc:         0.903800 loss:        0.325158
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.953660 loss:        0.133644
Test - acc:         0.893500 loss:        0.363729
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.953260 loss:        0.134528
Test - acc:         0.897900 loss:        0.353657
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.952440 loss:        0.132870
Test - acc:         0.901800 loss:        0.323007
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.953740 loss:        0.131303
Test - acc:         0.894800 loss:        0.348878
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.954280 loss:        0.131321
Test - acc:         0.892000 loss:        0.356730
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.954060 loss:        0.129543
Test - acc:         0.899900 loss:        0.338919
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.955920 loss:        0.126687
Test - acc:         0.903900 loss:        0.322135
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.955260 loss:        0.128745
Test - acc:         0.901200 loss:        0.340494
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.954440 loss:        0.128584
Test - acc:         0.890900 loss:        0.365932
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.944780 loss:        0.159587
Test - acc:         0.903600 loss:        0.318003
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.948020 loss:        0.150767
Test - acc:         0.908600 loss:        0.300426
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.948940 loss:        0.146574
Test - acc:         0.902400 loss:        0.316840
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.948360 loss:        0.147808
Test - acc:         0.897700 loss:        0.344698
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.949340 loss:        0.146796
Test - acc:         0.898600 loss:        0.326384
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.950320 loss:        0.143311
Test - acc:         0.897100 loss:        0.338715
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.950160 loss:        0.144466
Test - acc:         0.898300 loss:        0.330870
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.949840 loss:        0.144404
Test - acc:         0.892800 loss:        0.343739
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.949940 loss:        0.142318
Test - acc:         0.897500 loss:        0.345424
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.950300 loss:        0.143582
Test - acc:         0.900300 loss:        0.318531
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.950720 loss:        0.142463
Test - acc:         0.900000 loss:        0.329236
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.951160 loss:        0.140716
Test - acc:         0.898300 loss:        0.325974
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.949480 loss:        0.144078
Test - acc:         0.902600 loss:        0.322246
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.951640 loss:        0.138629
Test - acc:         0.893100 loss:        0.345613
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.950100 loss:        0.142555
Test - acc:         0.897900 loss:        0.330264
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.950520 loss:        0.139480
Test - acc:         0.886400 loss:        0.388197
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.950540 loss:        0.143648
Test - acc:         0.896600 loss:        0.349712
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.951880 loss:        0.138291
Test - acc:         0.900600 loss:        0.334516
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.951280 loss:        0.138168
Test - acc:         0.895200 loss:        0.336487
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.952140 loss:        0.135649
Test - acc:         0.898400 loss:        0.339353
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.949900 loss:        0.141613
Test - acc:         0.902800 loss:        0.325491
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.952100 loss:        0.136984
Test - acc:         0.896300 loss:        0.341223
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.953800 loss:        0.134253
Test - acc:         0.896700 loss:        0.336103
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.950880 loss:        0.142029
Test - acc:         0.896600 loss:        0.347243
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.950680 loss:        0.139648
Test - acc:         0.896900 loss:        0.353080
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.950900 loss:        0.136949
Test - acc:         0.888600 loss:        0.372001
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.952120 loss:        0.137469
Test - acc:         0.900500 loss:        0.343635
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.949840 loss:        0.140380
Test - acc:         0.890700 loss:        0.366916
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.950580 loss:        0.138808
Test - acc:         0.900400 loss:        0.337957
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.952420 loss:        0.137397
Test - acc:         0.900000 loss:        0.327600
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.951140 loss:        0.139498
Test - acc:         0.900600 loss:        0.342405
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.953000 loss:        0.134997
Test - acc:         0.898600 loss:        0.335803
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.953560 loss:        0.134782
Test - acc:         0.893300 loss:        0.375906
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.950360 loss:        0.141768
Test - acc:         0.903500 loss:        0.329452
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.952520 loss:        0.136105
Test - acc:         0.897300 loss:        0.348223
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.951840 loss:        0.135782
Test - acc:         0.893900 loss:        0.348674
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.951380 loss:        0.136514
Test - acc:         0.897100 loss:        0.334756
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.951360 loss:        0.139646
Test - acc:         0.905100 loss:        0.309648
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.953260 loss:        0.135464
Test - acc:         0.893600 loss:        0.359258
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.954740 loss:        0.131961
Test - acc:         0.896900 loss:        0.338582
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.951900 loss:        0.135004
Test - acc:         0.881900 loss:        0.410170
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.954300 loss:        0.129741
Test - acc:         0.901700 loss:        0.332476
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.954100 loss:        0.130694
Test - acc:         0.898200 loss:        0.354812
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.952880 loss:        0.134221
Test - acc:         0.900400 loss:        0.341794
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.953960 loss:        0.133669
Test - acc:         0.900200 loss:        0.336911
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.953040 loss:        0.133060
Test - acc:         0.895000 loss:        0.361791
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.953240 loss:        0.135504
Test - acc:         0.896800 loss:        0.345273
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.954340 loss:        0.130643
Test - acc:         0.902600 loss:        0.318725
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.954340 loss:        0.131170
Test - acc:         0.896300 loss:        0.350526
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.952120 loss:        0.134272
Test - acc:         0.898700 loss:        0.349445
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.956280 loss:        0.124072
Test - acc:         0.909200 loss:        0.290199
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.965080 loss:        0.102548
Test - acc:         0.911700 loss:        0.287864
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.967460 loss:        0.095651
Test - acc:         0.912800 loss:        0.284525
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.969800 loss:        0.087620
Test - acc:         0.914600 loss:        0.286486
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.971760 loss:        0.084534
Test - acc:         0.913400 loss:        0.285930
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.971180 loss:        0.084300
Test - acc:         0.915300 loss:        0.288099
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.972660 loss:        0.079684
Test - acc:         0.915000 loss:        0.289060
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.973000 loss:        0.077585
Test - acc:         0.916300 loss:        0.288482
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.974080 loss:        0.076014
Test - acc:         0.913900 loss:        0.292031
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.973800 loss:        0.076022
Test - acc:         0.912600 loss:        0.295412
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.975180 loss:        0.072992
Test - acc:         0.914200 loss:        0.294781
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.975040 loss:        0.071663
Test - acc:         0.913400 loss:        0.295738
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.976280 loss:        0.068847
Test - acc:         0.914700 loss:        0.297887
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.975760 loss:        0.069251
Test - acc:         0.913400 loss:        0.301777
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.976700 loss:        0.066603
Test - acc:         0.914800 loss:        0.300647
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.977980 loss:        0.065296
Test - acc:         0.912900 loss:        0.302526
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.977560 loss:        0.065309
Test - acc:         0.915000 loss:        0.304435
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.977600 loss:        0.064186
Test - acc:         0.911300 loss:        0.311206
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.977320 loss:        0.064537
Test - acc:         0.914800 loss:        0.311220
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.978180 loss:        0.062393
Test - acc:         0.911800 loss:        0.310468
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.978660 loss:        0.061085
Test - acc:         0.913200 loss:        0.307735
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.978740 loss:        0.061048
Test - acc:         0.914500 loss:        0.310285
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.979520 loss:        0.059879
Test - acc:         0.914800 loss:        0.307111
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.980260 loss:        0.057591
Test - acc:         0.912000 loss:        0.309707
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.980720 loss:        0.057219
Test - acc:         0.914700 loss:        0.314328
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.979860 loss:        0.058601
Test - acc:         0.915000 loss:        0.317577
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.980780 loss:        0.055296
Test - acc:         0.913900 loss:        0.315043
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.980600 loss:        0.057743
Test - acc:         0.914400 loss:        0.316355
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.981140 loss:        0.054451
Test - acc:         0.911300 loss:        0.324721
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.981000 loss:        0.055765
Test - acc:         0.914500 loss:        0.316344
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.981100 loss:        0.055216
Test - acc:         0.915700 loss:        0.320729
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.981660 loss:        0.054537
Test - acc:         0.916900 loss:        0.319291
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.980980 loss:        0.054954
Test - acc:         0.914800 loss:        0.323103
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.981380 loss:        0.053005
Test - acc:         0.914700 loss:        0.316405
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.982280 loss:        0.050680
Test - acc:         0.913500 loss:        0.324312
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.981000 loss:        0.053103
Test - acc:         0.915100 loss:        0.322265
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.981960 loss:        0.052628
Test - acc:         0.913600 loss:        0.323720
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.982580 loss:        0.050687
Test - acc:         0.913400 loss:        0.324391
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.982200 loss:        0.051929
Test - acc:         0.912900 loss:        0.332858
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.982860 loss:        0.050542
Test - acc:         0.913800 loss:        0.326211
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.983080 loss:        0.050298
Test - acc:         0.914600 loss:        0.328848
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.982240 loss:        0.049976
Test - acc:         0.914500 loss:        0.326741
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.983980 loss:        0.047848
Test - acc:         0.911900 loss:        0.332750
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.983540 loss:        0.048940
Test - acc:         0.913100 loss:        0.329380
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.983880 loss:        0.047182
Test - acc:         0.914100 loss:        0.329049
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.983780 loss:        0.047316
Test - acc:         0.916100 loss:        0.327136
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.984380 loss:        0.045995
Test - acc:         0.915400 loss:        0.336510
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.984120 loss:        0.047531
Test - acc:         0.916400 loss:        0.333218
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.983900 loss:        0.047990
Test - acc:         0.914600 loss:        0.329885
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.983560 loss:        0.047581
Test - acc:         0.912900 loss:        0.337288
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.956620 loss:        0.125118
Test - acc:         0.904700 loss:        0.337420
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.960940 loss:        0.111416
Test - acc:         0.904800 loss:        0.321096
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.964760 loss:        0.100735
Test - acc:         0.906600 loss:        0.318202
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.964160 loss:        0.098963
Test - acc:         0.907800 loss:        0.319644
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.966060 loss:        0.096384
Test - acc:         0.909100 loss:        0.317151
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.967860 loss:        0.090855
Test - acc:         0.908600 loss:        0.317319
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.967200 loss:        0.091702
Test - acc:         0.909900 loss:        0.312018
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.967700 loss:        0.090161
Test - acc:         0.909700 loss:        0.317735
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.969120 loss:        0.086891
Test - acc:         0.909500 loss:        0.310938
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.970840 loss:        0.082768
Test - acc:         0.909000 loss:        0.318915
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.971020 loss:        0.081917
Test - acc:         0.911200 loss:        0.314096
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.970360 loss:        0.085208
Test - acc:         0.908900 loss:        0.321935
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.971880 loss:        0.079474
Test - acc:         0.910100 loss:        0.319738
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.971780 loss:        0.081122
Test - acc:         0.910000 loss:        0.316571
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.971600 loss:        0.080090
Test - acc:         0.909600 loss:        0.319198
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.971040 loss:        0.079654
Test - acc:         0.910700 loss:        0.317684
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.973400 loss:        0.077195
Test - acc:         0.909200 loss:        0.323429
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.972720 loss:        0.078214
Test - acc:         0.911600 loss:        0.320526
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.973600 loss:        0.076716
Test - acc:         0.911000 loss:        0.314529
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.973520 loss:        0.074610
Test - acc:         0.910800 loss:        0.318859
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.973340 loss:        0.075567
Test - acc:         0.910200 loss:        0.318766
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.972740 loss:        0.074944
Test - acc:         0.909300 loss:        0.323084
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.972440 loss:        0.077792
Test - acc:         0.910500 loss:        0.321316
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.974880 loss:        0.071888
Test - acc:         0.909400 loss:        0.321566
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.974840 loss:        0.072224
Test - acc:         0.908700 loss:        0.325987
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.975100 loss:        0.072315
Test - acc:         0.910000 loss:        0.323968
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.974660 loss:        0.070763
Test - acc:         0.909100 loss:        0.327841
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.975100 loss:        0.071506
Test - acc:         0.910000 loss:        0.335088
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.975200 loss:        0.070139
Test - acc:         0.908400 loss:        0.329345
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.975420 loss:        0.070491
Test - acc:         0.909000 loss:        0.329972
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.975460 loss:        0.070480
Test - acc:         0.910700 loss:        0.325233
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.974820 loss:        0.071760
Test - acc:         0.908400 loss:        0.327699
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.975500 loss:        0.069407
Test - acc:         0.909300 loss:        0.332100
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.975760 loss:        0.069587
Test - acc:         0.909500 loss:        0.330639
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.976440 loss:        0.068040
Test - acc:         0.908500 loss:        0.335189
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.975180 loss:        0.070006
Test - acc:         0.909100 loss:        0.333888
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.975720 loss:        0.069034
Test - acc:         0.910500 loss:        0.332555
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.976740 loss:        0.066492
Test - acc:         0.908700 loss:        0.338983
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.976400 loss:        0.067055
Test - acc:         0.909100 loss:        0.329160
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.977780 loss:        0.063256
Test - acc:         0.911000 loss:        0.336537
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.977800 loss:        0.065446
Test - acc:         0.910200 loss:        0.333553
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.976220 loss:        0.067381
Test - acc:         0.909200 loss:        0.335287
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.977420 loss:        0.063809
Test - acc:         0.911500 loss:        0.336053
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.977220 loss:        0.064918
Test - acc:         0.909400 loss:        0.336018
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.977660 loss:        0.064692
Test - acc:         0.905700 loss:        0.345029
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.978380 loss:        0.063202
Test - acc:         0.910000 loss:        0.337001
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.978700 loss:        0.062809
Test - acc:         0.909000 loss:        0.348122
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.978320 loss:        0.061464
Test - acc:         0.910300 loss:        0.338482
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.977520 loss:        0.063469
Test - acc:         0.908700 loss:        0.346079
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.978640 loss:        0.061514
Test - acc:         0.907100 loss:        0.355409
Sparsity :          0.9844
Wdecay :        0.000500
