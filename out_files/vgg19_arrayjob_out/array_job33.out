Running --prune_criterion global_magnitude --seed 43 --prune_freq 39 --prune_rate 0.5 --comment=vgg19_crit=global_magnitude_pf=39_seed=43 --save_model=pre-finetune/vgg19_global_magnitude_pf39_s43
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "global_magnitude",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_global_magnitude_pf39_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.105300 loss:        2.601138
Test - acc:         0.109400 loss:        2.297950
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.109500 loss:        2.296811
Test - acc:         0.121000 loss:        2.290626
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.135060 loss:        2.237199
Test - acc:         0.195300 loss:        2.051474
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.236160 loss:        1.912667
Test - acc:         0.262600 loss:        1.785715
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.333360 loss:        1.685028
Test - acc:         0.349500 loss:        1.665372
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.444340 loss:        1.475077
Test - acc:         0.474900 loss:        1.444693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.547480 loss:        1.252955
Test - acc:         0.515300 loss:        1.357213
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.628240 loss:        1.060720
Test - acc:         0.621900 loss:        1.115540
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.682960 loss:        0.919136
Test - acc:         0.642600 loss:        1.043907
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.722260 loss:        0.830122
Test - acc:         0.668000 loss:        1.003053
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.744180 loss:        0.775274
Test - acc:         0.625800 loss:        1.237646
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.763340 loss:        0.715959
Test - acc:         0.698600 loss:        0.897794
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.772800 loss:        0.692598
Test - acc:         0.643300 loss:        1.246807
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.783280 loss:        0.663185
Test - acc:         0.740300 loss:        0.836798
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.791500 loss:        0.640620
Test - acc:         0.727400 loss:        0.895406
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.793680 loss:        0.635948
Test - acc:         0.722900 loss:        0.857718
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.801200 loss:        0.611050
Test - acc:         0.686800 loss:        1.000325
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.803660 loss:        0.604088
Test - acc:         0.787200 loss:        0.699029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.808180 loss:        0.589044
Test - acc:         0.729800 loss:        0.866465
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.818440 loss:        0.569596
Test - acc:         0.746700 loss:        0.787903
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.817640 loss:        0.565120
Test - acc:         0.740400 loss:        0.878687
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.818520 loss:        0.559278
Test - acc:         0.729600 loss:        0.940122
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.825000 loss:        0.546039
Test - acc:         0.771700 loss:        0.706058
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.829120 loss:        0.535224
Test - acc:         0.712800 loss:        0.952716
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.826080 loss:        0.540991
Test - acc:         0.765300 loss:        0.804311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.829180 loss:        0.530004
Test - acc:         0.777800 loss:        0.723334
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.834860 loss:        0.515444
Test - acc:         0.759500 loss:        0.789311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.833080 loss:        0.519963
Test - acc:         0.705800 loss:        1.057780
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.838420 loss:        0.500873
Test - acc:         0.757500 loss:        0.824629
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.839660 loss:        0.498996
Test - acc:         0.768500 loss:        0.803411
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.840520 loss:        0.494644
Test - acc:         0.676400 loss:        1.210812
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.840660 loss:        0.494944
Test - acc:         0.712200 loss:        1.064473
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.839980 loss:        0.493296
Test - acc:         0.793100 loss:        0.669570
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.843080 loss:        0.486499
Test - acc:         0.807000 loss:        0.617336
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.843280 loss:        0.483888
Test - acc:         0.794200 loss:        0.667158
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.843480 loss:        0.484887
Test - acc:         0.765100 loss:        0.799214
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.843420 loss:        0.488633
Test - acc:         0.765000 loss:        0.769964
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.845880 loss:        0.479952
Test - acc:         0.704900 loss:        1.180704
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.847740 loss:        0.470945
Test - acc:         0.763200 loss:        0.836014
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.852020 loss:        0.453213
Test - acc:         0.723500 loss:        0.956319
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.850820 loss:        0.463812
Test - acc:         0.740700 loss:        0.907963
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.852820 loss:        0.459975
Test - acc:         0.802200 loss:        0.629193
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.851180 loss:        0.457112
Test - acc:         0.813800 loss:        0.589809
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.852340 loss:        0.453854
Test - acc:         0.770400 loss:        0.749669
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.855880 loss:        0.449102
Test - acc:         0.771000 loss:        0.787130
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.852420 loss:        0.452886
Test - acc:         0.781000 loss:        0.684535
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.853520 loss:        0.450212
Test - acc:         0.789900 loss:        0.658433
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.856500 loss:        0.443013
Test - acc:         0.798000 loss:        0.640110
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.858540 loss:        0.436694
Test - acc:         0.729600 loss:        0.867122
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.854280 loss:        0.450001
Test - acc:         0.763600 loss:        0.771093
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.859620 loss:        0.432174
Test - acc:         0.718400 loss:        0.961896
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.857160 loss:        0.443639
Test - acc:         0.714600 loss:        1.041913
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.858220 loss:        0.436337
Test - acc:         0.832200 loss:        0.518862
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.859440 loss:        0.430936
Test - acc:         0.848300 loss:        0.474163
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.862060 loss:        0.423072
Test - acc:         0.735900 loss:        0.883990
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.861060 loss:        0.426779
Test - acc:         0.794600 loss:        0.620065
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.858300 loss:        0.433596
Test - acc:         0.803400 loss:        0.645874
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.862620 loss:        0.421219
Test - acc:         0.814000 loss:        0.573342
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.862180 loss:        0.423287
Test - acc:         0.776000 loss:        0.770044
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.862560 loss:        0.416697
Test - acc:         0.825300 loss:        0.544026
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.864000 loss:        0.410377
Test - acc:         0.815700 loss:        0.591204
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.865300 loss:        0.410966
Test - acc:         0.778500 loss:        0.768517
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.861900 loss:        0.421835
Test - acc:         0.815300 loss:        0.587815
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.865220 loss:        0.411758
Test - acc:         0.849000 loss:        0.468284
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.865720 loss:        0.409979
Test - acc:         0.680100 loss:        1.188645
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.864380 loss:        0.410578
Test - acc:         0.643000 loss:        1.503695
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.867120 loss:        0.405080
Test - acc:         0.758600 loss:        0.750941
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.863680 loss:        0.409873
Test - acc:         0.818100 loss:        0.568839
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.869300 loss:        0.400569
Test - acc:         0.715800 loss:        0.924837
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.867160 loss:        0.403429
Test - acc:         0.748400 loss:        0.848108
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.866560 loss:        0.405998
Test - acc:         0.759900 loss:        0.720355
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.870180 loss:        0.395469
Test - acc:         0.815800 loss:        0.604067
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.868380 loss:        0.401073
Test - acc:         0.713100 loss:        1.125212
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.868800 loss:        0.395874
Test - acc:         0.773800 loss:        0.690956
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.868740 loss:        0.396991
Test - acc:         0.816000 loss:        0.582503
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.868820 loss:        0.397680
Test - acc:         0.814600 loss:        0.554769
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.868420 loss:        0.395504
Test - acc:         0.830900 loss:        0.518110
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.869860 loss:        0.391121
Test - acc:         0.812400 loss:        0.602803
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.870720 loss:        0.389308
Test - acc:         0.832200 loss:        0.540838
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.872860 loss:        0.382764
Test - acc:         0.779300 loss:        0.710103
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.870660 loss:        0.389902
Test - acc:         0.786500 loss:        0.686213
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.872060 loss:        0.383167
Test - acc:         0.829800 loss:        0.536523
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.873080 loss:        0.383462
Test - acc:         0.849200 loss:        0.467047
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.872940 loss:        0.383878
Test - acc:         0.819500 loss:        0.560606
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.872120 loss:        0.383832
Test - acc:         0.805600 loss:        0.607784
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.872660 loss:        0.383645
Test - acc:         0.808000 loss:        0.616195
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.874100 loss:        0.375489
Test - acc:         0.850200 loss:        0.465230
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.874280 loss:        0.378467
Test - acc:         0.796800 loss:        0.647575
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.874680 loss:        0.379541
Test - acc:         0.733300 loss:        0.954893
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.875300 loss:        0.372936
Test - acc:         0.819600 loss:        0.603639
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.874160 loss:        0.379989
Test - acc:         0.823200 loss:        0.537780
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.874140 loss:        0.377541
Test - acc:         0.795800 loss:        0.631951
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.873340 loss:        0.382265
Test - acc:         0.822300 loss:        0.552982
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.871080 loss:        0.393748
Test - acc:         0.820600 loss:        0.567599
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.871000 loss:        0.391802
Test - acc:         0.810900 loss:        0.580242
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.874240 loss:        0.377467
Test - acc:         0.844100 loss:        0.505928
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.872160 loss:        0.388661
Test - acc:         0.802400 loss:        0.618357
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.874820 loss:        0.381668
Test - acc:         0.793000 loss:        0.659245
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.872880 loss:        0.381169
Test - acc:         0.827900 loss:        0.575053
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.875740 loss:        0.373235
Test - acc:         0.780300 loss:        0.673007
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.875660 loss:        0.375309
Test - acc:         0.856500 loss:        0.446516
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.877580 loss:        0.364831
Test - acc:         0.834000 loss:        0.497193
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.876840 loss:        0.371585
Test - acc:         0.829300 loss:        0.530777
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.875720 loss:        0.373867
Test - acc:         0.800500 loss:        0.633928
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.876580 loss:        0.367223
Test - acc:         0.830300 loss:        0.560532
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.875260 loss:        0.373757
Test - acc:         0.791000 loss:        0.642811
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.874740 loss:        0.372705
Test - acc:         0.837700 loss:        0.496178
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.873940 loss:        0.375301
Test - acc:         0.832000 loss:        0.505850
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.875300 loss:        0.373252
Test - acc:         0.711900 loss:        1.020656
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.873480 loss:        0.378330
Test - acc:         0.819500 loss:        0.596327
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.878400 loss:        0.364727
Test - acc:         0.784500 loss:        0.689192
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.878500 loss:        0.366769
Test - acc:         0.816700 loss:        0.586602
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.878020 loss:        0.367476
Test - acc:         0.834200 loss:        0.511664
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.877340 loss:        0.366727
Test - acc:         0.832900 loss:        0.517103
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.881260 loss:        0.357886
Test - acc:         0.843900 loss:        0.478664
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.877120 loss:        0.369386
Test - acc:         0.773100 loss:        0.681071
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.879100 loss:        0.363979
Test - acc:         0.827000 loss:        0.520130
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.883400 loss:        0.352155
Test - acc:         0.852100 loss:        0.452884
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.883740 loss:        0.347495
Test - acc:         0.799500 loss:        0.632737
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.878740 loss:        0.359901
Test - acc:         0.800900 loss:        0.669456
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.878060 loss:        0.359943
Test - acc:         0.770300 loss:        0.751523
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.882860 loss:        0.352986
Test - acc:         0.827500 loss:        0.551988
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.882180 loss:        0.356033
Test - acc:         0.833800 loss:        0.513162
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.882080 loss:        0.354355
Test - acc:         0.825200 loss:        0.544499
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.882480 loss:        0.351612
Test - acc:         0.805200 loss:        0.657453
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.877920 loss:        0.359939
Test - acc:         0.755600 loss:        0.776354
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.882700 loss:        0.350682
Test - acc:         0.837300 loss:        0.514662
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.882540 loss:        0.351489
Test - acc:         0.773200 loss:        0.759049
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.879200 loss:        0.358258
Test - acc:         0.801200 loss:        0.626036
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.881120 loss:        0.354949
Test - acc:         0.794400 loss:        0.679044
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.881320 loss:        0.354046
Test - acc:         0.841600 loss:        0.489852
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.881700 loss:        0.351726
Test - acc:         0.810800 loss:        0.588608
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.880500 loss:        0.356987
Test - acc:         0.680600 loss:        1.314127
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.881560 loss:        0.353153
Test - acc:         0.753800 loss:        0.859621
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.882380 loss:        0.350723
Test - acc:         0.839000 loss:        0.482199
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.874740 loss:        0.373795
Test - acc:         0.792700 loss:        0.661179
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.879680 loss:        0.361622
Test - acc:         0.793600 loss:        0.688102
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.882060 loss:        0.352973
Test - acc:         0.850300 loss:        0.457896
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.884240 loss:        0.345870
Test - acc:         0.800600 loss:        0.632139
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.881400 loss:        0.354051
Test - acc:         0.847100 loss:        0.466526
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.880680 loss:        0.352685
Test - acc:         0.818000 loss:        0.579051
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.880200 loss:        0.357917
Test - acc:         0.782000 loss:        0.716246
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.883760 loss:        0.346826
Test - acc:         0.846700 loss:        0.468918
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.882960 loss:        0.351643
Test - acc:         0.805400 loss:        0.625805
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.880460 loss:        0.354567
Test - acc:         0.858000 loss:        0.425469
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.881700 loss:        0.348447
Test - acc:         0.763700 loss:        0.703428
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.883700 loss:        0.351937
Test - acc:         0.833900 loss:        0.521036
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.883520 loss:        0.350691
Test - acc:         0.819300 loss:        0.570118
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.881420 loss:        0.352896
Test - acc:         0.818200 loss:        0.538536
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.879040 loss:        0.360520
Test - acc:         0.810200 loss:        0.596170
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.936500 loss:        0.190658
Test - acc:         0.918500 loss:        0.245187
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.955400 loss:        0.134401
Test - acc:         0.925400 loss:        0.238688
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.962220 loss:        0.113487
Test - acc:         0.922900 loss:        0.239833
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.966120 loss:        0.101989
Test - acc:         0.925800 loss:        0.237926
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.969840 loss:        0.090400
Test - acc:         0.923100 loss:        0.247293
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.973000 loss:        0.080068
Test - acc:         0.925300 loss:        0.249624
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.977000 loss:        0.070918
Test - acc:         0.926900 loss:        0.244838
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.978920 loss:        0.063535
Test - acc:         0.922500 loss:        0.271516
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.060409
Test - acc:         0.927200 loss:        0.256625
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.981680 loss:        0.054835
Test - acc:         0.923500 loss:        0.269877
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.982540 loss:        0.051973
Test - acc:         0.928100 loss:        0.267018
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.984420 loss:        0.047925
Test - acc:         0.922800 loss:        0.278467
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.985040 loss:        0.045581
Test - acc:         0.925800 loss:        0.275204
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.984180 loss:        0.046598
Test - acc:         0.926400 loss:        0.263613
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.984900 loss:        0.044026
Test - acc:         0.925200 loss:        0.276934
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.985240 loss:        0.043167
Test - acc:         0.923000 loss:        0.296956
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.986380 loss:        0.040224
Test - acc:         0.925500 loss:        0.291655
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.985340 loss:        0.042358
Test - acc:         0.923700 loss:        0.291838
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.987280 loss:        0.039337
Test - acc:         0.925200 loss:        0.286241
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.987120 loss:        0.038659
Test - acc:         0.919700 loss:        0.319847
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.986860 loss:        0.039268
Test - acc:         0.925900 loss:        0.294596
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.986160 loss:        0.039886
Test - acc:         0.923300 loss:        0.297289
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.986720 loss:        0.038377
Test - acc:         0.924200 loss:        0.299940
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.985540 loss:        0.042069
Test - acc:         0.920600 loss:        0.300161
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.985900 loss:        0.041603
Test - acc:         0.916000 loss:        0.320758
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.984680 loss:        0.043818
Test - acc:         0.923300 loss:        0.296566
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.985880 loss:        0.042088
Test - acc:         0.922600 loss:        0.305806
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.985300 loss:        0.044207
Test - acc:         0.924300 loss:        0.289197
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.985100 loss:        0.044543
Test - acc:         0.917700 loss:        0.318041
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.985580 loss:        0.043295
Test - acc:         0.923000 loss:        0.303612
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.984780 loss:        0.043903
Test - acc:         0.921000 loss:        0.301818
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.984180 loss:        0.046023
Test - acc:         0.920400 loss:        0.291651
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.985800 loss:        0.042206
Test - acc:         0.914600 loss:        0.339447
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.984640 loss:        0.045808
Test - acc:         0.915700 loss:        0.320291
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.984560 loss:        0.047006
Test - acc:         0.914600 loss:        0.329404
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.983780 loss:        0.048321
Test - acc:         0.917800 loss:        0.315658
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.984120 loss:        0.047861
Test - acc:         0.915500 loss:        0.315957
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.983620 loss:        0.048035
Test - acc:         0.917200 loss:        0.323810
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.984820 loss:        0.044843
Test - acc:         0.918800 loss:        0.319499
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.983100 loss:        0.051074
Test - acc:         0.913500 loss:        0.319766
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.983320 loss:        0.050267
Test - acc:         0.910500 loss:        0.355519
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.982320 loss:        0.052198
Test - acc:         0.907500 loss:        0.365870
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.982040 loss:        0.053995
Test - acc:         0.905900 loss:        0.379003
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.981140 loss:        0.055172
Test - acc:         0.910700 loss:        0.332332
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.983500 loss:        0.050464
Test - acc:         0.917900 loss:        0.330558
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.982740 loss:        0.052268
Test - acc:         0.914400 loss:        0.325240
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.985760 loss:        0.043308
Test - acc:         0.917400 loss:        0.320330
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.985320 loss:        0.044225
Test - acc:         0.914600 loss:        0.330142
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.986360 loss:        0.041495
Test - acc:         0.919000 loss:        0.313099
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.985460 loss:        0.044048
Test - acc:         0.915300 loss:        0.313579
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.985140 loss:        0.043961
Test - acc:         0.909100 loss:        0.358831
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.986680 loss:        0.040093
Test - acc:         0.918900 loss:        0.310067
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.986860 loss:        0.039884
Test - acc:         0.898000 loss:        0.405587
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.985000 loss:        0.043689
Test - acc:         0.911900 loss:        0.343129
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.985140 loss:        0.045738
Test - acc:         0.910500 loss:        0.351196
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.985320 loss:        0.043414
Test - acc:         0.912000 loss:        0.342125
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.985800 loss:        0.043633
Test - acc:         0.909200 loss:        0.356898
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.986660 loss:        0.040221
Test - acc:         0.912200 loss:        0.345375
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.984220 loss:        0.047435
Test - acc:         0.907900 loss:        0.370117
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.983680 loss:        0.049534
Test - acc:         0.907300 loss:        0.364581
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.984820 loss:        0.044630
Test - acc:         0.909200 loss:        0.357549
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.985140 loss:        0.045745
Test - acc:         0.912700 loss:        0.343041
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.983420 loss:        0.048711
Test - acc:         0.914000 loss:        0.337885
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.984600 loss:        0.045712
Test - acc:         0.917000 loss:        0.330525
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.985440 loss:        0.044203
Test - acc:         0.904900 loss:        0.356573
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.983340 loss:        0.048362
Test - acc:         0.916700 loss:        0.315165
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.983360 loss:        0.049239
Test - acc:         0.907800 loss:        0.351405
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.982240 loss:        0.051000
Test - acc:         0.914200 loss:        0.323328
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.984620 loss:        0.045860
Test - acc:         0.902700 loss:        0.388555
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.984900 loss:        0.045001
Test - acc:         0.920900 loss:        0.292972
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.982820 loss:        0.050066
Test - acc:         0.912700 loss:        0.336906
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.983640 loss:        0.048655
Test - acc:         0.919800 loss:        0.309672
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.983560 loss:        0.048017
Test - acc:         0.915400 loss:        0.327060
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.984280 loss:        0.045725
Test - acc:         0.909500 loss:        0.352670
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.983480 loss:        0.049526
Test - acc:         0.914700 loss:        0.334873
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.983680 loss:        0.049302
Test - acc:         0.916800 loss:        0.314312
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.984940 loss:        0.046780
Test - acc:         0.912500 loss:        0.333741
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.983200 loss:        0.049745
Test - acc:         0.909800 loss:        0.356115
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.984160 loss:        0.047023
Test - acc:         0.917100 loss:        0.317486
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.984560 loss:        0.046561
Test - acc:         0.907700 loss:        0.363530
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.985280 loss:        0.045359
Test - acc:         0.913800 loss:        0.334311
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.984000 loss:        0.047947
Test - acc:         0.910400 loss:        0.342171
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.981220 loss:        0.054908
Test - acc:         0.911800 loss:        0.350994
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.984660 loss:        0.046571
Test - acc:         0.913100 loss:        0.330874
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.978140 loss:        0.065737
Test - acc:         0.912500 loss:        0.306843
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.981160 loss:        0.055542
Test - acc:         0.914500 loss:        0.328494
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.982240 loss:        0.051792
Test - acc:         0.909400 loss:        0.375359
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.982720 loss:        0.050410
Test - acc:         0.920600 loss:        0.297169
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.984680 loss:        0.047229
Test - acc:         0.909000 loss:        0.350284
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.982880 loss:        0.050285
Test - acc:         0.913800 loss:        0.368340
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.984200 loss:        0.047587
Test - acc:         0.916300 loss:        0.329069
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.985620 loss:        0.043431
Test - acc:         0.917300 loss:        0.323275
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.984580 loss:        0.045564
Test - acc:         0.913300 loss:        0.330499
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.984320 loss:        0.044412
Test - acc:         0.903100 loss:        0.380513
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.984280 loss:        0.046721
Test - acc:         0.912400 loss:        0.344909
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.986360 loss:        0.040258
Test - acc:         0.912500 loss:        0.362676
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.984440 loss:        0.046367
Test - acc:         0.911300 loss:        0.364311
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.982000 loss:        0.052454
Test - acc:         0.911400 loss:        0.357185
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.985100 loss:        0.046212
Test - acc:         0.911500 loss:        0.348685
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.983100 loss:        0.050767
Test - acc:         0.913900 loss:        0.343513
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.992060 loss:        0.025700
Test - acc:         0.929400 loss:        0.274354
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.995900 loss:        0.015222
Test - acc:         0.932200 loss:        0.269587
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.996280 loss:        0.013463
Test - acc:         0.932200 loss:        0.271954
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.997480 loss:        0.010212
Test - acc:         0.933200 loss:        0.273499
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.997520 loss:        0.009249
Test - acc:         0.933900 loss:        0.275823
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.997800 loss:        0.009249
Test - acc:         0.933300 loss:        0.276274
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.997940 loss:        0.008130
Test - acc:         0.933200 loss:        0.280431
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.998180 loss:        0.007503
Test - acc:         0.935200 loss:        0.279870
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.998460 loss:        0.007106
Test - acc:         0.934900 loss:        0.282297
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.998280 loss:        0.007000
Test - acc:         0.934300 loss:        0.280952
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.998820 loss:        0.006075
Test - acc:         0.933100 loss:        0.285375
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.998500 loss:        0.006404
Test - acc:         0.934300 loss:        0.284181
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.998540 loss:        0.006152
Test - acc:         0.932400 loss:        0.288096
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.998600 loss:        0.006200
Test - acc:         0.933100 loss:        0.287010
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.998540 loss:        0.005886
Test - acc:         0.933900 loss:        0.286251
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.998880 loss:        0.005277
Test - acc:         0.934000 loss:        0.283796
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.998900 loss:        0.004994
Test - acc:         0.932700 loss:        0.284946
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.998960 loss:        0.004695
Test - acc:         0.933400 loss:        0.287671
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999100 loss:        0.004693
Test - acc:         0.932900 loss:        0.288091
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999140 loss:        0.004433
Test - acc:         0.932900 loss:        0.292559
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999160 loss:        0.004428
Test - acc:         0.932100 loss:        0.290300
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999220 loss:        0.004535
Test - acc:         0.934000 loss:        0.292817
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.004583
Test - acc:         0.934100 loss:        0.290603
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.980060 loss:        0.058867
Test - acc:         0.918600 loss:        0.336641
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.986580 loss:        0.040892
Test - acc:         0.923200 loss:        0.313421
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.987520 loss:        0.036841
Test - acc:         0.926100 loss:        0.307085
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.989040 loss:        0.032232
Test - acc:         0.923300 loss:        0.307475
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.990780 loss:        0.029114
Test - acc:         0.925000 loss:        0.314146
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.992000 loss:        0.026690
Test - acc:         0.926300 loss:        0.307039
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.991740 loss:        0.026036
Test - acc:         0.925400 loss:        0.310178
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.992640 loss:        0.023653
Test - acc:         0.925000 loss:        0.307915
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.993340 loss:        0.022574
Test - acc:         0.927000 loss:        0.307779
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.992800 loss:        0.022322
Test - acc:         0.926500 loss:        0.308663
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.994060 loss:        0.020909
Test - acc:         0.927700 loss:        0.306966
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.994460 loss:        0.018655
Test - acc:         0.927000 loss:        0.309906
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.994840 loss:        0.017503
Test - acc:         0.928200 loss:        0.311733
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.994620 loss:        0.017998
Test - acc:         0.926200 loss:        0.316487
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.994880 loss:        0.017098
Test - acc:         0.926000 loss:        0.315501
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.994160 loss:        0.018088
Test - acc:         0.926700 loss:        0.313300
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.995480 loss:        0.015797
Test - acc:         0.927000 loss:        0.312265
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.995480 loss:        0.015666
Test - acc:         0.926400 loss:        0.315510
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.995540 loss:        0.014954
Test - acc:         0.928600 loss:        0.313167
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.995980 loss:        0.014090
Test - acc:         0.926600 loss:        0.316261
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.996360 loss:        0.013190
Test - acc:         0.927600 loss:        0.315159
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.996240 loss:        0.013303
Test - acc:         0.925800 loss:        0.321770
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.996440 loss:        0.012397
Test - acc:         0.927700 loss:        0.319857
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.996420 loss:        0.012676
Test - acc:         0.928400 loss:        0.318340
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.996300 loss:        0.012935
Test - acc:         0.927100 loss:        0.322491
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.996340 loss:        0.012498
Test - acc:         0.927800 loss:        0.324479
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.996500 loss:        0.012674
Test - acc:         0.927300 loss:        0.323718
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.996140 loss:        0.012192
Test - acc:         0.928400 loss:        0.326593
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.996760 loss:        0.011566
Test - acc:         0.927000 loss:        0.330665
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.996480 loss:        0.011836
Test - acc:         0.927400 loss:        0.326896
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.997160 loss:        0.010547
Test - acc:         0.928100 loss:        0.327813
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.996700 loss:        0.011089
Test - acc:         0.928300 loss:        0.323403
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.996700 loss:        0.010589
Test - acc:         0.929400 loss:        0.321403
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.996940 loss:        0.010082
Test - acc:         0.928900 loss:        0.326369
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.997240 loss:        0.010047
Test - acc:         0.930000 loss:        0.324592
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.997140 loss:        0.009928
Test - acc:         0.930600 loss:        0.322301
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.997120 loss:        0.010558
Test - acc:         0.930000 loss:        0.323260
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.997220 loss:        0.010001
Test - acc:         0.929100 loss:        0.327677
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.997620 loss:        0.009332
Test - acc:         0.928700 loss:        0.321507
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.921240 loss:        0.243203
Test - acc:         0.891900 loss:        0.383575
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.946760 loss:        0.155575
Test - acc:         0.898600 loss:        0.357001
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.954420 loss:        0.133138
Test - acc:         0.903700 loss:        0.342026
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.959020 loss:        0.119361
Test - acc:         0.905400 loss:        0.329595
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.962360 loss:        0.109782
Test - acc:         0.907100 loss:        0.322072
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.965060 loss:        0.101003
Test - acc:         0.909100 loss:        0.325566
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.967880 loss:        0.096621
Test - acc:         0.910600 loss:        0.329924
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.967880 loss:        0.094330
Test - acc:         0.909200 loss:        0.331434
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.970680 loss:        0.086573
Test - acc:         0.909100 loss:        0.328979
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.970920 loss:        0.084207
Test - acc:         0.908800 loss:        0.329378
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.972420 loss:        0.080064
Test - acc:         0.913000 loss:        0.326588
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.973400 loss:        0.077636
Test - acc:         0.911500 loss:        0.326910
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.974040 loss:        0.075111
Test - acc:         0.912300 loss:        0.328055
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.974740 loss:        0.073649
Test - acc:         0.912700 loss:        0.326235
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.976860 loss:        0.069839
Test - acc:         0.914300 loss:        0.321381
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.976000 loss:        0.071332
Test - acc:         0.913500 loss:        0.323528
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.977400 loss:        0.068116
Test - acc:         0.913500 loss:        0.324451
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.977800 loss:        0.065091
Test - acc:         0.912500 loss:        0.327132
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.978920 loss:        0.064280
Test - acc:         0.911500 loss:        0.327166
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.979580 loss:        0.062144
Test - acc:         0.913000 loss:        0.334852
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.976900 loss:        0.065019
Test - acc:         0.910400 loss:        0.333203
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.979120 loss:        0.058640
Test - acc:         0.912500 loss:        0.337232
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.980940 loss:        0.057236
Test - acc:         0.912900 loss:        0.341860
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.978960 loss:        0.058962
Test - acc:         0.912100 loss:        0.335075
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.980200 loss:        0.058969
Test - acc:         0.912900 loss:        0.324360
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.980600 loss:        0.057163
Test - acc:         0.913300 loss:        0.333602
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.981420 loss:        0.055420
Test - acc:         0.913300 loss:        0.334038
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.981880 loss:        0.054600
Test - acc:         0.913800 loss:        0.336121
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.981040 loss:        0.054998
Test - acc:         0.913900 loss:        0.336248
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.982140 loss:        0.052583
Test - acc:         0.913800 loss:        0.336715
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.981560 loss:        0.054576
Test - acc:         0.914600 loss:        0.335267
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.982400 loss:        0.052458
Test - acc:         0.914200 loss:        0.337884
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.982340 loss:        0.053354
Test - acc:         0.916500 loss:        0.328680
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.982280 loss:        0.052546
Test - acc:         0.913700 loss:        0.333443
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.982420 loss:        0.051344
Test - acc:         0.917100 loss:        0.326854
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.983480 loss:        0.048345
Test - acc:         0.916300 loss:        0.328992
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.984680 loss:        0.045605
Test - acc:         0.915500 loss:        0.335140
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.984120 loss:        0.046800
Test - acc:         0.912500 loss:        0.341369
Sparsity :          0.9961
Wdecay :        0.000500
