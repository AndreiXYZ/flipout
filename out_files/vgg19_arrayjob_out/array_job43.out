Running --prune_criterion random --seed 44 --prune_freq 117 --prune_rate 0.5 --comment=vgg19_crit=random_pf=117_seed=44 --save_model=pre-finetune/vgg19_random_pf117_s44
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "random",
    "prune_freq": 117,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_random_pf117_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.108300 loss:        2.474734
Test - acc:         0.119300 loss:        2.267837
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.150540 loss:        2.171051
Test - acc:         0.194700 loss:        1.934882
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.241500 loss:        1.878880
Test - acc:         0.265700 loss:        1.874334
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.306100 loss:        1.759676
Test - acc:         0.321600 loss:        1.772883
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.438900 loss:        1.470889
Test - acc:         0.480900 loss:        1.401025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.559400 loss:        1.215320
Test - acc:         0.570900 loss:        1.236438
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.640540 loss:        1.022796
Test - acc:         0.562900 loss:        1.313802
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.692820 loss:        0.908134
Test - acc:         0.458400 loss:        1.942289
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.726620 loss:        0.830650
Test - acc:         0.698100 loss:        0.963013
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.750900 loss:        0.760590
Test - acc:         0.690900 loss:        0.970304
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.770860 loss:        0.699995
Test - acc:         0.733400 loss:        0.833592
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.780940 loss:        0.667889
Test - acc:         0.743900 loss:        0.787249
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.789600 loss:        0.649976
Test - acc:         0.738800 loss:        0.849763
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.796420 loss:        0.625437
Test - acc:         0.722200 loss:        0.895129
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.802720 loss:        0.608729
Test - acc:         0.740500 loss:        0.775364
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.806240 loss:        0.596568
Test - acc:         0.700700 loss:        1.022337
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.812280 loss:        0.587819
Test - acc:         0.763300 loss:        0.771137
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.812580 loss:        0.580530
Test - acc:         0.772300 loss:        0.710877
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.818180 loss:        0.560021
Test - acc:         0.742400 loss:        0.803548
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.821800 loss:        0.557051
Test - acc:         0.728400 loss:        0.884478
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.827740 loss:        0.535048
Test - acc:         0.737700 loss:        0.793409
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.828660 loss:        0.530681
Test - acc:         0.763400 loss:        0.765218
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.825700 loss:        0.538257
Test - acc:         0.761600 loss:        0.788449
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.826500 loss:        0.539644
Test - acc:         0.788700 loss:        0.688115
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.833200 loss:        0.521488
Test - acc:         0.765500 loss:        0.769116
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.835460 loss:        0.515609
Test - acc:         0.707000 loss:        0.956001
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.836300 loss:        0.512242
Test - acc:         0.744700 loss:        0.809497
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.839600 loss:        0.497366
Test - acc:         0.704800 loss:        1.055230
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.837900 loss:        0.502614
Test - acc:         0.815400 loss:        0.592117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.838720 loss:        0.504216
Test - acc:         0.766300 loss:        0.782455
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.846120 loss:        0.485764
Test - acc:         0.739600 loss:        0.860019
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.840980 loss:        0.499632
Test - acc:         0.767800 loss:        0.798063
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.844020 loss:        0.487687
Test - acc:         0.785900 loss:        0.677178
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.840220 loss:        0.499990
Test - acc:         0.769100 loss:        0.729718
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.844640 loss:        0.483766
Test - acc:         0.789700 loss:        0.670113
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.847620 loss:        0.479940
Test - acc:         0.780700 loss:        0.708346
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.847820 loss:        0.476032
Test - acc:         0.694800 loss:        0.955153
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.846740 loss:        0.480284
Test - acc:         0.744600 loss:        0.948408
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.849640 loss:        0.473404
Test - acc:         0.783500 loss:        0.690285
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.849180 loss:        0.473203
Test - acc:         0.751500 loss:        0.898768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.848480 loss:        0.472494
Test - acc:         0.721900 loss:        0.949589
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.850180 loss:        0.470560
Test - acc:         0.806800 loss:        0.602617
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.850760 loss:        0.465705
Test - acc:         0.799500 loss:        0.629135
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.848360 loss:        0.469438
Test - acc:         0.768300 loss:        0.786816
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.853020 loss:        0.464158
Test - acc:         0.746400 loss:        0.859362
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.853460 loss:        0.456696
Test - acc:         0.794600 loss:        0.663899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.852880 loss:        0.457089
Test - acc:         0.808800 loss:        0.618557
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.857000 loss:        0.449771
Test - acc:         0.782300 loss:        0.711926
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.854520 loss:        0.448991
Test - acc:         0.799400 loss:        0.636266
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.852680 loss:        0.454349
Test - acc:         0.806400 loss:        0.604921
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.856820 loss:        0.449033
Test - acc:         0.730100 loss:        0.955128
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.858580 loss:        0.441545
Test - acc:         0.746400 loss:        0.875038
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.856260 loss:        0.444644
Test - acc:         0.810500 loss:        0.590448
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.860220 loss:        0.437055
Test - acc:         0.784100 loss:        0.712976
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.859880 loss:        0.435651
Test - acc:         0.765100 loss:        0.776423
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.859060 loss:        0.438030
Test - acc:         0.740100 loss:        0.830604
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.859000 loss:        0.438488
Test - acc:         0.823400 loss:        0.542734
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.856360 loss:        0.440860
Test - acc:         0.795800 loss:        0.664613
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.861860 loss:        0.432752
Test - acc:         0.782700 loss:        0.685126
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.859580 loss:        0.437792
Test - acc:         0.791600 loss:        0.643237
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.857200 loss:        0.439134
Test - acc:         0.809500 loss:        0.619625
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.861160 loss:        0.429580
Test - acc:         0.780500 loss:        0.713928
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.863220 loss:        0.423986
Test - acc:         0.812500 loss:        0.587948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.861040 loss:        0.432553
Test - acc:         0.808100 loss:        0.666451
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.861880 loss:        0.430105
Test - acc:         0.725800 loss:        0.954056
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.861780 loss:        0.424264
Test - acc:         0.763900 loss:        0.728084
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.861920 loss:        0.429865
Test - acc:         0.782800 loss:        0.739310
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.861820 loss:        0.424858
Test - acc:         0.767300 loss:        0.743341
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.862740 loss:        0.423034
Test - acc:         0.838800 loss:        0.501345
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.863040 loss:        0.421400
Test - acc:         0.815000 loss:        0.581123
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.864320 loss:        0.423240
Test - acc:         0.834500 loss:        0.519555
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.866640 loss:        0.413950
Test - acc:         0.831700 loss:        0.523528
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.864360 loss:        0.418288
Test - acc:         0.820800 loss:        0.559322
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.867840 loss:        0.412379
Test - acc:         0.766000 loss:        0.757876
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.866600 loss:        0.416091
Test - acc:         0.836400 loss:        0.515224
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.868440 loss:        0.407757
Test - acc:         0.727800 loss:        0.905851
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.866020 loss:        0.416935
Test - acc:         0.839100 loss:        0.497251
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.864200 loss:        0.415742
Test - acc:         0.802200 loss:        0.627611
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.864520 loss:        0.417862
Test - acc:         0.797700 loss:        0.628908
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.862160 loss:        0.422781
Test - acc:         0.762500 loss:        0.798229
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.869200 loss:        0.404666
Test - acc:         0.828800 loss:        0.545756
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.865060 loss:        0.418578
Test - acc:         0.784700 loss:        0.725508
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.863580 loss:        0.419763
Test - acc:         0.799400 loss:        0.620531
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.868940 loss:        0.408306
Test - acc:         0.767800 loss:        0.764631
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.864000 loss:        0.415234
Test - acc:         0.800700 loss:        0.621845
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.865780 loss:        0.412683
Test - acc:         0.815800 loss:        0.579064
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.868760 loss:        0.407703
Test - acc:         0.833200 loss:        0.518468
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.869320 loss:        0.404598
Test - acc:         0.835900 loss:        0.509719
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.866560 loss:        0.410822
Test - acc:         0.791400 loss:        0.693861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.867500 loss:        0.408667
Test - acc:         0.814900 loss:        0.575468
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.863180 loss:        0.418969
Test - acc:         0.784600 loss:        0.721662
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.867560 loss:        0.407989
Test - acc:         0.799100 loss:        0.650269
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.862820 loss:        0.417657
Test - acc:         0.790200 loss:        0.684191
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.868660 loss:        0.403626
Test - acc:         0.833600 loss:        0.513746
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.869240 loss:        0.403502
Test - acc:         0.823200 loss:        0.558471
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.867820 loss:        0.412332
Test - acc:         0.786900 loss:        0.717720
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.868580 loss:        0.406730
Test - acc:         0.831200 loss:        0.537203
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.867420 loss:        0.410229
Test - acc:         0.833200 loss:        0.537067
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.869360 loss:        0.405386
Test - acc:         0.843300 loss:        0.494369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.865900 loss:        0.408322
Test - acc:         0.714400 loss:        0.986963
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.866920 loss:        0.411319
Test - acc:         0.833800 loss:        0.544343
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.866920 loss:        0.406242
Test - acc:         0.810500 loss:        0.583735
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.867760 loss:        0.405782
Test - acc:         0.767500 loss:        0.763915
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.866940 loss:        0.409973
Test - acc:         0.843800 loss:        0.479875
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.869500 loss:        0.401055
Test - acc:         0.801200 loss:        0.616615
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.868340 loss:        0.404501
Test - acc:         0.762200 loss:        0.812341
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.867980 loss:        0.402198
Test - acc:         0.816100 loss:        0.575990
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.869900 loss:        0.400487
Test - acc:         0.829200 loss:        0.556557
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.869440 loss:        0.404103
Test - acc:         0.819900 loss:        0.547084
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.868020 loss:        0.408310
Test - acc:         0.794600 loss:        0.705874
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.869240 loss:        0.403407
Test - acc:         0.812900 loss:        0.563207
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.868720 loss:        0.406881
Test - acc:         0.784700 loss:        0.681638
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.868360 loss:        0.411344
Test - acc:         0.745700 loss:        0.927781
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.871420 loss:        0.400819
Test - acc:         0.710400 loss:        1.072614
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.869500 loss:        0.401794
Test - acc:         0.804800 loss:        0.629328
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.870780 loss:        0.405644
Test - acc:         0.775700 loss:        0.718914
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.871500 loss:        0.398125
Test - acc:         0.774100 loss:        0.741777
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.788500 loss:        0.654400
Test - acc:         0.769300 loss:        0.713083
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.831440 loss:        0.516598
Test - acc:         0.795800 loss:        0.628892
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.845340 loss:        0.476751
Test - acc:         0.784300 loss:        0.650056
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.849840 loss:        0.461774
Test - acc:         0.808100 loss:        0.592681
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.850440 loss:        0.455070
Test - acc:         0.812900 loss:        0.597514
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.854500 loss:        0.445099
Test - acc:         0.785700 loss:        0.663323
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.851900 loss:        0.454715
Test - acc:         0.807500 loss:        0.606733
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.858840 loss:        0.437960
Test - acc:         0.761200 loss:        0.753730
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.857320 loss:        0.435561
Test - acc:         0.806300 loss:        0.598111
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.857120 loss:        0.437943
Test - acc:         0.796100 loss:        0.671187
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.858340 loss:        0.431491
Test - acc:         0.807300 loss:        0.604772
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.857560 loss:        0.433746
Test - acc:         0.762200 loss:        0.756127
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.858340 loss:        0.430078
Test - acc:         0.822400 loss:        0.574323
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.859400 loss:        0.431927
Test - acc:         0.834400 loss:        0.501454
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.861720 loss:        0.425992
Test - acc:         0.785900 loss:        0.708160
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.860080 loss:        0.430384
Test - acc:         0.803100 loss:        0.594263
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.864040 loss:        0.415137
Test - acc:         0.776400 loss:        0.776447
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.860640 loss:        0.426799
Test - acc:         0.780200 loss:        0.704626
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.859340 loss:        0.430510
Test - acc:         0.814700 loss:        0.562512
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.862120 loss:        0.420538
Test - acc:         0.818000 loss:        0.572038
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.862460 loss:        0.426124
Test - acc:         0.819200 loss:        0.576159
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.862220 loss:        0.426089
Test - acc:         0.782000 loss:        0.706016
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.859940 loss:        0.427237
Test - acc:         0.797700 loss:        0.624621
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.864980 loss:        0.414593
Test - acc:         0.789800 loss:        0.683081
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.862480 loss:        0.420347
Test - acc:         0.793000 loss:        0.628278
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.864660 loss:        0.416172
Test - acc:         0.816800 loss:        0.583695
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.863000 loss:        0.420824
Test - acc:         0.823800 loss:        0.532977
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.861560 loss:        0.420033
Test - acc:         0.828800 loss:        0.550296
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.859560 loss:        0.420976
Test - acc:         0.716900 loss:        0.987034
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.863100 loss:        0.416953
Test - acc:         0.770500 loss:        0.726242
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.862180 loss:        0.417033
Test - acc:         0.807300 loss:        0.595359
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.861740 loss:        0.419752
Test - acc:         0.797700 loss:        0.616525
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.867280 loss:        0.403648
Test - acc:         0.756100 loss:        0.757641
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.923060 loss:        0.231102
Test - acc:         0.910800 loss:        0.274033
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.941360 loss:        0.177412
Test - acc:         0.917100 loss:        0.260199
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.947900 loss:        0.156071
Test - acc:         0.918000 loss:        0.257973
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.953260 loss:        0.142599
Test - acc:         0.919700 loss:        0.250745
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.956520 loss:        0.129895
Test - acc:         0.920800 loss:        0.256581
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.959920 loss:        0.120993
Test - acc:         0.919600 loss:        0.268781
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.964180 loss:        0.109966
Test - acc:         0.922300 loss:        0.256829
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.966060 loss:        0.099996
Test - acc:         0.918800 loss:        0.282491
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.967620 loss:        0.096919
Test - acc:         0.921600 loss:        0.272585
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.970120 loss:        0.090418
Test - acc:         0.918400 loss:        0.290411
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.971620 loss:        0.085233
Test - acc:         0.919800 loss:        0.279895
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.972380 loss:        0.083926
Test - acc:         0.920700 loss:        0.274776
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.973280 loss:        0.081452
Test - acc:         0.910400 loss:        0.303442
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.973360 loss:        0.078948
Test - acc:         0.916400 loss:        0.293361
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.976200 loss:        0.071229
Test - acc:         0.918200 loss:        0.288852
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.975540 loss:        0.072813
Test - acc:         0.916300 loss:        0.299086
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.973360 loss:        0.076778
Test - acc:         0.913700 loss:        0.311258
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.975040 loss:        0.074797
Test - acc:         0.918800 loss:        0.287471
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.975720 loss:        0.071058
Test - acc:         0.917100 loss:        0.297179
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.977120 loss:        0.069015
Test - acc:         0.912200 loss:        0.317788
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.975840 loss:        0.069435
Test - acc:         0.914600 loss:        0.299654
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.976120 loss:        0.070450
Test - acc:         0.911000 loss:        0.321996
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.975960 loss:        0.072110
Test - acc:         0.914800 loss:        0.315990
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.976600 loss:        0.069513
Test - acc:         0.907800 loss:        0.339801
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.974500 loss:        0.075000
Test - acc:         0.908400 loss:        0.325819
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.974900 loss:        0.074802
Test - acc:         0.912300 loss:        0.310174
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.976940 loss:        0.070655
Test - acc:         0.913000 loss:        0.305711
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.975720 loss:        0.071242
Test - acc:         0.908900 loss:        0.332047
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.972780 loss:        0.079602
Test - acc:         0.900100 loss:        0.363921
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.973960 loss:        0.077086
Test - acc:         0.904500 loss:        0.353421
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.974800 loss:        0.074158
Test - acc:         0.902400 loss:        0.362662
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.975040 loss:        0.072556
Test - acc:         0.912100 loss:        0.314779
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.974960 loss:        0.072678
Test - acc:         0.912900 loss:        0.314833
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.973600 loss:        0.077200
Test - acc:         0.900800 loss:        0.376114
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.974160 loss:        0.076240
Test - acc:         0.907100 loss:        0.335281
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.975040 loss:        0.074046
Test - acc:         0.907600 loss:        0.339924
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.973020 loss:        0.080870
Test - acc:         0.906200 loss:        0.339784
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.974220 loss:        0.077818
Test - acc:         0.902600 loss:        0.345265
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.974200 loss:        0.076854
Test - acc:         0.903600 loss:        0.345359
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.973180 loss:        0.078326
Test - acc:         0.907900 loss:        0.325143
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.972360 loss:        0.081902
Test - acc:         0.904500 loss:        0.346007
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.974040 loss:        0.075862
Test - acc:         0.905800 loss:        0.342124
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.972580 loss:        0.081041
Test - acc:         0.906300 loss:        0.339531
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.972600 loss:        0.081226
Test - acc:         0.897700 loss:        0.361838
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.973540 loss:        0.078648
Test - acc:         0.905100 loss:        0.348357
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.973420 loss:        0.078314
Test - acc:         0.895800 loss:        0.383852
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.973240 loss:        0.079422
Test - acc:         0.895500 loss:        0.384074
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.973880 loss:        0.077672
Test - acc:         0.902500 loss:        0.357786
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.972860 loss:        0.079716
Test - acc:         0.902000 loss:        0.349705
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.974000 loss:        0.077131
Test - acc:         0.908100 loss:        0.331097
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.973320 loss:        0.079330
Test - acc:         0.904700 loss:        0.334899
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.974440 loss:        0.076567
Test - acc:         0.901900 loss:        0.369473
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.973060 loss:        0.080830
Test - acc:         0.901600 loss:        0.364649
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.974700 loss:        0.077568
Test - acc:         0.905900 loss:        0.343665
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.974120 loss:        0.077329
Test - acc:         0.897200 loss:        0.385143
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.973260 loss:        0.080129
Test - acc:         0.906400 loss:        0.337706
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.973400 loss:        0.080028
Test - acc:         0.900700 loss:        0.375468
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.974420 loss:        0.077496
Test - acc:         0.903200 loss:        0.350848
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.971660 loss:        0.083369
Test - acc:         0.904600 loss:        0.335414
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.973940 loss:        0.078572
Test - acc:         0.905400 loss:        0.355379
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.973580 loss:        0.077571
Test - acc:         0.904400 loss:        0.350501
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.974160 loss:        0.079348
Test - acc:         0.899200 loss:        0.379529
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.972580 loss:        0.082331
Test - acc:         0.904000 loss:        0.345076
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.973280 loss:        0.078281
Test - acc:         0.904300 loss:        0.331746
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.972640 loss:        0.080363
Test - acc:         0.909900 loss:        0.322249
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.972180 loss:        0.081401
Test - acc:         0.905000 loss:        0.355128
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.973640 loss:        0.079421
Test - acc:         0.904900 loss:        0.342950
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.973480 loss:        0.078363
Test - acc:         0.900300 loss:        0.351824
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.974440 loss:        0.076466
Test - acc:         0.897700 loss:        0.363518
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.975200 loss:        0.074431
Test - acc:         0.907000 loss:        0.349692
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.973280 loss:        0.078910
Test - acc:         0.906300 loss:        0.343811
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.972460 loss:        0.081186
Test - acc:         0.903500 loss:        0.355027
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.973480 loss:        0.079289
Test - acc:         0.901100 loss:        0.359838
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.974780 loss:        0.076108
Test - acc:         0.894100 loss:        0.391750
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.972280 loss:        0.082163
Test - acc:         0.909000 loss:        0.330408
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.974840 loss:        0.075546
Test - acc:         0.889400 loss:        0.412934
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.973840 loss:        0.078891
Test - acc:         0.902000 loss:        0.370570
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.974200 loss:        0.077291
Test - acc:         0.896600 loss:        0.372567
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.972300 loss:        0.079414
Test - acc:         0.903700 loss:        0.350824
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.975320 loss:        0.074458
Test - acc:         0.906100 loss:        0.335839
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.974060 loss:        0.077469
Test - acc:         0.893700 loss:        0.404485
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.975540 loss:        0.073375
Test - acc:         0.908900 loss:        0.330902
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.974600 loss:        0.077217
Test - acc:         0.898000 loss:        0.374831
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.975280 loss:        0.073861
Test - acc:         0.907800 loss:        0.330950
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.831940 loss:        0.514742
Test - acc:         0.836600 loss:        0.519209
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.884820 loss:        0.343321
Test - acc:         0.840500 loss:        0.492005
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.904440 loss:        0.287711
Test - acc:         0.853000 loss:        0.453628
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.912360 loss:        0.259995
Test - acc:         0.874700 loss:        0.404177
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.919380 loss:        0.239359
Test - acc:         0.887000 loss:        0.346690
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.925460 loss:        0.218326
Test - acc:         0.888400 loss:        0.372738
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.928280 loss:        0.210609
Test - acc:         0.880200 loss:        0.393775
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.932200 loss:        0.197754
Test - acc:         0.888000 loss:        0.353841
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.934260 loss:        0.191973
Test - acc:         0.875900 loss:        0.386383
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.937840 loss:        0.182462
Test - acc:         0.877000 loss:        0.412871
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.939340 loss:        0.176522
Test - acc:         0.883800 loss:        0.376203
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.942820 loss:        0.169650
Test - acc:         0.886700 loss:        0.362796
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.941580 loss:        0.171269
Test - acc:         0.883100 loss:        0.367295
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.947380 loss:        0.155867
Test - acc:         0.885000 loss:        0.379212
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.946100 loss:        0.158312
Test - acc:         0.889000 loss:        0.374931
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.948300 loss:        0.153757
Test - acc:         0.887900 loss:        0.377584
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.969900 loss:        0.089881
Test - acc:         0.916100 loss:        0.281778
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.977540 loss:        0.067291
Test - acc:         0.919500 loss:        0.285678
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.982620 loss:        0.055698
Test - acc:         0.921100 loss:        0.289270
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.982900 loss:        0.051907
Test - acc:         0.920400 loss:        0.290736
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.986440 loss:        0.043513
Test - acc:         0.920800 loss:        0.299246
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.985880 loss:        0.043596
Test - acc:         0.920000 loss:        0.301189
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.987440 loss:        0.037826
Test - acc:         0.921900 loss:        0.303201
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.988120 loss:        0.037206
Test - acc:         0.921500 loss:        0.305345
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.988860 loss:        0.034760
Test - acc:         0.920000 loss:        0.308854
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.988980 loss:        0.034676
Test - acc:         0.923300 loss:        0.308644
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.990060 loss:        0.031522
Test - acc:         0.922200 loss:        0.313637
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.990460 loss:        0.029175
Test - acc:         0.921800 loss:        0.316569
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.991420 loss:        0.028119
Test - acc:         0.921300 loss:        0.320051
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.991580 loss:        0.027445
Test - acc:         0.922300 loss:        0.321527
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.991900 loss:        0.026198
Test - acc:         0.921600 loss:        0.320899
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.992200 loss:        0.024975
Test - acc:         0.921700 loss:        0.328472
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.992460 loss:        0.024066
Test - acc:         0.923000 loss:        0.325722
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.993220 loss:        0.022112
Test - acc:         0.923000 loss:        0.331063
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.992900 loss:        0.022001
Test - acc:         0.924100 loss:        0.328358
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.993560 loss:        0.020717
Test - acc:         0.922700 loss:        0.329427
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.993600 loss:        0.020003
Test - acc:         0.922800 loss:        0.333471
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.993060 loss:        0.020766
Test - acc:         0.924800 loss:        0.330395
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.994180 loss:        0.018462
Test - acc:         0.925600 loss:        0.334367
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.994440 loss:        0.018074
Test - acc:         0.923300 loss:        0.341247
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.994100 loss:        0.018936
Test - acc:         0.925400 loss:        0.338056
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.994660 loss:        0.017056
Test - acc:         0.922500 loss:        0.342231
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.994800 loss:        0.016376
Test - acc:         0.923600 loss:        0.340520
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.995160 loss:        0.015496
Test - acc:         0.922100 loss:        0.345079
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.995480 loss:        0.014769
Test - acc:         0.923200 loss:        0.343528
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.995040 loss:        0.015985
Test - acc:         0.922200 loss:        0.344748
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.995520 loss:        0.014501
Test - acc:         0.922400 loss:        0.347650
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.995260 loss:        0.015025
Test - acc:         0.922200 loss:        0.349752
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.995340 loss:        0.014561
Test - acc:         0.921900 loss:        0.350310
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.995380 loss:        0.014708
Test - acc:         0.923700 loss:        0.346728
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.995600 loss:        0.013532
Test - acc:         0.922900 loss:        0.350443
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.996380 loss:        0.012305
Test - acc:         0.921700 loss:        0.356258
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.996120 loss:        0.013044
Test - acc:         0.922400 loss:        0.360710
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.995920 loss:        0.012440
Test - acc:         0.923200 loss:        0.356248
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.996220 loss:        0.011993
Test - acc:         0.921900 loss:        0.364008
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.996120 loss:        0.013033
Test - acc:         0.924100 loss:        0.367399
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.996120 loss:        0.012475
Test - acc:         0.922100 loss:        0.362486
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.996760 loss:        0.011182
Test - acc:         0.925400 loss:        0.361851
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.996380 loss:        0.011661
Test - acc:         0.924000 loss:        0.366198
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.996900 loss:        0.010761
Test - acc:         0.923600 loss:        0.363270
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.996020 loss:        0.012326
Test - acc:         0.924000 loss:        0.363705
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.996160 loss:        0.011892
Test - acc:         0.924100 loss:        0.361668
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.996180 loss:        0.012157
Test - acc:         0.924300 loss:        0.359332
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.996220 loss:        0.012208
Test - acc:         0.922800 loss:        0.365284
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.996940 loss:        0.010794
Test - acc:         0.923900 loss:        0.366901
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.996680 loss:        0.010943
Test - acc:         0.922500 loss:        0.368515
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.996440 loss:        0.011393
Test - acc:         0.923300 loss:        0.360667
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.996760 loss:        0.009988
Test - acc:         0.921300 loss:        0.369154
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.997380 loss:        0.009011
Test - acc:         0.922300 loss:        0.370249
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.996980 loss:        0.010358
Test - acc:         0.922800 loss:        0.370220
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.997020 loss:        0.009475
Test - acc:         0.921200 loss:        0.377254
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.997240 loss:        0.009079
Test - acc:         0.921200 loss:        0.377853
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.996520 loss:        0.010763
Test - acc:         0.920700 loss:        0.378899
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.996580 loss:        0.010730
Test - acc:         0.921600 loss:        0.381670
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.997440 loss:        0.008876
Test - acc:         0.921500 loss:        0.377709
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.997320 loss:        0.009944
Test - acc:         0.923700 loss:        0.372644
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.997520 loss:        0.009846
Test - acc:         0.924200 loss:        0.371993
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.997360 loss:        0.008974
Test - acc:         0.922900 loss:        0.377819
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.996260 loss:        0.011757
Test - acc:         0.921200 loss:        0.380830
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.996760 loss:        0.010986
Test - acc:         0.922100 loss:        0.372676
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.996600 loss:        0.010304
Test - acc:         0.921400 loss:        0.372588
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.997400 loss:        0.009312
Test - acc:         0.922100 loss:        0.368615
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.997320 loss:        0.008718
Test - acc:         0.925000 loss:        0.365312
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.997140 loss:        0.009061
Test - acc:         0.923000 loss:        0.377321
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.997200 loss:        0.008528
Test - acc:         0.922200 loss:        0.378453
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.996480 loss:        0.010753
Test - acc:         0.924100 loss:        0.372853
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.997180 loss:        0.009843
Test - acc:         0.924900 loss:        0.367389
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.997080 loss:        0.009149
Test - acc:         0.925400 loss:        0.368551
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.997160 loss:        0.009316
Test - acc:         0.925600 loss:        0.370364
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.997680 loss:        0.008306
Test - acc:         0.924500 loss:        0.374868
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.996740 loss:        0.009715
Test - acc:         0.924800 loss:        0.374239
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.997560 loss:        0.008812
Test - acc:         0.924500 loss:        0.373221
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.997040 loss:        0.009761
Test - acc:         0.924000 loss:        0.378957
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.997420 loss:        0.009000
Test - acc:         0.923800 loss:        0.371513
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.996820 loss:        0.009914
Test - acc:         0.920600 loss:        0.383767
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.997100 loss:        0.009081
Test - acc:         0.921800 loss:        0.381391
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.997440 loss:        0.008526
Test - acc:         0.923300 loss:        0.377671
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.997340 loss:        0.008213
Test - acc:         0.923300 loss:        0.381552
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.996920 loss:        0.010077
Test - acc:         0.924700 loss:        0.370159
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.997440 loss:        0.008556
Test - acc:         0.924800 loss:        0.374891
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.997720 loss:        0.007901
Test - acc:         0.923000 loss:        0.376145
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.997740 loss:        0.008297
Test - acc:         0.922300 loss:        0.381049
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.997820 loss:        0.007761
Test - acc:         0.922700 loss:        0.376040
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.997640 loss:        0.008061
Test - acc:         0.923900 loss:        0.374428
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.997960 loss:        0.007159
Test - acc:         0.922100 loss:        0.372785
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.997660 loss:        0.008822
Test - acc:         0.925000 loss:        0.381721
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.997880 loss:        0.007967
Test - acc:         0.923600 loss:        0.378842
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.997260 loss:        0.009470
Test - acc:         0.925100 loss:        0.373787
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.997220 loss:        0.009071
Test - acc:         0.923000 loss:        0.378920
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.997480 loss:        0.009174
Test - acc:         0.924100 loss:        0.374534
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.997440 loss:        0.008358
Test - acc:         0.921100 loss:        0.384147
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.997820 loss:        0.007258
Test - acc:         0.922700 loss:        0.382287
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.997880 loss:        0.007564
Test - acc:         0.922900 loss:        0.384429
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.997660 loss:        0.007932
Test - acc:         0.921700 loss:        0.383810
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.997240 loss:        0.008856
Test - acc:         0.920100 loss:        0.388192
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.997900 loss:        0.007586
Test - acc:         0.921100 loss:        0.392860
Sparsity :          0.7500
Wdecay :        0.000500
