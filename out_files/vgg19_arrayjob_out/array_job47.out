Running --prune_criterion random --seed 44 --prune_freq 70 --prune_rate 0.5 --comment=vgg19_crit=random_pf=70_seed=44 --save_model=pre-finetune/vgg19_random_pf70_s44
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "random",
    "prune_freq": 70,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_random_pf70_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.108300 loss:        2.474734
Test - acc:         0.119300 loss:        2.267837
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.150540 loss:        2.171051
Test - acc:         0.194700 loss:        1.934882
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.241500 loss:        1.878880
Test - acc:         0.265700 loss:        1.874334
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.306100 loss:        1.759676
Test - acc:         0.321600 loss:        1.772883
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.438900 loss:        1.470889
Test - acc:         0.480900 loss:        1.401025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.559400 loss:        1.215320
Test - acc:         0.570900 loss:        1.236438
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.640540 loss:        1.022796
Test - acc:         0.562900 loss:        1.313802
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.692820 loss:        0.908134
Test - acc:         0.458400 loss:        1.942289
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.726620 loss:        0.830650
Test - acc:         0.698100 loss:        0.963013
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.750900 loss:        0.760590
Test - acc:         0.690900 loss:        0.970304
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.770860 loss:        0.699995
Test - acc:         0.733400 loss:        0.833592
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.780940 loss:        0.667889
Test - acc:         0.743900 loss:        0.787249
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.789600 loss:        0.649976
Test - acc:         0.738800 loss:        0.849763
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.796420 loss:        0.625437
Test - acc:         0.722200 loss:        0.895129
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.802720 loss:        0.608729
Test - acc:         0.740500 loss:        0.775364
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.806240 loss:        0.596568
Test - acc:         0.700700 loss:        1.022337
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.812280 loss:        0.587819
Test - acc:         0.763300 loss:        0.771137
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.812580 loss:        0.580530
Test - acc:         0.772300 loss:        0.710877
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.818180 loss:        0.560021
Test - acc:         0.742400 loss:        0.803548
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.821800 loss:        0.557051
Test - acc:         0.728400 loss:        0.884478
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.827740 loss:        0.535048
Test - acc:         0.737700 loss:        0.793409
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.828660 loss:        0.530681
Test - acc:         0.763400 loss:        0.765218
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.825700 loss:        0.538257
Test - acc:         0.761600 loss:        0.788449
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.826500 loss:        0.539644
Test - acc:         0.788700 loss:        0.688115
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.833200 loss:        0.521488
Test - acc:         0.765500 loss:        0.769116
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.835460 loss:        0.515609
Test - acc:         0.707000 loss:        0.956001
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.836300 loss:        0.512242
Test - acc:         0.744700 loss:        0.809497
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.839600 loss:        0.497366
Test - acc:         0.704800 loss:        1.055230
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.837900 loss:        0.502614
Test - acc:         0.815400 loss:        0.592117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.838720 loss:        0.504216
Test - acc:         0.766300 loss:        0.782455
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.846120 loss:        0.485764
Test - acc:         0.739600 loss:        0.860019
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.840980 loss:        0.499632
Test - acc:         0.767800 loss:        0.798063
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.844020 loss:        0.487687
Test - acc:         0.785900 loss:        0.677178
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.840220 loss:        0.499990
Test - acc:         0.769100 loss:        0.729718
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.844640 loss:        0.483766
Test - acc:         0.789700 loss:        0.670113
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.847620 loss:        0.479940
Test - acc:         0.780700 loss:        0.708346
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.847820 loss:        0.476032
Test - acc:         0.694800 loss:        0.955153
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.846740 loss:        0.480284
Test - acc:         0.744600 loss:        0.948408
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.849640 loss:        0.473404
Test - acc:         0.783500 loss:        0.690285
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.849180 loss:        0.473203
Test - acc:         0.751500 loss:        0.898768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.848480 loss:        0.472494
Test - acc:         0.721900 loss:        0.949589
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.850180 loss:        0.470560
Test - acc:         0.806800 loss:        0.602617
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.850760 loss:        0.465705
Test - acc:         0.799500 loss:        0.629135
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.848360 loss:        0.469438
Test - acc:         0.768300 loss:        0.786816
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.853020 loss:        0.464158
Test - acc:         0.746400 loss:        0.859362
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.853460 loss:        0.456696
Test - acc:         0.794600 loss:        0.663899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.852880 loss:        0.457089
Test - acc:         0.808800 loss:        0.618557
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.857000 loss:        0.449771
Test - acc:         0.782300 loss:        0.711926
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.854520 loss:        0.448991
Test - acc:         0.799400 loss:        0.636266
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.852680 loss:        0.454349
Test - acc:         0.806400 loss:        0.604921
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.856820 loss:        0.449033
Test - acc:         0.730100 loss:        0.955128
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.858580 loss:        0.441545
Test - acc:         0.746400 loss:        0.875038
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.856260 loss:        0.444644
Test - acc:         0.810500 loss:        0.590448
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.860220 loss:        0.437055
Test - acc:         0.784100 loss:        0.712976
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.859880 loss:        0.435651
Test - acc:         0.765100 loss:        0.776423
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.859060 loss:        0.438030
Test - acc:         0.740100 loss:        0.830604
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.859000 loss:        0.438488
Test - acc:         0.823400 loss:        0.542734
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.856360 loss:        0.440860
Test - acc:         0.795800 loss:        0.664613
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.861860 loss:        0.432752
Test - acc:         0.782700 loss:        0.685126
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.859580 loss:        0.437792
Test - acc:         0.791600 loss:        0.643237
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.857200 loss:        0.439134
Test - acc:         0.809500 loss:        0.619625
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.861160 loss:        0.429580
Test - acc:         0.780500 loss:        0.713928
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.863220 loss:        0.423986
Test - acc:         0.812500 loss:        0.587948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.861040 loss:        0.432553
Test - acc:         0.808100 loss:        0.666451
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.861880 loss:        0.430105
Test - acc:         0.725800 loss:        0.954056
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.861780 loss:        0.424264
Test - acc:         0.763900 loss:        0.728084
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.861920 loss:        0.429865
Test - acc:         0.782800 loss:        0.739310
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.861820 loss:        0.424858
Test - acc:         0.767300 loss:        0.743341
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.862740 loss:        0.423034
Test - acc:         0.838800 loss:        0.501345
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.863040 loss:        0.421400
Test - acc:         0.815000 loss:        0.581123
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.784820 loss:        0.664742
Test - acc:         0.778800 loss:        0.710506
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.830460 loss:        0.524335
Test - acc:         0.791000 loss:        0.648882
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.835440 loss:        0.501807
Test - acc:         0.769300 loss:        0.754186
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.846200 loss:        0.475858
Test - acc:         0.805800 loss:        0.591402
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.848200 loss:        0.470238
Test - acc:         0.804800 loss:        0.580222
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.851320 loss:        0.456575
Test - acc:         0.746300 loss:        0.813884
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.847660 loss:        0.461497
Test - acc:         0.794700 loss:        0.639694
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.850820 loss:        0.453835
Test - acc:         0.784100 loss:        0.664852
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.851940 loss:        0.448319
Test - acc:         0.802000 loss:        0.602190
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.852680 loss:        0.446686
Test - acc:         0.766900 loss:        0.744220
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.854800 loss:        0.445030
Test - acc:         0.808300 loss:        0.621053
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.854740 loss:        0.443751
Test - acc:         0.804700 loss:        0.629899
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.857800 loss:        0.439477
Test - acc:         0.762600 loss:        0.775008
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.855520 loss:        0.440187
Test - acc:         0.699500 loss:        1.001377
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.857340 loss:        0.439049
Test - acc:         0.829600 loss:        0.521375
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.857780 loss:        0.434725
Test - acc:         0.779600 loss:        0.726984
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.857820 loss:        0.437914
Test - acc:         0.820600 loss:        0.553177
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.858060 loss:        0.428917
Test - acc:         0.776800 loss:        0.718653
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.857480 loss:        0.430181
Test - acc:         0.819200 loss:        0.588405
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.859960 loss:        0.428698
Test - acc:         0.766100 loss:        0.771195
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.857120 loss:        0.433629
Test - acc:         0.819000 loss:        0.582971
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.859220 loss:        0.430740
Test - acc:         0.818300 loss:        0.566440
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.858820 loss:        0.428979
Test - acc:         0.841100 loss:        0.491578
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.859620 loss:        0.424501
Test - acc:         0.813500 loss:        0.575423
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.857820 loss:        0.427789
Test - acc:         0.773000 loss:        0.743425
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.860340 loss:        0.426131
Test - acc:         0.798100 loss:        0.659820
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.859600 loss:        0.426799
Test - acc:         0.791500 loss:        0.677623
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.859340 loss:        0.429197
Test - acc:         0.793600 loss:        0.650975
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.861420 loss:        0.418961
Test - acc:         0.823400 loss:        0.553685
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.862280 loss:        0.417483
Test - acc:         0.832200 loss:        0.522754
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.860900 loss:        0.423761
Test - acc:         0.796800 loss:        0.664504
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.862400 loss:        0.417766
Test - acc:         0.828000 loss:        0.539393
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.859960 loss:        0.422335
Test - acc:         0.780800 loss:        0.719514
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.863620 loss:        0.415459
Test - acc:         0.816300 loss:        0.554918
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.863260 loss:        0.416590
Test - acc:         0.759300 loss:        0.757811
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.862500 loss:        0.420600
Test - acc:         0.831700 loss:        0.522356
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.864720 loss:        0.412148
Test - acc:         0.807600 loss:        0.625510
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.863680 loss:        0.417291
Test - acc:         0.837600 loss:        0.491462
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.859720 loss:        0.425436
Test - acc:         0.805000 loss:        0.621431
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.865000 loss:        0.419096
Test - acc:         0.783500 loss:        0.725606
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.864800 loss:        0.414996
Test - acc:         0.812600 loss:        0.573496
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.863820 loss:        0.415670
Test - acc:         0.807400 loss:        0.577989
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.862620 loss:        0.415680
Test - acc:         0.795100 loss:        0.680985
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.863620 loss:        0.415221
Test - acc:         0.790200 loss:        0.658436
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.861520 loss:        0.421849
Test - acc:         0.788800 loss:        0.667097
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.863700 loss:        0.411311
Test - acc:         0.776300 loss:        0.744018
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.864120 loss:        0.407595
Test - acc:         0.823100 loss:        0.571123
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.863920 loss:        0.415081
Test - acc:         0.817400 loss:        0.562465
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.866360 loss:        0.408385
Test - acc:         0.814700 loss:        0.567113
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.865320 loss:        0.412196
Test - acc:         0.743400 loss:        0.892448
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.862660 loss:        0.419338
Test - acc:         0.776000 loss:        0.743646
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.861880 loss:        0.421890
Test - acc:         0.836900 loss:        0.498090
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.864540 loss:        0.411692
Test - acc:         0.757800 loss:        0.794500
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.864700 loss:        0.410225
Test - acc:         0.825700 loss:        0.560851
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.866040 loss:        0.414882
Test - acc:         0.766000 loss:        0.784577
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.867640 loss:        0.408035
Test - acc:         0.795100 loss:        0.634233
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.867280 loss:        0.406130
Test - acc:         0.799500 loss:        0.621484
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.867020 loss:        0.404916
Test - acc:         0.776500 loss:        0.701281
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.866620 loss:        0.410513
Test - acc:         0.757500 loss:        0.776272
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.862240 loss:        0.415839
Test - acc:         0.828900 loss:        0.523283
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.863300 loss:        0.412554
Test - acc:         0.823100 loss:        0.578194
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.865240 loss:        0.409277
Test - acc:         0.835300 loss:        0.534304
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.864300 loss:        0.409858
Test - acc:         0.776400 loss:        0.760107
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.865880 loss:        0.408947
Test - acc:         0.817200 loss:        0.615810
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.866660 loss:        0.402919
Test - acc:         0.808100 loss:        0.585007
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.862920 loss:        0.414340
Test - acc:         0.820900 loss:        0.565416
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.865400 loss:        0.408322
Test - acc:         0.804100 loss:        0.600952
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.865700 loss:        0.406504
Test - acc:         0.817200 loss:        0.563301
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.865200 loss:        0.410067
Test - acc:         0.828700 loss:        0.545676
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.866400 loss:        0.409924
Test - acc:         0.787700 loss:        0.683258
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.766540 loss:        0.709329
Test - acc:         0.708500 loss:        0.894001
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.812760 loss:        0.561255
Test - acc:         0.715200 loss:        0.971052
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.829580 loss:        0.516542
Test - acc:         0.726400 loss:        0.845704
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.834240 loss:        0.502541
Test - acc:         0.815900 loss:        0.548090
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.838840 loss:        0.484813
Test - acc:         0.795700 loss:        0.630107
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.840220 loss:        0.483871
Test - acc:         0.703500 loss:        1.008978
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.838600 loss:        0.484977
Test - acc:         0.792400 loss:        0.642164
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.843760 loss:        0.471865
Test - acc:         0.817900 loss:        0.552015
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.842780 loss:        0.474972
Test - acc:         0.748500 loss:        0.866675
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.845800 loss:        0.464772
Test - acc:         0.769600 loss:        0.712065
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.905740 loss:        0.281039
Test - acc:         0.898700 loss:        0.311668
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.922780 loss:        0.231217
Test - acc:         0.901800 loss:        0.298105
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.930380 loss:        0.208554
Test - acc:         0.904700 loss:        0.295468
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.936340 loss:        0.190052
Test - acc:         0.907700 loss:        0.287209
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.940880 loss:        0.177792
Test - acc:         0.908800 loss:        0.284518
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.943060 loss:        0.169137
Test - acc:         0.905800 loss:        0.298581
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.946320 loss:        0.160842
Test - acc:         0.908700 loss:        0.284550
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.949300 loss:        0.151157
Test - acc:         0.908200 loss:        0.293878
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.951620 loss:        0.142702
Test - acc:         0.906000 loss:        0.297151
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.952180 loss:        0.139692
Test - acc:         0.904900 loss:        0.317331
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.954680 loss:        0.134131
Test - acc:         0.906900 loss:        0.300819
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.957520 loss:        0.126983
Test - acc:         0.904800 loss:        0.310152
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.956140 loss:        0.128318
Test - acc:         0.897100 loss:        0.351651
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.959080 loss:        0.122370
Test - acc:         0.907400 loss:        0.315307
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.960220 loss:        0.116963
Test - acc:         0.905300 loss:        0.329474
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.959260 loss:        0.119660
Test - acc:         0.905000 loss:        0.315432
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.961320 loss:        0.114168
Test - acc:         0.905100 loss:        0.318099
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.960600 loss:        0.116156
Test - acc:         0.900900 loss:        0.342264
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.961800 loss:        0.111281
Test - acc:         0.902700 loss:        0.336550
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.963260 loss:        0.109239
Test - acc:         0.903700 loss:        0.338230
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.962580 loss:        0.110295
Test - acc:         0.903900 loss:        0.346814
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.961540 loss:        0.113676
Test - acc:         0.903900 loss:        0.326482
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.961240 loss:        0.114891
Test - acc:         0.906400 loss:        0.323687
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.963240 loss:        0.108636
Test - acc:         0.890700 loss:        0.388760
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.961780 loss:        0.110735
Test - acc:         0.896300 loss:        0.354935
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.962620 loss:        0.109207
Test - acc:         0.894400 loss:        0.357249
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.960860 loss:        0.115081
Test - acc:         0.898500 loss:        0.337787
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.960800 loss:        0.116509
Test - acc:         0.886400 loss:        0.408307
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.961480 loss:        0.114325
Test - acc:         0.884700 loss:        0.386072
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.962620 loss:        0.108324
Test - acc:         0.898500 loss:        0.351120
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.961960 loss:        0.113546
Test - acc:         0.892600 loss:        0.372904
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.961040 loss:        0.114714
Test - acc:         0.903300 loss:        0.320337
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.961000 loss:        0.113825
Test - acc:         0.901900 loss:        0.332805
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.962680 loss:        0.109305
Test - acc:         0.888700 loss:        0.395309
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.960820 loss:        0.114995
Test - acc:         0.898600 loss:        0.345312
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.961160 loss:        0.113259
Test - acc:         0.896100 loss:        0.339380
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.961480 loss:        0.113995
Test - acc:         0.893400 loss:        0.365730
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.961460 loss:        0.114415
Test - acc:         0.879400 loss:        0.421448
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.962740 loss:        0.108847
Test - acc:         0.895800 loss:        0.359624
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.959560 loss:        0.115358
Test - acc:         0.891500 loss:        0.381153
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.960460 loss:        0.115284
Test - acc:         0.898200 loss:        0.340123
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.960740 loss:        0.112949
Test - acc:         0.896500 loss:        0.351399
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.961540 loss:        0.111669
Test - acc:         0.892700 loss:        0.372230
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.961960 loss:        0.110262
Test - acc:         0.884300 loss:        0.412553
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.961000 loss:        0.113398
Test - acc:         0.891800 loss:        0.384665
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.961560 loss:        0.111382
Test - acc:         0.889000 loss:        0.379962
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.961640 loss:        0.111334
Test - acc:         0.874900 loss:        0.457067
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.959660 loss:        0.114347
Test - acc:         0.900500 loss:        0.345521
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.962220 loss:        0.108599
Test - acc:         0.892700 loss:        0.381859
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.961480 loss:        0.111738
Test - acc:         0.892900 loss:        0.370772
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.961780 loss:        0.112612
Test - acc:         0.902400 loss:        0.340526
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.961340 loss:        0.112793
Test - acc:         0.892400 loss:        0.371357
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.961720 loss:        0.111540
Test - acc:         0.891100 loss:        0.384280
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.962060 loss:        0.111471
Test - acc:         0.896100 loss:        0.358071
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.963800 loss:        0.107419
Test - acc:         0.891300 loss:        0.383894
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.961860 loss:        0.112011
Test - acc:         0.894600 loss:        0.376069
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.963140 loss:        0.107953
Test - acc:         0.894300 loss:        0.384926
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.961320 loss:        0.113087
Test - acc:         0.891500 loss:        0.397692
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.964460 loss:        0.106743
Test - acc:         0.891800 loss:        0.377294
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.962380 loss:        0.110406
Test - acc:         0.894300 loss:        0.366806
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.647780 loss:        1.006357
Test - acc:         0.734000 loss:        0.791690
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.808960 loss:        0.566258
Test - acc:         0.793500 loss:        0.621429
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.843200 loss:        0.468081
Test - acc:         0.839600 loss:        0.487963
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.856840 loss:        0.422341
Test - acc:         0.814800 loss:        0.564210
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.868340 loss:        0.385846
Test - acc:         0.841600 loss:        0.470352
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.880400 loss:        0.353227
Test - acc:         0.854500 loss:        0.436189
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.885940 loss:        0.336044
Test - acc:         0.861100 loss:        0.427599
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.888600 loss:        0.323445
Test - acc:         0.849400 loss:        0.454626
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.894600 loss:        0.310281
Test - acc:         0.870400 loss:        0.396945
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.898460 loss:        0.294590
Test - acc:         0.853400 loss:        0.445374
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.900700 loss:        0.287862
Test - acc:         0.856600 loss:        0.437571
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.904840 loss:        0.278205
Test - acc:         0.843300 loss:        0.507189
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.905840 loss:        0.273637
Test - acc:         0.855500 loss:        0.449110
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.907040 loss:        0.271212
Test - acc:         0.859600 loss:        0.448986
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.913320 loss:        0.255393
Test - acc:         0.854300 loss:        0.438820
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.912260 loss:        0.252491
Test - acc:         0.868900 loss:        0.407396
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.914540 loss:        0.250155
Test - acc:         0.847500 loss:        0.469745
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.913500 loss:        0.247741
Test - acc:         0.866800 loss:        0.422578
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.919840 loss:        0.234764
Test - acc:         0.870600 loss:        0.398206
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.916340 loss:        0.241100
Test - acc:         0.859100 loss:        0.441243
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.920260 loss:        0.230602
Test - acc:         0.867200 loss:        0.419601
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.921920 loss:        0.223483
Test - acc:         0.881000 loss:        0.370947
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.921080 loss:        0.229176
Test - acc:         0.873300 loss:        0.404219
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.924200 loss:        0.217348
Test - acc:         0.869700 loss:        0.416332
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.925260 loss:        0.215344
Test - acc:         0.860600 loss:        0.445467
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.926040 loss:        0.212792
Test - acc:         0.874700 loss:        0.403992
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.926580 loss:        0.210401
Test - acc:         0.873100 loss:        0.392004
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.927580 loss:        0.205463
Test - acc:         0.871700 loss:        0.399336
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.929100 loss:        0.204461
Test - acc:         0.865200 loss:        0.427900
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.929380 loss:        0.203544
Test - acc:         0.869300 loss:        0.420999
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.929520 loss:        0.202813
Test - acc:         0.869400 loss:        0.427658
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.929300 loss:        0.200245
Test - acc:         0.872400 loss:        0.404285
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.931820 loss:        0.195779
Test - acc:         0.850600 loss:        0.506114
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.931740 loss:        0.197053
Test - acc:         0.868100 loss:        0.433302
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.931880 loss:        0.195972
Test - acc:         0.878700 loss:        0.385428
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.934160 loss:        0.191473
Test - acc:         0.870100 loss:        0.406632
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.934640 loss:        0.187664
Test - acc:         0.872400 loss:        0.407566
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.934060 loss:        0.186678
Test - acc:         0.866700 loss:        0.417560
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.932680 loss:        0.191257
Test - acc:         0.864100 loss:        0.422057
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.935440 loss:        0.186438
Test - acc:         0.875300 loss:        0.402588
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.958540 loss:        0.122411
Test - acc:         0.900600 loss:        0.322171
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.967240 loss:        0.099386
Test - acc:         0.901900 loss:        0.318966
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.971100 loss:        0.088216
Test - acc:         0.903700 loss:        0.322305
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.972220 loss:        0.081445
Test - acc:         0.902900 loss:        0.324217
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.976400 loss:        0.073361
Test - acc:         0.902900 loss:        0.327779
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.974980 loss:        0.074849
Test - acc:         0.904600 loss:        0.330818
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.977620 loss:        0.066020
Test - acc:         0.905100 loss:        0.335808
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.977980 loss:        0.065543
Test - acc:         0.904400 loss:        0.342616
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.978700 loss:        0.062822
Test - acc:         0.903500 loss:        0.339590
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.979760 loss:        0.059829
Test - acc:         0.902500 loss:        0.350036
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.980040 loss:        0.059847
Test - acc:         0.903000 loss:        0.347683
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.980800 loss:        0.056682
Test - acc:         0.903700 loss:        0.346171
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.981380 loss:        0.055804
Test - acc:         0.903000 loss:        0.353171
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.982160 loss:        0.052376
Test - acc:         0.904500 loss:        0.359060
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.982560 loss:        0.052166
Test - acc:         0.902100 loss:        0.362748
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.984840 loss:        0.048248
Test - acc:         0.905200 loss:        0.354705
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.983500 loss:        0.048750
Test - acc:         0.904500 loss:        0.356206
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.984400 loss:        0.045738
Test - acc:         0.904800 loss:        0.359433
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.984140 loss:        0.046017
Test - acc:         0.906200 loss:        0.357886
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.986520 loss:        0.041348
Test - acc:         0.906400 loss:        0.358625
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.985800 loss:        0.042721
Test - acc:         0.905000 loss:        0.371577
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.986560 loss:        0.041607
Test - acc:         0.906300 loss:        0.362661
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.987180 loss:        0.038870
Test - acc:         0.905600 loss:        0.375492
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.986320 loss:        0.040185
Test - acc:         0.904600 loss:        0.367837
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.986780 loss:        0.040185
Test - acc:         0.906200 loss:        0.370614
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.987040 loss:        0.038840
Test - acc:         0.905500 loss:        0.376547
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.987460 loss:        0.036425
Test - acc:         0.907200 loss:        0.377412
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.988120 loss:        0.035405
Test - acc:         0.905200 loss:        0.379270
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.988120 loss:        0.035821
Test - acc:         0.903900 loss:        0.383036
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.988700 loss:        0.034767
Test - acc:         0.903900 loss:        0.387033
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.371260 loss:        1.721096
Test - acc:         0.510500 loss:        1.350505
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.559340 loss:        1.239765
Test - acc:         0.623400 loss:        1.062419
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.638200 loss:        1.027702
Test - acc:         0.671800 loss:        0.927160
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.685480 loss:        0.897850
Test - acc:         0.714000 loss:        0.822447
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.720840 loss:        0.808300
Test - acc:         0.735400 loss:        0.761367
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.741700 loss:        0.743194
Test - acc:         0.755400 loss:        0.709704
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.761060 loss:        0.692372
Test - acc:         0.770200 loss:        0.664322
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.775100 loss:        0.650373
Test - acc:         0.777200 loss:        0.642342
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.784440 loss:        0.618614
Test - acc:         0.786800 loss:        0.626407
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.797120 loss:        0.589319
Test - acc:         0.795200 loss:        0.593773
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.805180 loss:        0.562602
Test - acc:         0.798000 loss:        0.581973
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.811180 loss:        0.544563
Test - acc:         0.806000 loss:        0.570257
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.819460 loss:        0.523396
Test - acc:         0.809400 loss:        0.561067
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.825560 loss:        0.508011
Test - acc:         0.815000 loss:        0.549558
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.828780 loss:        0.495495
Test - acc:         0.822700 loss:        0.515357
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.836480 loss:        0.473826
Test - acc:         0.818800 loss:        0.539466
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.841540 loss:        0.460939
Test - acc:         0.827400 loss:        0.513880
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.844800 loss:        0.447785
Test - acc:         0.824500 loss:        0.516259
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.850000 loss:        0.435893
Test - acc:         0.831000 loss:        0.506066
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.853680 loss:        0.422352
Test - acc:         0.827100 loss:        0.518384
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.856240 loss:        0.413570
Test - acc:         0.833100 loss:        0.495784
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.860480 loss:        0.403050
Test - acc:         0.830800 loss:        0.510699
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.864220 loss:        0.393268
Test - acc:         0.830600 loss:        0.502198
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.867180 loss:        0.386697
Test - acc:         0.833800 loss:        0.497424
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.868700 loss:        0.376647
Test - acc:         0.837500 loss:        0.485076
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.870040 loss:        0.373103
Test - acc:         0.834800 loss:        0.500007
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.874760 loss:        0.364870
Test - acc:         0.841400 loss:        0.478632
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.876520 loss:        0.358273
Test - acc:         0.841100 loss:        0.474666
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.880120 loss:        0.347890
Test - acc:         0.841800 loss:        0.479965
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.881240 loss:        0.344311
Test - acc:         0.847200 loss:        0.460424
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.881100 loss:        0.339144
Test - acc:         0.845200 loss:        0.467898
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.887740 loss:        0.326534
Test - acc:         0.846800 loss:        0.467620
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.889220 loss:        0.322561
Test - acc:         0.844400 loss:        0.465005
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.889680 loss:        0.319418
Test - acc:         0.845300 loss:        0.478641
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.893920 loss:        0.309776
Test - acc:         0.845400 loss:        0.471403
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.893220 loss:        0.308164
Test - acc:         0.845600 loss:        0.465791
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.895060 loss:        0.304346
Test - acc:         0.846900 loss:        0.463183
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.896680 loss:        0.300421
Test - acc:         0.845500 loss:        0.465526
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.898680 loss:        0.291221
Test - acc:         0.848300 loss:        0.472339
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.897600 loss:        0.293900
Test - acc:         0.852800 loss:        0.460590
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.899540 loss:        0.289172
Test - acc:         0.850300 loss:        0.465507
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.901000 loss:        0.285793
Test - acc:         0.849100 loss:        0.465007
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.904080 loss:        0.275556
Test - acc:         0.850400 loss:        0.451992
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.906980 loss:        0.270974
Test - acc:         0.846700 loss:        0.479855
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.907700 loss:        0.268201
Test - acc:         0.848700 loss:        0.474183
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.908780 loss:        0.262847
Test - acc:         0.854300 loss:        0.465591
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.908580 loss:        0.263380
Test - acc:         0.849900 loss:        0.475164
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.910440 loss:        0.257955
Test - acc:         0.857200 loss:        0.451227
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.909440 loss:        0.260338
Test - acc:         0.859000 loss:        0.445281
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.912800 loss:        0.251550
Test - acc:         0.842300 loss:        0.510859
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.913860 loss:        0.246834
Test - acc:         0.852200 loss:        0.464178
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.915620 loss:        0.244055
Test - acc:         0.853100 loss:        0.465721
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.915000 loss:        0.243644
Test - acc:         0.857500 loss:        0.453567
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.918680 loss:        0.236500
Test - acc:         0.854900 loss:        0.467801
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.915440 loss:        0.239032
Test - acc:         0.853700 loss:        0.464497
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.918500 loss:        0.235386
Test - acc:         0.858000 loss:        0.456109
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.917480 loss:        0.233710
Test - acc:         0.857200 loss:        0.450805
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.919060 loss:        0.229763
Test - acc:         0.856400 loss:        0.459693
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.921680 loss:        0.226024
Test - acc:         0.852200 loss:        0.473880
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.921880 loss:        0.225119
Test - acc:         0.858300 loss:        0.457349
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.922800 loss:        0.221678
Test - acc:         0.850300 loss:        0.501875
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.925360 loss:        0.218138
Test - acc:         0.855000 loss:        0.467775
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.923600 loss:        0.216547
Test - acc:         0.854700 loss:        0.463618
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.925960 loss:        0.214741
Test - acc:         0.858900 loss:        0.455168
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.927020 loss:        0.210779
Test - acc:         0.861200 loss:        0.462159
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.927220 loss:        0.210037
Test - acc:         0.852200 loss:        0.485002
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.928120 loss:        0.208900
Test - acc:         0.860900 loss:        0.457219
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.928280 loss:        0.205566
Test - acc:         0.861100 loss:        0.459627
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.928520 loss:        0.203421
Test - acc:         0.858900 loss:        0.461054
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.929540 loss:        0.201855
Test - acc:         0.856100 loss:        0.473683
Sparsity :          0.9375
Wdecay :        0.000500
