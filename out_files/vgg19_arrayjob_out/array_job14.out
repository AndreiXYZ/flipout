Running --prune_criterion magnitude --seed 42 --prune_freq 39 --prune_rate 0.5 --comment=vgg19_crit=magnitude_pf=39_seed=42 --save_model=pre-finetune/vgg19_magnitude_pf39_s42
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "magnitude",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_magnitude_pf39_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.106580 loss:        2.520409
Test - acc:         0.107600 loss:        2.294274
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.121620 loss:        2.275786
Test - acc:         0.128800 loss:        2.258387
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.167360 loss:        2.134492
Test - acc:         0.208100 loss:        1.905877
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.258380 loss:        1.859667
Test - acc:         0.287600 loss:        1.767105
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.322920 loss:        1.699367
Test - acc:         0.410600 loss:        1.501470
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.445160 loss:        1.456379
Test - acc:         0.508200 loss:        1.356367
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.546300 loss:        1.241191
Test - acc:         0.498200 loss:        1.477734
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.604540 loss:        1.101290
Test - acc:         0.483800 loss:        1.743679
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.650880 loss:        0.993529
Test - acc:         0.619700 loss:        1.166982
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.703160 loss:        0.882477
Test - acc:         0.659900 loss:        1.012752
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.733140 loss:        0.808711
Test - acc:         0.648500 loss:        1.121658
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.752060 loss:        0.763825
Test - acc:         0.697100 loss:        0.950123
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.763380 loss:        0.735478
Test - acc:         0.647600 loss:        1.210369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.773560 loss:        0.706368
Test - acc:         0.717500 loss:        0.901679
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.780940 loss:        0.681272
Test - acc:         0.682800 loss:        1.025874
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.791420 loss:        0.649883
Test - acc:         0.736800 loss:        0.872370
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.796700 loss:        0.634338
Test - acc:         0.700800 loss:        0.932917
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.801840 loss:        0.615529
Test - acc:         0.797400 loss:        0.635734
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.804520 loss:        0.605495
Test - acc:         0.786700 loss:        0.672975
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.811300 loss:        0.586166
Test - acc:         0.692500 loss:        1.006768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.818600 loss:        0.574597
Test - acc:         0.764600 loss:        0.752744
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.819720 loss:        0.564470
Test - acc:         0.732800 loss:        0.858983
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.822020 loss:        0.561122
Test - acc:         0.767600 loss:        0.719709
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.821520 loss:        0.558114
Test - acc:         0.744500 loss:        0.842082
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.827620 loss:        0.542572
Test - acc:         0.761300 loss:        0.789428
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.830100 loss:        0.535151
Test - acc:         0.741300 loss:        0.871493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.828600 loss:        0.532575
Test - acc:         0.787500 loss:        0.687972
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.833340 loss:        0.523183
Test - acc:         0.781500 loss:        0.714158
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.832740 loss:        0.525514
Test - acc:         0.761200 loss:        0.775278
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.831200 loss:        0.525716
Test - acc:         0.754700 loss:        0.881011
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.838100 loss:        0.512746
Test - acc:         0.776500 loss:        0.724635
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.839420 loss:        0.504651
Test - acc:         0.760400 loss:        0.863967
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.838440 loss:        0.504414
Test - acc:         0.785700 loss:        0.718698
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.838460 loss:        0.503714
Test - acc:         0.815300 loss:        0.567434
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.840220 loss:        0.497685
Test - acc:         0.662800 loss:        1.328712
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.842920 loss:        0.493377
Test - acc:         0.732000 loss:        0.900541
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.841740 loss:        0.493601
Test - acc:         0.813600 loss:        0.592560
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.841380 loss:        0.496394
Test - acc:         0.789100 loss:        0.683082
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.846480 loss:        0.485149
Test - acc:         0.747100 loss:        0.824019
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.860380 loss:        0.437306
Test - acc:         0.784000 loss:        0.748418
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.860820 loss:        0.433059
Test - acc:         0.748000 loss:        0.864293
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.861140 loss:        0.439461
Test - acc:         0.767000 loss:        0.794151
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.857220 loss:        0.441943
Test - acc:         0.798000 loss:        0.648820
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.860100 loss:        0.433352
Test - acc:         0.778200 loss:        0.739798
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.859000 loss:        0.440122
Test - acc:         0.815000 loss:        0.606635
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.862180 loss:        0.435272
Test - acc:         0.799800 loss:        0.668058
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.864820 loss:        0.429945
Test - acc:         0.803100 loss:        0.611516
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.861420 loss:        0.437240
Test - acc:         0.845600 loss:        0.474534
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.865760 loss:        0.419880
Test - acc:         0.805800 loss:        0.603961
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.862140 loss:        0.435768
Test - acc:         0.762300 loss:        0.784003
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.865960 loss:        0.422364
Test - acc:         0.726500 loss:        0.986153
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.860960 loss:        0.431756
Test - acc:         0.680300 loss:        1.181270
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.862880 loss:        0.429973
Test - acc:         0.799500 loss:        0.656182
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.863500 loss:        0.424039
Test - acc:         0.789800 loss:        0.685865
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.866580 loss:        0.419889
Test - acc:         0.752500 loss:        0.806806
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.862500 loss:        0.429121
Test - acc:         0.779800 loss:        0.755012
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.865180 loss:        0.418440
Test - acc:         0.736200 loss:        0.958843
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.865580 loss:        0.424816
Test - acc:         0.833500 loss:        0.502475
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.866440 loss:        0.418917
Test - acc:         0.810700 loss:        0.581314
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.863180 loss:        0.426412
Test - acc:         0.779000 loss:        0.698791
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.863960 loss:        0.425018
Test - acc:         0.800800 loss:        0.643018
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.866960 loss:        0.420443
Test - acc:         0.805500 loss:        0.611864
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.864960 loss:        0.422796
Test - acc:         0.807500 loss:        0.647006
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.867920 loss:        0.410128
Test - acc:         0.751400 loss:        0.846033
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.864700 loss:        0.423333
Test - acc:         0.770600 loss:        0.761816
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.867600 loss:        0.411014
Test - acc:         0.759500 loss:        0.753613
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.865860 loss:        0.414741
Test - acc:         0.761500 loss:        0.799969
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.863800 loss:        0.424828
Test - acc:         0.753300 loss:        0.824307
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.864300 loss:        0.420475
Test - acc:         0.822200 loss:        0.602898
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.865840 loss:        0.413751
Test - acc:         0.830700 loss:        0.545337
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.864200 loss:        0.423911
Test - acc:         0.810400 loss:        0.628966
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.868620 loss:        0.410474
Test - acc:         0.836300 loss:        0.525730
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.865480 loss:        0.413948
Test - acc:         0.776700 loss:        0.733514
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.866020 loss:        0.420244
Test - acc:         0.737700 loss:        0.888626
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.865420 loss:        0.415541
Test - acc:         0.797700 loss:        0.643699
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.866740 loss:        0.418588
Test - acc:         0.796200 loss:        0.666218
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.865800 loss:        0.420025
Test - acc:         0.803500 loss:        0.609575
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.867540 loss:        0.411377
Test - acc:         0.841100 loss:        0.499218
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.882040 loss:        0.366522
Test - acc:         0.833800 loss:        0.521722
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.881760 loss:        0.364566
Test - acc:         0.842300 loss:        0.526218
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.881540 loss:        0.365386
Test - acc:         0.803500 loss:        0.640186
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.877400 loss:        0.367958
Test - acc:         0.819200 loss:        0.609866
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.878240 loss:        0.369206
Test - acc:         0.795800 loss:        0.614982
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.883460 loss:        0.359978
Test - acc:         0.745800 loss:        0.947007
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.880060 loss:        0.365539
Test - acc:         0.800100 loss:        0.664082
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.882180 loss:        0.361508
Test - acc:         0.829800 loss:        0.546373
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.884160 loss:        0.362182
Test - acc:         0.764800 loss:        0.797488
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.881900 loss:        0.361604
Test - acc:         0.843000 loss:        0.513529
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.881180 loss:        0.359718
Test - acc:         0.849200 loss:        0.465930
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.882680 loss:        0.357163
Test - acc:         0.791200 loss:        0.692560
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.880500 loss:        0.362497
Test - acc:         0.832800 loss:        0.524390
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.882960 loss:        0.356281
Test - acc:         0.773900 loss:        0.737572
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.883640 loss:        0.358029
Test - acc:         0.802800 loss:        0.589372
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.882620 loss:        0.360117
Test - acc:         0.829000 loss:        0.554247
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.882300 loss:        0.358975
Test - acc:         0.809600 loss:        0.633656
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.884000 loss:        0.358392
Test - acc:         0.824800 loss:        0.559570
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.882860 loss:        0.359311
Test - acc:         0.808000 loss:        0.622715
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.885160 loss:        0.351924
Test - acc:         0.810500 loss:        0.574218
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.883560 loss:        0.354451
Test - acc:         0.823500 loss:        0.533095
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.883660 loss:        0.350474
Test - acc:         0.828400 loss:        0.540456
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.885060 loss:        0.347155
Test - acc:         0.804200 loss:        0.672316
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.884680 loss:        0.352312
Test - acc:         0.834400 loss:        0.518225
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.884240 loss:        0.353288
Test - acc:         0.846600 loss:        0.481382
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.884880 loss:        0.352252
Test - acc:         0.834000 loss:        0.527544
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.887680 loss:        0.341373
Test - acc:         0.819700 loss:        0.599939
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.883460 loss:        0.351880
Test - acc:         0.822000 loss:        0.536990
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.886360 loss:        0.348789
Test - acc:         0.804600 loss:        0.642103
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.847300 loss:        0.491067
Test - acc:         0.766100 loss:        0.845572
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.872140 loss:        0.401878
Test - acc:         0.833800 loss:        0.535803
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.878420 loss:        0.377934
Test - acc:         0.798100 loss:        0.642704
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.878180 loss:        0.374717
Test - acc:         0.792300 loss:        0.667593
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.881360 loss:        0.362497
Test - acc:         0.847900 loss:        0.490640
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.882740 loss:        0.363518
Test - acc:         0.836200 loss:        0.508919
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.880380 loss:        0.366243
Test - acc:         0.842900 loss:        0.484582
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.881680 loss:        0.365595
Test - acc:         0.760900 loss:        0.812241
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.883760 loss:        0.357766
Test - acc:         0.807200 loss:        0.597323
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.876180 loss:        0.388005
Test - acc:         0.820400 loss:        0.591665
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.880660 loss:        0.372451
Test - acc:         0.804500 loss:        0.679035
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.887340 loss:        0.346303
Test - acc:         0.803500 loss:        0.673662
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.886220 loss:        0.349402
Test - acc:         0.838800 loss:        0.539750
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.883700 loss:        0.352418
Test - acc:         0.765100 loss:        0.889578
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.886100 loss:        0.350817
Test - acc:         0.824200 loss:        0.574879
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.885160 loss:        0.353785
Test - acc:         0.779900 loss:        0.777947
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.886040 loss:        0.348888
Test - acc:         0.725600 loss:        0.989026
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.887060 loss:        0.344631
Test - acc:         0.808600 loss:        0.603009
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.879200 loss:        0.372401
Test - acc:         0.844400 loss:        0.523755
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.887060 loss:        0.346814
Test - acc:         0.800000 loss:        0.690955
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.889060 loss:        0.341873
Test - acc:         0.752400 loss:        0.801398
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.884900 loss:        0.348847
Test - acc:         0.798900 loss:        0.627712
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.885700 loss:        0.348522
Test - acc:         0.842500 loss:        0.472440
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.884940 loss:        0.351792
Test - acc:         0.755500 loss:        0.828636
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.886740 loss:        0.348484
Test - acc:         0.813800 loss:        0.586224
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.887820 loss:        0.338832
Test - acc:         0.807300 loss:        0.652237
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.884900 loss:        0.350778
Test - acc:         0.865800 loss:        0.412572
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.889240 loss:        0.335341
Test - acc:         0.838900 loss:        0.491072
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.887940 loss:        0.342728
Test - acc:         0.820200 loss:        0.607126
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.888260 loss:        0.337251
Test - acc:         0.842400 loss:        0.498135
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.887620 loss:        0.340651
Test - acc:         0.830400 loss:        0.553609
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.885500 loss:        0.347066
Test - acc:         0.816400 loss:        0.587554
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.887120 loss:        0.342570
Test - acc:         0.769100 loss:        0.833081
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.888800 loss:        0.336280
Test - acc:         0.817000 loss:        0.589196
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.887420 loss:        0.340698
Test - acc:         0.810600 loss:        0.637023
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.887600 loss:        0.341744
Test - acc:         0.846800 loss:        0.482639
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.887560 loss:        0.341064
Test - acc:         0.867200 loss:        0.409990
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.888440 loss:        0.338601
Test - acc:         0.823200 loss:        0.560202
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.886720 loss:        0.341447
Test - acc:         0.831800 loss:        0.531675
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.890360 loss:        0.333871
Test - acc:         0.777600 loss:        0.772317
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.889060 loss:        0.337527
Test - acc:         0.826700 loss:        0.552707
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.887500 loss:        0.336354
Test - acc:         0.848500 loss:        0.491815
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.889060 loss:        0.333198
Test - acc:         0.857500 loss:        0.460610
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.934420 loss:        0.197099
Test - acc:         0.911800 loss:        0.267760
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.947680 loss:        0.157204
Test - acc:         0.916600 loss:        0.252045
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.952880 loss:        0.139204
Test - acc:         0.918100 loss:        0.260061
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.956480 loss:        0.131375
Test - acc:         0.916600 loss:        0.256566
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.961220 loss:        0.117517
Test - acc:         0.920100 loss:        0.256596
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.960940 loss:        0.116742
Test - acc:         0.920200 loss:        0.251856
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.948900 loss:        0.153156
Test - acc:         0.914200 loss:        0.278807
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.954400 loss:        0.134389
Test - acc:         0.914800 loss:        0.281262
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.958600 loss:        0.122909
Test - acc:         0.915500 loss:        0.278680
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.960140 loss:        0.118682
Test - acc:         0.914700 loss:        0.276897
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.960480 loss:        0.114361
Test - acc:         0.912500 loss:        0.284709
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.963100 loss:        0.109417
Test - acc:         0.918300 loss:        0.278780
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.962920 loss:        0.107795
Test - acc:         0.912300 loss:        0.289154
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.965120 loss:        0.102296
Test - acc:         0.912500 loss:        0.298434
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.963120 loss:        0.108401
Test - acc:         0.915100 loss:        0.279894
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.964580 loss:        0.104914
Test - acc:         0.914300 loss:        0.293389
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.966720 loss:        0.101160
Test - acc:         0.912600 loss:        0.296915
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.965360 loss:        0.100900
Test - acc:         0.911500 loss:        0.297240
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.966160 loss:        0.098081
Test - acc:         0.915200 loss:        0.290782
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.965900 loss:        0.101391
Test - acc:         0.909000 loss:        0.303789
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.966720 loss:        0.097533
Test - acc:         0.915200 loss:        0.298490
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.965300 loss:        0.099530
Test - acc:         0.908100 loss:        0.310018
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.966400 loss:        0.099442
Test - acc:         0.907300 loss:        0.322708
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.965840 loss:        0.098690
Test - acc:         0.902500 loss:        0.342458
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.968140 loss:        0.095413
Test - acc:         0.914800 loss:        0.297026
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.964260 loss:        0.101957
Test - acc:         0.906400 loss:        0.312073
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.966520 loss:        0.097896
Test - acc:         0.905300 loss:        0.321651
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.965120 loss:        0.100941
Test - acc:         0.903500 loss:        0.324440
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.966040 loss:        0.101076
Test - acc:         0.905900 loss:        0.321100
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.965320 loss:        0.101035
Test - acc:         0.901800 loss:        0.339091
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.966420 loss:        0.098674
Test - acc:         0.905800 loss:        0.331216
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.965940 loss:        0.099633
Test - acc:         0.910100 loss:        0.314173
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.966700 loss:        0.098455
Test - acc:         0.901700 loss:        0.347021
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.965240 loss:        0.101362
Test - acc:         0.902200 loss:        0.338720
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.964840 loss:        0.101724
Test - acc:         0.899600 loss:        0.348545
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.964640 loss:        0.102091
Test - acc:         0.909100 loss:        0.308947
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.965340 loss:        0.100649
Test - acc:         0.906000 loss:        0.336961
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.966220 loss:        0.100939
Test - acc:         0.907500 loss:        0.319916
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.963940 loss:        0.105796
Test - acc:         0.902000 loss:        0.348372
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.965640 loss:        0.101189
Test - acc:         0.903900 loss:        0.328738
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.965240 loss:        0.102945
Test - acc:         0.910600 loss:        0.315751
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.963200 loss:        0.111090
Test - acc:         0.908500 loss:        0.323813
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.964520 loss:        0.103138
Test - acc:         0.901100 loss:        0.340951
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.963120 loss:        0.105851
Test - acc:         0.908700 loss:        0.315162
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.962900 loss:        0.106747
Test - acc:         0.905000 loss:        0.332080
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.904940 loss:        0.280038
Test - acc:         0.882000 loss:        0.368082
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.928420 loss:        0.207700
Test - acc:         0.889400 loss:        0.359978
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.933320 loss:        0.194393
Test - acc:         0.874300 loss:        0.403727
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.935540 loss:        0.186682
Test - acc:         0.892000 loss:        0.353025
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.936740 loss:        0.183142
Test - acc:         0.889300 loss:        0.348556
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.938700 loss:        0.178503
Test - acc:         0.896300 loss:        0.337075
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.939000 loss:        0.175738
Test - acc:         0.884600 loss:        0.354575
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.940640 loss:        0.172234
Test - acc:         0.891500 loss:        0.343465
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.941620 loss:        0.168747
Test - acc:         0.888200 loss:        0.379840
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.940580 loss:        0.169780
Test - acc:         0.883100 loss:        0.390751
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.940460 loss:        0.173224
Test - acc:         0.893600 loss:        0.336930
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.943480 loss:        0.164166
Test - acc:         0.894800 loss:        0.330904
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.943080 loss:        0.164082
Test - acc:         0.886100 loss:        0.363603
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.944740 loss:        0.161385
Test - acc:         0.896700 loss:        0.341483
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.944280 loss:        0.162157
Test - acc:         0.890600 loss:        0.357169
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.943580 loss:        0.162451
Test - acc:         0.894900 loss:        0.332597
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.944880 loss:        0.160088
Test - acc:         0.892700 loss:        0.339119
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.944900 loss:        0.157856
Test - acc:         0.879400 loss:        0.389072
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.944800 loss:        0.157542
Test - acc:         0.889700 loss:        0.376668
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.944980 loss:        0.161225
Test - acc:         0.888500 loss:        0.352793
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.946400 loss:        0.154123
Test - acc:         0.887300 loss:        0.364848
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.945460 loss:        0.159128
Test - acc:         0.883000 loss:        0.378223
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.946600 loss:        0.154915
Test - acc:         0.888700 loss:        0.370647
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.945420 loss:        0.159808
Test - acc:         0.892000 loss:        0.335685
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.946940 loss:        0.152691
Test - acc:         0.898400 loss:        0.339755
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.944520 loss:        0.161045
Test - acc:         0.895200 loss:        0.352747
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.947440 loss:        0.153049
Test - acc:         0.896100 loss:        0.345436
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.947700 loss:        0.151365
Test - acc:         0.885800 loss:        0.393524
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.946620 loss:        0.156354
Test - acc:         0.882000 loss:        0.409320
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.947020 loss:        0.154938
Test - acc:         0.897400 loss:        0.343064
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.946180 loss:        0.154229
Test - acc:         0.889200 loss:        0.363933
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.947740 loss:        0.152747
Test - acc:         0.888000 loss:        0.361706
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.945900 loss:        0.155251
Test - acc:         0.887700 loss:        0.354978
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.946760 loss:        0.153480
Test - acc:         0.891900 loss:        0.356684
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.946760 loss:        0.153289
Test - acc:         0.896000 loss:        0.338996
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.945480 loss:        0.157156
Test - acc:         0.889400 loss:        0.375355
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.946220 loss:        0.155483
Test - acc:         0.884700 loss:        0.377752
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.946640 loss:        0.153512
Test - acc:         0.885500 loss:        0.400060
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.947980 loss:        0.151490
Test - acc:         0.894200 loss:        0.353832
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.868620 loss:        0.384032
Test - acc:         0.855700 loss:        0.438114
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.890680 loss:        0.317896
Test - acc:         0.845000 loss:        0.467011
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.898300 loss:        0.297949
Test - acc:         0.860400 loss:        0.438750
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.898760 loss:        0.292124
Test - acc:         0.863800 loss:        0.408882
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.903840 loss:        0.280378
Test - acc:         0.861900 loss:        0.430947
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.905060 loss:        0.276198
Test - acc:         0.865400 loss:        0.410504
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.906180 loss:        0.271461
Test - acc:         0.853200 loss:        0.444419
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.905480 loss:        0.269050
Test - acc:         0.854600 loss:        0.469813
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.909860 loss:        0.263940
Test - acc:         0.856900 loss:        0.447584
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.906780 loss:        0.269928
Test - acc:         0.854200 loss:        0.463901
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.908480 loss:        0.267669
Test - acc:         0.872800 loss:        0.390102
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.910920 loss:        0.260281
Test - acc:         0.858000 loss:        0.456108
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.908140 loss:        0.268456
Test - acc:         0.853600 loss:        0.452615
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.910320 loss:        0.258731
Test - acc:         0.868500 loss:        0.405727
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.909880 loss:        0.263436
Test - acc:         0.855400 loss:        0.456844
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.910940 loss:        0.258915
Test - acc:         0.864800 loss:        0.416110
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.929020 loss:        0.205864
Test - acc:         0.890900 loss:        0.335743
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.935420 loss:        0.184719
Test - acc:         0.888800 loss:        0.337132
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.937700 loss:        0.178668
Test - acc:         0.891600 loss:        0.334613
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.939640 loss:        0.173292
Test - acc:         0.891300 loss:        0.339465
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.940600 loss:        0.171314
Test - acc:         0.892500 loss:        0.336835
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.943180 loss:        0.165177
Test - acc:         0.892500 loss:        0.341762
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.943200 loss:        0.164664
Test - acc:         0.893800 loss:        0.339901
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.942840 loss:        0.164343
Test - acc:         0.893200 loss:        0.338505
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.944440 loss:        0.159327
Test - acc:         0.893100 loss:        0.339054
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.943340 loss:        0.160109
Test - acc:         0.894600 loss:        0.336943
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.945720 loss:        0.157497
Test - acc:         0.897300 loss:        0.344033
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.944440 loss:        0.158743
Test - acc:         0.895500 loss:        0.342012
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.944900 loss:        0.158108
Test - acc:         0.896500 loss:        0.342448
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.945200 loss:        0.156696
Test - acc:         0.894200 loss:        0.345667
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.946220 loss:        0.152430
Test - acc:         0.894900 loss:        0.344085
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.948600 loss:        0.148114
Test - acc:         0.894400 loss:        0.346374
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.948200 loss:        0.150056
Test - acc:         0.894000 loss:        0.345048
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.946880 loss:        0.152170
Test - acc:         0.892600 loss:        0.347799
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.948380 loss:        0.147574
Test - acc:         0.893500 loss:        0.348590
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.947460 loss:        0.148973
Test - acc:         0.893600 loss:        0.350164
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.948340 loss:        0.147904
Test - acc:         0.896100 loss:        0.348354
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.948560 loss:        0.145898
Test - acc:         0.894500 loss:        0.348245
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.948840 loss:        0.144264
Test - acc:         0.895300 loss:        0.349258
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.721140 loss:        0.836804
Test - acc:         0.722500 loss:        0.843308
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.784420 loss:        0.627812
Test - acc:         0.652600 loss:        1.069501
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.800780 loss:        0.582226
Test - acc:         0.758700 loss:        0.726008
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.812680 loss:        0.545968
Test - acc:         0.733700 loss:        0.825482
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.818600 loss:        0.530224
Test - acc:         0.802100 loss:        0.593165
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.823620 loss:        0.510252
Test - acc:         0.810400 loss:        0.567069
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.827940 loss:        0.498118
Test - acc:         0.778500 loss:        0.670276
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.831660 loss:        0.483681
Test - acc:         0.826800 loss:        0.520644
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.835280 loss:        0.477855
Test - acc:         0.809300 loss:        0.561321
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.832980 loss:        0.479582
Test - acc:         0.825600 loss:        0.513164
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.838720 loss:        0.466744
Test - acc:         0.789000 loss:        0.639693
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.840280 loss:        0.458220
Test - acc:         0.830400 loss:        0.511741
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.842600 loss:        0.452348
Test - acc:         0.826300 loss:        0.520571
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.843540 loss:        0.452199
Test - acc:         0.829800 loss:        0.503875
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.847880 loss:        0.439769
Test - acc:         0.828100 loss:        0.509782
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.846680 loss:        0.444404
Test - acc:         0.836200 loss:        0.497106
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.846860 loss:        0.439244
Test - acc:         0.826600 loss:        0.525296
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.850720 loss:        0.433707
Test - acc:         0.829100 loss:        0.512162
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.849940 loss:        0.434416
Test - acc:         0.604000 loss:        1.429142
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.853340 loss:        0.426558
Test - acc:         0.625800 loss:        1.291995
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.852120 loss:        0.422928
Test - acc:         0.822400 loss:        0.517182
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.853740 loss:        0.421517
Test - acc:         0.786800 loss:        0.646684
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.854260 loss:        0.417034
Test - acc:         0.819400 loss:        0.545715
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.854720 loss:        0.417683
Test - acc:         0.839300 loss:        0.479870
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.855660 loss:        0.417104
Test - acc:         0.838800 loss:        0.487819
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.859260 loss:        0.410045
Test - acc:         0.842800 loss:        0.471269
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.858960 loss:        0.410064
Test - acc:         0.832000 loss:        0.495588
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.859020 loss:        0.408688
Test - acc:         0.832000 loss:        0.507992
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.859400 loss:        0.401977
Test - acc:         0.814300 loss:        0.564004
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.860880 loss:        0.404189
Test - acc:         0.837800 loss:        0.493309
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.858600 loss:        0.403960
Test - acc:         0.844800 loss:        0.468138
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.860660 loss:        0.403119
Test - acc:         0.399100 loss:        2.658959
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.860340 loss:        0.398502
Test - acc:         0.843100 loss:        0.468453
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.861440 loss:        0.401055
Test - acc:         0.566800 loss:        1.625570
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.861060 loss:        0.397627
Test - acc:         0.753200 loss:        0.778457
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.862360 loss:        0.394214
Test - acc:         0.633500 loss:        1.304188
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.862200 loss:        0.394508
Test - acc:         0.845300 loss:        0.470973
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.862620 loss:        0.389895
Test - acc:         0.840800 loss:        0.478190
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.865360 loss:        0.388870
Test - acc:         0.825500 loss:        0.537585
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.387360 loss:        1.719887
Test - acc:         0.392200 loss:        1.610607
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.481100 loss:        1.425320
Test - acc:         0.259300 loss:        2.657812
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.521380 loss:        1.325035
Test - acc:         0.184600 loss:        3.415673
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.542860 loss:        1.263795
Test - acc:         0.478700 loss:        1.458597
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.561000 loss:        1.222211
Test - acc:         0.533800 loss:        1.340483
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.577400 loss:        1.176766
Test - acc:         0.571300 loss:        1.185409
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.590140 loss:        1.147873
Test - acc:         0.100000 loss:       26.873860
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.600860 loss:        1.120896
Test - acc:         0.505000 loss:        1.419276
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.608240 loss:        1.100300
Test - acc:         0.100000 loss:       14.388960
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.613840 loss:        1.081689
Test - acc:         0.100000 loss:       18.901049
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.623860 loss:        1.061388
Test - acc:         0.100000 loss:       16.875851
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.629740 loss:        1.045972
Test - acc:         0.496700 loss:        1.517173
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.636660 loss:        1.027291
Test - acc:         0.100000 loss:        9.922585
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.644380 loss:        1.006556
Test - acc:         0.100000 loss:       17.186349
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.646560 loss:        1.002295
Test - acc:         0.624100 loss:        1.043608
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.650980 loss:        0.987463
Test - acc:         0.100000 loss:        9.033765
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.655860 loss:        0.974455
Test - acc:         0.420900 loss:        1.987186
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.658540 loss:        0.961870
Test - acc:         0.100000 loss:       14.576831
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.664760 loss:        0.952078
Test - acc:         0.519300 loss:        1.481368
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.668600 loss:        0.938431
Test - acc:         0.366300 loss:        2.297355
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.672800 loss:        0.930178
Test - acc:         0.376700 loss:        2.265344
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.673900 loss:        0.921903
Test - acc:         0.194400 loss:        4.192410
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.675100 loss:        0.919341
Test - acc:         0.665400 loss:        0.953290
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.679860 loss:        0.909785
Test - acc:         0.461100 loss:        1.819299
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.681580 loss:        0.900612
Test - acc:         0.526100 loss:        1.481795
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.684280 loss:        0.895780
Test - acc:         0.368700 loss:        2.324375
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.688320 loss:        0.891658
Test - acc:         0.651000 loss:        1.022210
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.690240 loss:        0.885448
Test - acc:         0.608100 loss:        1.125637
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.691460 loss:        0.875612
Test - acc:         0.614900 loss:        1.121235
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.691940 loss:        0.876246
Test - acc:         0.405100 loss:        1.953279
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.692660 loss:        0.875682
Test - acc:         0.587300 loss:        1.180292
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.694360 loss:        0.864743
Test - acc:         0.662000 loss:        0.983793
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.695560 loss:        0.860023
Test - acc:         0.693500 loss:        0.877265
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.700140 loss:        0.854139
Test - acc:         0.671800 loss:        0.963688
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.701620 loss:        0.853901
Test - acc:         0.173500 loss:        5.312346
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.700100 loss:        0.846271
Test - acc:         0.600900 loss:        1.158473
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.703380 loss:        0.842812
Test - acc:         0.599900 loss:        1.212721
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.705240 loss:        0.839908
Test - acc:         0.666500 loss:        0.974059
Sparsity :          0.9961
Wdecay :        0.000500
