Running --prune_criterion topflip --seed 43 --prune_freq 50 --prune_rate 0.5 --noise --comment=vgg19_crit=topflip_pf=50_seed=43 --save_model=pre-finetune/vgg19_topflip_pf50_s43
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "topflip",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_topflip_pf50_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.106800 loss:        2.887146
Test - acc:         0.106900 loss:        2.342092
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.150740 loss:        2.225399
Test - acc:         0.217800 loss:        1.992225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.234640 loss:        1.934278
Test - acc:         0.264700 loss:        1.843442
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.300120 loss:        1.767933
Test - acc:         0.338200 loss:        1.686450
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.392020 loss:        1.587929
Test - acc:         0.437300 loss:        1.548025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.492220 loss:        1.385459
Test - acc:         0.509400 loss:        1.337186
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.563300 loss:        1.221598
Test - acc:         0.571100 loss:        1.229948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.623540 loss:        1.073808
Test - acc:         0.625300 loss:        1.131032
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.674300 loss:        0.946742
Test - acc:         0.649400 loss:        1.065826
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.715740 loss:        0.855450
Test - acc:         0.646100 loss:        1.036136
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.737360 loss:        0.797917
Test - acc:         0.710400 loss:        0.907650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.757520 loss:        0.740845
Test - acc:         0.739900 loss:        0.834118
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.767060 loss:        0.710992
Test - acc:         0.757100 loss:        0.765124
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.774240 loss:        0.687791
Test - acc:         0.616100 loss:        1.443478
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.778580 loss:        0.675666
Test - acc:         0.758400 loss:        0.744959
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.783520 loss:        0.664763
Test - acc:         0.749500 loss:        0.786861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.789340 loss:        0.647328
Test - acc:         0.641700 loss:        1.267682
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.797920 loss:        0.624745
Test - acc:         0.704000 loss:        1.001233
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.798240 loss:        0.618439
Test - acc:         0.726200 loss:        0.885273
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.806440 loss:        0.602265
Test - acc:         0.693200 loss:        1.160592
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.802280 loss:        0.607501
Test - acc:         0.719900 loss:        0.932802
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.807540 loss:        0.589088
Test - acc:         0.730100 loss:        0.917314
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.809820 loss:        0.588532
Test - acc:         0.715300 loss:        0.942745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.810920 loss:        0.585595
Test - acc:         0.714900 loss:        0.915837
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.811620 loss:        0.582650
Test - acc:         0.728800 loss:        0.960459
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.813400 loss:        0.579243
Test - acc:         0.747100 loss:        0.819809
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.818040 loss:        0.561499
Test - acc:         0.780800 loss:        0.700761
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.816380 loss:        0.563682
Test - acc:         0.758900 loss:        0.750427
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.818180 loss:        0.555751
Test - acc:         0.765400 loss:        0.744770
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.820560 loss:        0.551154
Test - acc:         0.754200 loss:        0.816667
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.820700 loss:        0.553704
Test - acc:         0.772100 loss:        0.741460
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.823620 loss:        0.541712
Test - acc:         0.776900 loss:        0.711410
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.823200 loss:        0.544783
Test - acc:         0.787900 loss:        0.659339
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.824960 loss:        0.537488
Test - acc:         0.739300 loss:        0.842265
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.824540 loss:        0.538636
Test - acc:         0.747200 loss:        0.830881
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.825260 loss:        0.538525
Test - acc:         0.721800 loss:        0.859322
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.826340 loss:        0.537777
Test - acc:         0.759300 loss:        0.778132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.825240 loss:        0.536022
Test - acc:         0.699400 loss:        0.966428
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.828320 loss:        0.533694
Test - acc:         0.748900 loss:        0.794782
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.829740 loss:        0.524597
Test - acc:         0.779400 loss:        0.698693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.830560 loss:        0.526019
Test - acc:         0.765000 loss:        0.810277
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.829480 loss:        0.525207
Test - acc:         0.817700 loss:        0.535735
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.830560 loss:        0.524708
Test - acc:         0.800900 loss:        0.639905
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.832280 loss:        0.520831
Test - acc:         0.658700 loss:        1.128968
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.830180 loss:        0.520592
Test - acc:         0.752500 loss:        0.800119
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.830940 loss:        0.525489
Test - acc:         0.809500 loss:        0.578567
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.828380 loss:        0.522665
Test - acc:         0.742900 loss:        0.823478
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.833420 loss:        0.507824
Test - acc:         0.753900 loss:        0.753708
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.832900 loss:        0.511564
Test - acc:         0.644200 loss:        1.207186
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.831300 loss:        0.517602
Test - acc:         0.803600 loss:        0.605393
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.838320 loss:        0.492059
Test - acc:         0.767100 loss:        0.735565
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.842840 loss:        0.475046
Test - acc:         0.801600 loss:        0.627154
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.846620 loss:        0.469376
Test - acc:         0.823700 loss:        0.529785
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.850760 loss:        0.460041
Test - acc:         0.808600 loss:        0.595257
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.850660 loss:        0.456138
Test - acc:         0.751700 loss:        0.852653
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.848580 loss:        0.459159
Test - acc:         0.806800 loss:        0.584764
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.847740 loss:        0.459741
Test - acc:         0.805800 loss:        0.640645
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.848540 loss:        0.460192
Test - acc:         0.787500 loss:        0.668923
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.851720 loss:        0.452207
Test - acc:         0.825100 loss:        0.549283
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.852600 loss:        0.451599
Test - acc:         0.827100 loss:        0.546191
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.852580 loss:        0.450763
Test - acc:         0.849000 loss:        0.473592
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.851720 loss:        0.450359
Test - acc:         0.785000 loss:        0.709848
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.851080 loss:        0.454494
Test - acc:         0.818600 loss:        0.542737
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.852040 loss:        0.449287
Test - acc:         0.793800 loss:        0.665465
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.852420 loss:        0.447185
Test - acc:         0.808200 loss:        0.601258
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.852320 loss:        0.449648
Test - acc:         0.824100 loss:        0.558641
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.853440 loss:        0.446739
Test - acc:         0.804600 loss:        0.609050
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.853580 loss:        0.446291
Test - acc:         0.824200 loss:        0.555884
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.853180 loss:        0.441935
Test - acc:         0.779400 loss:        0.706781
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.852420 loss:        0.453336
Test - acc:         0.788800 loss:        0.682121
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.850300 loss:        0.450839
Test - acc:         0.808200 loss:        0.615930
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.853580 loss:        0.442550
Test - acc:         0.835300 loss:        0.504774
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.852160 loss:        0.445356
Test - acc:         0.762500 loss:        0.771460
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.852760 loss:        0.444923
Test - acc:         0.725700 loss:        0.892058
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.854300 loss:        0.443290
Test - acc:         0.777800 loss:        0.689414
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.852020 loss:        0.446936
Test - acc:         0.803000 loss:        0.618941
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.853820 loss:        0.444077
Test - acc:         0.822900 loss:        0.527932
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.854680 loss:        0.441496
Test - acc:         0.803100 loss:        0.639023
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.853420 loss:        0.447321
Test - acc:         0.773800 loss:        0.721355
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.856560 loss:        0.439655
Test - acc:         0.816300 loss:        0.565981
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.852560 loss:        0.446880
Test - acc:         0.798600 loss:        0.632687
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.853620 loss:        0.442685
Test - acc:         0.687500 loss:        1.154492
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.853580 loss:        0.442643
Test - acc:         0.815000 loss:        0.591988
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.854820 loss:        0.438515
Test - acc:         0.814400 loss:        0.596383
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.853620 loss:        0.444471
Test - acc:         0.763500 loss:        0.792793
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.854860 loss:        0.440286
Test - acc:         0.745700 loss:        0.828261
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.857900 loss:        0.433325
Test - acc:         0.813900 loss:        0.635678
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.855420 loss:        0.441345
Test - acc:         0.828400 loss:        0.543704
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.854820 loss:        0.438856
Test - acc:         0.803100 loss:        0.624369
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.854480 loss:        0.441111
Test - acc:         0.800100 loss:        0.637527
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.851660 loss:        0.448072
Test - acc:         0.834400 loss:        0.536577
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.854880 loss:        0.440801
Test - acc:         0.822400 loss:        0.541357
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.855740 loss:        0.435788
Test - acc:         0.797400 loss:        0.623234
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.853280 loss:        0.439297
Test - acc:         0.789700 loss:        0.657097
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.854220 loss:        0.440843
Test - acc:         0.792400 loss:        0.675596
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.855140 loss:        0.441327
Test - acc:         0.824300 loss:        0.561964
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.854260 loss:        0.441292
Test - acc:         0.827200 loss:        0.535660
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.853800 loss:        0.438128
Test - acc:         0.804700 loss:        0.594589
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.853800 loss:        0.439753
Test - acc:         0.775200 loss:        0.671820
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.855140 loss:        0.438078
Test - acc:         0.790800 loss:        0.650212
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.860460 loss:        0.423591
Test - acc:         0.686700 loss:        1.368477
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.861880 loss:        0.416024
Test - acc:         0.829700 loss:        0.544203
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.864500 loss:        0.410288
Test - acc:         0.797900 loss:        0.638570
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.867280 loss:        0.400940
Test - acc:         0.793400 loss:        0.654557
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.866020 loss:        0.402295
Test - acc:         0.794600 loss:        0.626656
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.867100 loss:        0.401263
Test - acc:         0.796300 loss:        0.636524
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.865900 loss:        0.400721
Test - acc:         0.827400 loss:        0.520361
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.866580 loss:        0.399225
Test - acc:         0.832300 loss:        0.510993
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.864280 loss:        0.400994
Test - acc:         0.809900 loss:        0.570606
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.870360 loss:        0.388861
Test - acc:         0.829500 loss:        0.537508
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.867820 loss:        0.396314
Test - acc:         0.762900 loss:        0.796319
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.868420 loss:        0.395742
Test - acc:         0.847200 loss:        0.477598
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.870860 loss:        0.388818
Test - acc:         0.754100 loss:        0.823519
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.865860 loss:        0.402677
Test - acc:         0.822800 loss:        0.548782
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.871240 loss:        0.384962
Test - acc:         0.836500 loss:        0.527152
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.869680 loss:        0.391492
Test - acc:         0.815600 loss:        0.562676
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.870380 loss:        0.391632
Test - acc:         0.818700 loss:        0.561657
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.867880 loss:        0.393513
Test - acc:         0.837300 loss:        0.503924
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.869800 loss:        0.390572
Test - acc:         0.799400 loss:        0.648483
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.867400 loss:        0.396400
Test - acc:         0.799600 loss:        0.669377
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.869180 loss:        0.390797
Test - acc:         0.780500 loss:        0.690747
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.868200 loss:        0.392237
Test - acc:         0.799500 loss:        0.592105
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.869700 loss:        0.385996
Test - acc:         0.822000 loss:        0.547987
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.868440 loss:        0.394416
Test - acc:         0.799500 loss:        0.637027
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.870120 loss:        0.394157
Test - acc:         0.800000 loss:        0.630264
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.870060 loss:        0.389232
Test - acc:         0.755100 loss:        0.837705
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.867620 loss:        0.394913
Test - acc:         0.820200 loss:        0.568519
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.870220 loss:        0.392438
Test - acc:         0.832900 loss:        0.513806
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.868940 loss:        0.393954
Test - acc:         0.763500 loss:        0.782594
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.870720 loss:        0.391049
Test - acc:         0.825200 loss:        0.548353
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.870000 loss:        0.390532
Test - acc:         0.832400 loss:        0.523904
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.869040 loss:        0.395018
Test - acc:         0.837900 loss:        0.509112
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.868900 loss:        0.395589
Test - acc:         0.768000 loss:        0.737355
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.867920 loss:        0.394351
Test - acc:         0.794900 loss:        0.678191
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.871120 loss:        0.389925
Test - acc:         0.811000 loss:        0.595722
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.858380 loss:        0.427579
Test - acc:         0.786600 loss:        0.654311
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.863020 loss:        0.415436
Test - acc:         0.739900 loss:        0.832507
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.867180 loss:        0.405503
Test - acc:         0.815800 loss:        0.597993
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.868680 loss:        0.395292
Test - acc:         0.776500 loss:        0.780392
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.869180 loss:        0.393909
Test - acc:         0.781200 loss:        0.755448
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.868820 loss:        0.395024
Test - acc:         0.836400 loss:        0.507268
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.869780 loss:        0.389837
Test - acc:         0.803600 loss:        0.692231
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.867920 loss:        0.395640
Test - acc:         0.801300 loss:        0.651220
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.870660 loss:        0.390130
Test - acc:         0.764200 loss:        0.696237
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.865720 loss:        0.398761
Test - acc:         0.746300 loss:        0.960065
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.867320 loss:        0.398605
Test - acc:         0.844600 loss:        0.469280
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.868680 loss:        0.394481
Test - acc:         0.818700 loss:        0.606382
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.867600 loss:        0.398138
Test - acc:         0.774800 loss:        0.767444
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.868120 loss:        0.395361
Test - acc:         0.835500 loss:        0.532964
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.869300 loss:        0.390798
Test - acc:         0.794700 loss:        0.685074
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.915360 loss:        0.256306
Test - acc:         0.898900 loss:        0.310707
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.927280 loss:        0.218712
Test - acc:         0.904100 loss:        0.301359
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.932240 loss:        0.202244
Test - acc:         0.905400 loss:        0.289474
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.935460 loss:        0.192037
Test - acc:         0.906500 loss:        0.289684
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.937400 loss:        0.186700
Test - acc:         0.905900 loss:        0.294325
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.939060 loss:        0.180879
Test - acc:         0.907800 loss:        0.286495
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.942240 loss:        0.172001
Test - acc:         0.905400 loss:        0.293895
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.943660 loss:        0.165237
Test - acc:         0.907300 loss:        0.294717
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.945780 loss:        0.161363
Test - acc:         0.907800 loss:        0.295179
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.946180 loss:        0.158557
Test - acc:         0.906800 loss:        0.293870
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.945700 loss:        0.156221
Test - acc:         0.905300 loss:        0.298634
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.948560 loss:        0.150694
Test - acc:         0.905200 loss:        0.310563
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.947360 loss:        0.151904
Test - acc:         0.910400 loss:        0.289647
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.951460 loss:        0.145091
Test - acc:         0.908000 loss:        0.297719
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.950440 loss:        0.145305
Test - acc:         0.902600 loss:        0.317680
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.949340 loss:        0.146680
Test - acc:         0.902800 loss:        0.321176
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.950520 loss:        0.143624
Test - acc:         0.910600 loss:        0.302470
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.951560 loss:        0.140557
Test - acc:         0.903000 loss:        0.325514
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.952060 loss:        0.140320
Test - acc:         0.897500 loss:        0.329109
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.952360 loss:        0.138327
Test - acc:         0.906300 loss:        0.312466
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.953000 loss:        0.137940
Test - acc:         0.903800 loss:        0.315232
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.952340 loss:        0.139191
Test - acc:         0.907500 loss:        0.303508
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.952680 loss:        0.135346
Test - acc:         0.901300 loss:        0.328045
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.951920 loss:        0.139203
Test - acc:         0.899700 loss:        0.324316
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.952500 loss:        0.138318
Test - acc:         0.905100 loss:        0.317746
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.951700 loss:        0.139970
Test - acc:         0.902500 loss:        0.315287
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.953480 loss:        0.136310
Test - acc:         0.898700 loss:        0.335783
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.953020 loss:        0.137010
Test - acc:         0.892400 loss:        0.368267
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.953440 loss:        0.137660
Test - acc:         0.895800 loss:        0.362557
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.952820 loss:        0.137345
Test - acc:         0.896600 loss:        0.334431
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.953840 loss:        0.135708
Test - acc:         0.895700 loss:        0.350529
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.953780 loss:        0.134147
Test - acc:         0.899600 loss:        0.340774
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.953180 loss:        0.134756
Test - acc:         0.898800 loss:        0.335046
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.952420 loss:        0.138229
Test - acc:         0.890000 loss:        0.362595
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.952980 loss:        0.137422
Test - acc:         0.900400 loss:        0.335143
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.953120 loss:        0.137394
Test - acc:         0.898900 loss:        0.345520
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.950180 loss:        0.142249
Test - acc:         0.895300 loss:        0.351775
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.953040 loss:        0.135736
Test - acc:         0.897200 loss:        0.350980
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.953060 loss:        0.138237
Test - acc:         0.894500 loss:        0.351014
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.951460 loss:        0.138533
Test - acc:         0.897500 loss:        0.345062
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.953200 loss:        0.135977
Test - acc:         0.890400 loss:        0.390038
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.950400 loss:        0.142241
Test - acc:         0.893600 loss:        0.354366
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.951540 loss:        0.140525
Test - acc:         0.897000 loss:        0.343585
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.951720 loss:        0.139230
Test - acc:         0.893300 loss:        0.356494
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.950200 loss:        0.142138
Test - acc:         0.893300 loss:        0.356673
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.953320 loss:        0.137765
Test - acc:         0.900000 loss:        0.359001
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.952620 loss:        0.135972
Test - acc:         0.889300 loss:        0.383300
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.950840 loss:        0.141559
Test - acc:         0.894600 loss:        0.361855
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.954780 loss:        0.134185
Test - acc:         0.900000 loss:        0.355750
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.951300 loss:        0.139892
Test - acc:         0.898700 loss:        0.325818
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.941940 loss:        0.167341
Test - acc:         0.900900 loss:        0.322586
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.944300 loss:        0.158852
Test - acc:         0.895100 loss:        0.355605
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.945740 loss:        0.155263
Test - acc:         0.897400 loss:        0.335643
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.945780 loss:        0.156592
Test - acc:         0.897800 loss:        0.339421
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.946840 loss:        0.154125
Test - acc:         0.887100 loss:        0.362593
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.947100 loss:        0.152812
Test - acc:         0.886300 loss:        0.381912
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.946020 loss:        0.153535
Test - acc:         0.889900 loss:        0.361886
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.949100 loss:        0.147308
Test - acc:         0.898100 loss:        0.340160
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.946440 loss:        0.153205
Test - acc:         0.884800 loss:        0.394655
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.946420 loss:        0.155352
Test - acc:         0.888900 loss:        0.369021
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.948560 loss:        0.149557
Test - acc:         0.895700 loss:        0.346618
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.948720 loss:        0.148997
Test - acc:         0.895600 loss:        0.365716
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.948640 loss:        0.148765
Test - acc:         0.891500 loss:        0.375752
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.949240 loss:        0.147373
Test - acc:         0.893100 loss:        0.370131
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.948680 loss:        0.150135
Test - acc:         0.895500 loss:        0.358799
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.945960 loss:        0.154767
Test - acc:         0.893800 loss:        0.346682
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.950040 loss:        0.145107
Test - acc:         0.889200 loss:        0.383128
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.949840 loss:        0.147924
Test - acc:         0.892900 loss:        0.367422
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.948960 loss:        0.146263
Test - acc:         0.902400 loss:        0.334331
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.949160 loss:        0.148191
Test - acc:         0.893000 loss:        0.347623
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.948100 loss:        0.149874
Test - acc:         0.893900 loss:        0.359931
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.947820 loss:        0.148778
Test - acc:         0.892800 loss:        0.371895
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.949300 loss:        0.145937
Test - acc:         0.891800 loss:        0.356699
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.948460 loss:        0.146937
Test - acc:         0.888700 loss:        0.353471
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.949420 loss:        0.147751
Test - acc:         0.889800 loss:        0.392960
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.949340 loss:        0.147316
Test - acc:         0.884800 loss:        0.381181
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.949400 loss:        0.146679
Test - acc:         0.891800 loss:        0.363121
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.949080 loss:        0.146860
Test - acc:         0.894200 loss:        0.362253
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.950060 loss:        0.143584
Test - acc:         0.890100 loss:        0.370448
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.950500 loss:        0.142896
Test - acc:         0.883600 loss:        0.395654
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.950400 loss:        0.142470
Test - acc:         0.898500 loss:        0.350260
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.950420 loss:        0.145388
Test - acc:         0.896100 loss:        0.343924
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.949200 loss:        0.147811
Test - acc:         0.892800 loss:        0.356907
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.950280 loss:        0.144624
Test - acc:         0.893300 loss:        0.362991
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.950680 loss:        0.142224
Test - acc:         0.889700 loss:        0.382411
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.947680 loss:        0.146463
Test - acc:         0.897500 loss:        0.346902
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.949360 loss:        0.145900
Test - acc:         0.892000 loss:        0.361556
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.951000 loss:        0.140389
Test - acc:         0.892700 loss:        0.375147
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.950100 loss:        0.144469
Test - acc:         0.898200 loss:        0.340527
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.950440 loss:        0.145114
Test - acc:         0.893700 loss:        0.363724
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.951520 loss:        0.140160
Test - acc:         0.895300 loss:        0.359412
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.950240 loss:        0.145048
Test - acc:         0.898000 loss:        0.345131
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.950500 loss:        0.145409
Test - acc:         0.894600 loss:        0.369455
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.951140 loss:        0.140992
Test - acc:         0.895200 loss:        0.356864
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.950700 loss:        0.143314
Test - acc:         0.896100 loss:        0.359947
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.952460 loss:        0.139483
Test - acc:         0.894100 loss:        0.352314
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.949920 loss:        0.140828
Test - acc:         0.885200 loss:        0.390453
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.948300 loss:        0.146353
Test - acc:         0.885800 loss:        0.381055
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.950700 loss:        0.142326
Test - acc:         0.893300 loss:        0.354346
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.950120 loss:        0.139334
Test - acc:         0.892100 loss:        0.361382
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.955240 loss:        0.131044
Test - acc:         0.906200 loss:        0.302634
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.962260 loss:        0.109896
Test - acc:         0.907700 loss:        0.303822
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.964320 loss:        0.104873
Test - acc:         0.910300 loss:        0.303820
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.967100 loss:        0.098348
Test - acc:         0.908800 loss:        0.305989
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.968720 loss:        0.091466
Test - acc:         0.909300 loss:        0.308614
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.968800 loss:        0.089678
Test - acc:         0.910600 loss:        0.310828
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.970680 loss:        0.087014
Test - acc:         0.909100 loss:        0.314106
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.970000 loss:        0.086830
Test - acc:         0.910100 loss:        0.318945
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.972160 loss:        0.081837
Test - acc:         0.909800 loss:        0.318882
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.971920 loss:        0.081194
Test - acc:         0.911500 loss:        0.317395
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.973480 loss:        0.078191
Test - acc:         0.911800 loss:        0.318954
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.973680 loss:        0.077553
Test - acc:         0.912800 loss:        0.318370
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.974500 loss:        0.074676
Test - acc:         0.910500 loss:        0.322444
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.974120 loss:        0.075971
Test - acc:         0.911900 loss:        0.327494
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.973620 loss:        0.075673
Test - acc:         0.912000 loss:        0.326842
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.975600 loss:        0.073075
Test - acc:         0.911000 loss:        0.326466
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.975420 loss:        0.071783
Test - acc:         0.911300 loss:        0.335589
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.974360 loss:        0.074294
Test - acc:         0.912200 loss:        0.330712
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.975760 loss:        0.070061
Test - acc:         0.912200 loss:        0.333804
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.977780 loss:        0.067111
Test - acc:         0.911300 loss:        0.335408
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.975940 loss:        0.069149
Test - acc:         0.912400 loss:        0.333825
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.976020 loss:        0.067939
Test - acc:         0.911800 loss:        0.337517
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.977900 loss:        0.064602
Test - acc:         0.910700 loss:        0.337643
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.978640 loss:        0.062736
Test - acc:         0.911800 loss:        0.336688
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.978120 loss:        0.065252
Test - acc:         0.912700 loss:        0.338691
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.978180 loss:        0.064707
Test - acc:         0.912200 loss:        0.334267
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.978080 loss:        0.064439
Test - acc:         0.910200 loss:        0.338548
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.978520 loss:        0.062576
Test - acc:         0.912000 loss:        0.345997
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.978580 loss:        0.062265
Test - acc:         0.911300 loss:        0.338615
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.978900 loss:        0.061735
Test - acc:         0.911800 loss:        0.340212
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.978800 loss:        0.061518
Test - acc:         0.910500 loss:        0.345108
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.979700 loss:        0.059857
Test - acc:         0.913100 loss:        0.343237
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.980960 loss:        0.057079
Test - acc:         0.911800 loss:        0.344963
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.980000 loss:        0.057311
Test - acc:         0.910900 loss:        0.347111
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.980100 loss:        0.056958
Test - acc:         0.910800 loss:        0.347030
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.980420 loss:        0.057624
Test - acc:         0.910000 loss:        0.355952
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.979820 loss:        0.057947
Test - acc:         0.910700 loss:        0.351584
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.979220 loss:        0.058535
Test - acc:         0.908700 loss:        0.355303
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.979280 loss:        0.059048
Test - acc:         0.908300 loss:        0.353365
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.980400 loss:        0.057612
Test - acc:         0.909600 loss:        0.352625
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.980480 loss:        0.056207
Test - acc:         0.910200 loss:        0.351507
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.980420 loss:        0.054794
Test - acc:         0.909200 loss:        0.361310
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.980480 loss:        0.055031
Test - acc:         0.910900 loss:        0.355128
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.981060 loss:        0.054883
Test - acc:         0.911200 loss:        0.355110
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.981320 loss:        0.055489
Test - acc:         0.910100 loss:        0.366194
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.981720 loss:        0.054738
Test - acc:         0.908900 loss:        0.355146
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.981440 loss:        0.053912
Test - acc:         0.911700 loss:        0.357062
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.981900 loss:        0.053513
Test - acc:         0.911100 loss:        0.359884
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.982140 loss:        0.051679
Test - acc:         0.910800 loss:        0.359442
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.982120 loss:        0.051719
Test - acc:         0.910900 loss:        0.359314
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.952940 loss:        0.134670
Test - acc:         0.902400 loss:        0.347997
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.958400 loss:        0.120204
Test - acc:         0.905000 loss:        0.345937
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.961260 loss:        0.112553
Test - acc:         0.904200 loss:        0.338988
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.963040 loss:        0.108446
Test - acc:         0.902800 loss:        0.345529
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.964900 loss:        0.100638
Test - acc:         0.908000 loss:        0.329822
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.963620 loss:        0.103075
Test - acc:         0.907600 loss:        0.332426
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.964740 loss:        0.100172
Test - acc:         0.906000 loss:        0.337448
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.964800 loss:        0.100153
Test - acc:         0.907400 loss:        0.336923
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.968040 loss:        0.093271
Test - acc:         0.907600 loss:        0.330546
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.967900 loss:        0.091386
Test - acc:         0.904200 loss:        0.341436
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.967800 loss:        0.091915
Test - acc:         0.907700 loss:        0.342455
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.968520 loss:        0.091004
Test - acc:         0.907600 loss:        0.339130
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.967140 loss:        0.092222
Test - acc:         0.907900 loss:        0.334782
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.968180 loss:        0.091513
Test - acc:         0.908000 loss:        0.335723
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.968520 loss:        0.090754
Test - acc:         0.907800 loss:        0.337483
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.969340 loss:        0.088720
Test - acc:         0.906400 loss:        0.340389
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.968000 loss:        0.090340
Test - acc:         0.907300 loss:        0.341428
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.970200 loss:        0.085512
Test - acc:         0.906200 loss:        0.343130
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.971020 loss:        0.083619
Test - acc:         0.906800 loss:        0.344038
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.971120 loss:        0.084632
Test - acc:         0.906800 loss:        0.339724
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.971480 loss:        0.082735
Test - acc:         0.908300 loss:        0.342806
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.969900 loss:        0.084151
Test - acc:         0.908900 loss:        0.342758
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.971720 loss:        0.080366
Test - acc:         0.905900 loss:        0.349753
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.971840 loss:        0.081292
Test - acc:         0.908000 loss:        0.343726
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.971680 loss:        0.083833
Test - acc:         0.909200 loss:        0.340131
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.970980 loss:        0.080742
Test - acc:         0.906900 loss:        0.352825
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.972400 loss:        0.080805
Test - acc:         0.907100 loss:        0.347992
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.972660 loss:        0.078191
Test - acc:         0.908900 loss:        0.346801
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.972880 loss:        0.078553
Test - acc:         0.908900 loss:        0.352085
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.972260 loss:        0.081471
Test - acc:         0.908900 loss:        0.344402
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.972840 loss:        0.076750
Test - acc:         0.909300 loss:        0.347073
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.973700 loss:        0.075816
Test - acc:         0.906800 loss:        0.352398
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.972000 loss:        0.078517
Test - acc:         0.909000 loss:        0.353637
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.972920 loss:        0.077556
Test - acc:         0.908400 loss:        0.352362
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.972740 loss:        0.076952
Test - acc:         0.909400 loss:        0.357275
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.973260 loss:        0.075658
Test - acc:         0.911000 loss:        0.353423
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.973820 loss:        0.075221
Test - acc:         0.909500 loss:        0.354284
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.972880 loss:        0.076802
Test - acc:         0.908500 loss:        0.363518
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.973220 loss:        0.076553
Test - acc:         0.907500 loss:        0.355794
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.973880 loss:        0.075635
Test - acc:         0.906900 loss:        0.356221
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.974120 loss:        0.073032
Test - acc:         0.904800 loss:        0.367172
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.974220 loss:        0.073565
Test - acc:         0.906000 loss:        0.356693
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.973740 loss:        0.076223
Test - acc:         0.907300 loss:        0.355029
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.974100 loss:        0.074351
Test - acc:         0.907500 loss:        0.349842
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.974840 loss:        0.072310
Test - acc:         0.908300 loss:        0.351892
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.974400 loss:        0.073132
Test - acc:         0.907800 loss:        0.349307
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.975200 loss:        0.072021
Test - acc:         0.906000 loss:        0.355183
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.974880 loss:        0.073389
Test - acc:         0.906800 loss:        0.360895
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.975920 loss:        0.069986
Test - acc:         0.908900 loss:        0.358237
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.975180 loss:        0.071385
Test - acc:         0.906900 loss:        0.363840
Sparsity :          0.9844
Wdecay :        0.000500
