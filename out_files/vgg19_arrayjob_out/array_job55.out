Running --prune_criterion random --seed 44 --prune_freq 39 --prune_rate 0.5 --comment=vgg19_crit=random_pf=39_seed=44 --save_model=pre-finetune/vgg19_random_pf39_s44
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "random",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_random_pf39_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.108300 loss:        2.474734
Test - acc:         0.119300 loss:        2.267837
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.150540 loss:        2.171051
Test - acc:         0.194700 loss:        1.934882
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.241500 loss:        1.878880
Test - acc:         0.265700 loss:        1.874334
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.306100 loss:        1.759676
Test - acc:         0.321600 loss:        1.772883
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.438900 loss:        1.470889
Test - acc:         0.480900 loss:        1.401025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.559400 loss:        1.215320
Test - acc:         0.570900 loss:        1.236438
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.640540 loss:        1.022796
Test - acc:         0.562900 loss:        1.313802
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.692820 loss:        0.908134
Test - acc:         0.458400 loss:        1.942289
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.726620 loss:        0.830650
Test - acc:         0.698100 loss:        0.963013
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.750900 loss:        0.760590
Test - acc:         0.690900 loss:        0.970304
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.770860 loss:        0.699995
Test - acc:         0.733400 loss:        0.833592
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.780940 loss:        0.667889
Test - acc:         0.743900 loss:        0.787249
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.789600 loss:        0.649976
Test - acc:         0.738800 loss:        0.849763
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.796420 loss:        0.625437
Test - acc:         0.722200 loss:        0.895129
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.802720 loss:        0.608729
Test - acc:         0.740500 loss:        0.775364
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.806240 loss:        0.596568
Test - acc:         0.700700 loss:        1.022337
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.812280 loss:        0.587819
Test - acc:         0.763300 loss:        0.771137
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.812580 loss:        0.580530
Test - acc:         0.772300 loss:        0.710877
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.818180 loss:        0.560021
Test - acc:         0.742400 loss:        0.803548
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.821800 loss:        0.557051
Test - acc:         0.728400 loss:        0.884478
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.827740 loss:        0.535048
Test - acc:         0.737700 loss:        0.793409
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.828660 loss:        0.530681
Test - acc:         0.763400 loss:        0.765218
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.825700 loss:        0.538257
Test - acc:         0.761600 loss:        0.788449
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.826500 loss:        0.539644
Test - acc:         0.788700 loss:        0.688115
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.833200 loss:        0.521488
Test - acc:         0.765500 loss:        0.769116
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.835460 loss:        0.515609
Test - acc:         0.707000 loss:        0.956001
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.836300 loss:        0.512242
Test - acc:         0.744700 loss:        0.809497
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.839600 loss:        0.497366
Test - acc:         0.704800 loss:        1.055230
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.837900 loss:        0.502614
Test - acc:         0.815400 loss:        0.592117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.838720 loss:        0.504216
Test - acc:         0.766300 loss:        0.782455
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.846120 loss:        0.485764
Test - acc:         0.739600 loss:        0.860019
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.840980 loss:        0.499632
Test - acc:         0.767800 loss:        0.798063
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.844020 loss:        0.487687
Test - acc:         0.785900 loss:        0.677178
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.840220 loss:        0.499990
Test - acc:         0.769100 loss:        0.729718
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.844640 loss:        0.483766
Test - acc:         0.789700 loss:        0.670113
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.847620 loss:        0.479940
Test - acc:         0.780700 loss:        0.708346
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.847820 loss:        0.476032
Test - acc:         0.694800 loss:        0.955153
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.846740 loss:        0.480284
Test - acc:         0.744600 loss:        0.948408
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.849640 loss:        0.473404
Test - acc:         0.783500 loss:        0.690285
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.768500 loss:        0.723802
Test - acc:         0.718600 loss:        0.993216
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.814000 loss:        0.575419
Test - acc:         0.679300 loss:        1.113013
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.824020 loss:        0.545038
Test - acc:         0.757800 loss:        0.799024
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.826740 loss:        0.529998
Test - acc:         0.777400 loss:        0.729089
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.834520 loss:        0.508657
Test - acc:         0.798000 loss:        0.633145
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.839500 loss:        0.500777
Test - acc:         0.812500 loss:        0.592072
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.841460 loss:        0.489408
Test - acc:         0.775800 loss:        0.715434
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.842420 loss:        0.485177
Test - acc:         0.756700 loss:        0.787451
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.844140 loss:        0.474129
Test - acc:         0.772500 loss:        0.719780
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.845840 loss:        0.472026
Test - acc:         0.801600 loss:        0.624031
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.846440 loss:        0.472822
Test - acc:         0.757100 loss:        0.817688
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.847220 loss:        0.466885
Test - acc:         0.728500 loss:        0.873800
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.847500 loss:        0.466652
Test - acc:         0.795800 loss:        0.634631
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.848620 loss:        0.463489
Test - acc:         0.699200 loss:        0.943325
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.851400 loss:        0.455915
Test - acc:         0.741100 loss:        0.858843
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.849260 loss:        0.456064
Test - acc:         0.786000 loss:        0.640403
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.855700 loss:        0.444855
Test - acc:         0.739300 loss:        0.817388
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.853500 loss:        0.452992
Test - acc:         0.799700 loss:        0.631760
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.851440 loss:        0.454360
Test - acc:         0.780300 loss:        0.737216
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.853580 loss:        0.445910
Test - acc:         0.740300 loss:        0.874955
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.854820 loss:        0.444838
Test - acc:         0.772100 loss:        0.747720
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.855760 loss:        0.443159
Test - acc:         0.811600 loss:        0.569805
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.852540 loss:        0.449888
Test - acc:         0.805900 loss:        0.632524
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.857300 loss:        0.436671
Test - acc:         0.751500 loss:        0.796716
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.436074
Test - acc:         0.794600 loss:        0.683681
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.856520 loss:        0.436884
Test - acc:         0.823900 loss:        0.524038
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.858000 loss:        0.434121
Test - acc:         0.756400 loss:        0.856289
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.858580 loss:        0.433801
Test - acc:         0.820200 loss:        0.564814
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.858780 loss:        0.431331
Test - acc:         0.811800 loss:        0.617822
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.857300 loss:        0.434057
Test - acc:         0.789600 loss:        0.640790
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.857140 loss:        0.431569
Test - acc:         0.685100 loss:        1.054193
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.859820 loss:        0.428430
Test - acc:         0.789300 loss:        0.649584
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.859520 loss:        0.429059
Test - acc:         0.861900 loss:        0.435983
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.860780 loss:        0.429037
Test - acc:         0.793400 loss:        0.686644
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.860180 loss:        0.425613
Test - acc:         0.795600 loss:        0.666734
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.862300 loss:        0.422265
Test - acc:         0.803800 loss:        0.602964
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.860240 loss:        0.425305
Test - acc:         0.767300 loss:        0.752545
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.858780 loss:        0.429792
Test - acc:         0.751200 loss:        0.861425
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.862760 loss:        0.422019
Test - acc:         0.810900 loss:        0.577640
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.768500 loss:        0.712083
Test - acc:         0.783300 loss:        0.659066
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.810280 loss:        0.575810
Test - acc:         0.768300 loss:        0.718080
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.825480 loss:        0.526970
Test - acc:         0.773600 loss:        0.709967
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.831180 loss:        0.513564
Test - acc:         0.743800 loss:        0.829696
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.834120 loss:        0.499694
Test - acc:         0.779200 loss:        0.703796
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.832480 loss:        0.500645
Test - acc:         0.796800 loss:        0.620266
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
slurmstepd: error: _is_a_lwp: open() /proc/141904/status failed: No such file or directory
LR =  0.1
Train - acc:        0.840780 loss:        0.483571
Test - acc:         0.817600 loss:        0.565115
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.837720 loss:        0.485342
Test - acc:         0.796200 loss:        0.658802
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.841100 loss:        0.479048
Test - acc:         0.752000 loss:        0.797238
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.842420 loss:        0.475967
Test - acc:         0.801100 loss:        0.661426
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.843200 loss:        0.472090
Test - acc:         0.753400 loss:        0.838558
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.846420 loss:        0.468026
Test - acc:         0.803700 loss:        0.613079
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.845520 loss:        0.467782
Test - acc:         0.794000 loss:        0.680486
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.846360 loss:        0.464734
Test - acc:         0.791400 loss:        0.675155
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.845360 loss:        0.465740
Test - acc:         0.789300 loss:        0.655952
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.843460 loss:        0.468082
Test - acc:         0.789400 loss:        0.662113
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.845320 loss:        0.460891
Test - acc:         0.723100 loss:        0.952196
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.847040 loss:        0.463079
Test - acc:         0.767100 loss:        0.721431
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.848220 loss:        0.459754
Test - acc:         0.801600 loss:        0.624072
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.848560 loss:        0.456676
Test - acc:         0.817000 loss:        0.571448
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.847440 loss:        0.457482
Test - acc:         0.824200 loss:        0.513396
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.849960 loss:        0.450083
Test - acc:         0.785900 loss:        0.669258
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.848500 loss:        0.451897
Test - acc:         0.789700 loss:        0.670444
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.848400 loss:        0.460339
Test - acc:         0.816700 loss:        0.542698
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.848060 loss:        0.455141
Test - acc:         0.765900 loss:        0.743376
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.851680 loss:        0.448378
Test - acc:         0.814600 loss:        0.563680
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.851020 loss:        0.450553
Test - acc:         0.782700 loss:        0.672417
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.851120 loss:        0.449004
Test - acc:         0.798500 loss:        0.605511
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.850220 loss:        0.449274
Test - acc:         0.759900 loss:        0.787684
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.852480 loss:        0.446385
Test - acc:         0.740100 loss:        0.813236
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.849760 loss:        0.453257
Test - acc:         0.831700 loss:        0.497619
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.851840 loss:        0.448114
Test - acc:         0.754500 loss:        0.796923
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.851380 loss:        0.445321
Test - acc:         0.742000 loss:        0.800728
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.849860 loss:        0.452754
Test - acc:         0.749700 loss:        0.792838
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.851900 loss:        0.442521
Test - acc:         0.763900 loss:        0.795453
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.850520 loss:        0.448379
Test - acc:         0.768800 loss:        0.724914
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.852980 loss:        0.439807
Test - acc:         0.755000 loss:        0.834681
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.851640 loss:        0.449045
Test - acc:         0.794100 loss:        0.638967
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.854160 loss:        0.438156
Test - acc:         0.799800 loss:        0.674539
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.637220 loss:        1.039764
Test - acc:         0.742700 loss:        0.779564
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.767980 loss:        0.697738
Test - acc:         0.762300 loss:        0.717685
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.787060 loss:        0.633965
Test - acc:         0.637300 loss:        1.171155
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.798040 loss:        0.605264
Test - acc:         0.767600 loss:        0.709261
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.806020 loss:        0.582474
Test - acc:         0.783700 loss:        0.654745
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.808420 loss:        0.569050
Test - acc:         0.704700 loss:        0.916850
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.811500 loss:        0.557370
Test - acc:         0.748300 loss:        0.781411
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.815780 loss:        0.551275
Test - acc:         0.753400 loss:        0.765434
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.820620 loss:        0.536538
Test - acc:         0.778300 loss:        0.693383
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.822280 loss:        0.530071
Test - acc:         0.736200 loss:        0.793406
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.822920 loss:        0.529583
Test - acc:         0.764100 loss:        0.720972
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.820120 loss:        0.532134
Test - acc:         0.786600 loss:        0.630932
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.824460 loss:        0.519642
Test - acc:         0.779600 loss:        0.674525
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.821180 loss:        0.522873
Test - acc:         0.801400 loss:        0.601522
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.824300 loss:        0.516297
Test - acc:         0.771700 loss:        0.716592
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.829480 loss:        0.509259
Test - acc:         0.729500 loss:        0.894841
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.829400 loss:        0.506286
Test - acc:         0.761100 loss:        0.759469
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.830480 loss:        0.504397
Test - acc:         0.767200 loss:        0.751185
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.827460 loss:        0.510544
Test - acc:         0.764900 loss:        0.736105
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.830680 loss:        0.501717
Test - acc:         0.790700 loss:        0.642328
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.831660 loss:        0.499847
Test - acc:         0.791900 loss:        0.642430
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.830660 loss:        0.503029
Test - acc:         0.767200 loss:        0.705015
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.830680 loss:        0.499755
Test - acc:         0.723800 loss:        0.820366
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.831480 loss:        0.493818
Test - acc:         0.792100 loss:        0.619557
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.833320 loss:        0.498097
Test - acc:         0.742400 loss:        0.771283
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.835980 loss:        0.493077
Test - acc:         0.812800 loss:        0.557672
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.831180 loss:        0.499174
Test - acc:         0.811600 loss:        0.556746
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.832440 loss:        0.493735
Test - acc:         0.768600 loss:        0.718391
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.835440 loss:        0.485468
Test - acc:         0.790800 loss:        0.618111
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.832420 loss:        0.495356
Test - acc:         0.764700 loss:        0.682335
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.834660 loss:        0.484479
Test - acc:         0.809000 loss:        0.575617
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.834680 loss:        0.489820
Test - acc:         0.770200 loss:        0.734515
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.835300 loss:        0.485509
Test - acc:         0.777300 loss:        0.675434
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.892400 loss:        0.317289
Test - acc:         0.882100 loss:        0.345448
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.910920 loss:        0.267275
Test - acc:         0.891600 loss:        0.327053
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.916360 loss:        0.244161
Test - acc:         0.896200 loss:        0.312130
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.921980 loss:        0.228804
Test - acc:         0.894600 loss:        0.310158
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.926100 loss:        0.217149
Test - acc:         0.892400 loss:        0.326379
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.930000 loss:        0.203332
Test - acc:         0.895400 loss:        0.322613
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.559920 loss:        1.240733
Test - acc:         0.718200 loss:        0.818368
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.751420 loss:        0.720988
Test - acc:         0.753000 loss:        0.745976
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.792460 loss:        0.600357
Test - acc:         0.805300 loss:        0.571294
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.813220 loss:        0.544313
Test - acc:         0.815000 loss:        0.552868
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.827280 loss:        0.502611
Test - acc:         0.817800 loss:        0.524266
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.837400 loss:        0.470858
Test - acc:         0.827700 loss:        0.514040
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.846100 loss:        0.449137
Test - acc:         0.825300 loss:        0.515832
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.851980 loss:        0.431561
Test - acc:         0.833400 loss:        0.490562
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.857600 loss:        0.414535
Test - acc:         0.820400 loss:        0.527122
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.860900 loss:        0.404900
Test - acc:         0.845000 loss:        0.462045
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.864400 loss:        0.394053
Test - acc:         0.833500 loss:        0.493636
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.867360 loss:        0.386401
Test - acc:         0.836100 loss:        0.476668
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.869400 loss:        0.376551
Test - acc:         0.841200 loss:        0.471399
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.873760 loss:        0.364304
Test - acc:         0.844000 loss:        0.464955
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.873460 loss:        0.365891
Test - acc:         0.836800 loss:        0.482626
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.875920 loss:        0.358611
Test - acc:         0.838100 loss:        0.478205
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.879160 loss:        0.352316
Test - acc:         0.853400 loss:        0.438446
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.878540 loss:        0.349680
Test - acc:         0.846300 loss:        0.457375
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.879940 loss:        0.343971
Test - acc:         0.837900 loss:        0.488324
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.883380 loss:        0.335124
Test - acc:         0.840300 loss:        0.477572
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.883760 loss:        0.337318
Test - acc:         0.835600 loss:        0.490357
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.885500 loss:        0.332847
Test - acc:         0.849100 loss:        0.447858
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.884700 loss:        0.333891
Test - acc:         0.831400 loss:        0.501935
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.886080 loss:        0.328371
Test - acc:         0.835000 loss:        0.484934
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.888780 loss:        0.322165
Test - acc:         0.843800 loss:        0.479959
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.889480 loss:        0.317769
Test - acc:         0.852500 loss:        0.454245
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.890820 loss:        0.316928
Test - acc:         0.852900 loss:        0.446705
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.888360 loss:        0.316794
Test - acc:         0.844400 loss:        0.468570
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.890560 loss:        0.316091
Test - acc:         0.851400 loss:        0.452480
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.893880 loss:        0.305154
Test - acc:         0.849600 loss:        0.458945
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.893000 loss:        0.308380
Test - acc:         0.857100 loss:        0.433468
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.896040 loss:        0.302553
Test - acc:         0.848600 loss:        0.466986
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.894120 loss:        0.305240
Test - acc:         0.848700 loss:        0.457420
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.896760 loss:        0.297765
Test - acc:         0.843200 loss:        0.476988
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.897640 loss:        0.294347
Test - acc:         0.847200 loss:        0.454055
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.897300 loss:        0.295790
Test - acc:         0.859000 loss:        0.424657
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.898160 loss:        0.294535
Test - acc:         0.845900 loss:        0.469616
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.899800 loss:        0.288838
Test - acc:         0.849700 loss:        0.451459
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.899040 loss:        0.290043
Test - acc:         0.843000 loss:        0.480686
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.448100 loss:        1.504820
Test - acc:         0.542900 loss:        1.282451
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.604840 loss:        1.107533
Test - acc:         0.637800 loss:        1.054391
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.656420 loss:        0.969072
Test - acc:         0.661300 loss:        0.977829
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.694120 loss:        0.880118
Test - acc:         0.691800 loss:        0.894208
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.714780 loss:        0.817086
Test - acc:         0.682200 loss:        0.935907
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.730980 loss:        0.772003
Test - acc:         0.706400 loss:        0.858250
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.742620 loss:        0.740633
Test - acc:         0.731400 loss:        0.789602
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.752700 loss:        0.715654
Test - acc:         0.730300 loss:        0.783408
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.760340 loss:        0.687826
Test - acc:         0.730900 loss:        0.781137
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.766820 loss:        0.669749
Test - acc:         0.757400 loss:        0.700521
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.772900 loss:        0.654283
Test - acc:         0.725700 loss:        0.830601
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.777780 loss:        0.638584
Test - acc:         0.757400 loss:        0.717191
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.786060 loss:        0.619164
Test - acc:         0.769100 loss:        0.690009
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.791280 loss:        0.605341
Test - acc:         0.761300 loss:        0.696988
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.790360 loss:        0.600531
Test - acc:         0.768500 loss:        0.678471
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.797060 loss:        0.588116
Test - acc:         0.770000 loss:        0.676476
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.799000 loss:        0.578892
Test - acc:         0.771300 loss:        0.676663
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.801900 loss:        0.564949
Test - acc:         0.787400 loss:        0.608887
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.803760 loss:        0.564289
Test - acc:         0.770300 loss:        0.672116
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.806960 loss:        0.555736
Test - acc:         0.780700 loss:        0.639342
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.809940 loss:        0.546380
Test - acc:         0.766400 loss:        0.676947
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.813460 loss:        0.539154
Test - acc:         0.792900 loss:        0.604262
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.812500 loss:        0.538447
Test - acc:         0.795400 loss:        0.592539
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.818320 loss:        0.526476
Test - acc:         0.792300 loss:        0.611818
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.817280 loss:        0.525194
Test - acc:         0.799500 loss:        0.582030
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.820480 loss:        0.514724
Test - acc:         0.788300 loss:        0.615484
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.820900 loss:        0.516371
Test - acc:         0.763700 loss:        0.710805
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.821960 loss:        0.515083
Test - acc:         0.797900 loss:        0.583571
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.823840 loss:        0.508050
Test - acc:         0.781600 loss:        0.636009
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.824360 loss:        0.504875
Test - acc:         0.803000 loss:        0.584758
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.828180 loss:        0.498789
Test - acc:         0.801700 loss:        0.582514
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.830200 loss:        0.493583
Test - acc:         0.802000 loss:        0.584920
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.829540 loss:        0.491351
Test - acc:         0.792600 loss:        0.625252
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.829980 loss:        0.490477
Test - acc:         0.801400 loss:        0.582796
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.831520 loss:        0.488471
Test - acc:         0.800900 loss:        0.589815
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.831160 loss:        0.482665
Test - acc:         0.801700 loss:        0.575957
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.834660 loss:        0.475089
Test - acc:         0.800600 loss:        0.593753
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.836060 loss:        0.475103
Test - acc:         0.777000 loss:        0.660184
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.836260 loss:        0.472957
Test - acc:         0.814000 loss:        0.548728
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.377140 loss:        1.702808
Test - acc:         0.428100 loss:        1.668619
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.531300 loss:        1.310269
Test - acc:         0.522300 loss:        1.351574
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.584160 loss:        1.167323
Test - acc:         0.609100 loss:        1.093473
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.614140 loss:        1.085642
Test - acc:         0.620400 loss:        1.072598
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.635060 loss:        1.027279
Test - acc:         0.638600 loss:        1.010229
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.648380 loss:        0.988059
Test - acc:         0.634700 loss:        1.035792
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.666240 loss:        0.947899
Test - acc:         0.654500 loss:        1.003007
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.678240 loss:        0.916349
Test - acc:         0.664900 loss:        0.950625
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.684220 loss:        0.893537
Test - acc:         0.667100 loss:        0.943610
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.692300 loss:        0.868505
Test - acc:         0.694500 loss:        0.876959
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.701260 loss:        0.852223
Test - acc:         0.701200 loss:        0.857450
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.709240 loss:        0.833061
Test - acc:         0.702600 loss:        0.852343
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.712340 loss:        0.819273
Test - acc:         0.696300 loss:        0.874529
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.717500 loss:        0.800829
Test - acc:         0.710500 loss:        0.845718
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.721640 loss:        0.792258
Test - acc:         0.694600 loss:        0.888547
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.730400 loss:        0.773349
Test - acc:         0.707000 loss:        0.837362
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.756600 loss:        0.699679
Test - acc:         0.754900 loss:        0.701175
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.764860 loss:        0.675959
Test - acc:         0.761600 loss:        0.684540
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.769180 loss:        0.663647
Test - acc:         0.762300 loss:        0.682126
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.768220 loss:        0.658302
Test - acc:         0.765100 loss:        0.675656
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.771680 loss:        0.649095
Test - acc:         0.763900 loss:        0.672269
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.772480 loss:        0.649398
Test - acc:         0.767300 loss:        0.664881
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.775560 loss:        0.637472
Test - acc:         0.766700 loss:        0.665445
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.775200 loss:        0.639192
Test - acc:         0.770300 loss:        0.656405
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.778440 loss:        0.632726
Test - acc:         0.770200 loss:        0.656673
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.780280 loss:        0.629034
Test - acc:         0.771300 loss:        0.654016
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.781880 loss:        0.627759
Test - acc:         0.770900 loss:        0.657598
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.781240 loss:        0.625675
Test - acc:         0.771100 loss:        0.652466
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.783680 loss:        0.621271
Test - acc:         0.770700 loss:        0.653845
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.785780 loss:        0.617323
Test - acc:         0.774300 loss:        0.644602
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.784660 loss:        0.615844
Test - acc:         0.775300 loss:        0.647905
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.787640 loss:        0.610622
Test - acc:         0.772000 loss:        0.647025
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.789480 loss:        0.603720
Test - acc:         0.770900 loss:        0.655091
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.788400 loss:        0.605801
Test - acc:         0.774700 loss:        0.643613
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.787960 loss:        0.603031
Test - acc:         0.774900 loss:        0.647269
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.788820 loss:        0.602133
Test - acc:         0.775700 loss:        0.651727
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.790760 loss:        0.596107
Test - acc:         0.773500 loss:        0.646525
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.791080 loss:        0.598985
Test - acc:         0.778700 loss:        0.638734
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.790500 loss:        0.594789
Test - acc:         0.781100 loss:        0.637519
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.154480 loss:        2.444881
Test - acc:         0.229900 loss:        2.167685
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.250480 loss:        2.078326
Test - acc:         0.245000 loss:        2.096103
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.272820 loss:        1.990237
Test - acc:         0.280800 loss:        2.024414
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.310680 loss:        1.930353
Test - acc:         0.290100 loss:        1.974718
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.341820 loss:        1.850336
Test - acc:         0.216900 loss:        2.227391
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.366340 loss:        1.792875
Test - acc:         0.100000 loss:        4.369962
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.379620 loss:        1.749068
Test - acc:         0.211700 loss:        2.460535
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.391540 loss:        1.706682
Test - acc:         0.255100 loss:        2.117888
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.397020 loss:        1.672953
Test - acc:         0.291300 loss:        1.961010
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.405680 loss:        1.635416
Test - acc:         0.100000 loss:        4.747752
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.413880 loss:        1.600716
Test - acc:         0.100000 loss:        4.732663
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.421960 loss:        1.571087
Test - acc:         0.405700 loss:        1.621700
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.431520 loss:        1.548492
Test - acc:         0.267700 loss:        2.095825
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.435260 loss:        1.528865
Test - acc:         0.369400 loss:        1.695578
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.447380 loss:        1.506964
Test - acc:         0.124100 loss:        3.431633
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.480220 loss:        1.470881
Test - acc:         0.255000 loss:        2.205665
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.500020 loss:        1.418049
Test - acc:         0.319800 loss:        1.974331
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.508100 loss:        1.391320
Test - acc:         0.236300 loss:        2.326425
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.514420 loss:        1.372794
Test - acc:         0.415700 loss:        1.665763
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.522180 loss:        1.352539
Test - acc:         0.481800 loss:        1.492105
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.528160 loss:        1.335428
Test - acc:         0.208300 loss:        2.605133
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.532000 loss:        1.327216
Test - acc:         0.498100 loss:        1.426154
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.539560 loss:        1.305796
Test - acc:         0.142600 loss:        3.641972
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.541080 loss:        1.294338
Test - acc:         0.416700 loss:        1.725862
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.543920 loss:        1.283329
Test - acc:         0.365800 loss:        1.923553
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.550840 loss:        1.269202
Test - acc:         0.365700 loss:        1.802824
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.559240 loss:        1.251717
Test - acc:         0.550000 loss:        1.281751
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.559640 loss:        1.239743
Test - acc:         0.360500 loss:        1.902249
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.564920 loss:        1.229821
Test - acc:         0.199600 loss:        3.161651
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.568820 loss:        1.215735
Test - acc:         0.176700 loss:        3.937356
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.568680 loss:        1.209574
Test - acc:         0.317100 loss:        2.160093
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.575380 loss:        1.199091
Test - acc:         0.510700 loss:        1.385618
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.576120 loss:        1.188921
Test - acc:         0.172900 loss:        3.661227
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.582260 loss:        1.179384
Test - acc:         0.211200 loss:        2.860516
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.584720 loss:        1.170097
Test - acc:         0.468800 loss:        1.605314
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.588240 loss:        1.162615
Test - acc:         0.501700 loss:        1.440659
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.591020 loss:        1.151327
Test - acc:         0.224400 loss:        3.218306
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.590960 loss:        1.148985
Test - acc:         0.345600 loss:        2.095829
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.597360 loss:        1.132439
Test - acc:         0.506900 loss:        1.460911
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.100940 loss:        2.605936
Test - acc:         0.100000 loss:        2.433632
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.379654
Test - acc:         0.100000 loss:        2.344722
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.120360 loss:        2.317100
Test - acc:         0.159600 loss:        2.301439
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.156160 loss:        2.273773
Test - acc:         0.100000 loss:        2.475772
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.161700 loss:        2.255605
Test - acc:         0.101200 loss:        2.389558
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.164060 loss:        2.245402
Test - acc:         0.149500 loss:        2.263739
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.166720 loss:        2.239378
Test - acc:         0.161800 loss:        2.238701
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.159820 loss:        2.234461
Test - acc:         0.157300 loss:        2.228965
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.156820 loss:        2.228704
Test - acc:         0.171000 loss:        2.240008
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.158960 loss:        2.226241
Test - acc:         0.157500 loss:        2.237050
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.156920 loss:        2.221035
Test - acc:         0.148300 loss:        2.303690
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.162160 loss:        2.218181
Test - acc:         0.109300 loss:        2.326562
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.151580 loss:        2.214841
Test - acc:         0.117500 loss:        2.267369
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.157560 loss:        2.211593
Test - acc:         0.163600 loss:        2.260210
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.157180 loss:        2.209634
Test - acc:         0.143600 loss:        2.232759
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.162240 loss:        2.206470
Test - acc:         0.115600 loss:        2.295824
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.159300 loss:        2.204923
Test - acc:         0.145300 loss:        2.241260
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.159020 loss:        2.203374
Test - acc:         0.147600 loss:        2.193811
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.159760 loss:        2.200507
Test - acc:         0.104800 loss:        2.366344
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.161420 loss:        2.196519
Test - acc:         0.113400 loss:        2.463336
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.161020 loss:        2.194256
Test - acc:         0.118800 loss:        2.286960
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.164380 loss:        2.193466
Test - acc:         0.112900 loss:        2.307460
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.160400 loss:        2.193008
Test - acc:         0.105800 loss:        2.407236
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.158580 loss:        2.190168
Test - acc:         0.146900 loss:        2.205578
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.156600 loss:        2.191527
Test - acc:         0.149000 loss:        2.200932
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.160900 loss:        2.186973
Test - acc:         0.131700 loss:        2.441385
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.166160 loss:        2.184985
Test - acc:         0.164200 loss:        2.189651
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.164660 loss:        2.186044
Test - acc:         0.132500 loss:        2.225028
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.161120 loss:        2.182684
Test - acc:         0.138000 loss:        2.282762
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.162900 loss:        2.182868
Test - acc:         0.119800 loss:        2.282251
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.166880 loss:        2.181791
Test - acc:         0.162500 loss:        2.174706
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.166640 loss:        2.180537
Test - acc:         0.102900 loss:        2.509575
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.162660 loss:        2.177789
Test - acc:         0.103300 loss:        2.549387
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.161420 loss:        2.177214
Test - acc:         0.106700 loss:        2.454080
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.163940 loss:        2.175980
Test - acc:         0.160400 loss:        2.185720
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.161760 loss:        2.175723
Test - acc:         0.127800 loss:        2.442056
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.164540 loss:        2.175304
Test - acc:         0.148000 loss:        2.213458
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.164700 loss:        2.174764
Test - acc:         0.160100 loss:        2.201118
Sparsity :          0.9961
Wdecay :        0.000500
