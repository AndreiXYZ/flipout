Running --prune_criterion global_magnitude --seed 42 --prune_freq 39 --prune_rate 0.5 --comment=vgg19_crit=global_magnitude_pf=39_seed=42 --save_model=pre-finetune/vgg19_global_magnitude_pf39_s42
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "global_magnitude",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_global_magnitude_pf39_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.106580 loss:        2.520409
Test - acc:         0.107600 loss:        2.294274
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.121620 loss:        2.275786
Test - acc:         0.128800 loss:        2.258387
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.167360 loss:        2.134492
Test - acc:         0.208100 loss:        1.905877
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.258380 loss:        1.859667
Test - acc:         0.287600 loss:        1.767105
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.322920 loss:        1.699367
Test - acc:         0.410600 loss:        1.501470
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.445160 loss:        1.456379
Test - acc:         0.508200 loss:        1.356367
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.546300 loss:        1.241191
Test - acc:         0.498200 loss:        1.477734
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.604540 loss:        1.101290
Test - acc:         0.483800 loss:        1.743679
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.650880 loss:        0.993529
Test - acc:         0.619700 loss:        1.166982
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.703160 loss:        0.882477
Test - acc:         0.659900 loss:        1.012752
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.733140 loss:        0.808711
Test - acc:         0.648500 loss:        1.121658
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.752060 loss:        0.763825
Test - acc:         0.697100 loss:        0.950123
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.763380 loss:        0.735478
Test - acc:         0.647600 loss:        1.210369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.773560 loss:        0.706368
Test - acc:         0.717500 loss:        0.901679
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.780940 loss:        0.681272
Test - acc:         0.682800 loss:        1.025874
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.791420 loss:        0.649883
Test - acc:         0.736800 loss:        0.872370
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.796700 loss:        0.634338
Test - acc:         0.700800 loss:        0.932917
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.801840 loss:        0.615529
Test - acc:         0.797400 loss:        0.635734
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.804520 loss:        0.605495
Test - acc:         0.786700 loss:        0.672975
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.811300 loss:        0.586166
Test - acc:         0.692500 loss:        1.006768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.818600 loss:        0.574597
Test - acc:         0.764600 loss:        0.752744
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.819720 loss:        0.564470
Test - acc:         0.732800 loss:        0.858983
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.822020 loss:        0.561122
Test - acc:         0.767600 loss:        0.719709
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.821520 loss:        0.558114
Test - acc:         0.744500 loss:        0.842082
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.827620 loss:        0.542572
Test - acc:         0.761300 loss:        0.789428
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.830100 loss:        0.535151
Test - acc:         0.741300 loss:        0.871493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.828600 loss:        0.532575
Test - acc:         0.787500 loss:        0.687972
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.833340 loss:        0.523183
Test - acc:         0.781500 loss:        0.714158
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.832740 loss:        0.525514
Test - acc:         0.761200 loss:        0.775278
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.831200 loss:        0.525716
Test - acc:         0.754700 loss:        0.881011
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.838100 loss:        0.512746
Test - acc:         0.776500 loss:        0.724635
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.839420 loss:        0.504651
Test - acc:         0.760400 loss:        0.863967
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.838440 loss:        0.504414
Test - acc:         0.785700 loss:        0.718698
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.838460 loss:        0.503714
Test - acc:         0.815300 loss:        0.567434
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.840220 loss:        0.497685
Test - acc:         0.662800 loss:        1.328712
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.842920 loss:        0.493377
Test - acc:         0.732000 loss:        0.900541
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.841740 loss:        0.493601
Test - acc:         0.813600 loss:        0.592560
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.841380 loss:        0.496394
Test - acc:         0.789100 loss:        0.683082
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.846480 loss:        0.485149
Test - acc:         0.747100 loss:        0.824019
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.845720 loss:        0.484925
Test - acc:         0.775900 loss:        0.730700
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.846140 loss:        0.483482
Test - acc:         0.774100 loss:        0.734032
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.846320 loss:        0.477160
Test - acc:         0.768900 loss:        0.774520
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.848800 loss:        0.473170
Test - acc:         0.779300 loss:        0.711399
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.847980 loss:        0.467543
Test - acc:         0.782300 loss:        0.688237
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.851100 loss:        0.464567
Test - acc:         0.787800 loss:        0.683402
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.848660 loss:        0.470969
Test - acc:         0.763700 loss:        0.790470
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.851540 loss:        0.465318
Test - acc:         0.824000 loss:        0.548599
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.849820 loss:        0.468125
Test - acc:         0.815800 loss:        0.583107
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.851560 loss:        0.463714
Test - acc:         0.737500 loss:        0.908745
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.847460 loss:        0.474401
Test - acc:         0.769800 loss:        0.805283
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.854920 loss:        0.456229
Test - acc:         0.777600 loss:        0.704827
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.853340 loss:        0.453599
Test - acc:         0.679200 loss:        1.107712
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.852900 loss:        0.457863
Test - acc:         0.785100 loss:        0.752962
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.854140 loss:        0.452291
Test - acc:         0.739200 loss:        0.831337
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.852220 loss:        0.461698
Test - acc:         0.717900 loss:        0.976825
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.853220 loss:        0.461739
Test - acc:         0.810300 loss:        0.608315
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.855820 loss:        0.443066
Test - acc:         0.743200 loss:        0.857049
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.856500 loss:        0.450512
Test - acc:         0.715900 loss:        1.050227
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.855040 loss:        0.452331
Test - acc:         0.816100 loss:        0.629649
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.856540 loss:        0.449209
Test - acc:         0.773200 loss:        0.754343
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.858640 loss:        0.441071
Test - acc:         0.810300 loss:        0.608598
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.857580 loss:        0.449602
Test - acc:         0.802800 loss:        0.624989
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.857280 loss:        0.445982
Test - acc:         0.784300 loss:        0.720142
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.860880 loss:        0.439689
Test - acc:         0.824100 loss:        0.529842
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.857340 loss:        0.445407
Test - acc:         0.775600 loss:        0.722940
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.858880 loss:        0.439097
Test - acc:         0.742700 loss:        0.865283
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.860760 loss:        0.435329
Test - acc:         0.795000 loss:        0.627228
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.846980 loss:        0.480166
Test - acc:         0.791000 loss:        0.683072
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.861400 loss:        0.435621
Test - acc:         0.842300 loss:        0.503677
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.859520 loss:        0.439137
Test - acc:         0.805500 loss:        0.609690
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.858660 loss:        0.439242
Test - acc:         0.813500 loss:        0.576117
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.859760 loss:        0.436903
Test - acc:         0.736700 loss:        0.900541
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.860680 loss:        0.434896
Test - acc:         0.800400 loss:        0.627701
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.859560 loss:        0.441664
Test - acc:         0.791800 loss:        0.690658
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.860760 loss:        0.426967
Test - acc:         0.810400 loss:        0.612291
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.860420 loss:        0.431766
Test - acc:         0.792100 loss:        0.653750
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.862860 loss:        0.432125
Test - acc:         0.714500 loss:        0.936172
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.858880 loss:        0.440001
Test - acc:         0.820700 loss:        0.557697
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.864600 loss:        0.422826
Test - acc:         0.814300 loss:        0.565616
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.860360 loss:        0.436869
Test - acc:         0.759300 loss:        0.803187
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.863540 loss:        0.423209
Test - acc:         0.780700 loss:        0.734517
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.863800 loss:        0.423415
Test - acc:         0.780500 loss:        0.769537
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.861080 loss:        0.433655
Test - acc:         0.760200 loss:        0.797578
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.864480 loss:        0.422414
Test - acc:         0.748000 loss:        0.846771
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.861760 loss:        0.426779
Test - acc:         0.826300 loss:        0.578435
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.863020 loss:        0.423508
Test - acc:         0.775900 loss:        0.693724
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.865140 loss:        0.420875
Test - acc:         0.751000 loss:        0.819144
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.863920 loss:        0.419027
Test - acc:         0.739000 loss:        0.924323
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.863840 loss:        0.427074
Test - acc:         0.838900 loss:        0.506228
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.865360 loss:        0.419673
Test - acc:         0.790000 loss:        0.675896
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.863880 loss:        0.420267
Test - acc:         0.823000 loss:        0.555100
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.859640 loss:        0.434144
Test - acc:         0.739300 loss:        0.871285
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.847220 loss:        0.487259
Test - acc:         0.752300 loss:        0.777387
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.854840 loss:        0.462914
Test - acc:         0.733800 loss:        0.946877
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.860060 loss:        0.441501
Test - acc:         0.829800 loss:        0.533171
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.863660 loss:        0.432875
Test - acc:         0.785500 loss:        0.769021
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.860220 loss:        0.433233
Test - acc:         0.776100 loss:        0.727269
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.865600 loss:        0.420928
Test - acc:         0.838700 loss:        0.505070
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.865060 loss:        0.422409
Test - acc:         0.765900 loss:        0.800296
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.866540 loss:        0.411474
Test - acc:         0.808800 loss:        0.601589
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.868120 loss:        0.409083
Test - acc:         0.826100 loss:        0.537310
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.866000 loss:        0.415196
Test - acc:         0.760800 loss:        0.752235
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.863320 loss:        0.420705
Test - acc:         0.828500 loss:        0.564334
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.866740 loss:        0.410421
Test - acc:         0.803200 loss:        0.630255
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.869220 loss:        0.403696
Test - acc:         0.779400 loss:        0.750948
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.864820 loss:        0.412430
Test - acc:         0.745300 loss:        0.850261
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.865560 loss:        0.410762
Test - acc:         0.811000 loss:        0.618376
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.870960 loss:        0.396982
Test - acc:         0.807700 loss:        0.663041
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.866400 loss:        0.406238
Test - acc:         0.813600 loss:        0.595317
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.866600 loss:        0.404212
Test - acc:         0.745500 loss:        0.772446
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.868360 loss:        0.409150
Test - acc:         0.705000 loss:        1.155329
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.870980 loss:        0.398541
Test - acc:         0.832000 loss:        0.549377
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.869160 loss:        0.401917
Test - acc:         0.818900 loss:        0.550888
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.869340 loss:        0.407698
Test - acc:         0.838000 loss:        0.515878
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.867040 loss:        0.408984
Test - acc:         0.811200 loss:        0.609302
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.871340 loss:        0.399962
Test - acc:         0.795900 loss:        0.685833
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.865460 loss:        0.409792
Test - acc:         0.822000 loss:        0.609294
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.877200 loss:        0.382835
Test - acc:         0.773800 loss:        0.741203
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.876940 loss:        0.379278
Test - acc:         0.807500 loss:        0.624843
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.875080 loss:        0.386558
Test - acc:         0.841300 loss:        0.495269
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.871640 loss:        0.394246
Test - acc:         0.820200 loss:        0.601568
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.872100 loss:        0.393536
Test - acc:         0.787000 loss:        0.680778
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.874400 loss:        0.388255
Test - acc:         0.787200 loss:        0.683324
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.875700 loss:        0.384378
Test - acc:         0.738000 loss:        0.893925
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.874440 loss:        0.383683
Test - acc:         0.777200 loss:        0.779252
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.875740 loss:        0.384337
Test - acc:         0.819300 loss:        0.602238
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.878740 loss:        0.376887
Test - acc:         0.849600 loss:        0.505421
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.874480 loss:        0.380983
Test - acc:         0.782500 loss:        0.721223
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.873680 loss:        0.386674
Test - acc:         0.812900 loss:        0.597333
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.875760 loss:        0.383755
Test - acc:         0.788800 loss:        0.730975
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.877000 loss:        0.381939
Test - acc:         0.787400 loss:        0.654187
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.875160 loss:        0.382404
Test - acc:         0.815200 loss:        0.640459
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.872360 loss:        0.388806
Test - acc:         0.683000 loss:        1.081156
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.874800 loss:        0.387789
Test - acc:         0.812000 loss:        0.580085
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.874740 loss:        0.382276
Test - acc:         0.786500 loss:        0.697410
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.877160 loss:        0.376278
Test - acc:         0.794700 loss:        0.650663
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.875160 loss:        0.383531
Test - acc:         0.819800 loss:        0.600598
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.874120 loss:        0.385162
Test - acc:         0.819900 loss:        0.581222
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.874500 loss:        0.383752
Test - acc:         0.812900 loss:        0.580165
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.874380 loss:        0.387139
Test - acc:         0.833500 loss:        0.551179
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.877020 loss:        0.376002
Test - acc:         0.817500 loss:        0.582530
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.873340 loss:        0.388393
Test - acc:         0.774200 loss:        0.819928
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.875340 loss:        0.384115
Test - acc:         0.836500 loss:        0.515220
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.874480 loss:        0.382098
Test - acc:         0.847400 loss:        0.488102
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.877260 loss:        0.377417
Test - acc:         0.780000 loss:        0.744585
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.877160 loss:        0.375864
Test - acc:         0.796200 loss:        0.651625
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.874480 loss:        0.386333
Test - acc:         0.794600 loss:        0.725167
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.873660 loss:        0.385132
Test - acc:         0.855200 loss:        0.454653
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.877000 loss:        0.376983
Test - acc:         0.844400 loss:        0.483604
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.874760 loss:        0.384124
Test - acc:         0.837200 loss:        0.519243
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.933880 loss:        0.202872
Test - acc:         0.915400 loss:        0.261892
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.950440 loss:        0.151097
Test - acc:         0.922200 loss:        0.249550
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.956360 loss:        0.130398
Test - acc:         0.922600 loss:        0.241650
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.961600 loss:        0.115715
Test - acc:         0.920700 loss:        0.249561
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.966660 loss:        0.100219
Test - acc:         0.922200 loss:        0.253932
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.968860 loss:        0.095313
Test - acc:         0.924800 loss:        0.256767
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.972060 loss:        0.086135
Test - acc:         0.924000 loss:        0.258712
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.973720 loss:        0.081059
Test - acc:         0.924900 loss:        0.261832
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.976160 loss:        0.071924
Test - acc:         0.922500 loss:        0.267496
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.977440 loss:        0.067855
Test - acc:         0.924200 loss:        0.269384
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.978900 loss:        0.063888
Test - acc:         0.924900 loss:        0.279341
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.979620 loss:        0.060057
Test - acc:         0.923600 loss:        0.281215
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.980420 loss:        0.057835
Test - acc:         0.918900 loss:        0.291403
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.057510
Test - acc:         0.921900 loss:        0.290565
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.981920 loss:        0.054212
Test - acc:         0.922700 loss:        0.296134
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.982980 loss:        0.050691
Test - acc:         0.921100 loss:        0.304778
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.982680 loss:        0.051198
Test - acc:         0.920300 loss:        0.298779
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.982720 loss:        0.051806
Test - acc:         0.915200 loss:        0.327727
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.983720 loss:        0.049207
Test - acc:         0.920900 loss:        0.315815
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.983240 loss:        0.050435
Test - acc:         0.917000 loss:        0.308244
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.984320 loss:        0.049181
Test - acc:         0.918000 loss:        0.316078
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.055274
Test - acc:         0.918500 loss:        0.312530
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.983500 loss:        0.049744
Test - acc:         0.917600 loss:        0.325239
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.981840 loss:        0.054275
Test - acc:         0.920900 loss:        0.299533
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.983780 loss:        0.048639
Test - acc:         0.919100 loss:        0.310181
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.981580 loss:        0.054598
Test - acc:         0.917900 loss:        0.312009
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.982080 loss:        0.054880
Test - acc:         0.912000 loss:        0.339935
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.982140 loss:        0.053811
Test - acc:         0.915600 loss:        0.324948
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.982080 loss:        0.054486
Test - acc:         0.918300 loss:        0.313177
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.982960 loss:        0.050368
Test - acc:         0.915600 loss:        0.325505
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.981080 loss:        0.057035
Test - acc:         0.914000 loss:        0.335553
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.056262
Test - acc:         0.920100 loss:        0.300868
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.983400 loss:        0.051470
Test - acc:         0.917700 loss:        0.321746
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.980680 loss:        0.057562
Test - acc:         0.913700 loss:        0.326811
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.979720 loss:        0.061277
Test - acc:         0.913600 loss:        0.332576
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.981740 loss:        0.054445
Test - acc:         0.912900 loss:        0.336419
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.981520 loss:        0.054050
Test - acc:         0.911000 loss:        0.341483
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.979740 loss:        0.061495
Test - acc:         0.909800 loss:        0.337784
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.978840 loss:        0.062902
Test - acc:         0.907000 loss:        0.353920
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.979120 loss:        0.060539
Test - acc:         0.912700 loss:        0.336081
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.980460 loss:        0.060136
Test - acc:         0.915100 loss:        0.335682
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.979660 loss:        0.060888
Test - acc:         0.899300 loss:        0.382186
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.978660 loss:        0.063832
Test - acc:         0.906800 loss:        0.354963
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.979780 loss:        0.061082
Test - acc:         0.912200 loss:        0.320218
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.978260 loss:        0.064429
Test - acc:         0.916600 loss:        0.304306
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.980820 loss:        0.058590
Test - acc:         0.910300 loss:        0.339418
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.981580 loss:        0.057065
Test - acc:         0.915500 loss:        0.321559
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.983840 loss:        0.048546
Test - acc:         0.908300 loss:        0.351751
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.985360 loss:        0.043865
Test - acc:         0.918700 loss:        0.327651
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.983160 loss:        0.050976
Test - acc:         0.912400 loss:        0.333634
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.983640 loss:        0.049979
Test - acc:         0.908700 loss:        0.360648
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.984540 loss:        0.047789
Test - acc:         0.913100 loss:        0.354142
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.982960 loss:        0.050794
Test - acc:         0.915800 loss:        0.337176
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.983940 loss:        0.050330
Test - acc:         0.912700 loss:        0.339934
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.982540 loss:        0.051326
Test - acc:         0.919900 loss:        0.325566
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.982900 loss:        0.052317
Test - acc:         0.907300 loss:        0.345120
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.982360 loss:        0.053666
Test - acc:         0.910500 loss:        0.349168
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.981440 loss:        0.054854
Test - acc:         0.902500 loss:        0.398248
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.982940 loss:        0.051910
Test - acc:         0.905500 loss:        0.366001
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.981680 loss:        0.055175
Test - acc:         0.913700 loss:        0.334300
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.982900 loss:        0.053877
Test - acc:         0.916800 loss:        0.318251
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.983160 loss:        0.051371
Test - acc:         0.905700 loss:        0.364705
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.983060 loss:        0.051906
Test - acc:         0.910600 loss:        0.349056
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.982920 loss:        0.051124
Test - acc:         0.905600 loss:        0.381247
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.980100 loss:        0.059335
Test - acc:         0.903400 loss:        0.373455
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.982300 loss:        0.053100
Test - acc:         0.895600 loss:        0.423309
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.980640 loss:        0.057335
Test - acc:         0.900200 loss:        0.402675
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.981160 loss:        0.058117
Test - acc:         0.913000 loss:        0.332339
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.981700 loss:        0.055765
Test - acc:         0.906500 loss:        0.353256
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.984000 loss:        0.048561
Test - acc:         0.907400 loss:        0.351903
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.981700 loss:        0.054882
Test - acc:         0.907200 loss:        0.386199
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.981220 loss:        0.056988
Test - acc:         0.909600 loss:        0.369348
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.980820 loss:        0.058269
Test - acc:         0.912400 loss:        0.342929
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.980180 loss:        0.057378
Test - acc:         0.910900 loss:        0.355030
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.983300 loss:        0.050906
Test - acc:         0.903100 loss:        0.380508
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.983100 loss:        0.049517
Test - acc:         0.904700 loss:        0.398201
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.980460 loss:        0.058924
Test - acc:         0.912100 loss:        0.349272
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.983720 loss:        0.051060
Test - acc:         0.907400 loss:        0.353368
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.983140 loss:        0.050853
Test - acc:         0.904200 loss:        0.369739
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.981000 loss:        0.057404
Test - acc:         0.904700 loss:        0.373121
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.060307
Test - acc:         0.898100 loss:        0.404086
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.981080 loss:        0.057514
Test - acc:         0.912900 loss:        0.362600
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.981520 loss:        0.057382
Test - acc:         0.903900 loss:        0.388567
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.980660 loss:        0.058037
Test - acc:         0.910500 loss:        0.347970
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.974980 loss:        0.075018
Test - acc:         0.907900 loss:        0.351583
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.060335
Test - acc:         0.907900 loss:        0.357332
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.980280 loss:        0.059683
Test - acc:         0.910600 loss:        0.339333
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.983200 loss:        0.050635
Test - acc:         0.903900 loss:        0.377088
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.981980 loss:        0.053273
Test - acc:         0.905900 loss:        0.382286
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.980940 loss:        0.055736
Test - acc:         0.906200 loss:        0.373865
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.980820 loss:        0.058781
Test - acc:         0.914100 loss:        0.331902
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.984180 loss:        0.047897
Test - acc:         0.910000 loss:        0.350229
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.055920
Test - acc:         0.895000 loss:        0.444435
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.982920 loss:        0.051119
Test - acc:         0.917600 loss:        0.317371
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.983480 loss:        0.052186
Test - acc:         0.911400 loss:        0.350690
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.982320 loss:        0.052670
Test - acc:         0.912800 loss:        0.340674
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.980780 loss:        0.057627
Test - acc:         0.898400 loss:        0.407238
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.983320 loss:        0.052119
Test - acc:         0.912500 loss:        0.324530
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.982160 loss:        0.052381
Test - acc:         0.906700 loss:        0.373226
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.982400 loss:        0.052526
Test - acc:         0.911000 loss:        0.348646
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.991780 loss:        0.026230
Test - acc:         0.925800 loss:        0.287777
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.995200 loss:        0.017481
Test - acc:         0.926300 loss:        0.285286
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.995620 loss:        0.014636
Test - acc:         0.928400 loss:        0.282992
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.997060 loss:        0.011862
Test - acc:         0.928600 loss:        0.284437
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.997100 loss:        0.011188
Test - acc:         0.928700 loss:        0.283094
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.997260 loss:        0.010535
Test - acc:         0.929000 loss:        0.288523
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.997560 loss:        0.009592
Test - acc:         0.928900 loss:        0.286850
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.997660 loss:        0.009270
Test - acc:         0.929300 loss:        0.286605
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.997960 loss:        0.007928
Test - acc:         0.930300 loss:        0.291587
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.998060 loss:        0.007677
Test - acc:         0.930700 loss:        0.287029
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.998420 loss:        0.006842
Test - acc:         0.931900 loss:        0.287696
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.998540 loss:        0.007154
Test - acc:         0.931900 loss:        0.287855
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.998360 loss:        0.006553
Test - acc:         0.930300 loss:        0.291551
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.998220 loss:        0.007118
Test - acc:         0.930200 loss:        0.291111
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.998660 loss:        0.006001
Test - acc:         0.932600 loss:        0.292598
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.998620 loss:        0.005762
Test - acc:         0.930100 loss:        0.295028
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.005606
Test - acc:         0.930900 loss:        0.291526
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.998720 loss:        0.005551
Test - acc:         0.933300 loss:        0.291913
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.998860 loss:        0.005530
Test - acc:         0.933100 loss:        0.292445
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.004852
Test - acc:         0.933200 loss:        0.292853
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999160 loss:        0.004598
Test - acc:         0.932300 loss:        0.295388
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.998760 loss:        0.005440
Test - acc:         0.932600 loss:        0.295556
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999120 loss:        0.004478
Test - acc:         0.932800 loss:        0.292499
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.978280 loss:        0.066795
Test - acc:         0.916500 loss:        0.341029
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.984520 loss:        0.047364
Test - acc:         0.917800 loss:        0.330244
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.986320 loss:        0.040522
Test - acc:         0.916200 loss:        0.326587
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.986900 loss:        0.038009
Test - acc:         0.918800 loss:        0.327476
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.988320 loss:        0.035562
Test - acc:         0.921400 loss:        0.327319
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.990440 loss:        0.029951
Test - acc:         0.922000 loss:        0.320141
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.991560 loss:        0.028109
Test - acc:         0.923800 loss:        0.319841
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.991580 loss:        0.026087
Test - acc:         0.922400 loss:        0.321131
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.991520 loss:        0.026834
Test - acc:         0.923400 loss:        0.321624
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.992100 loss:        0.025099
Test - acc:         0.920800 loss:        0.327643
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.992740 loss:        0.023149
Test - acc:         0.923900 loss:        0.316221
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.993540 loss:        0.021450
Test - acc:         0.923300 loss:        0.320709
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.993500 loss:        0.021526
Test - acc:         0.921600 loss:        0.323725
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.993700 loss:        0.020737
Test - acc:         0.923700 loss:        0.322983
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.993460 loss:        0.021328
Test - acc:         0.925900 loss:        0.320582
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.994200 loss:        0.019896
Test - acc:         0.924700 loss:        0.321379
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.993900 loss:        0.019574
Test - acc:         0.925300 loss:        0.321294
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.994600 loss:        0.017083
Test - acc:         0.924600 loss:        0.323026
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.995000 loss:        0.017622
Test - acc:         0.923200 loss:        0.323802
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.995020 loss:        0.016916
Test - acc:         0.926400 loss:        0.322434
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.995580 loss:        0.015502
Test - acc:         0.922500 loss:        0.325560
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.995480 loss:        0.015239
Test - acc:         0.923900 loss:        0.324414
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.995580 loss:        0.015487
Test - acc:         0.925800 loss:        0.327582
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.995760 loss:        0.014681
Test - acc:         0.925400 loss:        0.325029
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.995480 loss:        0.014790
Test - acc:         0.925700 loss:        0.329409
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.995840 loss:        0.014065
Test - acc:         0.924800 loss:        0.327471
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.995680 loss:        0.014567
Test - acc:         0.924800 loss:        0.332606
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.995660 loss:        0.014170
Test - acc:         0.924300 loss:        0.330455
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.995560 loss:        0.015075
Test - acc:         0.924600 loss:        0.333906
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.995840 loss:        0.013754
Test - acc:         0.924400 loss:        0.330480
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.995980 loss:        0.013557
Test - acc:         0.924600 loss:        0.334676
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.995980 loss:        0.013346
Test - acc:         0.925100 loss:        0.328347
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.996380 loss:        0.012972
Test - acc:         0.924900 loss:        0.324738
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.996640 loss:        0.011912
Test - acc:         0.925300 loss:        0.327724
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.996620 loss:        0.011953
Test - acc:         0.924400 loss:        0.334639
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.996260 loss:        0.012658
Test - acc:         0.925300 loss:        0.328533
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.996440 loss:        0.012081
Test - acc:         0.925900 loss:        0.330363
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.996920 loss:        0.010969
Test - acc:         0.926700 loss:        0.333735
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.997340 loss:        0.010163
Test - acc:         0.925300 loss:        0.328910
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.915400 loss:        0.266801
Test - acc:         0.883700 loss:        0.399126
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.941200 loss:        0.174958
Test - acc:         0.891000 loss:        0.384461
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.951400 loss:        0.144794
Test - acc:         0.897500 loss:        0.362314
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.955780 loss:        0.132537
Test - acc:         0.899000 loss:        0.350261
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.958020 loss:        0.124914
Test - acc:         0.901500 loss:        0.342970
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.961880 loss:        0.112445
Test - acc:         0.904100 loss:        0.346941
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.964180 loss:        0.110906
Test - acc:         0.903000 loss:        0.349574
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.964940 loss:        0.103326
Test - acc:         0.905700 loss:        0.334672
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.966560 loss:        0.097186
Test - acc:         0.904800 loss:        0.338133
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.968800 loss:        0.094298
Test - acc:         0.904300 loss:        0.341654
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.969320 loss:        0.093353
Test - acc:         0.906400 loss:        0.338202
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.969840 loss:        0.089407
Test - acc:         0.907900 loss:        0.345673
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.972100 loss:        0.082421
Test - acc:         0.908000 loss:        0.341809
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.972340 loss:        0.081407
Test - acc:         0.906700 loss:        0.354745
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.972540 loss:        0.080257
Test - acc:         0.911300 loss:        0.341324
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.972940 loss:        0.079096
Test - acc:         0.909200 loss:        0.347492
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.973480 loss:        0.078245
Test - acc:         0.909000 loss:        0.343087
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.974940 loss:        0.074352
Test - acc:         0.910500 loss:        0.336189
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.975500 loss:        0.072881
Test - acc:         0.907200 loss:        0.346648
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.976440 loss:        0.069434
Test - acc:         0.908600 loss:        0.351520
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.975320 loss:        0.071621
Test - acc:         0.910300 loss:        0.338785
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.976660 loss:        0.070529
Test - acc:         0.909800 loss:        0.349403
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.977260 loss:        0.065999
Test - acc:         0.910200 loss:        0.347094
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.977100 loss:        0.066863
Test - acc:         0.909400 loss:        0.342722
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.977320 loss:        0.066651
Test - acc:         0.910800 loss:        0.351052
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.978720 loss:        0.064841
Test - acc:         0.909800 loss:        0.350420
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.978080 loss:        0.063716
Test - acc:         0.912000 loss:        0.345152
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.979240 loss:        0.062402
Test - acc:         0.911600 loss:        0.349166
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.979760 loss:        0.060131
Test - acc:         0.912000 loss:        0.352765
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.979740 loss:        0.061714
Test - acc:         0.910900 loss:        0.354966
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.979840 loss:        0.059019
Test - acc:         0.912100 loss:        0.350474
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.979040 loss:        0.061375
Test - acc:         0.910800 loss:        0.348685
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.980520 loss:        0.059029
Test - acc:         0.913200 loss:        0.347057
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.980440 loss:        0.057579
Test - acc:         0.910800 loss:        0.354387
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.979300 loss:        0.059160
Test - acc:         0.912600 loss:        0.350312
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.981100 loss:        0.055022
Test - acc:         0.913400 loss:        0.347752
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.981920 loss:        0.053197
Test - acc:         0.911900 loss:        0.353183
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.981640 loss:        0.054349
Test - acc:         0.911500 loss:        0.357462
Sparsity :          0.9961
Wdecay :        0.000500
