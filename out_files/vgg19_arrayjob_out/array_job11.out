Running --prune_criterion random --seed 42 --prune_freq 50 --prune_rate 0.5 --comment=vgg19_crit=random_pf=50_seed=42 --save_model=pre-finetune/vgg19_random_pf50_s42
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "random",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_random_pf50_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.106580 loss:        2.520409
Test - acc:         0.107600 loss:        2.294274
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.121620 loss:        2.275786
Test - acc:         0.128800 loss:        2.258387
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.167360 loss:        2.134492
Test - acc:         0.208100 loss:        1.905877
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.258380 loss:        1.859667
Test - acc:         0.287600 loss:        1.767105
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.322920 loss:        1.699367
Test - acc:         0.410600 loss:        1.501470
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.445160 loss:        1.456379
Test - acc:         0.508200 loss:        1.356367
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.546300 loss:        1.241191
Test - acc:         0.498200 loss:        1.477734
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.604540 loss:        1.101290
Test - acc:         0.483800 loss:        1.743679
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.650880 loss:        0.993529
Test - acc:         0.619700 loss:        1.166982
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.703160 loss:        0.882477
Test - acc:         0.659900 loss:        1.012752
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.733140 loss:        0.808711
Test - acc:         0.648500 loss:        1.121658
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.752060 loss:        0.763825
Test - acc:         0.697100 loss:        0.950123
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.763380 loss:        0.735478
Test - acc:         0.647600 loss:        1.210369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.773560 loss:        0.706368
Test - acc:         0.717500 loss:        0.901679
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.780940 loss:        0.681272
Test - acc:         0.682800 loss:        1.025874
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.791420 loss:        0.649883
Test - acc:         0.736800 loss:        0.872370
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.796700 loss:        0.634338
Test - acc:         0.700800 loss:        0.932917
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.801840 loss:        0.615529
Test - acc:         0.797400 loss:        0.635734
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.804520 loss:        0.605495
Test - acc:         0.786700 loss:        0.672975
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.811300 loss:        0.586166
Test - acc:         0.692500 loss:        1.006768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.818600 loss:        0.574597
Test - acc:         0.764600 loss:        0.752744
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.819720 loss:        0.564470
Test - acc:         0.732800 loss:        0.858983
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.822020 loss:        0.561122
Test - acc:         0.767600 loss:        0.719709
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.821520 loss:        0.558114
Test - acc:         0.744500 loss:        0.842082
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.827620 loss:        0.542572
Test - acc:         0.761300 loss:        0.789428
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.830100 loss:        0.535151
Test - acc:         0.741300 loss:        0.871493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.828600 loss:        0.532575
Test - acc:         0.787500 loss:        0.687972
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.833340 loss:        0.523183
Test - acc:         0.781500 loss:        0.714158
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.832740 loss:        0.525514
Test - acc:         0.761200 loss:        0.775278
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.831200 loss:        0.525716
Test - acc:         0.754700 loss:        0.881011
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.838100 loss:        0.512746
Test - acc:         0.776500 loss:        0.724635
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.839420 loss:        0.504651
Test - acc:         0.760400 loss:        0.863967
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.838440 loss:        0.504414
Test - acc:         0.785700 loss:        0.718698
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.838460 loss:        0.503714
Test - acc:         0.815300 loss:        0.567434
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.840220 loss:        0.497685
Test - acc:         0.662800 loss:        1.328712
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.842920 loss:        0.493377
Test - acc:         0.732000 loss:        0.900541
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.841740 loss:        0.493601
Test - acc:         0.813600 loss:        0.592560
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.841380 loss:        0.496394
Test - acc:         0.789100 loss:        0.683082
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.846480 loss:        0.485149
Test - acc:         0.747100 loss:        0.824019
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.844420 loss:        0.494730
Test - acc:         0.760400 loss:        0.826573
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.844160 loss:        0.488179
Test - acc:         0.778400 loss:        0.727026
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.844580 loss:        0.484431
Test - acc:         0.789900 loss:        0.706319
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.848200 loss:        0.477011
Test - acc:         0.799900 loss:        0.643258
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.848440 loss:        0.471032
Test - acc:         0.698000 loss:        1.080359
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.845960 loss:        0.481293
Test - acc:         0.705900 loss:        1.076335
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.848320 loss:        0.476802
Test - acc:         0.775000 loss:        0.792201
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.849920 loss:        0.475338
Test - acc:         0.805700 loss:        0.608078
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.850620 loss:        0.476133
Test - acc:         0.752100 loss:        0.806073
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.850460 loss:        0.467818
Test - acc:         0.784000 loss:        0.724835
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.850640 loss:        0.472186
Test - acc:         0.675200 loss:        1.208476
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.773280 loss:        0.707359
Test - acc:         0.738300 loss:        0.897441
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.811100 loss:        0.586477
Test - acc:         0.686700 loss:        1.098659
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.821640 loss:        0.558811
Test - acc:         0.775000 loss:        0.701516
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.829120 loss:        0.528213
Test - acc:         0.723700 loss:        0.883212
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.831280 loss:        0.523552
Test - acc:         0.801700 loss:        0.609660
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.834000 loss:        0.516862
Test - acc:         0.788500 loss:        0.707596
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.838860 loss:        0.501530
Test - acc:         0.654800 loss:        1.404620
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.838100 loss:        0.507918
Test - acc:         0.768300 loss:        0.734592
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.839460 loss:        0.496223
Test - acc:         0.795000 loss:        0.652486
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.839400 loss:        0.501089
Test - acc:         0.685900 loss:        1.118646
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.841800 loss:        0.493294
Test - acc:         0.802300 loss:        0.631163
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.844900 loss:        0.487686
Test - acc:         0.761800 loss:        0.774874
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.843180 loss:        0.485163
Test - acc:         0.805900 loss:        0.603917
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.845040 loss:        0.489182
Test - acc:         0.694600 loss:        1.037995
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.842900 loss:        0.488722
Test - acc:         0.782500 loss:        0.730363
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.845820 loss:        0.481199
Test - acc:         0.759600 loss:        0.746345
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.846720 loss:        0.477427
Test - acc:         0.748600 loss:        0.833438
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.845280 loss:        0.483620
Test - acc:         0.792200 loss:        0.673477
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.847500 loss:        0.475137
Test - acc:         0.820100 loss:        0.560119
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.846620 loss:        0.474994
Test - acc:         0.755500 loss:        0.781621
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.847920 loss:        0.477171
Test - acc:         0.731600 loss:        0.931961
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.845300 loss:        0.482557
Test - acc:         0.814700 loss:        0.571622
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.848440 loss:        0.473483
Test - acc:         0.788900 loss:        0.680505
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.847720 loss:        0.472862
Test - acc:         0.766500 loss:        0.749050
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.848500 loss:        0.471152
Test - acc:         0.726100 loss:        0.979442
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.846620 loss:        0.475450
Test - acc:         0.760000 loss:        0.745728
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.848720 loss:        0.466476
Test - acc:         0.807000 loss:        0.594202
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.850460 loss:        0.463011
Test - acc:         0.791900 loss:        0.684498
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.849840 loss:        0.462768
Test - acc:         0.809800 loss:        0.568477
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.853740 loss:        0.452051
Test - acc:         0.785300 loss:        0.693523
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.853680 loss:        0.452706
Test - acc:         0.786000 loss:        0.661061
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.852440 loss:        0.452964
Test - acc:         0.807200 loss:        0.630320
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.852240 loss:        0.453965
Test - acc:         0.749100 loss:        0.810575
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.854760 loss:        0.449787
Test - acc:         0.765600 loss:        0.787687
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.856040 loss:        0.446800
Test - acc:         0.757700 loss:        0.760245
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.858420 loss:        0.436984
Test - acc:         0.805500 loss:        0.568828
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.854160 loss:        0.447027
Test - acc:         0.751600 loss:        0.899890
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.857920 loss:        0.438259
Test - acc:         0.791900 loss:        0.691531
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.856440 loss:        0.442169
Test - acc:         0.796400 loss:        0.646815
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.856620 loss:        0.434349
Test - acc:         0.731700 loss:        0.935902
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.857240 loss:        0.437647
Test - acc:         0.798800 loss:        0.632203
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.859400 loss:        0.433649
Test - acc:         0.802600 loss:        0.648622
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.855920 loss:        0.436072
Test - acc:         0.778400 loss:        0.743011
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.858640 loss:        0.431873
Test - acc:         0.764500 loss:        0.841029
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.857960 loss:        0.434975
Test - acc:         0.793200 loss:        0.634094
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.858340 loss:        0.431953
Test - acc:         0.742400 loss:        0.845329
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.857640 loss:        0.434222
Test - acc:         0.778000 loss:        0.686937
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.861980 loss:        0.421509
Test - acc:         0.701600 loss:        1.001209
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.860180 loss:        0.429703
Test - acc:         0.799200 loss:        0.630983
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.861380 loss:        0.420399
Test - acc:         0.810700 loss:        0.586214
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.749020 loss:        0.760703
Test - acc:         0.692300 loss:        1.016294
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.807220 loss:        0.586890
Test - acc:         0.748600 loss:        0.808367
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.819420 loss:        0.544134
Test - acc:         0.739500 loss:        0.791928
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.825600 loss:        0.525908
Test - acc:         0.764400 loss:        0.740973
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.833460 loss:        0.504762
Test - acc:         0.753200 loss:        0.784630
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.835000 loss:        0.505449
Test - acc:         0.745700 loss:        0.751381
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.836000 loss:        0.493228
Test - acc:         0.803000 loss:        0.601509
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.838720 loss:        0.487638
Test - acc:         0.795500 loss:        0.615606
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.837420 loss:        0.490732
Test - acc:         0.811500 loss:        0.575091
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.844140 loss:        0.475240
Test - acc:         0.738000 loss:        0.865451
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.840440 loss:        0.476407
Test - acc:         0.719300 loss:        1.006239
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.842940 loss:        0.471327
Test - acc:         0.819300 loss:        0.542163
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.844320 loss:        0.466943
Test - acc:         0.776000 loss:        0.655881
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.846420 loss:        0.468298
Test - acc:         0.805700 loss:        0.583146
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.843580 loss:        0.464640
Test - acc:         0.821200 loss:        0.534857
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.845560 loss:        0.462596
Test - acc:         0.813400 loss:        0.565324
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.847560 loss:        0.459299
Test - acc:         0.803200 loss:        0.618679
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.847080 loss:        0.458716
Test - acc:         0.787000 loss:        0.637379
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.849800 loss:        0.450471
Test - acc:         0.800400 loss:        0.626813
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.849300 loss:        0.451710
Test - acc:         0.814700 loss:        0.550409
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.846000 loss:        0.455166
Test - acc:         0.693600 loss:        1.058182
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.849500 loss:        0.447194
Test - acc:         0.761600 loss:        0.721092
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.847540 loss:        0.453048
Test - acc:         0.797400 loss:        0.631315
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.850680 loss:        0.445296
Test - acc:         0.745600 loss:        0.803706
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.852160 loss:        0.445710
Test - acc:         0.802600 loss:        0.603194
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.853580 loss:        0.442049
Test - acc:         0.741200 loss:        0.827786
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.848560 loss:        0.452813
Test - acc:         0.834000 loss:        0.496670
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.849720 loss:        0.447842
Test - acc:         0.793300 loss:        0.633137
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.853780 loss:        0.439901
Test - acc:         0.825600 loss:        0.522026
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.849660 loss:        0.453467
Test - acc:         0.757400 loss:        0.758045
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.850660 loss:        0.444979
Test - acc:         0.698400 loss:        0.926457
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.853780 loss:        0.436591
Test - acc:         0.818700 loss:        0.561558
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.850640 loss:        0.445272
Test - acc:         0.755800 loss:        0.809397
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.851740 loss:        0.447554
Test - acc:         0.757200 loss:        0.751377
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.855200 loss:        0.436132
Test - acc:         0.798100 loss:        0.617217
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.854700 loss:        0.431626
Test - acc:         0.810600 loss:        0.571036
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.852080 loss:        0.441400
Test - acc:         0.806600 loss:        0.588328
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.854140 loss:        0.436794
Test - acc:         0.797200 loss:        0.617794
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.856460 loss:        0.432689
Test - acc:         0.823600 loss:        0.532991
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.854920 loss:        0.436011
Test - acc:         0.741700 loss:        0.845389
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.859060 loss:        0.424836
Test - acc:         0.785800 loss:        0.705249
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.852820 loss:        0.435642
Test - acc:         0.776800 loss:        0.764362
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.851960 loss:        0.437452
Test - acc:         0.795700 loss:        0.654211
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.855800 loss:        0.435295
Test - acc:         0.806800 loss:        0.616907
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.854620 loss:        0.434220
Test - acc:         0.825600 loss:        0.522839
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.855520 loss:        0.430552
Test - acc:         0.831800 loss:        0.492777
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.855200 loss:        0.432932
Test - acc:         0.800700 loss:        0.626721
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.852900 loss:        0.431102
Test - acc:         0.819900 loss:        0.560955
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.857580 loss:        0.426134
Test - acc:         0.792300 loss:        0.659863
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.854920 loss:        0.431904
Test - acc:         0.793800 loss:        0.656847
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.739640 loss:        0.777189
Test - acc:         0.824600 loss:        0.530338
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.847500 loss:        0.449587
Test - acc:         0.842900 loss:        0.473847
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.870400 loss:        0.381762
Test - acc:         0.856500 loss:        0.435183
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.882220 loss:        0.344217
Test - acc:         0.867700 loss:        0.397199
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.890600 loss:        0.318039
Test - acc:         0.868400 loss:        0.392063
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.896480 loss:        0.300294
Test - acc:         0.874700 loss:        0.381292
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.901900 loss:        0.290289
Test - acc:         0.873500 loss:        0.385941
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.902820 loss:        0.283546
Test - acc:         0.866900 loss:        0.389787
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.908700 loss:        0.267383
Test - acc:         0.865300 loss:        0.413984
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.911920 loss:        0.259046
Test - acc:         0.877000 loss:        0.376599
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.914420 loss:        0.251226
Test - acc:         0.872200 loss:        0.400049
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.914880 loss:        0.248582
Test - acc:         0.880100 loss:        0.366988
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.916460 loss:        0.242627
Test - acc:         0.865100 loss:        0.423272
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.918120 loss:        0.235981
Test - acc:         0.873900 loss:        0.390569
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.919240 loss:        0.233453
Test - acc:         0.857100 loss:        0.449778
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.920000 loss:        0.227746
Test - acc:         0.879000 loss:        0.370825
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.922080 loss:        0.225294
Test - acc:         0.866500 loss:        0.421086
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.923420 loss:        0.224053
Test - acc:         0.868500 loss:        0.409393
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.925080 loss:        0.216823
Test - acc:         0.872300 loss:        0.409439
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.925200 loss:        0.215666
Test - acc:         0.876200 loss:        0.385897
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.926220 loss:        0.212256
Test - acc:         0.879800 loss:        0.366096
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.926040 loss:        0.213453
Test - acc:         0.879000 loss:        0.399598
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.928600 loss:        0.207641
Test - acc:         0.883900 loss:        0.386369
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.927300 loss:        0.209151
Test - acc:         0.874000 loss:        0.399044
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.928880 loss:        0.201868
Test - acc:         0.886800 loss:        0.368362
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.929720 loss:        0.203242
Test - acc:         0.876900 loss:        0.386176
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.930860 loss:        0.198374
Test - acc:         0.872700 loss:        0.393688
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.931460 loss:        0.201564
Test - acc:         0.867400 loss:        0.426433
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.933280 loss:        0.191768
Test - acc:         0.871100 loss:        0.410313
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.931120 loss:        0.201093
Test - acc:         0.875900 loss:        0.401672
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.932960 loss:        0.191221
Test - acc:         0.881800 loss:        0.387783
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.933240 loss:        0.194299
Test - acc:         0.879700 loss:        0.383392
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.935100 loss:        0.188505
Test - acc:         0.875400 loss:        0.387660
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.933640 loss:        0.189818
Test - acc:         0.873400 loss:        0.415291
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.932240 loss:        0.192633
Test - acc:         0.871800 loss:        0.412258
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.936200 loss:        0.184768
Test - acc:         0.881300 loss:        0.378570
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.936300 loss:        0.182018
Test - acc:         0.876000 loss:        0.412144
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.934240 loss:        0.190508
Test - acc:         0.881200 loss:        0.368690
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.936460 loss:        0.182018
Test - acc:         0.876000 loss:        0.395406
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.934780 loss:        0.184823
Test - acc:         0.880200 loss:        0.380748
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.937540 loss:        0.179618
Test - acc:         0.877200 loss:        0.399381
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.938440 loss:        0.178133
Test - acc:         0.880300 loss:        0.396088
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.937240 loss:        0.183157
Test - acc:         0.882000 loss:        0.377426
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.937820 loss:        0.177146
Test - acc:         0.880500 loss:        0.372709
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.940340 loss:        0.173626
Test - acc:         0.876700 loss:        0.402518
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.935700 loss:        0.179147
Test - acc:         0.875000 loss:        0.403925
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.939120 loss:        0.173815
Test - acc:         0.883500 loss:        0.382945
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.939060 loss:        0.174131
Test - acc:         0.878100 loss:        0.400193
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.939820 loss:        0.172379
Test - acc:         0.875200 loss:        0.404220
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.939760 loss:        0.173697
Test - acc:         0.872800 loss:        0.421006
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.466280 loss:        1.453958
Test - acc:         0.589700 loss:        1.153823
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.649020 loss:        0.990329
Test - acc:         0.666700 loss:        0.958196
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.711680 loss:        0.826063
Test - acc:         0.703400 loss:        0.872558
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.745020 loss:        0.732455
Test - acc:         0.758600 loss:        0.697174
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.767540 loss:        0.670477
Test - acc:         0.752300 loss:        0.722924
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.781540 loss:        0.626934
Test - acc:         0.788600 loss:        0.614734
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.796140 loss:        0.587808
Test - acc:         0.767900 loss:        0.680712
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.805500 loss:        0.564734
Test - acc:         0.773700 loss:        0.661398
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.813400 loss:        0.536657
Test - acc:         0.784900 loss:        0.631670
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.820780 loss:        0.516915
Test - acc:         0.809900 loss:        0.546406
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.828060 loss:        0.500279
Test - acc:         0.789500 loss:        0.629758
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.832400 loss:        0.483337
Test - acc:         0.800600 loss:        0.587034
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.835900 loss:        0.477802
Test - acc:         0.815400 loss:        0.530592
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.839680 loss:        0.463206
Test - acc:         0.817100 loss:        0.538205
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.845000 loss:        0.448876
Test - acc:         0.804800 loss:        0.573275
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.848240 loss:        0.438304
Test - acc:         0.810000 loss:        0.564349
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.848860 loss:        0.434072
Test - acc:         0.812800 loss:        0.548452
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.849580 loss:        0.430832
Test - acc:         0.801400 loss:        0.593739
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.855520 loss:        0.417517
Test - acc:         0.833400 loss:        0.492761
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.857000 loss:        0.410867
Test - acc:         0.817200 loss:        0.536706
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.858540 loss:        0.408146
Test - acc:         0.806800 loss:        0.563486
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.863300 loss:        0.396487
Test - acc:         0.830500 loss:        0.503421
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.864080 loss:        0.391426
Test - acc:         0.828300 loss:        0.504056
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.865940 loss:        0.388870
Test - acc:         0.823100 loss:        0.523514
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.868140 loss:        0.382480
Test - acc:         0.812400 loss:        0.560766
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.870940 loss:        0.372421
Test - acc:         0.838700 loss:        0.471450
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.871420 loss:        0.373374
Test - acc:         0.807500 loss:        0.574473
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.873060 loss:        0.366093
Test - acc:         0.823500 loss:        0.524696
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.873720 loss:        0.360459
Test - acc:         0.837400 loss:        0.478882
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.876380 loss:        0.356079
Test - acc:         0.837600 loss:        0.469927
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.877060 loss:        0.354528
Test - acc:         0.825800 loss:        0.523024
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.879560 loss:        0.351027
Test - acc:         0.829300 loss:        0.505777
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.879180 loss:        0.347259
Test - acc:         0.842800 loss:        0.469972
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.879420 loss:        0.346067
Test - acc:         0.822400 loss:        0.533038
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.883160 loss:        0.337550
Test - acc:         0.824800 loss:        0.526766
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.882220 loss:        0.341214
Test - acc:         0.829300 loss:        0.515327
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.883720 loss:        0.337456
Test - acc:         0.828900 loss:        0.521936
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.884180 loss:        0.336751
Test - acc:         0.847400 loss:        0.457744
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.885680 loss:        0.328461
Test - acc:         0.835100 loss:        0.489031
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.887580 loss:        0.324099
Test - acc:         0.841000 loss:        0.473271
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.886540 loss:        0.325600
Test - acc:         0.831900 loss:        0.535853
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.890060 loss:        0.317276
Test - acc:         0.830000 loss:        0.508059
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.887160 loss:        0.321814
Test - acc:         0.844900 loss:        0.479851
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.888240 loss:        0.318873
Test - acc:         0.841700 loss:        0.493949
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.890660 loss:        0.313577
Test - acc:         0.845300 loss:        0.459794
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.893780 loss:        0.309518
Test - acc:         0.846800 loss:        0.468609
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.890700 loss:        0.310270
Test - acc:         0.820700 loss:        0.571960
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.892640 loss:        0.308623
Test - acc:         0.846400 loss:        0.466948
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.892560 loss:        0.308041
Test - acc:         0.848700 loss:        0.450152
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.896980 loss:        0.299242
Test - acc:         0.847900 loss:        0.471891
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.241000 loss:        2.068530
Test - acc:         0.317700 loss:        1.822607
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.341720 loss:        1.748549
Test - acc:         0.395700 loss:        1.619759
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.420500 loss:        1.574747
Test - acc:         0.449200 loss:        1.463765
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.477840 loss:        1.447928
Test - acc:         0.518600 loss:        1.346372
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.522540 loss:        1.345285
Test - acc:         0.559500 loss:        1.255878
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.553500 loss:        1.264945
Test - acc:         0.586900 loss:        1.179221
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.576360 loss:        1.193111
Test - acc:         0.605100 loss:        1.115385
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.599720 loss:        1.129882
Test - acc:         0.622700 loss:        1.069443
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.612880 loss:        1.089605
Test - acc:         0.635200 loss:        1.039704
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.626580 loss:        1.049688
Test - acc:         0.635000 loss:        1.027202
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.644000 loss:        1.008621
Test - acc:         0.659100 loss:        0.967940
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.654920 loss:        0.976737
Test - acc:         0.669500 loss:        0.940918
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.664740 loss:        0.951952
Test - acc:         0.681300 loss:        0.909801
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.673940 loss:        0.924288
Test - acc:         0.682000 loss:        0.904953
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.686540 loss:        0.894494
Test - acc:         0.688000 loss:        0.885367
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.693480 loss:        0.875737
Test - acc:         0.698900 loss:        0.862546
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.698080 loss:        0.856232
Test - acc:         0.698000 loss:        0.855174
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.704720 loss:        0.838975
Test - acc:         0.706700 loss:        0.831212
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.714660 loss:        0.819735
Test - acc:         0.699400 loss:        0.859860
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.716260 loss:        0.809603
Test - acc:         0.717400 loss:        0.809413
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.722640 loss:        0.788034
Test - acc:         0.717000 loss:        0.809648
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.728660 loss:        0.775533
Test - acc:         0.723300 loss:        0.789161
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.732380 loss:        0.760725
Test - acc:         0.733000 loss:        0.768676
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.739660 loss:        0.742794
Test - acc:         0.734300 loss:        0.759763
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.742560 loss:        0.737699
Test - acc:         0.736800 loss:        0.758186
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.746640 loss:        0.723304
Test - acc:         0.741700 loss:        0.754566
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.751440 loss:        0.709714
Test - acc:         0.733200 loss:        0.763497
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.755780 loss:        0.701035
Test - acc:         0.745500 loss:        0.743505
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.758460 loss:        0.689691
Test - acc:         0.749600 loss:        0.718700
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.763480 loss:        0.679087
Test - acc:         0.748000 loss:        0.735165
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.768880 loss:        0.664435
Test - acc:         0.754700 loss:        0.713621
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.767720 loss:        0.659606
Test - acc:         0.754900 loss:        0.713574
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.769620 loss:        0.658337
Test - acc:         0.752700 loss:        0.713400
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.774100 loss:        0.646264
Test - acc:         0.742300 loss:        0.742150
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.777620 loss:        0.635819
Test - acc:         0.757400 loss:        0.700300
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.780880 loss:        0.625082
Test - acc:         0.758900 loss:        0.696030
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.784900 loss:        0.620042
Test - acc:         0.750000 loss:        0.724281
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.785220 loss:        0.612880
Test - acc:         0.768900 loss:        0.669931
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.787560 loss:        0.608995
Test - acc:         0.763800 loss:        0.688532
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.790620 loss:        0.602598
Test - acc:         0.772500 loss:        0.658858
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.791680 loss:        0.595683
Test - acc:         0.772200 loss:        0.657407
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.795320 loss:        0.592580
Test - acc:         0.765900 loss:        0.672576
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.797500 loss:        0.580654
Test - acc:         0.775600 loss:        0.657158
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.798780 loss:        0.576190
Test - acc:         0.772800 loss:        0.661012
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.802140 loss:        0.569091
Test - acc:         0.776100 loss:        0.649366
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.802100 loss:        0.565102
Test - acc:         0.774500 loss:        0.647820
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.806360 loss:        0.560459
Test - acc:         0.738100 loss:        0.791564
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.806200 loss:        0.552440
Test - acc:         0.782500 loss:        0.631529
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.808760 loss:        0.548514
Test - acc:         0.779300 loss:        0.631769
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.808660 loss:        0.550479
Test - acc:         0.783800 loss:        0.631263
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.258480 loss:        2.086562
Test - acc:         0.328400 loss:        1.823378
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.346800 loss:        1.767770
Test - acc:         0.388000 loss:        1.637791
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.408060 loss:        1.625944
Test - acc:         0.437300 loss:        1.548099
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.454400 loss:        1.520460
Test - acc:         0.486200 loss:        1.422286
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.490440 loss:        1.435000
Test - acc:         0.515800 loss:        1.355212
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.516000 loss:        1.367374
Test - acc:         0.545700 loss:        1.295235
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.536200 loss:        1.314173
Test - acc:         0.565100 loss:        1.233885
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.553000 loss:        1.263841
Test - acc:         0.578200 loss:        1.191079
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.568600 loss:        1.221169
Test - acc:         0.556900 loss:        1.250273
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.577140 loss:        1.188174
Test - acc:         0.601400 loss:        1.131991
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.592120 loss:        1.152851
Test - acc:         0.598600 loss:        1.141664
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.597800 loss:        1.132317
Test - acc:         0.602700 loss:        1.107583
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.609820 loss:        1.108939
Test - acc:         0.599800 loss:        1.131220
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.615440 loss:        1.084672
Test - acc:         0.618600 loss:        1.087304
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.620360 loss:        1.069470
Test - acc:         0.624100 loss:        1.061155
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.627100 loss:        1.052721
Test - acc:         0.629000 loss:        1.050794
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.632820 loss:        1.036660
Test - acc:         0.633000 loss:        1.033813
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.640800 loss:        1.017993
Test - acc:         0.647800 loss:        0.989422
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.645600 loss:        1.004162
Test - acc:         0.639900 loss:        1.020225
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.651920 loss:        0.986064
Test - acc:         0.650700 loss:        0.981860
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.658020 loss:        0.971757
Test - acc:         0.646600 loss:        1.003898
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.659600 loss:        0.964683
Test - acc:         0.666200 loss:        0.941004
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.665380 loss:        0.950928
Test - acc:         0.643200 loss:        1.037904
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.666880 loss:        0.941254
Test - acc:         0.656300 loss:        0.974396
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.673340 loss:        0.928961
Test - acc:         0.663300 loss:        0.937296
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.677620 loss:        0.913588
Test - acc:         0.681500 loss:        0.916777
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.680700 loss:        0.908118
Test - acc:         0.683100 loss:        0.913729
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.683860 loss:        0.896427
Test - acc:         0.679200 loss:        0.904813
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.689000 loss:        0.888506
Test - acc:         0.693200 loss:        0.870831
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.691580 loss:        0.874472
Test - acc:         0.677500 loss:        0.909030
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.693900 loss:        0.872503
Test - acc:         0.677900 loss:        0.921645
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.695340 loss:        0.864118
Test - acc:         0.684800 loss:        0.890070
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.700560 loss:        0.850756
Test - acc:         0.698300 loss:        0.861936
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.703460 loss:        0.847785
Test - acc:         0.694000 loss:        0.871995
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.706740 loss:        0.837782
Test - acc:         0.703600 loss:        0.843094
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.708180 loss:        0.827220
Test - acc:         0.703400 loss:        0.850013
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.709580 loss:        0.827801
Test - acc:         0.706700 loss:        0.837424
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.716200 loss:        0.814118
Test - acc:         0.697300 loss:        0.853236
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.711980 loss:        0.817677
Test - acc:         0.711700 loss:        0.812003
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.714360 loss:        0.808815
Test - acc:         0.714500 loss:        0.819311
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.720340 loss:        0.800654
Test - acc:         0.705200 loss:        0.853753
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.720140 loss:        0.796901
Test - acc:         0.711600 loss:        0.825189
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.720500 loss:        0.789230
Test - acc:         0.714500 loss:        0.812978
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.726620 loss:        0.783144
Test - acc:         0.718300 loss:        0.815200
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.726800 loss:        0.779099
Test - acc:         0.726700 loss:        0.794492
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.728880 loss:        0.771863
Test - acc:         0.721900 loss:        0.799301
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.730100 loss:        0.770526
Test - acc:         0.726600 loss:        0.778291
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.731580 loss:        0.762476
Test - acc:         0.714000 loss:        0.827737
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.733900 loss:        0.753611
Test - acc:         0.727200 loss:        0.787107
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.736380 loss:        0.750742
Test - acc:         0.725800 loss:        0.784632
Sparsity :          0.9844
Wdecay :        0.000500
