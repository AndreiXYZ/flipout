Running --prune_criterion global_magnitude --seed 43 --prune_freq 117 --prune_rate 0.5 --comment=vgg19_crit=global_magnitude_pf=117_seed=43 --save_model=pre-finetune/vgg19_global_magnitude_pf117_s43
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "global_magnitude",
    "prune_freq": 117,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_global_magnitude_pf117_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.105300 loss:        2.601138
Test - acc:         0.109400 loss:        2.297950
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.109500 loss:        2.296811
Test - acc:         0.121000 loss:        2.290626
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.135060 loss:        2.237199
Test - acc:         0.195300 loss:        2.051474
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.236160 loss:        1.912667
Test - acc:         0.262600 loss:        1.785715
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.333360 loss:        1.685028
Test - acc:         0.349500 loss:        1.665372
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.444340 loss:        1.475077
Test - acc:         0.474900 loss:        1.444693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.547480 loss:        1.252955
Test - acc:         0.515300 loss:        1.357213
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.628240 loss:        1.060720
Test - acc:         0.621900 loss:        1.115540
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.682960 loss:        0.919136
Test - acc:         0.642600 loss:        1.043907
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.722260 loss:        0.830122
Test - acc:         0.668000 loss:        1.003053
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.744180 loss:        0.775274
Test - acc:         0.625800 loss:        1.237646
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.763340 loss:        0.715959
Test - acc:         0.698600 loss:        0.897794
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.772800 loss:        0.692598
Test - acc:         0.643300 loss:        1.246807
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.783280 loss:        0.663185
Test - acc:         0.740300 loss:        0.836798
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.791500 loss:        0.640620
Test - acc:         0.727400 loss:        0.895406
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.793680 loss:        0.635948
Test - acc:         0.722900 loss:        0.857718
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.801200 loss:        0.611050
Test - acc:         0.686800 loss:        1.000325
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.803660 loss:        0.604088
Test - acc:         0.787200 loss:        0.699029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.808180 loss:        0.589044
Test - acc:         0.729800 loss:        0.866465
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.818440 loss:        0.569596
Test - acc:         0.746700 loss:        0.787903
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.817640 loss:        0.565120
Test - acc:         0.740400 loss:        0.878687
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.818520 loss:        0.559278
Test - acc:         0.729600 loss:        0.940122
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.825000 loss:        0.546039
Test - acc:         0.771700 loss:        0.706058
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.829120 loss:        0.535224
Test - acc:         0.712800 loss:        0.952716
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.826080 loss:        0.540991
Test - acc:         0.765300 loss:        0.804311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.829180 loss:        0.530004
Test - acc:         0.777800 loss:        0.723334
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.834860 loss:        0.515444
Test - acc:         0.759500 loss:        0.789311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.833080 loss:        0.519963
Test - acc:         0.705800 loss:        1.057780
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.838420 loss:        0.500873
Test - acc:         0.757500 loss:        0.824629
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.839660 loss:        0.498996
Test - acc:         0.768500 loss:        0.803411
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.840520 loss:        0.494644
Test - acc:         0.676400 loss:        1.210812
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.840660 loss:        0.494944
Test - acc:         0.712200 loss:        1.064473
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.839980 loss:        0.493296
Test - acc:         0.793100 loss:        0.669570
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.843080 loss:        0.486499
Test - acc:         0.807000 loss:        0.617336
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.843280 loss:        0.483888
Test - acc:         0.794200 loss:        0.667158
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.843480 loss:        0.484887
Test - acc:         0.765100 loss:        0.799214
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.843420 loss:        0.488633
Test - acc:         0.765000 loss:        0.769964
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.845880 loss:        0.479952
Test - acc:         0.704900 loss:        1.180704
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.847740 loss:        0.470945
Test - acc:         0.763200 loss:        0.836014
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.848960 loss:        0.468243
Test - acc:         0.767700 loss:        0.790092
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.849860 loss:        0.472595
Test - acc:         0.799800 loss:        0.640374
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.851620 loss:        0.464365
Test - acc:         0.737900 loss:        1.000664
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.849000 loss:        0.469655
Test - acc:         0.794700 loss:        0.648971
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.852740 loss:        0.458308
Test - acc:         0.772600 loss:        0.740477
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.851760 loss:        0.460629
Test - acc:         0.764200 loss:        0.768356
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.852440 loss:        0.459701
Test - acc:         0.824600 loss:        0.551497
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.853960 loss:        0.455432
Test - acc:         0.770000 loss:        0.753559
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.853760 loss:        0.450759
Test - acc:         0.765000 loss:        0.778837
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.852020 loss:        0.453369
Test - acc:         0.763100 loss:        0.781117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.853780 loss:        0.454691
Test - acc:         0.786800 loss:        0.697041
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.855860 loss:        0.449234
Test - acc:         0.706300 loss:        1.038955
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.854740 loss:        0.450947
Test - acc:         0.760000 loss:        0.800164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.854380 loss:        0.448492
Test - acc:         0.745700 loss:        0.904669
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.857000 loss:        0.446824
Test - acc:         0.818900 loss:        0.570430
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.857600 loss:        0.441885
Test - acc:         0.677200 loss:        1.257951
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.856520 loss:        0.442883
Test - acc:         0.835100 loss:        0.516913
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.858440 loss:        0.436299
Test - acc:         0.766300 loss:        0.785000
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.855940 loss:        0.443078
Test - acc:         0.779200 loss:        0.680428
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.858600 loss:        0.436619
Test - acc:         0.802300 loss:        0.653361
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.858300 loss:        0.439553
Test - acc:         0.789100 loss:        0.669027
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.862160 loss:        0.422611
Test - acc:         0.781400 loss:        0.743858
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.859620 loss:        0.434089
Test - acc:         0.755400 loss:        0.795991
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.863180 loss:        0.430666
Test - acc:         0.843900 loss:        0.484440
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.862740 loss:        0.430180
Test - acc:         0.817900 loss:        0.579326
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.859260 loss:        0.435166
Test - acc:         0.797700 loss:        0.670136
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.862320 loss:        0.427011
Test - acc:         0.777300 loss:        0.777830
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.862340 loss:        0.430751
Test - acc:         0.779300 loss:        0.719918
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.864000 loss:        0.420051
Test - acc:         0.836300 loss:        0.506845
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.862700 loss:        0.424257
Test - acc:         0.764600 loss:        0.743010
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.863880 loss:        0.421460
Test - acc:         0.812900 loss:        0.626851
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.861860 loss:        0.427796
Test - acc:         0.751600 loss:        0.799758
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.862840 loss:        0.421127
Test - acc:         0.852300 loss:        0.457138
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.864380 loss:        0.420978
Test - acc:         0.768400 loss:        0.745303
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.862360 loss:        0.424135
Test - acc:         0.756100 loss:        0.758979
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.865340 loss:        0.417179
Test - acc:         0.806400 loss:        0.610813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.862560 loss:        0.421758
Test - acc:         0.808700 loss:        0.594398
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.862960 loss:        0.422137
Test - acc:         0.788400 loss:        0.688208
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.863380 loss:        0.419014
Test - acc:         0.817500 loss:        0.559834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.865240 loss:        0.418643
Test - acc:         0.818300 loss:        0.577821
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.863500 loss:        0.419742
Test - acc:         0.784100 loss:        0.698125
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.863260 loss:        0.419766
Test - acc:         0.774500 loss:        0.748665
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.866660 loss:        0.413004
Test - acc:         0.793700 loss:        0.665631
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.868180 loss:        0.408813
Test - acc:         0.796100 loss:        0.656701
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.865160 loss:        0.413741
Test - acc:         0.810700 loss:        0.598802
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.862980 loss:        0.419536
Test - acc:         0.811800 loss:        0.612017
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.865260 loss:        0.416437
Test - acc:         0.801100 loss:        0.621391
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.868780 loss:        0.402739
Test - acc:         0.806100 loss:        0.612109
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.866840 loss:        0.411342
Test - acc:         0.824000 loss:        0.538075
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.866920 loss:        0.408364
Test - acc:         0.843600 loss:        0.509813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.865160 loss:        0.414274
Test - acc:         0.836900 loss:        0.508037
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.863560 loss:        0.414960
Test - acc:         0.823300 loss:        0.531290
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.868480 loss:        0.405932
Test - acc:         0.814500 loss:        0.577153
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.868120 loss:        0.404279
Test - acc:         0.785400 loss:        0.691709
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.865680 loss:        0.413230
Test - acc:         0.813500 loss:        0.568627
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.866340 loss:        0.411092
Test - acc:         0.769900 loss:        0.767432
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.406480
Test - acc:         0.828300 loss:        0.564571
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.866180 loss:        0.411970
Test - acc:         0.767700 loss:        0.757638
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.869000 loss:        0.405491
Test - acc:         0.738700 loss:        0.921707
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.869200 loss:        0.402511
Test - acc:         0.784800 loss:        0.698934
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.870760 loss:        0.399933
Test - acc:         0.823800 loss:        0.551613
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.867060 loss:        0.408723
Test - acc:         0.822500 loss:        0.591477
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.871900 loss:        0.398030
Test - acc:         0.827100 loss:        0.567581
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.869380 loss:        0.404872
Test - acc:         0.799900 loss:        0.646741
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.870560 loss:        0.399703
Test - acc:         0.802100 loss:        0.640003
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.866940 loss:        0.406301
Test - acc:         0.795000 loss:        0.658980
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.868100 loss:        0.402825
Test - acc:         0.814000 loss:        0.598032
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.870020 loss:        0.398086
Test - acc:         0.840600 loss:        0.486131
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.866660 loss:        0.409396
Test - acc:         0.816900 loss:        0.575073
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.867500 loss:        0.403428
Test - acc:         0.803800 loss:        0.611693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.870940 loss:        0.393997
Test - acc:         0.823300 loss:        0.589868
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.871340 loss:        0.399286
Test - acc:         0.773500 loss:        0.842477
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.868660 loss:        0.402063
Test - acc:         0.799000 loss:        0.622545
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.868960 loss:        0.398977
Test - acc:         0.751500 loss:        0.914274
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.871220 loss:        0.397907
Test - acc:         0.797200 loss:        0.669362
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.872200 loss:        0.393757
Test - acc:         0.799700 loss:        0.645776
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.870860 loss:        0.401695
Test - acc:         0.811200 loss:        0.580239
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.872200 loss:        0.394888
Test - acc:         0.827100 loss:        0.547094
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.870600 loss:        0.395836
Test - acc:         0.854100 loss:        0.452436
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.871620 loss:        0.391898
Test - acc:         0.767900 loss:        0.775667
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.866600 loss:        0.404743
Test - acc:         0.814800 loss:        0.592590
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.870120 loss:        0.397591
Test - acc:         0.796200 loss:        0.647885
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.872960 loss:        0.386666
Test - acc:         0.837800 loss:        0.505676
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.871120 loss:        0.391173
Test - acc:         0.811500 loss:        0.612090
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.873560 loss:        0.387239
Test - acc:         0.826300 loss:        0.575603
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.872580 loss:        0.388294
Test - acc:         0.795900 loss:        0.650743
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.873100 loss:        0.388033
Test - acc:         0.725000 loss:        0.897838
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.871160 loss:        0.391461
Test - acc:         0.817700 loss:        0.577142
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.872560 loss:        0.387170
Test - acc:         0.822500 loss:        0.562166
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.872760 loss:        0.386061
Test - acc:         0.760800 loss:        0.850672
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.875620 loss:        0.379782
Test - acc:         0.825100 loss:        0.525199
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.875480 loss:        0.380862
Test - acc:         0.797900 loss:        0.661247
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.874000 loss:        0.381848
Test - acc:         0.793100 loss:        0.656674
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.871800 loss:        0.391807
Test - acc:         0.807000 loss:        0.604082
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.874600 loss:        0.381073
Test - acc:         0.821200 loss:        0.569611
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.878080 loss:        0.372263
Test - acc:         0.819600 loss:        0.572801
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.873080 loss:        0.383460
Test - acc:         0.792000 loss:        0.654577
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.875020 loss:        0.382059
Test - acc:         0.753500 loss:        0.794393
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.876740 loss:        0.377024
Test - acc:         0.809200 loss:        0.571340
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.871940 loss:        0.381577
Test - acc:         0.789500 loss:        0.687150
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.875440 loss:        0.381867
Test - acc:         0.826900 loss:        0.564682
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.874440 loss:        0.380768
Test - acc:         0.839500 loss:        0.526833
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.876300 loss:        0.372242
Test - acc:         0.808900 loss:        0.601854
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.876220 loss:        0.376800
Test - acc:         0.797600 loss:        0.650644
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.873840 loss:        0.379416
Test - acc:         0.776400 loss:        0.743450
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.873640 loss:        0.378201
Test - acc:         0.834000 loss:        0.511591
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.876080 loss:        0.376036
Test - acc:         0.809500 loss:        0.587309
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.876460 loss:        0.374990
Test - acc:         0.801200 loss:        0.618874
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.879760 loss:        0.366506
Test - acc:         0.822700 loss:        0.563221
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.877000 loss:        0.368847
Test - acc:         0.793400 loss:        0.648851
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.875480 loss:        0.375966
Test - acc:         0.794300 loss:        0.649766
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.933700 loss:        0.200657
Test - acc:         0.917000 loss:        0.248185
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.951420 loss:        0.146179
Test - acc:         0.920100 loss:        0.242200
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.958520 loss:        0.123653
Test - acc:         0.924300 loss:        0.236596
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.962920 loss:        0.109456
Test - acc:         0.925000 loss:        0.238155
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.967160 loss:        0.097796
Test - acc:         0.925500 loss:        0.248395
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.970660 loss:        0.087249
Test - acc:         0.924500 loss:        0.252955
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.974100 loss:        0.078161
Test - acc:         0.924400 loss:        0.248045
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.975500 loss:        0.072233
Test - acc:         0.923200 loss:        0.256115
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.976580 loss:        0.069069
Test - acc:         0.923000 loss:        0.266996
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.979280 loss:        0.063355
Test - acc:         0.923800 loss:        0.273234
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.980720 loss:        0.057446
Test - acc:         0.923900 loss:        0.267338
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.982240 loss:        0.054213
Test - acc:         0.918200 loss:        0.290586
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.981320 loss:        0.054712
Test - acc:         0.926700 loss:        0.279026
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.982240 loss:        0.053368
Test - acc:         0.921800 loss:        0.273860
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.983820 loss:        0.048921
Test - acc:         0.923500 loss:        0.287068
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.982840 loss:        0.050270
Test - acc:         0.918400 loss:        0.316279
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.982840 loss:        0.049889
Test - acc:         0.922000 loss:        0.309011
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.984080 loss:        0.048371
Test - acc:         0.922900 loss:        0.295070
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.983820 loss:        0.047744
Test - acc:         0.923000 loss:        0.295975
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.983160 loss:        0.048355
Test - acc:         0.919600 loss:        0.304730
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.985060 loss:        0.046206
Test - acc:         0.922400 loss:        0.297947
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.984280 loss:        0.046576
Test - acc:         0.919900 loss:        0.307234
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.984500 loss:        0.046412
Test - acc:         0.920700 loss:        0.302409
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.983840 loss:        0.049135
Test - acc:         0.921300 loss:        0.301784
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.983380 loss:        0.049055
Test - acc:         0.918500 loss:        0.303132
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.983100 loss:        0.052460
Test - acc:         0.917900 loss:        0.319656
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.983540 loss:        0.050326
Test - acc:         0.916200 loss:        0.301301
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.984080 loss:        0.047918
Test - acc:         0.912200 loss:        0.333265
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.983140 loss:        0.049777
Test - acc:         0.913200 loss:        0.327330
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.982240 loss:        0.051244
Test - acc:         0.915700 loss:        0.325400
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.981540 loss:        0.054418
Test - acc:         0.913100 loss:        0.341844
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.981820 loss:        0.053556
Test - acc:         0.911600 loss:        0.335460
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.981160 loss:        0.054633
Test - acc:         0.910200 loss:        0.337816
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.981380 loss:        0.055591
Test - acc:         0.912600 loss:        0.328253
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.981600 loss:        0.053715
Test - acc:         0.913800 loss:        0.312677
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.981580 loss:        0.056050
Test - acc:         0.910100 loss:        0.348281
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.981040 loss:        0.055449
Test - acc:         0.911000 loss:        0.333306
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.981220 loss:        0.054957
Test - acc:         0.915200 loss:        0.326759
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.981280 loss:        0.055473
Test - acc:         0.908700 loss:        0.360604
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.980180 loss:        0.058285
Test - acc:         0.911900 loss:        0.327920
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.981280 loss:        0.054981
Test - acc:         0.905800 loss:        0.368700
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.978420 loss:        0.064543
Test - acc:         0.912000 loss:        0.332532
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.981160 loss:        0.057573
Test - acc:         0.913300 loss:        0.322895
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.979540 loss:        0.060379
Test - acc:         0.909400 loss:        0.337467
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.977880 loss:        0.065228
Test - acc:         0.908300 loss:        0.346963
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.981340 loss:        0.055932
Test - acc:         0.912000 loss:        0.351119
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.979180 loss:        0.059470
Test - acc:         0.913800 loss:        0.339607
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.979240 loss:        0.060694
Test - acc:         0.887100 loss:        0.430583
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.979080 loss:        0.062718
Test - acc:         0.903500 loss:        0.365377
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.979320 loss:        0.060931
Test - acc:         0.908500 loss:        0.348260
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.980200 loss:        0.060107
Test - acc:         0.910000 loss:        0.332906
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.981060 loss:        0.056533
Test - acc:         0.913900 loss:        0.336219
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.980720 loss:        0.057978
Test - acc:         0.912900 loss:        0.330372
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.979600 loss:        0.061814
Test - acc:         0.904200 loss:        0.365118
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.980160 loss:        0.060305
Test - acc:         0.912600 loss:        0.327636
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.980280 loss:        0.058049
Test - acc:         0.908300 loss:        0.344319
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.978960 loss:        0.061561
Test - acc:         0.902900 loss:        0.370564
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.981120 loss:        0.056864
Test - acc:         0.910800 loss:        0.336152
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.980220 loss:        0.058811
Test - acc:         0.912700 loss:        0.337423
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.058919
Test - acc:         0.905700 loss:        0.354401
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.979960 loss:        0.059894
Test - acc:         0.903600 loss:        0.371730
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.977900 loss:        0.066294
Test - acc:         0.913000 loss:        0.314046
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.059827
Test - acc:         0.913700 loss:        0.322330
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.979220 loss:        0.060727
Test - acc:         0.905800 loss:        0.349646
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.979480 loss:        0.062656
Test - acc:         0.906300 loss:        0.362522
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.980800 loss:        0.055653
Test - acc:         0.908500 loss:        0.349249
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.978980 loss:        0.061079
Test - acc:         0.899900 loss:        0.385066
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.980320 loss:        0.058941
Test - acc:         0.903300 loss:        0.379150
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.980620 loss:        0.058442
Test - acc:         0.907800 loss:        0.370035
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.061309
Test - acc:         0.908700 loss:        0.346898
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.980460 loss:        0.057794
Test - acc:         0.904700 loss:        0.351312
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.980740 loss:        0.058534
Test - acc:         0.906800 loss:        0.352983
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.980540 loss:        0.059650
Test - acc:         0.907000 loss:        0.357170
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.980020 loss:        0.060278
Test - acc:         0.903800 loss:        0.361120
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.979780 loss:        0.060165
Test - acc:         0.906800 loss:        0.353371
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.979560 loss:        0.059813
Test - acc:         0.905700 loss:        0.354956
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.981280 loss:        0.054633
Test - acc:         0.893600 loss:        0.391472
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.980460 loss:        0.059557
Test - acc:         0.917300 loss:        0.335737
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.980900 loss:        0.056669
Test - acc:         0.896400 loss:        0.389977
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.981780 loss:        0.055834
Test - acc:         0.907900 loss:        0.356312
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.980860 loss:        0.059686
Test - acc:         0.902600 loss:        0.361348
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.980800 loss:        0.057567
Test - acc:         0.914400 loss:        0.320006
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.980540 loss:        0.056864
Test - acc:         0.911100 loss:        0.340749
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.980940 loss:        0.057188
Test - acc:         0.914000 loss:        0.313450
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.983160 loss:        0.052052
Test - acc:         0.904900 loss:        0.392012
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.981840 loss:        0.054518
Test - acc:         0.904300 loss:        0.374397
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.980300 loss:        0.057156
Test - acc:         0.912500 loss:        0.335297
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.981040 loss:        0.056685
Test - acc:         0.918600 loss:        0.308522
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.981380 loss:        0.056730
Test - acc:         0.903000 loss:        0.365457
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.979940 loss:        0.060556
Test - acc:         0.912600 loss:        0.318039
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.054535
Test - acc:         0.896900 loss:        0.409389
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.057786
Test - acc:         0.915300 loss:        0.305926
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.980840 loss:        0.057023
Test - acc:         0.902300 loss:        0.373727
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.981540 loss:        0.057181
Test - acc:         0.901700 loss:        0.379555
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.982320 loss:        0.053699
Test - acc:         0.920500 loss:        0.306010
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.058036
Test - acc:         0.906100 loss:        0.341784
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.981760 loss:        0.054392
Test - acc:         0.904800 loss:        0.349685
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.980560 loss:        0.058801
Test - acc:         0.909700 loss:        0.339358
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.983260 loss:        0.051077
Test - acc:         0.905700 loss:        0.355919
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.980800 loss:        0.055879
Test - acc:         0.912200 loss:        0.329650
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.992580 loss:        0.024355
Test - acc:         0.925800 loss:        0.281527
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.996020 loss:        0.013860
Test - acc:         0.927900 loss:        0.280717
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.997400 loss:        0.010035
Test - acc:         0.929900 loss:        0.278071
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.997860 loss:        0.008628
Test - acc:         0.932300 loss:        0.278173
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.998280 loss:        0.007274
Test - acc:         0.932100 loss:        0.281240
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.998320 loss:        0.006626
Test - acc:         0.932800 loss:        0.285741
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.998460 loss:        0.006221
Test - acc:         0.933100 loss:        0.285310
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.998820 loss:        0.005014
Test - acc:         0.933400 loss:        0.289430
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.998700 loss:        0.005414
Test - acc:         0.932700 loss:        0.291646
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.998900 loss:        0.004467
Test - acc:         0.935300 loss:        0.289226
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.999040 loss:        0.004174
Test - acc:         0.933400 loss:        0.292348
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.999000 loss:        0.004081
Test - acc:         0.935700 loss:        0.290679
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.003196
Test - acc:         0.935200 loss:        0.292360
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.003757
Test - acc:         0.935700 loss:        0.291369
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.003034
Test - acc:         0.935800 loss:        0.292621
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.003270
Test - acc:         0.936100 loss:        0.292428
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.003010
Test - acc:         0.935400 loss:        0.297145
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.002914
Test - acc:         0.934600 loss:        0.301611
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.002415
Test - acc:         0.935100 loss:        0.296608
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.002659
Test - acc:         0.935400 loss:        0.297698
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.002189
Test - acc:         0.934800 loss:        0.297368
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.002272
Test - acc:         0.936800 loss:        0.299017
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.002463
Test - acc:         0.936800 loss:        0.300973
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.002307
Test - acc:         0.937000 loss:        0.299837
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.002260
Test - acc:         0.936300 loss:        0.300517
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.002371
Test - acc:         0.936000 loss:        0.297019
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.002342
Test - acc:         0.937400 loss:        0.296084
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.002348
Test - acc:         0.937100 loss:        0.299458
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.001857
Test - acc:         0.936200 loss:        0.302942
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.001936
Test - acc:         0.937300 loss:        0.302639
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.001824
Test - acc:         0.936500 loss:        0.301937
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.001899
Test - acc:         0.936400 loss:        0.300169
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.001895
Test - acc:         0.937000 loss:        0.300270
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001614
Test - acc:         0.937000 loss:        0.302205
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.001839
Test - acc:         0.937700 loss:        0.301454
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.002032
Test - acc:         0.937100 loss:        0.306537
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001624
Test - acc:         0.936200 loss:        0.306757
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.001692
Test - acc:         0.936200 loss:        0.304578
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.001718
Test - acc:         0.936100 loss:        0.303901
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.001534
Test - acc:         0.936700 loss:        0.306688
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.001869
Test - acc:         0.937700 loss:        0.308019
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.001406
Test - acc:         0.936900 loss:        0.304981
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001428
Test - acc:         0.936200 loss:        0.306522
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001506
Test - acc:         0.937900 loss:        0.305494
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001558
Test - acc:         0.936000 loss:        0.307200
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001385
Test - acc:         0.936100 loss:        0.307838
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.001432
Test - acc:         0.936700 loss:        0.306960
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.001864
Test - acc:         0.935600 loss:        0.310501
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001352
Test - acc:         0.936500 loss:        0.308456
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.001775
Test - acc:         0.935700 loss:        0.309778
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.001483
Test - acc:         0.936600 loss:        0.305296
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.001561
Test - acc:         0.936200 loss:        0.303499
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001442
Test - acc:         0.937200 loss:        0.305320
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.001283
Test - acc:         0.937800 loss:        0.303315
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.001699
Test - acc:         0.937300 loss:        0.307934
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.001437
Test - acc:         0.938200 loss:        0.307941
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001343
Test - acc:         0.938200 loss:        0.304316
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001363
Test - acc:         0.938100 loss:        0.305048
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001394
Test - acc:         0.936300 loss:        0.309477
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.001367
Test - acc:         0.936400 loss:        0.307569
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.001383
Test - acc:         0.936200 loss:        0.307212
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001260
Test - acc:         0.937600 loss:        0.307031
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001373
Test - acc:         0.936500 loss:        0.307566
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.001293
Test - acc:         0.936700 loss:        0.306131
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.001211
Test - acc:         0.937400 loss:        0.308680
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.001455
Test - acc:         0.936900 loss:        0.313970
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001391
Test - acc:         0.935800 loss:        0.315292
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.001297
Test - acc:         0.937500 loss:        0.310900
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.001568
Test - acc:         0.935800 loss:        0.311937
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001550
Test - acc:         0.935600 loss:        0.311454
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.001244
Test - acc:         0.936400 loss:        0.313653
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.001434
Test - acc:         0.937000 loss:        0.313294
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001150
Test - acc:         0.936600 loss:        0.312873
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001150
Test - acc:         0.936900 loss:        0.314553
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.001360
Test - acc:         0.937100 loss:        0.307821
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.001358
Test - acc:         0.936400 loss:        0.311673
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001245
Test - acc:         0.936400 loss:        0.308244
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001186
Test - acc:         0.937400 loss:        0.309132
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.001268
Test - acc:         0.936700 loss:        0.308219
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.001150
Test - acc:         0.937400 loss:        0.308063
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.001169
Test - acc:         0.938300 loss:        0.308289
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.001052
Test - acc:         0.936500 loss:        0.311153
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001190
Test - acc:         0.937200 loss:        0.310335
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.001399
Test - acc:         0.937100 loss:        0.310291
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001217
Test - acc:         0.937700 loss:        0.308163
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001093
Test - acc:         0.938500 loss:        0.307085
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.001234
Test - acc:         0.938400 loss:        0.310918
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.001069
Test - acc:         0.937500 loss:        0.312873
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001332
Test - acc:         0.936700 loss:        0.313350
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.001218
Test - acc:         0.937100 loss:        0.313041
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.001161
Test - acc:         0.937700 loss:        0.310504
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.001196
Test - acc:         0.937600 loss:        0.311711
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001278
Test - acc:         0.936900 loss:        0.314341
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001352
Test - acc:         0.937200 loss:        0.315431
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.001105
Test - acc:         0.937200 loss:        0.315314
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.001180
Test - acc:         0.936000 loss:        0.314404
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.001180
Test - acc:         0.935800 loss:        0.315122
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.001329
Test - acc:         0.935600 loss:        0.321141
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.001062
Test - acc:         0.935500 loss:        0.320384
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.001070
Test - acc:         0.935000 loss:        0.323982
Sparsity :          0.7500
Wdecay :        0.000500
