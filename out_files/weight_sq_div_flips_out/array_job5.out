Running --model resnet18 --noise --prune_criterion weight_squared_div_flips --seed 42 --prune_freq 32 --prune_rate 0.5 --comment=resnet18_crit=weight_squared_div_flips_pf=32_seed=42 --save_model=pre-finetune/resnet18_weight_squared_div_flips_pf32_s42 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_squared_div_flips_pf32_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11164352
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.319760 loss:        2.027701
Test - acc:         0.373600 loss:        1.709656
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.483520 loss:        1.421054
Test - acc:         0.526700 loss:        1.263736
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.596320 loss:        1.129661
Test - acc:         0.595600 loss:        1.171379
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.673280 loss:        0.927933
Test - acc:         0.610100 loss:        1.134739
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.723460 loss:        0.790148
Test - acc:         0.705200 loss:        0.872062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.766400 loss:        0.675023
Test - acc:         0.746900 loss:        0.720768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.790000 loss:        0.608257
Test - acc:         0.766300 loss:        0.692052
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.805140 loss:        0.568890
Test - acc:         0.777000 loss:        0.661023
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.812440 loss:        0.540974
Test - acc:         0.748500 loss:        0.765813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.823580 loss:        0.513555
Test - acc:         0.742700 loss:        0.756514
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.831620 loss:        0.490783
Test - acc:         0.778800 loss:        0.671311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.834560 loss:        0.478143
Test - acc:         0.793000 loss:        0.626211
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.841480 loss:        0.462775
Test - acc:         0.792800 loss:        0.604948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838760 loss:        0.468699
Test - acc:         0.789100 loss:        0.624176
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844780 loss:        0.450396
Test - acc:         0.816100 loss:        0.559841
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.846280 loss:        0.448807
Test - acc:         0.764600 loss:        0.719807
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.850540 loss:        0.438978
Test - acc:         0.818300 loss:        0.530640
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.850840 loss:        0.433470
Test - acc:         0.796600 loss:        0.604748
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.853620 loss:        0.425959
Test - acc:         0.841700 loss:        0.488544
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.854620 loss:        0.421329
Test - acc:         0.795500 loss:        0.623324
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.859220 loss:        0.417579
Test - acc:         0.823000 loss:        0.516899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.856440 loss:        0.420470
Test - acc:         0.758600 loss:        0.712926
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.858640 loss:        0.411732
Test - acc:         0.793800 loss:        0.625225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.858100 loss:        0.413444
Test - acc:         0.793100 loss:        0.628817
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.862000 loss:        0.407803
Test - acc:         0.819100 loss:        0.573117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.861660 loss:        0.407108
Test - acc:         0.805500 loss:        0.599522
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.864180 loss:        0.397832
Test - acc:         0.836000 loss:        0.468893
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.863120 loss:        0.402166
Test - acc:         0.833700 loss:        0.486745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.863140 loss:        0.396906
Test - acc:         0.855500 loss:        0.423829
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.867600 loss:        0.394886
Test - acc:         0.810100 loss:        0.579276
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.865620 loss:        0.394207
Test - acc:         0.853400 loss:        0.434255
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.866820 loss:        0.393427
Test - acc:         0.698200 loss:        1.116456
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.877420 loss:        0.362634
Test - acc:         0.846800 loss:        0.455617
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.873380 loss:        0.368564
Test - acc:         0.831700 loss:        0.507364
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.875440 loss:        0.365068
Test - acc:         0.821000 loss:        0.565046
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.874400 loss:        0.366790
Test - acc:         0.834400 loss:        0.508456
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.870940 loss:        0.377726
Test - acc:         0.825600 loss:        0.533083
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.873120 loss:        0.371148
Test - acc:         0.807100 loss:        0.611533
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.871260 loss:        0.374071
Test - acc:         0.819700 loss:        0.560501
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.874880 loss:        0.370255
Test - acc:         0.815900 loss:        0.573783
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.874040 loss:        0.369530
Test - acc:         0.824900 loss:        0.524725
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.875180 loss:        0.365613
Test - acc:         0.789000 loss:        0.690826
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.874620 loss:        0.366430
Test - acc:         0.826100 loss:        0.528012
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.874460 loss:        0.364981
Test - acc:         0.817800 loss:        0.558633
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.874200 loss:        0.369775
Test - acc:         0.835700 loss:        0.490388
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.872800 loss:        0.369942
Test - acc:         0.839100 loss:        0.502220
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.876180 loss:        0.364538
Test - acc:         0.793500 loss:        0.687555
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.873900 loss:        0.370998
Test - acc:         0.813600 loss:        0.558575
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.877480 loss:        0.361059
Test - acc:         0.803400 loss:        0.622095
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.873920 loss:        0.370344
Test - acc:         0.840200 loss:        0.488977
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.874200 loss:        0.365064
Test - acc:         0.818800 loss:        0.559513
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.873960 loss:        0.370267
Test - acc:         0.839100 loss:        0.504473
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.874620 loss:        0.366692
Test - acc:         0.859000 loss:        0.427885
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.878600 loss:        0.355871
Test - acc:         0.827900 loss:        0.524577
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.876020 loss:        0.361625
Test - acc:         0.834100 loss:        0.496292
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.877700 loss:        0.361509
Test - acc:         0.827000 loss:        0.535045
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.875140 loss:        0.365934
Test - acc:         0.858100 loss:        0.430024
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.875560 loss:        0.362119
Test - acc:         0.826900 loss:        0.501709
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.875020 loss:        0.365657
Test - acc:         0.819200 loss:        0.551177
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.875520 loss:        0.367509
Test - acc:         0.814000 loss:        0.565094
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.877200 loss:        0.361246
Test - acc:         0.832800 loss:        0.520728
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.874500 loss:        0.367684
Test - acc:         0.818500 loss:        0.554735
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.875020 loss:        0.364442
Test - acc:         0.842900 loss:        0.462346
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.875260 loss:        0.363474
Test - acc:         0.821800 loss:        0.564639
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.887480 loss:        0.334747
Test - acc:         0.840500 loss:        0.481062
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.884740 loss:        0.335065
Test - acc:         0.834100 loss:        0.526816
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.882440 loss:        0.341703
Test - acc:         0.848100 loss:        0.443718
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.879560 loss:        0.353026
Test - acc:         0.844500 loss:        0.468907
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.882180 loss:        0.344364
Test - acc:         0.837500 loss:        0.504801
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.882700 loss:        0.345385
Test - acc:         0.848700 loss:        0.458992
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.880880 loss:        0.345944
Test - acc:         0.833300 loss:        0.494252
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.881640 loss:        0.344924
Test - acc:         0.833000 loss:        0.491287
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.882540 loss:        0.343212
Test - acc:         0.849000 loss:        0.456414
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.881860 loss:        0.346649
Test - acc:         0.828200 loss:        0.518524
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.883500 loss:        0.341569
Test - acc:         0.841300 loss:        0.486350
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.880820 loss:        0.344874
Test - acc:         0.839700 loss:        0.496813
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.883080 loss:        0.344984
Test - acc:         0.793000 loss:        0.653494
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.881140 loss:        0.343952
Test - acc:         0.849400 loss:        0.440071
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.882080 loss:        0.346001
Test - acc:         0.839200 loss:        0.472797
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.884200 loss:        0.340159
Test - acc:         0.858000 loss:        0.421148
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.882780 loss:        0.345415
Test - acc:         0.828300 loss:        0.518068
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.883240 loss:        0.341143
Test - acc:         0.857700 loss:        0.434466
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.882980 loss:        0.342626
Test - acc:         0.840500 loss:        0.484577
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.884340 loss:        0.338561
Test - acc:         0.818300 loss:        0.609494
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.883780 loss:        0.340662
Test - acc:         0.820400 loss:        0.558780
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.878000 loss:        0.348665
Test - acc:         0.852700 loss:        0.437925
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.880700 loss:        0.345679
Test - acc:         0.862100 loss:        0.420077
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.884320 loss:        0.341145
Test - acc:         0.825100 loss:        0.524766
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.881040 loss:        0.348390
Test - acc:         0.825100 loss:        0.537497
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.884760 loss:        0.339962
Test - acc:         0.829600 loss:        0.510337
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.879920 loss:        0.348563
Test - acc:         0.833600 loss:        0.510298
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.884000 loss:        0.341393
Test - acc:         0.854900 loss:        0.441917
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.883240 loss:        0.339663
Test - acc:         0.845500 loss:        0.472914
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.881380 loss:        0.347052
Test - acc:         0.845900 loss:        0.471103
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.881660 loss:        0.344162
Test - acc:         0.838100 loss:        0.501323
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.880700 loss:        0.345603
Test - acc:         0.829200 loss:        0.511725
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.894100 loss:        0.308410
Test - acc:         0.843700 loss:        0.501370
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.894960 loss:        0.308509
Test - acc:         0.829500 loss:        0.533894
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.888920 loss:        0.319929
Test - acc:         0.851500 loss:        0.443133
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.890200 loss:        0.321238
Test - acc:         0.854300 loss:        0.443512
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.891720 loss:        0.317360
Test - acc:         0.841800 loss:        0.490858
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.890520 loss:        0.319533
Test - acc:         0.858500 loss:        0.419634
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.888240 loss:        0.324437
Test - acc:         0.845300 loss:        0.469710
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.889540 loss:        0.320832
Test - acc:         0.853600 loss:        0.444157
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.889860 loss:        0.321139
Test - acc:         0.853400 loss:        0.438529
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.887060 loss:        0.326638
Test - acc:         0.863500 loss:        0.414870
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.890400 loss:        0.319253
Test - acc:         0.853000 loss:        0.441469
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.888420 loss:        0.326744
Test - acc:         0.836800 loss:        0.518239
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.888020 loss:        0.322077
Test - acc:         0.858400 loss:        0.421383
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.890480 loss:        0.321291
Test - acc:         0.852100 loss:        0.467465
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.889600 loss:        0.321910
Test - acc:         0.832300 loss:        0.513801
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.888880 loss:        0.321793
Test - acc:         0.840100 loss:        0.490046
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.887460 loss:        0.328681
Test - acc:         0.851900 loss:        0.464830
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.888060 loss:        0.325332
Test - acc:         0.821700 loss:        0.527811
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.892160 loss:        0.317445
Test - acc:         0.833100 loss:        0.517816
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.887760 loss:        0.326339
Test - acc:         0.863800 loss:        0.402229
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.888920 loss:        0.321953
Test - acc:         0.865300 loss:        0.399509
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.889540 loss:        0.322292
Test - acc:         0.788800 loss:        0.648838
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.890320 loss:        0.317333
Test - acc:         0.807300 loss:        0.617073
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.888800 loss:        0.326752
Test - acc:         0.834200 loss:        0.519875
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.889000 loss:        0.323556
Test - acc:         0.831400 loss:        0.503882
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.889640 loss:        0.320401
Test - acc:         0.805900 loss:        0.591038
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.888540 loss:        0.325806
Test - acc:         0.865700 loss:        0.394020
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.888200 loss:        0.323787
Test - acc:         0.857900 loss:        0.420854
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.889060 loss:        0.322361
Test - acc:         0.848000 loss:        0.460281
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.890740 loss:        0.322678
Test - acc:         0.869100 loss:        0.371790
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.890740 loss:        0.319622
Test - acc:         0.765800 loss:        0.829652
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.888340 loss:        0.323735
Test - acc:         0.786400 loss:        0.677587
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.898980 loss:        0.291008
Test - acc:         0.840600 loss:        0.481258
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.900100 loss:        0.294844
Test - acc:         0.847900 loss:        0.466240
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.896920 loss:        0.298428
Test - acc:         0.821700 loss:        0.547813
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.893920 loss:        0.308491
Test - acc:         0.861500 loss:        0.413799
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.897460 loss:        0.299335
Test - acc:         0.833200 loss:        0.527456
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.894480 loss:        0.307036
Test - acc:         0.849700 loss:        0.455781
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.895160 loss:        0.305442
Test - acc:         0.864500 loss:        0.430326
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.897800 loss:        0.299300
Test - acc:         0.858400 loss:        0.423179
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.894740 loss:        0.305024
Test - acc:         0.836700 loss:        0.519114
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.894700 loss:        0.306183
Test - acc:         0.831100 loss:        0.484096
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.892940 loss:        0.308494
Test - acc:         0.838200 loss:        0.487897
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.894960 loss:        0.305571
Test - acc:         0.865400 loss:        0.407128
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.895600 loss:        0.301355
Test - acc:         0.860100 loss:        0.439051
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.895880 loss:        0.303757
Test - acc:         0.868700 loss:        0.403803
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.895900 loss:        0.301006
Test - acc:         0.855500 loss:        0.437454
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.897200 loss:        0.298355
Test - acc:         0.857100 loss:        0.450455
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.893620 loss:        0.305500
Test - acc:         0.851900 loss:        0.464804
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.895640 loss:        0.305712
Test - acc:         0.873300 loss:        0.389298
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.896140 loss:        0.302707
Test - acc:         0.856900 loss:        0.430370
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.894280 loss:        0.303269
Test - acc:         0.848100 loss:        0.464924
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.895280 loss:        0.301892
Test - acc:         0.864000 loss:        0.421717
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.895520 loss:        0.304919
Test - acc:         0.870200 loss:        0.387716
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.938820 loss:        0.179848
Test - acc:         0.923400 loss:        0.223722
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.954000 loss:        0.139163
Test - acc:         0.926900 loss:        0.215426
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.957500 loss:        0.125459
Test - acc:         0.929100 loss:        0.211468
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.962100 loss:        0.111704
Test - acc:         0.929800 loss:        0.216424
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.963600 loss:        0.107824
Test - acc:         0.929700 loss:        0.212060
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.967740 loss:        0.097901
Test - acc:         0.931700 loss:        0.211815
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.969320 loss:        0.091867
Test - acc:         0.931600 loss:        0.212720
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.970720 loss:        0.087891
Test - acc:         0.931200 loss:        0.214456
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.972600 loss:        0.082108
Test - acc:         0.931100 loss:        0.220833
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.975340 loss:        0.075850
Test - acc:         0.933100 loss:        0.220175
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.974080 loss:        0.077420
Test - acc:         0.934500 loss:        0.213040
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.975560 loss:        0.072697
Test - acc:         0.932000 loss:        0.218792
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.977120 loss:        0.068797
Test - acc:         0.932700 loss:        0.226648
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.977300 loss:        0.068217
Test - acc:         0.929700 loss:        0.226949
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.978560 loss:        0.062734
Test - acc:         0.932500 loss:        0.219793
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.978080 loss:        0.065079
Test - acc:         0.933400 loss:        0.218149
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.978580 loss:        0.061978
Test - acc:         0.931600 loss:        0.228241
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.980100 loss:        0.060586
Test - acc:         0.932300 loss:        0.229040
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.979960 loss:        0.059565
Test - acc:         0.930000 loss:        0.231471
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.980400 loss:        0.060093
Test - acc:         0.930700 loss:        0.236020
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.981120 loss:        0.058104
Test - acc:         0.933200 loss:        0.223671
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.980220 loss:        0.059153
Test - acc:         0.930700 loss:        0.232179
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.981220 loss:        0.057056
Test - acc:         0.927500 loss:        0.250135
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.058140
Test - acc:         0.930000 loss:        0.241829
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.979960 loss:        0.058840
Test - acc:         0.927600 loss:        0.248343
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.980240 loss:        0.057828
Test - acc:         0.930200 loss:        0.243393
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.980520 loss:        0.058837
Test - acc:         0.925000 loss:        0.265220
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.980240 loss:        0.057514
Test - acc:         0.928600 loss:        0.242548
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.981660 loss:        0.055552
Test - acc:         0.925400 loss:        0.257415
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.980040 loss:        0.059147
Test - acc:         0.926900 loss:        0.246311
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.980920 loss:        0.057106
Test - acc:         0.922900 loss:        0.270292
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.978700 loss:        0.063717
Test - acc:         0.926300 loss:        0.253253
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.978340 loss:        0.063477
Test - acc:         0.925600 loss:        0.269561
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.979240 loss:        0.062421
Test - acc:         0.923300 loss:        0.260054
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.979720 loss:        0.060783
Test - acc:         0.929400 loss:        0.248647
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.979300 loss:        0.061596
Test - acc:         0.926100 loss:        0.254139
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.977500 loss:        0.065191
Test - acc:         0.923300 loss:        0.263813
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.978680 loss:        0.062254
Test - acc:         0.922100 loss:        0.274583
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.976740 loss:        0.066504
Test - acc:         0.920200 loss:        0.273321
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.977720 loss:        0.067214
Test - acc:         0.922300 loss:        0.267104
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.976800 loss:        0.068351
Test - acc:         0.924100 loss:        0.260969
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.977540 loss:        0.065779
Test - acc:         0.915700 loss:        0.296739
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.967900 loss:        0.093855
Test - acc:         0.920700 loss:        0.266780
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.973960 loss:        0.075599
Test - acc:         0.924600 loss:        0.257709
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.973520 loss:        0.076676
Test - acc:         0.922800 loss:        0.263244
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.973280 loss:        0.078837
Test - acc:         0.920600 loss:        0.271768
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.974200 loss:        0.076015
Test - acc:         0.924400 loss:        0.259224
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.976280 loss:        0.072495
Test - acc:         0.921500 loss:        0.273499
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.975460 loss:        0.073275
Test - acc:         0.918200 loss:        0.289934
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.974940 loss:        0.073300
Test - acc:         0.922700 loss:        0.269441
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.975760 loss:        0.071822
Test - acc:         0.918200 loss:        0.282762
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.975840 loss:        0.071136
Test - acc:         0.921000 loss:        0.279505
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.975540 loss:        0.072079
Test - acc:         0.920600 loss:        0.282652
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.974720 loss:        0.073947
Test - acc:         0.915300 loss:        0.287721
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.975120 loss:        0.072621
Test - acc:         0.918600 loss:        0.277153
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.975660 loss:        0.071556
Test - acc:         0.915500 loss:        0.277250
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.975300 loss:        0.073483
Test - acc:         0.918000 loss:        0.284387
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.975560 loss:        0.072546
Test - acc:         0.921600 loss:        0.270470
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.974600 loss:        0.074891
Test - acc:         0.922200 loss:        0.274867
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.974800 loss:        0.074316
Test - acc:         0.920500 loss:        0.265683
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.974340 loss:        0.074782
Test - acc:         0.918900 loss:        0.279840
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.975400 loss:        0.073374
Test - acc:         0.917800 loss:        0.284998
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.975340 loss:        0.072990
Test - acc:         0.920600 loss:        0.271151
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.974980 loss:        0.073426
Test - acc:         0.915500 loss:        0.295578
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.973500 loss:        0.076932
Test - acc:         0.914900 loss:        0.294386
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.974640 loss:        0.073566
Test - acc:         0.914500 loss:        0.297511
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.974000 loss:        0.077120
Test - acc:         0.919000 loss:        0.277121
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.973080 loss:        0.075870
Test - acc:         0.911800 loss:        0.310042
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.975820 loss:        0.073384
Test - acc:         0.920500 loss:        0.276857
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.974360 loss:        0.074843
Test - acc:         0.921100 loss:        0.266661
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.975520 loss:        0.072415
Test - acc:         0.920300 loss:        0.276329
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.974720 loss:        0.072542
Test - acc:         0.916000 loss:        0.286751
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.975060 loss:        0.072906
Test - acc:         0.916100 loss:        0.296836
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.973100 loss:        0.076708
Test - acc:         0.920600 loss:        0.279614
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.948760 loss:        0.146774
Test - acc:         0.909600 loss:        0.286651
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.957220 loss:        0.123969
Test - acc:         0.914500 loss:        0.281930
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.960380 loss:        0.118272
Test - acc:         0.898300 loss:        0.341412
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.958040 loss:        0.120570
Test - acc:         0.912800 loss:        0.282005
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.959620 loss:        0.115263
Test - acc:         0.908800 loss:        0.296518
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.960500 loss:        0.112885
Test - acc:         0.900600 loss:        0.317952
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.962720 loss:        0.108476
Test - acc:         0.912700 loss:        0.281962
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.963880 loss:        0.104587
Test - acc:         0.909500 loss:        0.298708
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.962180 loss:        0.109170
Test - acc:         0.913500 loss:        0.275844
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.960800 loss:        0.112520
Test - acc:         0.914500 loss:        0.286130
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.964100 loss:        0.103803
Test - acc:         0.913400 loss:        0.291704
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.962160 loss:        0.109420
Test - acc:         0.911300 loss:        0.289744
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.962580 loss:        0.107135
Test - acc:         0.916000 loss:        0.280302
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.963840 loss:        0.105292
Test - acc:         0.908300 loss:        0.291917
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.963960 loss:        0.103571
Test - acc:         0.910200 loss:        0.292578
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.965020 loss:        0.102792
Test - acc:         0.912500 loss:        0.290219
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.964560 loss:        0.104181
Test - acc:         0.911700 loss:        0.305823
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.963780 loss:        0.105979
Test - acc:         0.908300 loss:        0.307894
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.963960 loss:        0.104045
Test - acc:         0.911200 loss:        0.299869
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.963760 loss:        0.104618
Test - acc:         0.913700 loss:        0.282754
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.964500 loss:        0.100427
Test - acc:         0.913400 loss:        0.282156
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.963520 loss:        0.102816
Test - acc:         0.908400 loss:        0.313006
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.965160 loss:        0.100278
Test - acc:         0.917100 loss:        0.282181
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.965340 loss:        0.100762
Test - acc:         0.910300 loss:        0.302775
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.964340 loss:        0.103368
Test - acc:         0.919600 loss:        0.274008
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.964640 loss:        0.101524
Test - acc:         0.917000 loss:        0.283854
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.976300 loss:        0.071883
Test - acc:         0.926900 loss:        0.239423
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.981640 loss:        0.059227
Test - acc:         0.930500 loss:        0.234054
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.983400 loss:        0.054571
Test - acc:         0.930300 loss:        0.233932
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.983660 loss:        0.052707
Test - acc:         0.930500 loss:        0.233758
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.985300 loss:        0.049765
Test - acc:         0.931500 loss:        0.233128
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.986740 loss:        0.045718
Test - acc:         0.931500 loss:        0.233871
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.920980 loss:        0.230341
Test - acc:         0.901100 loss:        0.306120
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.935860 loss:        0.188848
Test - acc:         0.904000 loss:        0.291035
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.942680 loss:        0.169347
Test - acc:         0.907300 loss:        0.285171
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.945620 loss:        0.158915
Test - acc:         0.910100 loss:        0.279630
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.946960 loss:        0.154853
Test - acc:         0.912100 loss:        0.273003
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.948580 loss:        0.150512
Test - acc:         0.913500 loss:        0.271434
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.951440 loss:        0.143977
Test - acc:         0.913900 loss:        0.265573
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.952080 loss:        0.139834
Test - acc:         0.912400 loss:        0.271101
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.953740 loss:        0.137572
Test - acc:         0.913000 loss:        0.269663
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.953600 loss:        0.136412
Test - acc:         0.914200 loss:        0.268018
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.954080 loss:        0.133531
Test - acc:         0.915000 loss:        0.266381
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.957140 loss:        0.127931
Test - acc:         0.916000 loss:        0.265757
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.957200 loss:        0.125924
Test - acc:         0.914900 loss:        0.267840
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.956880 loss:        0.124720
Test - acc:         0.916400 loss:        0.265847
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.960120 loss:        0.119954
Test - acc:         0.914900 loss:        0.268074
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.959560 loss:        0.121097
Test - acc:         0.915400 loss:        0.266971
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.959580 loss:        0.119428
Test - acc:         0.914300 loss:        0.268749
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.960160 loss:        0.117514
Test - acc:         0.915200 loss:        0.266224
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.961820 loss:        0.114834
Test - acc:         0.918100 loss:        0.264124
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.961420 loss:        0.115442
Test - acc:         0.915200 loss:        0.267376
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.960380 loss:        0.112845
Test - acc:         0.916500 loss:        0.264811
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.961320 loss:        0.115357
Test - acc:         0.917400 loss:        0.266250
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.962740 loss:        0.111108
Test - acc:         0.916600 loss:        0.266261
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.962940 loss:        0.110940
Test - acc:         0.920500 loss:        0.261202
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.963080 loss:        0.109087
Test - acc:         0.918300 loss:        0.263564
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.962740 loss:        0.110098
Test - acc:         0.917300 loss:        0.265294
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.964640 loss:        0.106537
Test - acc:         0.918300 loss:        0.268688
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.964480 loss:        0.105519
Test - acc:         0.917800 loss:        0.268184
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.965020 loss:        0.104369
Test - acc:         0.921000 loss:        0.270441
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.963860 loss:        0.107901
Test - acc:         0.916600 loss:        0.270841
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.964020 loss:        0.104781
Test - acc:         0.918000 loss:        0.266153
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.964100 loss:        0.105983
Test - acc:         0.918500 loss:        0.269034
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.800800 loss:        0.586428
Test - acc:         0.831600 loss:        0.505288
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.848400 loss:        0.443685
Test - acc:         0.847200 loss:        0.460687
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.862320 loss:        0.402148
Test - acc:         0.857400 loss:        0.431500
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.870000 loss:        0.378338
Test - acc:         0.860500 loss:        0.412472
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.873000 loss:        0.366714
Test - acc:         0.864100 loss:        0.403552
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.878200 loss:        0.357384
Test - acc:         0.866800 loss:        0.394669
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.880920 loss:        0.348476
Test - acc:         0.868800 loss:        0.386873
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.884140 loss:        0.337506
Test - acc:         0.873100 loss:        0.380808
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.884660 loss:        0.330015
Test - acc:         0.872300 loss:        0.383029
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.888560 loss:        0.322509
Test - acc:         0.874500 loss:        0.372347
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.889300 loss:        0.324376
Test - acc:         0.876800 loss:        0.373030
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.889940 loss:        0.320063
Test - acc:         0.876500 loss:        0.371224
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.893240 loss:        0.311554
Test - acc:         0.879500 loss:        0.366485
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.892560 loss:        0.307664
Test - acc:         0.877200 loss:        0.365592
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.895240 loss:        0.305238
Test - acc:         0.881500 loss:        0.362449
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.895060 loss:        0.300873
Test - acc:         0.880900 loss:        0.362340
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.897640 loss:        0.301569
Test - acc:         0.878700 loss:        0.367612
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.897380 loss:        0.297696
Test - acc:         0.879200 loss:        0.365056
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.897900 loss:        0.296155
Test - acc:         0.881000 loss:        0.361542
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.897660 loss:        0.295936
Test - acc:         0.880600 loss:        0.359288
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.899900 loss:        0.291848
Test - acc:         0.880800 loss:        0.353869
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.899780 loss:        0.290255
Test - acc:         0.885200 loss:        0.353055
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.899460 loss:        0.289358
Test - acc:         0.882900 loss:        0.355610
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.901400 loss:        0.284120
Test - acc:         0.883900 loss:        0.357548
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.902240 loss:        0.284373
Test - acc:         0.885100 loss:        0.353551
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.902560 loss:        0.282351
Test - acc:         0.885200 loss:        0.355152
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.902400 loss:        0.285067
Test - acc:         0.885200 loss:        0.353643
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.903440 loss:        0.281569
Test - acc:         0.886200 loss:        0.351025
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.903800 loss:        0.280047
Test - acc:         0.885100 loss:        0.353903
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.904400 loss:        0.276009
Test - acc:         0.886500 loss:        0.352033
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.904420 loss:        0.277309
Test - acc:         0.885800 loss:        0.352673
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.904440 loss:        0.278344
Test - acc:         0.884100 loss:        0.351904
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.695220 loss:        0.875278
Test - acc:         0.753800 loss:        0.719617
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.758700 loss:        0.711104
Test - acc:         0.772400 loss:        0.659481
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.770720 loss:        0.665790
Test - acc:         0.785900 loss:        0.622430
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.780980 loss:        0.635841
Test - acc:         0.792900 loss:        0.604413
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.788340 loss:        0.619507
Test - acc:         0.795300 loss:        0.592048
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.792500 loss:        0.606270
Test - acc:         0.796500 loss:        0.583828
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.797120 loss:        0.592841
Test - acc:         0.801800 loss:        0.575358
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.799660 loss:        0.582743
Test - acc:         0.799400 loss:        0.568408
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.801000 loss:        0.574959
Test - acc:         0.806900 loss:        0.560736
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.804660 loss:        0.570514
Test - acc:         0.808200 loss:        0.553880
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.806860 loss:        0.561887
Test - acc:         0.806300 loss:        0.558049
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.810780 loss:        0.558616
Test - acc:         0.807300 loss:        0.558556
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.809280 loss:        0.553812
Test - acc:         0.812100 loss:        0.550639
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.811000 loss:        0.549453
Test - acc:         0.809200 loss:        0.544700
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.812580 loss:        0.545739
Test - acc:         0.814800 loss:        0.534055
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.814000 loss:        0.542909
Test - acc:         0.813400 loss:        0.536220
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.814560 loss:        0.539952
Test - acc:         0.816800 loss:        0.531775
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.815380 loss:        0.533593
Test - acc:         0.815600 loss:        0.526682
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.814580 loss:        0.533025
Test - acc:         0.815600 loss:        0.529900
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.819360 loss:        0.526898
Test - acc:         0.817300 loss:        0.525169
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.819580 loss:        0.526930
Test - acc:         0.814200 loss:        0.529476
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.818220 loss:        0.528885
Test - acc:         0.819900 loss:        0.523691
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.819840 loss:        0.527782
Test - acc:         0.817100 loss:        0.520757
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.819560 loss:        0.525755
Test - acc:         0.821200 loss:        0.514733
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.820940 loss:        0.518650
Test - acc:         0.817600 loss:        0.519997
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.821940 loss:        0.517768
Test - acc:         0.821300 loss:        0.516686
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.824100 loss:        0.514881
Test - acc:         0.821500 loss:        0.511178
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.824540 loss:        0.514649
Test - acc:         0.822800 loss:        0.518217
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.823840 loss:        0.511888
Test - acc:         0.821700 loss:        0.510082
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.824420 loss:        0.511329
Test - acc:         0.823900 loss:        0.515233
Sparsity :          0.9990
Wdecay :        0.000500
