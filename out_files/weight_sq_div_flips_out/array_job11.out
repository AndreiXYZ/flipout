Running --model resnet18 --noise --prune_criterion weight_squared_div_flips --seed 44 --prune_freq 117 --prune_rate 0.5 --comment=resnet18_crit=weight_squared_div_flips_pf=117_seed=44 --save_model=pre-finetune/resnet18_weight_squared_div_flips_pf117_s44 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 117,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_squared_div_flips_pf117_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11164352
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.303340 loss:        2.045872
Test - acc:         0.449200 loss:        1.518474
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.486940 loss:        1.408749
Test - acc:         0.553600 loss:        1.216369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.595040 loss:        1.130263
Test - acc:         0.627400 loss:        1.061316
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.662820 loss:        0.954220
Test - acc:         0.666000 loss:        0.963036
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.710980 loss:        0.823103
Test - acc:         0.700400 loss:        0.900422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.754080 loss:        0.701555
Test - acc:         0.719300 loss:        0.843699
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.784640 loss:        0.626890
Test - acc:         0.762900 loss:        0.702303
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.800120 loss:        0.580976
Test - acc:         0.678300 loss:        1.056559
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.809040 loss:        0.553956
Test - acc:         0.750400 loss:        0.725537
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.817400 loss:        0.527065
Test - acc:         0.750100 loss:        0.776029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.828080 loss:        0.504549
Test - acc:         0.806100 loss:        0.560775
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.827040 loss:        0.498283
Test - acc:         0.787800 loss:        0.637411
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.835700 loss:        0.482435
Test - acc:         0.794000 loss:        0.615528
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838900 loss:        0.468737
Test - acc:         0.820400 loss:        0.537681
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844200 loss:        0.459359
Test - acc:         0.804800 loss:        0.583139
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845740 loss:        0.448696
Test - acc:         0.757400 loss:        0.755301
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.845840 loss:        0.444416
Test - acc:         0.784900 loss:        0.670693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.849560 loss:        0.437153
Test - acc:         0.836300 loss:        0.472858
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.852000 loss:        0.431503
Test - acc:         0.805300 loss:        0.564962
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.853780 loss:        0.428139
Test - acc:         0.822000 loss:        0.534250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.417603
Test - acc:         0.776100 loss:        0.709650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.857220 loss:        0.416609
Test - acc:         0.822800 loss:        0.534768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.418318
Test - acc:         0.835400 loss:        0.503948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.857700 loss:        0.416371
Test - acc:         0.797500 loss:        0.599861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.861780 loss:        0.406843
Test - acc:         0.789200 loss:        0.638827
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.858420 loss:        0.412091
Test - acc:         0.812200 loss:        0.574223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.861500 loss:        0.402603
Test - acc:         0.834200 loss:        0.487177
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.861100 loss:        0.403876
Test - acc:         0.788300 loss:        0.694953
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.862260 loss:        0.397948
Test - acc:         0.834000 loss:        0.496371
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.861600 loss:        0.404324
Test - acc:         0.767800 loss:        0.741355
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.863460 loss:        0.399084
Test - acc:         0.813800 loss:        0.548094
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.862380 loss:        0.399397
Test - acc:         0.821100 loss:        0.548047
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.866720 loss:        0.392388
Test - acc:         0.831100 loss:        0.522133
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.863960 loss:        0.395103
Test - acc:         0.791300 loss:        0.647549
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.865880 loss:        0.393367
Test - acc:         0.748300 loss:        0.861032
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.867420 loss:        0.392783
Test - acc:         0.793100 loss:        0.629516
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.867160 loss:        0.387712
Test - acc:         0.855000 loss:        0.438008
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.387555
Test - acc:         0.817900 loss:        0.557149
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.867240 loss:        0.389858
Test - acc:         0.841200 loss:        0.478896
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.869000 loss:        0.387413
Test - acc:         0.832700 loss:        0.490588
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.864980 loss:        0.394477
Test - acc:         0.841800 loss:        0.463607
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.869720 loss:        0.383677
Test - acc:         0.843400 loss:        0.466556
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.870200 loss:        0.381420
Test - acc:         0.781100 loss:        0.700399
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.869540 loss:        0.383390
Test - acc:         0.801500 loss:        0.623825
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.870180 loss:        0.382817
Test - acc:         0.783300 loss:        0.685600
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.870820 loss:        0.376388
Test - acc:         0.813000 loss:        0.573551
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.869700 loss:        0.380625
Test - acc:         0.832900 loss:        0.502137
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.869460 loss:        0.382523
Test - acc:         0.762700 loss:        0.762488
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.869520 loss:        0.381779
Test - acc:         0.829900 loss:        0.524733
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.867960 loss:        0.379790
Test - acc:         0.834400 loss:        0.510441
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.872280 loss:        0.373012
Test - acc:         0.811000 loss:        0.582797
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.873420 loss:        0.372036
Test - acc:         0.843100 loss:        0.475100
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.870400 loss:        0.378922
Test - acc:         0.856800 loss:        0.411110
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.874820 loss:        0.371978
Test - acc:         0.797100 loss:        0.629058
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.870640 loss:        0.378161
Test - acc:         0.767800 loss:        0.727634
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.872540 loss:        0.372711
Test - acc:         0.797500 loss:        0.611661
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.871460 loss:        0.373897
Test - acc:         0.854300 loss:        0.436976
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.872720 loss:        0.374363
Test - acc:         0.833100 loss:        0.507876
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.872780 loss:        0.371153
Test - acc:         0.867000 loss:        0.394288
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.874740 loss:        0.367565
Test - acc:         0.849200 loss:        0.475920
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.874780 loss:        0.369434
Test - acc:         0.845800 loss:        0.448344
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.871860 loss:        0.376186
Test - acc:         0.781900 loss:        0.695914
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.873440 loss:        0.371171
Test - acc:         0.834900 loss:        0.500511
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.871740 loss:        0.371933
Test - acc:         0.852100 loss:        0.444918
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.871780 loss:        0.371990
Test - acc:         0.841500 loss:        0.489206
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.873980 loss:        0.370594
Test - acc:         0.855200 loss:        0.438623
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.872280 loss:        0.372062
Test - acc:         0.828900 loss:        0.518650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.873000 loss:        0.370841
Test - acc:         0.805500 loss:        0.613012
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.874360 loss:        0.368795
Test - acc:         0.826500 loss:        0.525628
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.877540 loss:        0.359595
Test - acc:         0.854300 loss:        0.434711
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.874280 loss:        0.367340
Test - acc:         0.835400 loss:        0.502133
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.869000 loss:        0.378338
Test - acc:         0.826300 loss:        0.545173
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.873220 loss:        0.370593
Test - acc:         0.835900 loss:        0.487525
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.874460 loss:        0.366382
Test - acc:         0.800900 loss:        0.632965
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.873380 loss:        0.370189
Test - acc:         0.821100 loss:        0.583991
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.878020 loss:        0.361024
Test - acc:         0.850600 loss:        0.461314
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.878060 loss:        0.360828
Test - acc:         0.827400 loss:        0.521085
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.874360 loss:        0.368305
Test - acc:         0.777500 loss:        0.671557
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.875820 loss:        0.365397
Test - acc:         0.774800 loss:        0.758583
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.874240 loss:        0.367884
Test - acc:         0.844000 loss:        0.473741
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.877000 loss:        0.365813
Test - acc:         0.838900 loss:        0.479518
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.873900 loss:        0.369830
Test - acc:         0.798500 loss:        0.653937
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.873300 loss:        0.369927
Test - acc:         0.855600 loss:        0.433112
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.874540 loss:        0.371067
Test - acc:         0.807400 loss:        0.583515
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.875160 loss:        0.369557
Test - acc:         0.817700 loss:        0.560351
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.877100 loss:        0.363132
Test - acc:         0.853400 loss:        0.439046
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.874980 loss:        0.361904
Test - acc:         0.828800 loss:        0.528302
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.877260 loss:        0.363573
Test - acc:         0.839200 loss:        0.502353
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.876540 loss:        0.363945
Test - acc:         0.729200 loss:        0.967693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.873780 loss:        0.369268
Test - acc:         0.841300 loss:        0.497874
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.877060 loss:        0.364654
Test - acc:         0.837300 loss:        0.489758
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.873580 loss:        0.369464
Test - acc:         0.841500 loss:        0.478789
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.875720 loss:        0.363625
Test - acc:         0.818600 loss:        0.561999
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.878360 loss:        0.360600
Test - acc:         0.861600 loss:        0.403242
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.874600 loss:        0.368205
Test - acc:         0.820900 loss:        0.537126
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.874360 loss:        0.368110
Test - acc:         0.846500 loss:        0.462511
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.877940 loss:        0.360022
Test - acc:         0.840300 loss:        0.497762
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.874320 loss:        0.366885
Test - acc:         0.806200 loss:        0.608950
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.877240 loss:        0.362035
Test - acc:         0.825400 loss:        0.523248
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.877100 loss:        0.362948
Test - acc:         0.818200 loss:        0.560487
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.876200 loss:        0.364892
Test - acc:         0.840900 loss:        0.494074
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.875220 loss:        0.367843
Test - acc:         0.810600 loss:        0.588674
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.879140 loss:        0.359609
Test - acc:         0.828000 loss:        0.538197
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.872880 loss:        0.371898
Test - acc:         0.849500 loss:        0.461650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.874720 loss:        0.364760
Test - acc:         0.811900 loss:        0.594411
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.874860 loss:        0.366808
Test - acc:         0.815400 loss:        0.600375
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.873640 loss:        0.366182
Test - acc:         0.837400 loss:        0.480663
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.874940 loss:        0.365754
Test - acc:         0.801600 loss:        0.608958
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.877820 loss:        0.356576
Test - acc:         0.839500 loss:        0.487813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.877060 loss:        0.358896
Test - acc:         0.846100 loss:        0.471395
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.877620 loss:        0.361139
Test - acc:         0.836500 loss:        0.492408
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.876640 loss:        0.367713
Test - acc:         0.847200 loss:        0.456855
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.874840 loss:        0.367901
Test - acc:         0.771000 loss:        0.774561
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.875620 loss:        0.365004
Test - acc:         0.829700 loss:        0.514313
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.876540 loss:        0.360185
Test - acc:         0.819300 loss:        0.531342
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.878400 loss:        0.359304
Test - acc:         0.827300 loss:        0.532782
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.874060 loss:        0.366304
Test - acc:         0.808200 loss:        0.588574
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.883940 loss:        0.339986
Test - acc:         0.844100 loss:        0.463797
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.884520 loss:        0.338216
Test - acc:         0.850500 loss:        0.442551
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.884980 loss:        0.338752
Test - acc:         0.814500 loss:        0.592603
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.879940 loss:        0.350649
Test - acc:         0.850200 loss:        0.456724
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.881000 loss:        0.348407
Test - acc:         0.839200 loss:        0.487734
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.879360 loss:        0.353110
Test - acc:         0.829000 loss:        0.512261
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.880120 loss:        0.350125
Test - acc:         0.828900 loss:        0.539446
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.882360 loss:        0.348499
Test - acc:         0.833200 loss:        0.521548
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.882040 loss:        0.346194
Test - acc:         0.835500 loss:        0.502150
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.882520 loss:        0.342068
Test - acc:         0.787900 loss:        0.673834
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.880920 loss:        0.350500
Test - acc:         0.841500 loss:        0.482911
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.881700 loss:        0.346106
Test - acc:         0.809100 loss:        0.610474
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.880820 loss:        0.347813
Test - acc:         0.809000 loss:        0.585936
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.881460 loss:        0.347848
Test - acc:         0.823400 loss:        0.558816
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.882920 loss:        0.344266
Test - acc:         0.826700 loss:        0.559464
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.880200 loss:        0.352504
Test - acc:         0.852900 loss:        0.458942
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.881280 loss:        0.352935
Test - acc:         0.834800 loss:        0.509802
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.882420 loss:        0.345381
Test - acc:         0.846700 loss:        0.478654
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.880000 loss:        0.350711
Test - acc:         0.855600 loss:        0.434120
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.881320 loss:        0.349663
Test - acc:         0.825700 loss:        0.538109
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.881680 loss:        0.347350
Test - acc:         0.813800 loss:        0.575994
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.882500 loss:        0.343786
Test - acc:         0.855900 loss:        0.440243
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.881740 loss:        0.349072
Test - acc:         0.833700 loss:        0.502747
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.882240 loss:        0.346778
Test - acc:         0.850500 loss:        0.438358
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.879340 loss:        0.350628
Test - acc:         0.842200 loss:        0.475235
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.882540 loss:        0.344452
Test - acc:         0.846600 loss:        0.449073
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.880380 loss:        0.351155
Test - acc:         0.857000 loss:        0.431800
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.880080 loss:        0.351226
Test - acc:         0.792900 loss:        0.630861
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.881040 loss:        0.349111
Test - acc:         0.791800 loss:        0.682558
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.879680 loss:        0.351544
Test - acc:         0.840500 loss:        0.483799
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.879820 loss:        0.344776
Test - acc:         0.778900 loss:        0.769812
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.879460 loss:        0.350232
Test - acc:         0.851200 loss:        0.461527
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.881260 loss:        0.353960
Test - acc:         0.800400 loss:        0.637751
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.929520 loss:        0.205478
Test - acc:         0.922000 loss:        0.224256
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.947780 loss:        0.156155
Test - acc:         0.925600 loss:        0.214871
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.954160 loss:        0.136244
Test - acc:         0.928200 loss:        0.208511
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.958980 loss:        0.121552
Test - acc:         0.930500 loss:        0.205310
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.963100 loss:        0.112409
Test - acc:         0.930100 loss:        0.205628
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.964140 loss:        0.103110
Test - acc:         0.932600 loss:        0.202623
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.969000 loss:        0.093274
Test - acc:         0.936600 loss:        0.200962
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.970400 loss:        0.089711
Test - acc:         0.932400 loss:        0.205808
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.972000 loss:        0.082312
Test - acc:         0.930000 loss:        0.218965
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.973580 loss:        0.076595
Test - acc:         0.933200 loss:        0.218417
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.974600 loss:        0.074197
Test - acc:         0.930900 loss:        0.217528
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.977020 loss:        0.069824
Test - acc:         0.931600 loss:        0.217694
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.978280 loss:        0.064064
Test - acc:         0.932600 loss:        0.220815
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.976960 loss:        0.067454
Test - acc:         0.930300 loss:        0.235680
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.979660 loss:        0.060816
Test - acc:         0.930400 loss:        0.232567
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.977860 loss:        0.063830
Test - acc:         0.927900 loss:        0.248278
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.980100 loss:        0.058732
Test - acc:         0.931500 loss:        0.231035
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.980760 loss:        0.056852
Test - acc:         0.927100 loss:        0.245991
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.981160 loss:        0.055375
Test - acc:         0.926800 loss:        0.245034
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.981520 loss:        0.055236
Test - acc:         0.930900 loss:        0.248554
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.055680
Test - acc:         0.927200 loss:        0.243051
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.981420 loss:        0.055729
Test - acc:         0.928000 loss:        0.251387
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.981700 loss:        0.053958
Test - acc:         0.924700 loss:        0.256371
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.981180 loss:        0.055732
Test - acc:         0.928400 loss:        0.247348
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.982780 loss:        0.051905
Test - acc:         0.927300 loss:        0.253277
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.980400 loss:        0.058480
Test - acc:         0.922600 loss:        0.265706
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.980040 loss:        0.058737
Test - acc:         0.925600 loss:        0.259527
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.980760 loss:        0.056914
Test - acc:         0.923700 loss:        0.264079
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.982140 loss:        0.054308
Test - acc:         0.924400 loss:        0.263403
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.979420 loss:        0.058939
Test - acc:         0.924800 loss:        0.267548
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.059069
Test - acc:         0.913800 loss:        0.316714
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.979660 loss:        0.061256
Test - acc:         0.927200 loss:        0.247916
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.980440 loss:        0.057237
Test - acc:         0.923100 loss:        0.267270
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.981340 loss:        0.057654
Test - acc:         0.922100 loss:        0.275665
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.978560 loss:        0.063916
Test - acc:         0.925700 loss:        0.264525
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.979520 loss:        0.061968
Test - acc:         0.925600 loss:        0.252448
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.981000 loss:        0.057062
Test - acc:         0.926500 loss:        0.251052
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.980560 loss:        0.058833
Test - acc:         0.913800 loss:        0.305229
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.977660 loss:        0.063241
Test - acc:         0.912400 loss:        0.310292
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.978500 loss:        0.061446
Test - acc:         0.919300 loss:        0.284588
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.978600 loss:        0.062492
Test - acc:         0.919700 loss:        0.277156
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.977900 loss:        0.064529
Test - acc:         0.922200 loss:        0.281844
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.979860 loss:        0.061539
Test - acc:         0.913500 loss:        0.301308
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.978900 loss:        0.063659
Test - acc:         0.926400 loss:        0.253772
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.978620 loss:        0.062787
Test - acc:         0.927900 loss:        0.255997
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.978820 loss:        0.061825
Test - acc:         0.907200 loss:        0.326855
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.976540 loss:        0.067726
Test - acc:         0.922000 loss:        0.271031
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.976220 loss:        0.066852
Test - acc:         0.920800 loss:        0.272655
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.979120 loss:        0.062680
Test - acc:         0.921900 loss:        0.277425
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.977520 loss:        0.067368
Test - acc:         0.917700 loss:        0.279936
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.979520 loss:        0.062401
Test - acc:         0.925200 loss:        0.266232
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.978580 loss:        0.064299
Test - acc:         0.927800 loss:        0.250072
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.977860 loss:        0.066587
Test - acc:         0.921000 loss:        0.281873
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.977620 loss:        0.065040
Test - acc:         0.920900 loss:        0.280478
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.979680 loss:        0.061640
Test - acc:         0.919700 loss:        0.291824
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.978340 loss:        0.063250
Test - acc:         0.923300 loss:        0.272386
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.977460 loss:        0.068075
Test - acc:         0.904400 loss:        0.331601
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.976360 loss:        0.069170
Test - acc:         0.924100 loss:        0.270705
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.976820 loss:        0.068263
Test - acc:         0.920500 loss:        0.261929
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.977820 loss:        0.064687
Test - acc:         0.924700 loss:        0.260581
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.977680 loss:        0.065721
Test - acc:         0.923500 loss:        0.261048
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.977700 loss:        0.066439
Test - acc:         0.922600 loss:        0.270048
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.978900 loss:        0.063242
Test - acc:         0.917500 loss:        0.287106
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.978340 loss:        0.065512
Test - acc:         0.913600 loss:        0.294795
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.977840 loss:        0.064366
Test - acc:         0.908900 loss:        0.336796
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.978040 loss:        0.065099
Test - acc:         0.916700 loss:        0.282051
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.976360 loss:        0.067678
Test - acc:         0.918200 loss:        0.281188
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.977920 loss:        0.067827
Test - acc:         0.919400 loss:        0.271727
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.978500 loss:        0.064400
Test - acc:         0.921500 loss:        0.274768
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.978920 loss:        0.062808
Test - acc:         0.917300 loss:        0.299540
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.979520 loss:        0.062360
Test - acc:         0.917600 loss:        0.285560
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.977140 loss:        0.067880
Test - acc:         0.921700 loss:        0.276330
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.979160 loss:        0.062498
Test - acc:         0.920500 loss:        0.289012
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.977240 loss:        0.068215
Test - acc:         0.920600 loss:        0.276482
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.980480 loss:        0.059176
Test - acc:         0.911600 loss:        0.309674
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.977860 loss:        0.066253
Test - acc:         0.919200 loss:        0.288134
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.977580 loss:        0.065296
Test - acc:         0.922200 loss:        0.274700
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.978560 loss:        0.062231
Test - acc:         0.919900 loss:        0.277022
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.978660 loss:        0.063925
Test - acc:         0.917200 loss:        0.297246
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.977420 loss:        0.065870
Test - acc:         0.917200 loss:        0.304113
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.978500 loss:        0.063771
Test - acc:         0.922400 loss:        0.273312
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.979920 loss:        0.058329
Test - acc:         0.918000 loss:        0.295170
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.980360 loss:        0.060165
Test - acc:         0.924100 loss:        0.275284
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.978860 loss:        0.061263
Test - acc:         0.919300 loss:        0.289851
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.984720 loss:        0.047904
Test - acc:         0.930500 loss:        0.254854
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.989500 loss:        0.034935
Test - acc:         0.929400 loss:        0.267934
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.986480 loss:        0.041002
Test - acc:         0.923400 loss:        0.275801
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.984600 loss:        0.047161
Test - acc:         0.928400 loss:        0.254268
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.985340 loss:        0.044846
Test - acc:         0.925400 loss:        0.275307
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.985400 loss:        0.043636
Test - acc:         0.924500 loss:        0.273584
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.983240 loss:        0.048884
Test - acc:         0.926000 loss:        0.279393
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.982800 loss:        0.052535
Test - acc:         0.929200 loss:        0.259870
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.983540 loss:        0.049097
Test - acc:         0.924800 loss:        0.278097
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.985340 loss:        0.045899
Test - acc:         0.925200 loss:        0.273678
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.983280 loss:        0.052039
Test - acc:         0.928100 loss:        0.265395
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.985300 loss:        0.044871
Test - acc:         0.922200 loss:        0.282300
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.982760 loss:        0.051874
Test - acc:         0.924600 loss:        0.270690
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.983240 loss:        0.050916
Test - acc:         0.916200 loss:        0.311740
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.981620 loss:        0.054123
Test - acc:         0.921000 loss:        0.290908
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.983720 loss:        0.050107
Test - acc:         0.914900 loss:        0.306716
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.991740 loss:        0.027074
Test - acc:         0.940400 loss:        0.218097
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.995300 loss:        0.016923
Test - acc:         0.941800 loss:        0.216106
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.996940 loss:        0.013654
Test - acc:         0.941600 loss:        0.213287
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.997440 loss:        0.011384
Test - acc:         0.944200 loss:        0.211829
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.997660 loss:        0.010755
Test - acc:         0.943600 loss:        0.212385
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.997760 loss:        0.010137
Test - acc:         0.944100 loss:        0.208355
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.998460 loss:        0.008308
Test - acc:         0.944400 loss:        0.209822
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.998560 loss:        0.008091
Test - acc:         0.946000 loss:        0.210454
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.998500 loss:        0.007631
Test - acc:         0.945400 loss:        0.211997
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.998900 loss:        0.006765
Test - acc:         0.945900 loss:        0.211517
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.998920 loss:        0.006749
Test - acc:         0.946500 loss:        0.210454
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.006787
Test - acc:         0.946200 loss:        0.210844
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.998880 loss:        0.006422
Test - acc:         0.947300 loss:        0.209839
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.006434
Test - acc:         0.946000 loss:        0.212881
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.999120 loss:        0.005668
Test - acc:         0.945900 loss:        0.209828
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.999380 loss:        0.005316
Test - acc:         0.946000 loss:        0.209510
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.004958
Test - acc:         0.946400 loss:        0.210987
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.005244
Test - acc:         0.946200 loss:        0.209770
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999380 loss:        0.005137
Test - acc:         0.946200 loss:        0.211214
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999220 loss:        0.005267
Test - acc:         0.946900 loss:        0.210496
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.004699
Test - acc:         0.946600 loss:        0.208475
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.004694
Test - acc:         0.947100 loss:        0.208864
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.004885
Test - acc:         0.947700 loss:        0.209941
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.004358
Test - acc:         0.947100 loss:        0.208641
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.004255
Test - acc:         0.947700 loss:        0.209826
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.004154
Test - acc:         0.947300 loss:        0.209495
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004200
Test - acc:         0.947100 loss:        0.208724
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.004051
Test - acc:         0.947200 loss:        0.209993
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.004043
Test - acc:         0.947400 loss:        0.210332
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.004008
Test - acc:         0.946500 loss:        0.209307
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.003883
Test - acc:         0.947300 loss:        0.208526
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.003946
Test - acc:         0.947500 loss:        0.210177
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.004007
Test - acc:         0.948100 loss:        0.207725
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003776
Test - acc:         0.947300 loss:        0.208856
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.003944
Test - acc:         0.946700 loss:        0.207319
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.003965
Test - acc:         0.946800 loss:        0.207239
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.003363
Test - acc:         0.947700 loss:        0.207458
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.003606
Test - acc:         0.947700 loss:        0.206393
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.003365
Test - acc:         0.947100 loss:        0.205399
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.003677
Test - acc:         0.947600 loss:        0.206791
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.003596
Test - acc:         0.947100 loss:        0.207134
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.003607
Test - acc:         0.947100 loss:        0.208252
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003448
Test - acc:         0.946900 loss:        0.209589
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.003256
Test - acc:         0.946000 loss:        0.209133
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.003202
Test - acc:         0.946200 loss:        0.210272
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.003131
Test - acc:         0.946700 loss:        0.209192
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003719
Test - acc:         0.947400 loss:        0.207389
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003297
Test - acc:         0.947900 loss:        0.205428
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.003283
Test - acc:         0.947300 loss:        0.207195
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.003298
Test - acc:         0.948300 loss:        0.205113
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.003266
Test - acc:         0.948200 loss:        0.206082
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.003267
Test - acc:         0.948300 loss:        0.207289
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.002989
Test - acc:         0.948600 loss:        0.204947
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.003115
Test - acc:         0.948800 loss:        0.204632
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003173
Test - acc:         0.946800 loss:        0.205204
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002978
Test - acc:         0.947800 loss:        0.206064
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.003215
Test - acc:         0.948400 loss:        0.206275
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.002968
Test - acc:         0.948800 loss:        0.204916
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.003105
Test - acc:         0.947300 loss:        0.203611
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002919
Test - acc:         0.948400 loss:        0.202863
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.003103
Test - acc:         0.948700 loss:        0.202258
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.003192
Test - acc:         0.949200 loss:        0.200835
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.003050
Test - acc:         0.948200 loss:        0.203148
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002812
Test - acc:         0.947900 loss:        0.201628
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002975
Test - acc:         0.948400 loss:        0.203772
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002695
Test - acc:         0.948700 loss:        0.201879
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002867
Test - acc:         0.948500 loss:        0.203499
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.002811
Test - acc:         0.948700 loss:        0.201944
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.002909
Test - acc:         0.949400 loss:        0.202579
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.002944
Test - acc:         0.950000 loss:        0.202131
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002653
Test - acc:         0.949100 loss:        0.202575
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002873
Test - acc:         0.949800 loss:        0.202885
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002904
Test - acc:         0.950400 loss:        0.203193
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002835
Test - acc:         0.949900 loss:        0.203378
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002741
Test - acc:         0.948500 loss:        0.203650
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002658
Test - acc:         0.949100 loss:        0.202923
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.002918
Test - acc:         0.949700 loss:        0.202618
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002971
Test - acc:         0.950100 loss:        0.200980
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002698
Test - acc:         0.950200 loss:        0.200781
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002664
Test - acc:         0.949500 loss:        0.199807
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002727
Test - acc:         0.949300 loss:        0.202256
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002822
Test - acc:         0.948700 loss:        0.202281
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.002765
Test - acc:         0.949800 loss:        0.201206
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002564
Test - acc:         0.948400 loss:        0.201281
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002646
Test - acc:         0.949900 loss:        0.200291
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002609
Test - acc:         0.949600 loss:        0.199965
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002563
Test - acc:         0.948400 loss:        0.200095
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002561
Test - acc:         0.950600 loss:        0.199320
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002509
Test - acc:         0.949200 loss:        0.201174
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002541
Test - acc:         0.948300 loss:        0.201551
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002631
Test - acc:         0.949100 loss:        0.203552
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002655
Test - acc:         0.949400 loss:        0.202603
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002699
Test - acc:         0.948300 loss:        0.198468
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002717
Test - acc:         0.948600 loss:        0.200839
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002641
Test - acc:         0.948800 loss:        0.199860
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002520
Test - acc:         0.949700 loss:        0.201856
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.002759
Test - acc:         0.949400 loss:        0.198895
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002683
Test - acc:         0.949400 loss:        0.199791
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.002701
Test - acc:         0.949800 loss:        0.198917
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002644
Test - acc:         0.948800 loss:        0.200029
Sparsity :          0.7500
Wdecay :        0.000500
