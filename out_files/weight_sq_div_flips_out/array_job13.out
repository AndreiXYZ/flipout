Running --model resnet18 --noise --prune_criterion weight_squared_div_flips --seed 44 --prune_freq 50 --prune_rate 0.5 --comment=resnet18_crit=weight_squared_div_flips_pf=50_seed=44 --save_model=pre-finetune/resnet18_weight_squared_div_flips_pf50_s44 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_squared_div_flips_pf50_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11164352
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.303340 loss:        2.045872
Test - acc:         0.449200 loss:        1.518474
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.486940 loss:        1.408749
Test - acc:         0.553600 loss:        1.216369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.595040 loss:        1.130263
Test - acc:         0.627400 loss:        1.061316
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.662820 loss:        0.954220
Test - acc:         0.666000 loss:        0.963036
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.710980 loss:        0.823103
Test - acc:         0.700400 loss:        0.900422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.754080 loss:        0.701555
Test - acc:         0.719300 loss:        0.843699
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.784640 loss:        0.626890
Test - acc:         0.762900 loss:        0.702303
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.800120 loss:        0.580976
Test - acc:         0.678300 loss:        1.056559
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.809040 loss:        0.553956
Test - acc:         0.750400 loss:        0.725537
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.817400 loss:        0.527065
Test - acc:         0.750100 loss:        0.776029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.828080 loss:        0.504549
Test - acc:         0.806100 loss:        0.560775
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.827040 loss:        0.498283
Test - acc:         0.787800 loss:        0.637411
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.835700 loss:        0.482435
Test - acc:         0.794000 loss:        0.615528
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838900 loss:        0.468737
Test - acc:         0.820400 loss:        0.537681
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844200 loss:        0.459359
Test - acc:         0.804800 loss:        0.583139
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845740 loss:        0.448696
Test - acc:         0.757400 loss:        0.755301
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.845840 loss:        0.444416
Test - acc:         0.784900 loss:        0.670693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.849560 loss:        0.437153
Test - acc:         0.836300 loss:        0.472858
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.852000 loss:        0.431503
Test - acc:         0.805300 loss:        0.564962
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.853780 loss:        0.428139
Test - acc:         0.822000 loss:        0.534250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.417603
Test - acc:         0.776100 loss:        0.709650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.857220 loss:        0.416609
Test - acc:         0.822800 loss:        0.534768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.418318
Test - acc:         0.835400 loss:        0.503948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.857700 loss:        0.416371
Test - acc:         0.797500 loss:        0.599861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.861780 loss:        0.406843
Test - acc:         0.789200 loss:        0.638827
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.858420 loss:        0.412091
Test - acc:         0.812200 loss:        0.574223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.861500 loss:        0.402603
Test - acc:         0.834200 loss:        0.487177
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.861100 loss:        0.403876
Test - acc:         0.788300 loss:        0.694953
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.862260 loss:        0.397948
Test - acc:         0.834000 loss:        0.496371
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.861600 loss:        0.404324
Test - acc:         0.767800 loss:        0.741355
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.863460 loss:        0.399084
Test - acc:         0.813800 loss:        0.548094
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.862380 loss:        0.399397
Test - acc:         0.821100 loss:        0.548047
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.866720 loss:        0.392388
Test - acc:         0.831100 loss:        0.522133
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.863960 loss:        0.395103
Test - acc:         0.791300 loss:        0.647549
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.865880 loss:        0.393367
Test - acc:         0.748300 loss:        0.861032
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.867420 loss:        0.392783
Test - acc:         0.793100 loss:        0.629516
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.867160 loss:        0.387712
Test - acc:         0.855000 loss:        0.438008
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.387555
Test - acc:         0.817900 loss:        0.557149
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.867240 loss:        0.389858
Test - acc:         0.841200 loss:        0.478896
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.869000 loss:        0.387413
Test - acc:         0.832700 loss:        0.490588
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.864980 loss:        0.394477
Test - acc:         0.841800 loss:        0.463607
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.869720 loss:        0.383677
Test - acc:         0.843400 loss:        0.466556
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.870200 loss:        0.381420
Test - acc:         0.781100 loss:        0.700399
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.869540 loss:        0.383390
Test - acc:         0.801500 loss:        0.623825
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.870180 loss:        0.382817
Test - acc:         0.783300 loss:        0.685600
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.870820 loss:        0.376388
Test - acc:         0.813000 loss:        0.573551
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.869700 loss:        0.380625
Test - acc:         0.832900 loss:        0.502137
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.869460 loss:        0.382523
Test - acc:         0.762700 loss:        0.762488
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.869520 loss:        0.381779
Test - acc:         0.829900 loss:        0.524733
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.867960 loss:        0.379790
Test - acc:         0.834400 loss:        0.510441
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.882500 loss:        0.347693
Test - acc:         0.837400 loss:        0.474986
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.878940 loss:        0.351707
Test - acc:         0.830000 loss:        0.533033
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.878280 loss:        0.357011
Test - acc:         0.838900 loss:        0.482885
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.875640 loss:        0.364956
Test - acc:         0.803500 loss:        0.596207
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.876760 loss:        0.360239
Test - acc:         0.796000 loss:        0.634223
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.878840 loss:        0.358397
Test - acc:         0.804200 loss:        0.587145
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.876740 loss:        0.357950
Test - acc:         0.826400 loss:        0.529886
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.876480 loss:        0.361367
Test - acc:         0.860400 loss:        0.417996
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.877820 loss:        0.356023
Test - acc:         0.838400 loss:        0.480024
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.878740 loss:        0.353887
Test - acc:         0.826400 loss:        0.529297
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.875500 loss:        0.362996
Test - acc:         0.857000 loss:        0.424084
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.877320 loss:        0.362563
Test - acc:         0.847400 loss:        0.473949
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.879040 loss:        0.359514
Test - acc:         0.826500 loss:        0.517637
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.877520 loss:        0.357685
Test - acc:         0.843900 loss:        0.471193
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.877020 loss:        0.359015
Test - acc:         0.861300 loss:        0.407045
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.876000 loss:        0.358891
Test - acc:         0.831500 loss:        0.534973
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.879660 loss:        0.354114
Test - acc:         0.824700 loss:        0.547341
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.877040 loss:        0.358571
Test - acc:         0.835100 loss:        0.500914
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.877000 loss:        0.358299
Test - acc:         0.843600 loss:        0.463848
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.878740 loss:        0.354453
Test - acc:         0.835400 loss:        0.481484
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.879540 loss:        0.354344
Test - acc:         0.820800 loss:        0.531458
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.876340 loss:        0.362053
Test - acc:         0.822500 loss:        0.555665
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.876900 loss:        0.356531
Test - acc:         0.838300 loss:        0.498027
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.876180 loss:        0.358623
Test - acc:         0.837200 loss:        0.479853
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.878480 loss:        0.357506
Test - acc:         0.808300 loss:        0.587545
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.880380 loss:        0.349793
Test - acc:         0.853700 loss:        0.443100
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.879900 loss:        0.351151
Test - acc:         0.801400 loss:        0.624811
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.879300 loss:        0.357552
Test - acc:         0.840300 loss:        0.482020
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.878840 loss:        0.355027
Test - acc:         0.816500 loss:        0.555522
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.878280 loss:        0.354735
Test - acc:         0.847200 loss:        0.475933
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.878880 loss:        0.355211
Test - acc:         0.834000 loss:        0.509560
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.877400 loss:        0.359425
Test - acc:         0.812300 loss:        0.585773
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.878560 loss:        0.357655
Test - acc:         0.841200 loss:        0.488161
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.879900 loss:        0.354399
Test - acc:         0.808400 loss:        0.598589
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.877300 loss:        0.356360
Test - acc:         0.846600 loss:        0.454351
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.878000 loss:        0.359055
Test - acc:         0.817200 loss:        0.580507
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.879280 loss:        0.352106
Test - acc:         0.797800 loss:        0.616073
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.879660 loss:        0.352521
Test - acc:         0.834200 loss:        0.509273
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.875240 loss:        0.362186
Test - acc:         0.794900 loss:        0.631488
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.879020 loss:        0.354858
Test - acc:         0.812300 loss:        0.594645
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.875960 loss:        0.361069
Test - acc:         0.852700 loss:        0.438704
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.875400 loss:        0.360328
Test - acc:         0.803000 loss:        0.626198
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.878260 loss:        0.353356
Test - acc:         0.854800 loss:        0.437452
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.878240 loss:        0.353874
Test - acc:         0.857600 loss:        0.427444
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.879560 loss:        0.355271
Test - acc:         0.819000 loss:        0.543084
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.879260 loss:        0.357784
Test - acc:         0.842300 loss:        0.477412
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.878840 loss:        0.354178
Test - acc:         0.836100 loss:        0.487433
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.880040 loss:        0.354578
Test - acc:         0.841500 loss:        0.483040
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.880380 loss:        0.351255
Test - acc:         0.837400 loss:        0.493161
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.877780 loss:        0.359681
Test - acc:         0.832100 loss:        0.511541
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.889460 loss:        0.324542
Test - acc:         0.818900 loss:        0.537815
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.886900 loss:        0.331000
Test - acc:         0.824800 loss:        0.509728
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.886940 loss:        0.331916
Test - acc:         0.843200 loss:        0.500448
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.884960 loss:        0.337974
Test - acc:         0.825600 loss:        0.531351
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.884140 loss:        0.338569
Test - acc:         0.813000 loss:        0.592667
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.886840 loss:        0.334749
Test - acc:         0.843400 loss:        0.498781
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.883600 loss:        0.339202
Test - acc:         0.834700 loss:        0.492380
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.885260 loss:        0.335039
Test - acc:         0.830600 loss:        0.510452
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.884900 loss:        0.336676
Test - acc:         0.868500 loss:        0.401565
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.885220 loss:        0.334399
Test - acc:         0.833800 loss:        0.533743
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.886380 loss:        0.333887
Test - acc:         0.837600 loss:        0.494819
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.884200 loss:        0.341186
Test - acc:         0.836200 loss:        0.496095
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.885340 loss:        0.336213
Test - acc:         0.819900 loss:        0.569266
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.883800 loss:        0.338123
Test - acc:         0.856500 loss:        0.425678
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.884420 loss:        0.339526
Test - acc:         0.835000 loss:        0.503598
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.884420 loss:        0.337244
Test - acc:         0.854400 loss:        0.437731
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.884080 loss:        0.338995
Test - acc:         0.816000 loss:        0.576304
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.885240 loss:        0.336718
Test - acc:         0.859400 loss:        0.428140
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.884420 loss:        0.337546
Test - acc:         0.845700 loss:        0.460170
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.884720 loss:        0.337465
Test - acc:         0.845100 loss:        0.477439
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.884640 loss:        0.338742
Test - acc:         0.846800 loss:        0.456920
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.885840 loss:        0.336958
Test - acc:         0.863300 loss:        0.420642
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.883960 loss:        0.338281
Test - acc:         0.833200 loss:        0.520120
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.881740 loss:        0.343507
Test - acc:         0.840600 loss:        0.485780
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.886380 loss:        0.335296
Test - acc:         0.859000 loss:        0.418281
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.886420 loss:        0.333411
Test - acc:         0.856800 loss:        0.424060
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.885360 loss:        0.332742
Test - acc:         0.853100 loss:        0.430753
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.885220 loss:        0.337853
Test - acc:         0.849300 loss:        0.446112
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.884560 loss:        0.337164
Test - acc:         0.830100 loss:        0.522855
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.883860 loss:        0.338971
Test - acc:         0.810600 loss:        0.585034
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.885000 loss:        0.336776
Test - acc:         0.846800 loss:        0.458478
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.883900 loss:        0.332970
Test - acc:         0.864000 loss:        0.407795
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.884720 loss:        0.340275
Test - acc:         0.869200 loss:        0.397457
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.885940 loss:        0.338717
Test - acc:         0.844100 loss:        0.457101
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.885700 loss:        0.332042
Test - acc:         0.859900 loss:        0.425148
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.885360 loss:        0.333315
Test - acc:         0.859700 loss:        0.426206
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.885640 loss:        0.336228
Test - acc:         0.847900 loss:        0.475049
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.884260 loss:        0.338359
Test - acc:         0.811900 loss:        0.588662
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.886940 loss:        0.334168
Test - acc:         0.861200 loss:        0.424158
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.884640 loss:        0.336518
Test - acc:         0.834600 loss:        0.532063
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.884300 loss:        0.337209
Test - acc:         0.827600 loss:        0.518064
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.887540 loss:        0.333809
Test - acc:         0.870100 loss:        0.397192
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.883920 loss:        0.334811
Test - acc:         0.845300 loss:        0.473309
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.883140 loss:        0.338352
Test - acc:         0.831200 loss:        0.502760
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.884960 loss:        0.338843
Test - acc:         0.780600 loss:        0.684497
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.884840 loss:        0.336522
Test - acc:         0.822800 loss:        0.552706
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.885500 loss:        0.335449
Test - acc:         0.799200 loss:        0.629723
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.886600 loss:        0.331635
Test - acc:         0.814100 loss:        0.596967
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.883420 loss:        0.339339
Test - acc:         0.806100 loss:        0.590951
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.883260 loss:        0.341454
Test - acc:         0.832200 loss:        0.506705
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.932300 loss:        0.204178
Test - acc:         0.921500 loss:        0.232098
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.947940 loss:        0.156531
Test - acc:         0.925200 loss:        0.223805
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.954380 loss:        0.137196
Test - acc:         0.927600 loss:        0.214934
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.957800 loss:        0.125957
Test - acc:         0.929600 loss:        0.210743
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.960320 loss:        0.117497
Test - acc:         0.927200 loss:        0.213734
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.962640 loss:        0.110177
Test - acc:         0.931600 loss:        0.207093
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.965660 loss:        0.100997
Test - acc:         0.931800 loss:        0.211345
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.967800 loss:        0.094196
Test - acc:         0.932300 loss:        0.213055
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.969320 loss:        0.091511
Test - acc:         0.928300 loss:        0.212601
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.971340 loss:        0.085693
Test - acc:         0.931200 loss:        0.215325
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.971080 loss:        0.082401
Test - acc:         0.930900 loss:        0.222244
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.974160 loss:        0.076201
Test - acc:         0.929300 loss:        0.215186
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.975060 loss:        0.074520
Test - acc:         0.932700 loss:        0.218115
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.976020 loss:        0.071029
Test - acc:         0.932000 loss:        0.221510
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.977980 loss:        0.067584
Test - acc:         0.926300 loss:        0.237926
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.977000 loss:        0.068044
Test - acc:         0.931100 loss:        0.227012
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.061175
Test - acc:         0.932900 loss:        0.224953
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.979340 loss:        0.061587
Test - acc:         0.930800 loss:        0.240998
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.981100 loss:        0.058514
Test - acc:         0.931300 loss:        0.233036
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.980480 loss:        0.058543
Test - acc:         0.927300 loss:        0.245022
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.979620 loss:        0.059414
Test - acc:         0.930200 loss:        0.236871
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.981780 loss:        0.055041
Test - acc:         0.926500 loss:        0.241693
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.981620 loss:        0.054482
Test - acc:         0.928100 loss:        0.251122
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.981700 loss:        0.054501
Test - acc:         0.928000 loss:        0.241602
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.982160 loss:        0.054300
Test - acc:         0.925700 loss:        0.245972
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.978960 loss:        0.059816
Test - acc:         0.926700 loss:        0.247510
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.981840 loss:        0.054682
Test - acc:         0.931100 loss:        0.236515
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.982340 loss:        0.054450
Test - acc:         0.926900 loss:        0.254526
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.979360 loss:        0.059164
Test - acc:         0.926700 loss:        0.255701
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.981040 loss:        0.057025
Test - acc:         0.929600 loss:        0.248129
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.979620 loss:        0.058803
Test - acc:         0.921300 loss:        0.265652
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.981080 loss:        0.057295
Test - acc:         0.931000 loss:        0.244042
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.982540 loss:        0.052548
Test - acc:         0.924100 loss:        0.261480
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.980580 loss:        0.056946
Test - acc:         0.922700 loss:        0.261166
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.978940 loss:        0.062322
Test - acc:         0.922600 loss:        0.274727
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.979980 loss:        0.060028
Test - acc:         0.925400 loss:        0.259913
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.981420 loss:        0.056175
Test - acc:         0.925000 loss:        0.269947
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.979100 loss:        0.060811
Test - acc:         0.923000 loss:        0.264961
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.057591
Test - acc:         0.921500 loss:        0.274910
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.977560 loss:        0.065187
Test - acc:         0.923200 loss:        0.268905
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.978980 loss:        0.063397
Test - acc:         0.916300 loss:        0.289149
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.977940 loss:        0.063154
Test - acc:         0.920600 loss:        0.270484
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.979040 loss:        0.060645
Test - acc:         0.921100 loss:        0.282003
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.979300 loss:        0.061689
Test - acc:         0.926700 loss:        0.259024
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.979660 loss:        0.062039
Test - acc:         0.924300 loss:        0.274174
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.978260 loss:        0.063127
Test - acc:         0.925600 loss:        0.257798
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.978080 loss:        0.064373
Test - acc:         0.919400 loss:        0.285063
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.978220 loss:        0.063869
Test - acc:         0.918900 loss:        0.279641
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.978600 loss:        0.063695
Test - acc:         0.926700 loss:        0.265923
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.060686
Test - acc:         0.921400 loss:        0.272397
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.982820 loss:        0.053393
Test - acc:         0.932300 loss:        0.241287
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.984840 loss:        0.048142
Test - acc:         0.928400 loss:        0.261015
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.983460 loss:        0.050218
Test - acc:         0.921300 loss:        0.270394
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.984200 loss:        0.046450
Test - acc:         0.926900 loss:        0.258938
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.984580 loss:        0.046669
Test - acc:         0.924400 loss:        0.283635
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.985240 loss:        0.044684
Test - acc:         0.929000 loss:        0.251168
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.983200 loss:        0.049954
Test - acc:         0.922600 loss:        0.278223
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.982360 loss:        0.052494
Test - acc:         0.925900 loss:        0.257386
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.983020 loss:        0.051623
Test - acc:         0.924400 loss:        0.266057
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.982860 loss:        0.051491
Test - acc:         0.926600 loss:        0.258024
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.982580 loss:        0.051801
Test - acc:         0.921600 loss:        0.283347
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.983320 loss:        0.050943
Test - acc:         0.923200 loss:        0.281252
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.981460 loss:        0.055073
Test - acc:         0.921900 loss:        0.275450
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.980580 loss:        0.058520
Test - acc:         0.921500 loss:        0.277950
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.981300 loss:        0.054270
Test - acc:         0.920500 loss:        0.294733
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.979640 loss:        0.059889
Test - acc:         0.912700 loss:        0.298060
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.982200 loss:        0.054729
Test - acc:         0.923600 loss:        0.264078
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.982220 loss:        0.053831
Test - acc:         0.921600 loss:        0.270197
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.980940 loss:        0.056395
Test - acc:         0.925800 loss:        0.266019
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.981240 loss:        0.055614
Test - acc:         0.925100 loss:        0.271472
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.982920 loss:        0.052673
Test - acc:         0.922200 loss:        0.284934
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.978980 loss:        0.059869
Test - acc:         0.924000 loss:        0.254666
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.981800 loss:        0.055172
Test - acc:         0.928500 loss:        0.263843
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.979080 loss:        0.061927
Test - acc:         0.925200 loss:        0.252535
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.982000 loss:        0.053303
Test - acc:         0.921200 loss:        0.268913
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.980280 loss:        0.057335
Test - acc:         0.918700 loss:        0.282923
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.979420 loss:        0.059379
Test - acc:         0.922400 loss:        0.274087
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.980660 loss:        0.056233
Test - acc:         0.920900 loss:        0.279774
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.057585
Test - acc:         0.921500 loss:        0.277384
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.980760 loss:        0.057607
Test - acc:         0.919900 loss:        0.278333
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.980660 loss:        0.058292
Test - acc:         0.921600 loss:        0.278683
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.980640 loss:        0.056761
Test - acc:         0.917000 loss:        0.291651
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.981060 loss:        0.056574
Test - acc:         0.911600 loss:        0.313344
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.979640 loss:        0.059610
Test - acc:         0.925300 loss:        0.260159
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.980560 loss:        0.058132
Test - acc:         0.926200 loss:        0.258127
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.981320 loss:        0.055988
Test - acc:         0.919900 loss:        0.286151
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.981040 loss:        0.056729
Test - acc:         0.923600 loss:        0.263173
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.980360 loss:        0.056849
Test - acc:         0.927400 loss:        0.249678
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.980820 loss:        0.056045
Test - acc:         0.916200 loss:        0.296931
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.982940 loss:        0.052770
Test - acc:         0.927600 loss:        0.270315
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.982840 loss:        0.051667
Test - acc:         0.919000 loss:        0.284827
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.980540 loss:        0.056595
Test - acc:         0.922300 loss:        0.279011
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.982060 loss:        0.053968
Test - acc:         0.928000 loss:        0.248405
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.980600 loss:        0.058632
Test - acc:         0.919300 loss:        0.289226
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.980760 loss:        0.057201
Test - acc:         0.924100 loss:        0.263040
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.979920 loss:        0.058090
Test - acc:         0.922100 loss:        0.276447
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.980880 loss:        0.055976
Test - acc:         0.923600 loss:        0.261117
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.980780 loss:        0.057034
Test - acc:         0.920800 loss:        0.282612
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.980520 loss:        0.057435
Test - acc:         0.923300 loss:        0.262130
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.980720 loss:        0.058130
Test - acc:         0.923500 loss:        0.267707
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.985880 loss:        0.046255
Test - acc:         0.937400 loss:        0.215971
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.990720 loss:        0.033380
Test - acc:         0.939800 loss:        0.210337
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.993060 loss:        0.026806
Test - acc:         0.939700 loss:        0.208290
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.993200 loss:        0.025315
Test - acc:         0.942000 loss:        0.207706
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.994920 loss:        0.021314
Test - acc:         0.941700 loss:        0.208435
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.995380 loss:        0.019951
Test - acc:         0.941700 loss:        0.210432
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.995280 loss:        0.019294
Test - acc:         0.941400 loss:        0.208571
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.995360 loss:        0.019375
Test - acc:         0.941600 loss:        0.207060
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.996200 loss:        0.017091
Test - acc:         0.941700 loss:        0.209088
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.996120 loss:        0.016784
Test - acc:         0.941500 loss:        0.207625
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.996620 loss:        0.016046
Test - acc:         0.941600 loss:        0.207806
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.996740 loss:        0.015123
Test - acc:         0.942000 loss:        0.207375
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.996600 loss:        0.015430
Test - acc:         0.943200 loss:        0.206830
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.996900 loss:        0.014889
Test - acc:         0.942200 loss:        0.207782
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.997320 loss:        0.013604
Test - acc:         0.942200 loss:        0.208614
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.996860 loss:        0.013691
Test - acc:         0.942000 loss:        0.207383
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.997620 loss:        0.012767
Test - acc:         0.941800 loss:        0.207498
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.997460 loss:        0.013001
Test - acc:         0.942100 loss:        0.206646
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.997580 loss:        0.012159
Test - acc:         0.942500 loss:        0.207258
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.997840 loss:        0.011996
Test - acc:         0.942300 loss:        0.211685
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.997920 loss:        0.010932
Test - acc:         0.942900 loss:        0.207840
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.997960 loss:        0.011206
Test - acc:         0.943600 loss:        0.209465
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.998080 loss:        0.011091
Test - acc:         0.942300 loss:        0.208334
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.998000 loss:        0.010515
Test - acc:         0.941600 loss:        0.208914
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.998420 loss:        0.010264
Test - acc:         0.942900 loss:        0.207570
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.998120 loss:        0.010458
Test - acc:         0.943500 loss:        0.208906
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.998040 loss:        0.010311
Test - acc:         0.942700 loss:        0.207457
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.998020 loss:        0.010271
Test - acc:         0.941200 loss:        0.209576
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.998300 loss:        0.010111
Test - acc:         0.941700 loss:        0.212433
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.998460 loss:        0.009345
Test - acc:         0.942500 loss:        0.208453
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.998660 loss:        0.009111
Test - acc:         0.941600 loss:        0.209391
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.998620 loss:        0.009243
Test - acc:         0.941700 loss:        0.209812
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.998340 loss:        0.009598
Test - acc:         0.942800 loss:        0.212257
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.008773
Test - acc:         0.942800 loss:        0.210940
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.998520 loss:        0.009176
Test - acc:         0.942900 loss:        0.211492
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.998840 loss:        0.008287
Test - acc:         0.943400 loss:        0.211893
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.998540 loss:        0.008815
Test - acc:         0.943300 loss:        0.210335
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.998500 loss:        0.008708
Test - acc:         0.942900 loss:        0.210079
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.998820 loss:        0.008303
Test - acc:         0.943000 loss:        0.208243
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.998240 loss:        0.009183
Test - acc:         0.942800 loss:        0.209107
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.008299
Test - acc:         0.944300 loss:        0.208377
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.998760 loss:        0.007847
Test - acc:         0.943000 loss:        0.211114
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.998860 loss:        0.007842
Test - acc:         0.942200 loss:        0.212161
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.998480 loss:        0.008528
Test - acc:         0.941600 loss:        0.213569
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.007559
Test - acc:         0.943200 loss:        0.212686
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.007527
Test - acc:         0.942900 loss:        0.212349
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.008019
Test - acc:         0.943600 loss:        0.211791
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.998840 loss:        0.007887
Test - acc:         0.942900 loss:        0.212071
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.007811
Test - acc:         0.942700 loss:        0.212966
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.998900 loss:        0.007618
Test - acc:         0.943500 loss:        0.211310
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.982740 loss:        0.060091
Test - acc:         0.937200 loss:        0.220184
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.988380 loss:        0.043173
Test - acc:         0.937500 loss:        0.214624
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.989420 loss:        0.038959
Test - acc:         0.936500 loss:        0.213756
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.991400 loss:        0.034378
Test - acc:         0.937600 loss:        0.213907
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.992460 loss:        0.031472
Test - acc:         0.937500 loss:        0.212480
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.992020 loss:        0.029976
Test - acc:         0.938900 loss:        0.214617
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.991860 loss:        0.030362
Test - acc:         0.937400 loss:        0.213039
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.992980 loss:        0.028630
Test - acc:         0.938800 loss:        0.211286
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.993320 loss:        0.027214
Test - acc:         0.938500 loss:        0.211018
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.993820 loss:        0.025409
Test - acc:         0.937900 loss:        0.213074
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.994080 loss:        0.025151
Test - acc:         0.938000 loss:        0.212014
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.994580 loss:        0.023178
Test - acc:         0.938400 loss:        0.215226
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.994620 loss:        0.023023
Test - acc:         0.938100 loss:        0.215595
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.995280 loss:        0.022060
Test - acc:         0.937900 loss:        0.214018
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.995160 loss:        0.021190
Test - acc:         0.939000 loss:        0.213296
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.994900 loss:        0.021141
Test - acc:         0.938700 loss:        0.212388
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.995480 loss:        0.020298
Test - acc:         0.938500 loss:        0.213775
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.995680 loss:        0.019521
Test - acc:         0.939600 loss:        0.212377
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.995640 loss:        0.019493
Test - acc:         0.939900 loss:        0.214825
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.994920 loss:        0.020702
Test - acc:         0.940100 loss:        0.213168
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.996120 loss:        0.018022
Test - acc:         0.939300 loss:        0.214419
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.996000 loss:        0.018225
Test - acc:         0.940200 loss:        0.214680
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.995760 loss:        0.018989
Test - acc:         0.941500 loss:        0.212821
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.996160 loss:        0.017665
Test - acc:         0.940100 loss:        0.216238
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.996140 loss:        0.016865
Test - acc:         0.939800 loss:        0.215115
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.996480 loss:        0.016213
Test - acc:         0.938900 loss:        0.213931
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.996860 loss:        0.015759
Test - acc:         0.940300 loss:        0.215669
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.996240 loss:        0.017141
Test - acc:         0.940700 loss:        0.215894
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.996620 loss:        0.015837
Test - acc:         0.940200 loss:        0.218869
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.996260 loss:        0.016737
Test - acc:         0.939400 loss:        0.220270
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.996740 loss:        0.015642
Test - acc:         0.939000 loss:        0.220008
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.996840 loss:        0.015213
Test - acc:         0.940200 loss:        0.222341
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.996640 loss:        0.015089
Test - acc:         0.938800 loss:        0.219668
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.996300 loss:        0.015630
Test - acc:         0.939100 loss:        0.219168
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.996900 loss:        0.015094
Test - acc:         0.940000 loss:        0.219130
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.996560 loss:        0.015540
Test - acc:         0.940700 loss:        0.218809
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.997060 loss:        0.014181
Test - acc:         0.940800 loss:        0.217750
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.997240 loss:        0.013881
Test - acc:         0.940100 loss:        0.219327
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.997440 loss:        0.013790
Test - acc:         0.939300 loss:        0.217779
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.997480 loss:        0.013063
Test - acc:         0.939400 loss:        0.220356
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.997300 loss:        0.013366
Test - acc:         0.940000 loss:        0.219537
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.996880 loss:        0.014490
Test - acc:         0.939800 loss:        0.219195
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.996960 loss:        0.013851
Test - acc:         0.940300 loss:        0.220014
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.997580 loss:        0.013145
Test - acc:         0.939800 loss:        0.220301
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.997360 loss:        0.013036
Test - acc:         0.940700 loss:        0.221444
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.997140 loss:        0.013765
Test - acc:         0.938900 loss:        0.223866
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.997340 loss:        0.013335
Test - acc:         0.940500 loss:        0.220862
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.997160 loss:        0.012897
Test - acc:         0.939800 loss:        0.222018
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.997520 loss:        0.012383
Test - acc:         0.939800 loss:        0.222244
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.997020 loss:        0.012969
Test - acc:         0.939900 loss:        0.221345
Sparsity :          0.9844
Wdecay :        0.000500
