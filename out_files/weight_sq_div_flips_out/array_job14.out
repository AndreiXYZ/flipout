Running --model resnet18 --noise --prune_criterion weight_squared_div_flips --seed 44 --prune_freq 39 --prune_rate 0.5 --comment=resnet18_crit=weight_squared_div_flips_pf=39_seed=44 --save_model=pre-finetune/resnet18_weight_squared_div_flips_pf39_s44 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_squared_div_flips_pf39_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11164352
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.303340 loss:        2.045872
Test - acc:         0.449200 loss:        1.518474
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.486940 loss:        1.408749
Test - acc:         0.553600 loss:        1.216369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.595040 loss:        1.130263
Test - acc:         0.627400 loss:        1.061316
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.662820 loss:        0.954220
Test - acc:         0.666000 loss:        0.963036
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.710980 loss:        0.823103
Test - acc:         0.700400 loss:        0.900422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.754080 loss:        0.701555
Test - acc:         0.719300 loss:        0.843699
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.784640 loss:        0.626890
Test - acc:         0.762900 loss:        0.702303
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.800120 loss:        0.580976
Test - acc:         0.678300 loss:        1.056559
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.809040 loss:        0.553956
Test - acc:         0.750400 loss:        0.725537
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.817400 loss:        0.527065
Test - acc:         0.750100 loss:        0.776029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.828080 loss:        0.504549
Test - acc:         0.806100 loss:        0.560775
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.827040 loss:        0.498283
Test - acc:         0.787800 loss:        0.637411
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.835700 loss:        0.482435
Test - acc:         0.794000 loss:        0.615528
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838900 loss:        0.468737
Test - acc:         0.820400 loss:        0.537681
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844200 loss:        0.459359
Test - acc:         0.804800 loss:        0.583139
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845740 loss:        0.448696
Test - acc:         0.757400 loss:        0.755301
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.845840 loss:        0.444416
Test - acc:         0.784900 loss:        0.670693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.849560 loss:        0.437153
Test - acc:         0.836300 loss:        0.472858
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.852000 loss:        0.431503
Test - acc:         0.805300 loss:        0.564962
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.853780 loss:        0.428139
Test - acc:         0.822000 loss:        0.534250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.417603
Test - acc:         0.776100 loss:        0.709650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.857220 loss:        0.416609
Test - acc:         0.822800 loss:        0.534768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.418318
Test - acc:         0.835400 loss:        0.503948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.857700 loss:        0.416371
Test - acc:         0.797500 loss:        0.599861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.861780 loss:        0.406843
Test - acc:         0.789200 loss:        0.638827
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.858420 loss:        0.412091
Test - acc:         0.812200 loss:        0.574223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.861500 loss:        0.402603
Test - acc:         0.834200 loss:        0.487177
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.861100 loss:        0.403876
Test - acc:         0.788300 loss:        0.694953
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.862260 loss:        0.397948
Test - acc:         0.834000 loss:        0.496371
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.861600 loss:        0.404324
Test - acc:         0.767800 loss:        0.741355
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.863460 loss:        0.399084
Test - acc:         0.813800 loss:        0.548094
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.862380 loss:        0.399397
Test - acc:         0.821100 loss:        0.548047
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.866720 loss:        0.392388
Test - acc:         0.831100 loss:        0.522133
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.863960 loss:        0.395103
Test - acc:         0.791300 loss:        0.647549
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.865880 loss:        0.393367
Test - acc:         0.748300 loss:        0.861032
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.867420 loss:        0.392783
Test - acc:         0.793100 loss:        0.629516
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.867160 loss:        0.387712
Test - acc:         0.855000 loss:        0.438008
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.387555
Test - acc:         0.817900 loss:        0.557149
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.867240 loss:        0.389858
Test - acc:         0.841200 loss:        0.478896
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.877880 loss:        0.356527
Test - acc:         0.850700 loss:        0.448123
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.875640 loss:        0.365851
Test - acc:         0.846400 loss:        0.446399
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.875800 loss:        0.363871
Test - acc:         0.836100 loss:        0.495892
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.874040 loss:        0.369850
Test - acc:         0.846400 loss:        0.464852
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.874600 loss:        0.364044
Test - acc:         0.819500 loss:        0.579822
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.873400 loss:        0.369713
Test - acc:         0.818000 loss:        0.550596
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.875260 loss:        0.365200
Test - acc:         0.818100 loss:        0.586464
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.875000 loss:        0.367150
Test - acc:         0.830600 loss:        0.508117
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.872760 loss:        0.371225
Test - acc:         0.781600 loss:        0.671163
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.875560 loss:        0.361992
Test - acc:         0.812700 loss:        0.570011
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.876520 loss:        0.361859
Test - acc:         0.828800 loss:        0.539721
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.875420 loss:        0.363847
Test - acc:         0.843900 loss:        0.454854
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.875220 loss:        0.363093
Test - acc:         0.837200 loss:        0.483467
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.873740 loss:        0.368480
Test - acc:         0.792600 loss:        0.675602
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.876020 loss:        0.364224
Test - acc:         0.823600 loss:        0.535998
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.875720 loss:        0.364384
Test - acc:         0.815100 loss:        0.543414
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.875760 loss:        0.359450
Test - acc:         0.804100 loss:        0.588562
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.876640 loss:        0.358147
Test - acc:         0.808100 loss:        0.580954
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.876020 loss:        0.359750
Test - acc:         0.827000 loss:        0.514261
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.876820 loss:        0.360234
Test - acc:         0.785800 loss:        0.684027
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.877360 loss:        0.357985
Test - acc:         0.824900 loss:        0.533442
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.876860 loss:        0.363327
Test - acc:         0.852600 loss:        0.439566
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.876400 loss:        0.364640
Test - acc:         0.836800 loss:        0.498833
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.875480 loss:        0.361553
Test - acc:         0.835800 loss:        0.481029
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.874500 loss:        0.364675
Test - acc:         0.863900 loss:        0.417681
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.875720 loss:        0.360134
Test - acc:         0.868100 loss:        0.403013
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.877720 loss:        0.356189
Test - acc:         0.815600 loss:        0.578014
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.877160 loss:        0.360881
Test - acc:         0.816500 loss:        0.578313
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.875800 loss:        0.363155
Test - acc:         0.840100 loss:        0.482451
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.876200 loss:        0.361911
Test - acc:         0.828800 loss:        0.520711
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.878500 loss:        0.353169
Test - acc:         0.826000 loss:        0.553930
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.878600 loss:        0.356397
Test - acc:         0.826700 loss:        0.510339
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.876440 loss:        0.361769
Test - acc:         0.855200 loss:        0.432338
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.878520 loss:        0.356407
Test - acc:         0.833000 loss:        0.533570
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.878340 loss:        0.356857
Test - acc:         0.826000 loss:        0.511871
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.878600 loss:        0.359461
Test - acc:         0.822800 loss:        0.515207
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.878180 loss:        0.360148
Test - acc:         0.853300 loss:        0.438680
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.879560 loss:        0.354178
Test - acc:         0.836300 loss:        0.502155
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.874660 loss:        0.364130
Test - acc:         0.792000 loss:        0.647747
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.889120 loss:        0.324398
Test - acc:         0.815800 loss:        0.586065
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.887700 loss:        0.330406
Test - acc:         0.851700 loss:        0.467004
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.884500 loss:        0.336694
Test - acc:         0.864400 loss:        0.408908
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.884020 loss:        0.336367
Test - acc:         0.843600 loss:        0.479215
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.884100 loss:        0.339413
Test - acc:         0.856100 loss:        0.433247
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.883260 loss:        0.343162
Test - acc:         0.781000 loss:        0.723882
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.883380 loss:        0.339304
Test - acc:         0.827900 loss:        0.499689
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.883360 loss:        0.342239
Test - acc:         0.839100 loss:        0.488847
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.884340 loss:        0.336785
Test - acc:         0.849600 loss:        0.456935
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.884860 loss:        0.337959
Test - acc:         0.842600 loss:        0.496326
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.884920 loss:        0.335612
Test - acc:         0.697200 loss:        1.162237
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.881860 loss:        0.344137
Test - acc:         0.850800 loss:        0.456628
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.885820 loss:        0.336084
Test - acc:         0.855800 loss:        0.433188
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.880940 loss:        0.344995
Test - acc:         0.820800 loss:        0.555079
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.884620 loss:        0.336194
Test - acc:         0.847000 loss:        0.479736
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.884440 loss:        0.336200
Test - acc:         0.845300 loss:        0.484258
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.883380 loss:        0.339885
Test - acc:         0.831500 loss:        0.505006
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.884940 loss:        0.337518
Test - acc:         0.841200 loss:        0.481870
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.884460 loss:        0.340581
Test - acc:         0.844700 loss:        0.471546
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.884420 loss:        0.338722
Test - acc:         0.806300 loss:        0.629550
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.882780 loss:        0.339678
Test - acc:         0.822600 loss:        0.555410
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.882840 loss:        0.343467
Test - acc:         0.835300 loss:        0.499774
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.884840 loss:        0.339052
Test - acc:         0.840300 loss:        0.499535
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.883420 loss:        0.342808
Test - acc:         0.833900 loss:        0.503386
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.882940 loss:        0.342845
Test - acc:         0.854400 loss:        0.434073
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.886240 loss:        0.333409
Test - acc:         0.816400 loss:        0.582265
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.884500 loss:        0.332815
Test - acc:         0.842800 loss:        0.477036
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.884220 loss:        0.338024
Test - acc:         0.804100 loss:        0.646398
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.883000 loss:        0.341130
Test - acc:         0.829700 loss:        0.517698
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.884660 loss:        0.341225
Test - acc:         0.837900 loss:        0.501962
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.880780 loss:        0.341850
Test - acc:         0.849300 loss:        0.465446
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.884880 loss:        0.339894
Test - acc:         0.847500 loss:        0.468040
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.886660 loss:        0.334841
Test - acc:         0.745800 loss:        0.901768
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.881860 loss:        0.343319
Test - acc:         0.836500 loss:        0.485238
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.885040 loss:        0.338339
Test - acc:         0.779900 loss:        0.745725
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.883620 loss:        0.338909
Test - acc:         0.838000 loss:        0.492670
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.886000 loss:        0.334979
Test - acc:         0.822700 loss:        0.535283
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.884140 loss:        0.335987
Test - acc:         0.855100 loss:        0.446005
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.883840 loss:        0.340434
Test - acc:         0.830300 loss:        0.525458
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.895120 loss:        0.304618
Test - acc:         0.872600 loss:        0.381103
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.893460 loss:        0.308850
Test - acc:         0.846100 loss:        0.472136
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.892420 loss:        0.313373
Test - acc:         0.846700 loss:        0.460052
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.891520 loss:        0.318387
Test - acc:         0.869000 loss:        0.379094
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.890360 loss:        0.317498
Test - acc:         0.837100 loss:        0.505480
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.891240 loss:        0.318553
Test - acc:         0.829400 loss:        0.554849
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.890400 loss:        0.320152
Test - acc:         0.861000 loss:        0.423471
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.889140 loss:        0.324366
Test - acc:         0.857100 loss:        0.440443
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.890920 loss:        0.316811
Test - acc:         0.863300 loss:        0.415297
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.890100 loss:        0.320863
Test - acc:         0.811300 loss:        0.587520
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.890960 loss:        0.318203
Test - acc:         0.862700 loss:        0.427525
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.890760 loss:        0.320118
Test - acc:         0.822600 loss:        0.526594
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.892300 loss:        0.317924
Test - acc:         0.781300 loss:        0.685679
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.889540 loss:        0.320021
Test - acc:         0.804100 loss:        0.666586
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.890620 loss:        0.316896
Test - acc:         0.857600 loss:        0.444894
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.888220 loss:        0.325842
Test - acc:         0.877200 loss:        0.370897
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.891540 loss:        0.318867
Test - acc:         0.822200 loss:        0.545388
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.891520 loss:        0.314683
Test - acc:         0.852500 loss:        0.433469
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.891200 loss:        0.316564
Test - acc:         0.855600 loss:        0.435539
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.890760 loss:        0.319846
Test - acc:         0.819000 loss:        0.579294
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.891920 loss:        0.315864
Test - acc:         0.848500 loss:        0.449299
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.890840 loss:        0.318772
Test - acc:         0.848700 loss:        0.457825
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.890720 loss:        0.319921
Test - acc:         0.844900 loss:        0.477523
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.889980 loss:        0.319200
Test - acc:         0.831300 loss:        0.534013
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.890320 loss:        0.322000
Test - acc:         0.865400 loss:        0.411592
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.891700 loss:        0.317579
Test - acc:         0.862400 loss:        0.414469
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.890420 loss:        0.319174
Test - acc:         0.842900 loss:        0.480005
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.888020 loss:        0.324412
Test - acc:         0.841200 loss:        0.491484
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.890040 loss:        0.317096
Test - acc:         0.842900 loss:        0.489288
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.889260 loss:        0.321310
Test - acc:         0.849400 loss:        0.449710
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.890580 loss:        0.315077
Test - acc:         0.846800 loss:        0.483494
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.891420 loss:        0.317243
Test - acc:         0.822300 loss:        0.582550
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.887760 loss:        0.324413
Test - acc:         0.794500 loss:        0.654464
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.937000 loss:        0.191215
Test - acc:         0.925200 loss:        0.223562
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.950540 loss:        0.145906
Test - acc:         0.927500 loss:        0.214799
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.957340 loss:        0.128523
Test - acc:         0.929600 loss:        0.206292
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.960320 loss:        0.116614
Test - acc:         0.932300 loss:        0.204813
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.964200 loss:        0.108249
Test - acc:         0.930900 loss:        0.206177
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.965580 loss:        0.101340
Test - acc:         0.932400 loss:        0.200980
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.969020 loss:        0.092736
Test - acc:         0.933400 loss:        0.200162
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.970620 loss:        0.088288
Test - acc:         0.935600 loss:        0.201521
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.972120 loss:        0.081912
Test - acc:         0.934000 loss:        0.204461
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.973760 loss:        0.079305
Test - acc:         0.934500 loss:        0.202919
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.974880 loss:        0.074761
Test - acc:         0.932400 loss:        0.214443
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.976000 loss:        0.072016
Test - acc:         0.934500 loss:        0.205612
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.976560 loss:        0.070222
Test - acc:         0.936300 loss:        0.206834
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.977180 loss:        0.069198
Test - acc:         0.935100 loss:        0.210087
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.978480 loss:        0.065584
Test - acc:         0.932000 loss:        0.225941
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.977940 loss:        0.066229
Test - acc:         0.933500 loss:        0.216220
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.979560 loss:        0.061729
Test - acc:         0.935800 loss:        0.214229
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.979700 loss:        0.060603
Test - acc:         0.933900 loss:        0.225532
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.982380 loss:        0.054390
Test - acc:         0.934100 loss:        0.223577
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.981420 loss:        0.056123
Test - acc:         0.929200 loss:        0.243394
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.980780 loss:        0.057024
Test - acc:         0.928700 loss:        0.245388
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.980680 loss:        0.056289
Test - acc:         0.928100 loss:        0.244525
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.981100 loss:        0.056470
Test - acc:         0.932800 loss:        0.235121
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.981980 loss:        0.053477
Test - acc:         0.929900 loss:        0.243443
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.981060 loss:        0.056104
Test - acc:         0.933200 loss:        0.233599
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.981120 loss:        0.056430
Test - acc:         0.932900 loss:        0.233932
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.982200 loss:        0.053526
Test - acc:         0.930900 loss:        0.245584
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.980900 loss:        0.055598
Test - acc:         0.929200 loss:        0.245698
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.982200 loss:        0.054131
Test - acc:         0.928600 loss:        0.256751
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.982060 loss:        0.053071
Test - acc:         0.929600 loss:        0.248541
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.980980 loss:        0.056213
Test - acc:         0.924100 loss:        0.266679
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.980560 loss:        0.057617
Test - acc:         0.923100 loss:        0.262171
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.980740 loss:        0.055957
Test - acc:         0.919800 loss:        0.284386
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.980900 loss:        0.055863
Test - acc:         0.927700 loss:        0.252265
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.978980 loss:        0.062348
Test - acc:         0.922900 loss:        0.270366
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.979660 loss:        0.060164
Test - acc:         0.923400 loss:        0.263327
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.980040 loss:        0.060691
Test - acc:         0.922400 loss:        0.273031
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.980040 loss:        0.058516
Test - acc:         0.922200 loss:        0.270948
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.981200 loss:        0.058605
Test - acc:         0.926000 loss:        0.252743
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.979220 loss:        0.063086
Test - acc:         0.923100 loss:        0.258402
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.979580 loss:        0.061624
Test - acc:         0.925300 loss:        0.255165
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.979140 loss:        0.062816
Test - acc:         0.919700 loss:        0.279642
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.978620 loss:        0.064652
Test - acc:         0.926600 loss:        0.251808
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.978980 loss:        0.063409
Test - acc:         0.914400 loss:        0.298511
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.977640 loss:        0.065391
Test - acc:         0.919200 loss:        0.283795
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.978980 loss:        0.065277
Test - acc:         0.928900 loss:        0.243420
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.982040 loss:        0.055251
Test - acc:         0.926000 loss:        0.254645
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.982760 loss:        0.052203
Test - acc:         0.925800 loss:        0.268983
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.980940 loss:        0.057750
Test - acc:         0.925500 loss:        0.271192
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.982620 loss:        0.053677
Test - acc:         0.920500 loss:        0.286861
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.982680 loss:        0.053105
Test - acc:         0.921700 loss:        0.280134
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.981560 loss:        0.054227
Test - acc:         0.923800 loss:        0.277080
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.980760 loss:        0.056959
Test - acc:         0.915000 loss:        0.290597
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.980480 loss:        0.058335
Test - acc:         0.923500 loss:        0.276740
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.981540 loss:        0.055098
Test - acc:         0.920800 loss:        0.284759
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.981200 loss:        0.055898
Test - acc:         0.918800 loss:        0.294062
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.979360 loss:        0.061436
Test - acc:         0.915000 loss:        0.324067
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.978800 loss:        0.061963
Test - acc:         0.926300 loss:        0.262518
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.980520 loss:        0.059014
Test - acc:         0.920700 loss:        0.268891
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.978440 loss:        0.064598
Test - acc:         0.924000 loss:        0.251221
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.980740 loss:        0.057465
Test - acc:         0.919900 loss:        0.290136
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.982360 loss:        0.055081
Test - acc:         0.922800 loss:        0.271566
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.979940 loss:        0.058983
Test - acc:         0.918600 loss:        0.289946
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.978960 loss:        0.061742
Test - acc:         0.917600 loss:        0.296511
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.978520 loss:        0.062071
Test - acc:         0.922100 loss:        0.267438
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.978860 loss:        0.061605
Test - acc:         0.922300 loss:        0.269902
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.979600 loss:        0.061354
Test - acc:         0.924200 loss:        0.258886
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.979700 loss:        0.059807
Test - acc:         0.921900 loss:        0.281156
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.978400 loss:        0.063825
Test - acc:         0.921100 loss:        0.287422
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.058090
Test - acc:         0.924700 loss:        0.273537
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.980540 loss:        0.058514
Test - acc:         0.924500 loss:        0.270551
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.978600 loss:        0.064829
Test - acc:         0.921400 loss:        0.267634
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.979420 loss:        0.061568
Test - acc:         0.926200 loss:        0.255624
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.979000 loss:        0.062283
Test - acc:         0.921700 loss:        0.275854
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.978360 loss:        0.062204
Test - acc:         0.919400 loss:        0.271218
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.978840 loss:        0.061394
Test - acc:         0.927500 loss:        0.267856
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.978180 loss:        0.062485
Test - acc:         0.918000 loss:        0.290416
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.981000 loss:        0.057371
Test - acc:         0.921200 loss:        0.272152
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.979940 loss:        0.061158
Test - acc:         0.922600 loss:        0.272500
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.977700 loss:        0.065046
Test - acc:         0.922100 loss:        0.276677
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.978640 loss:        0.063425
Test - acc:         0.923900 loss:        0.264129
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.978600 loss:        0.062130
Test - acc:         0.919700 loss:        0.276180
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.979280 loss:        0.063118
Test - acc:         0.925100 loss:        0.267315
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.978400 loss:        0.063362
Test - acc:         0.922500 loss:        0.277691
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.971120 loss:        0.086557
Test - acc:         0.925300 loss:        0.253984
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.975260 loss:        0.073344
Test - acc:         0.923100 loss:        0.261258
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.975880 loss:        0.072120
Test - acc:         0.927800 loss:        0.246925
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.976040 loss:        0.069980
Test - acc:         0.920300 loss:        0.263741
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.977660 loss:        0.066930
Test - acc:         0.919500 loss:        0.273169
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.976120 loss:        0.069919
Test - acc:         0.922500 loss:        0.260609
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.976780 loss:        0.068102
Test - acc:         0.924400 loss:        0.256570
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.977200 loss:        0.067821
Test - acc:         0.922600 loss:        0.276165
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.975660 loss:        0.070201
Test - acc:         0.917900 loss:        0.291343
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.977000 loss:        0.068179
Test - acc:         0.920200 loss:        0.279300
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.976380 loss:        0.070033
Test - acc:         0.925800 loss:        0.259397
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.976760 loss:        0.068441
Test - acc:         0.920000 loss:        0.274249
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.976360 loss:        0.070582
Test - acc:         0.916100 loss:        0.296443
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.978420 loss:        0.064909
Test - acc:         0.923500 loss:        0.267981
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.977840 loss:        0.066211
Test - acc:         0.922800 loss:        0.272373
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.978660 loss:        0.063847
Test - acc:         0.923100 loss:        0.267795
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.986840 loss:        0.042388
Test - acc:         0.935000 loss:        0.225841
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.989980 loss:        0.034913
Test - acc:         0.937000 loss:        0.222054
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.991980 loss:        0.029222
Test - acc:         0.936700 loss:        0.222660
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.993200 loss:        0.027585
Test - acc:         0.936300 loss:        0.222315
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.993500 loss:        0.024389
Test - acc:         0.938300 loss:        0.220849
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.994400 loss:        0.023315
Test - acc:         0.937400 loss:        0.221537
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.994060 loss:        0.023328
Test - acc:         0.938000 loss:        0.220183
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.994700 loss:        0.021890
Test - acc:         0.938600 loss:        0.220265
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.995440 loss:        0.020191
Test - acc:         0.937400 loss:        0.222312
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.995540 loss:        0.020186
Test - acc:         0.938100 loss:        0.221906
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.995000 loss:        0.019647
Test - acc:         0.937700 loss:        0.224094
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.995540 loss:        0.018957
Test - acc:         0.939300 loss:        0.219939
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.995640 loss:        0.018864
Test - acc:         0.938900 loss:        0.218882
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.995740 loss:        0.018650
Test - acc:         0.938800 loss:        0.219301
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.996320 loss:        0.017545
Test - acc:         0.939000 loss:        0.218839
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.995940 loss:        0.017682
Test - acc:         0.939800 loss:        0.217529
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.995960 loss:        0.017385
Test - acc:         0.939000 loss:        0.221771
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.996360 loss:        0.016352
Test - acc:         0.938800 loss:        0.221461
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.996620 loss:        0.015758
Test - acc:         0.938400 loss:        0.221232
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.996480 loss:        0.016086
Test - acc:         0.938700 loss:        0.221691
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.996700 loss:        0.015718
Test - acc:         0.939700 loss:        0.221087
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.996500 loss:        0.015603
Test - acc:         0.939500 loss:        0.220689
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.996920 loss:        0.015247
Test - acc:         0.938900 loss:        0.218484
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.963940 loss:        0.109379
Test - acc:         0.922200 loss:        0.254758
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.972680 loss:        0.084101
Test - acc:         0.925200 loss:        0.244861
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.974860 loss:        0.077166
Test - acc:         0.926300 loss:        0.239786
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.977760 loss:        0.071505
Test - acc:         0.927900 loss:        0.242069
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.977900 loss:        0.068591
Test - acc:         0.928100 loss:        0.241234
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.979280 loss:        0.065174
Test - acc:         0.929000 loss:        0.239957
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.982040 loss:        0.060320
Test - acc:         0.929400 loss:        0.240888
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.981200 loss:        0.059502
Test - acc:         0.929200 loss:        0.238347
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.981840 loss:        0.059206
Test - acc:         0.927400 loss:        0.241051
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.982260 loss:        0.056720
Test - acc:         0.928300 loss:        0.240069
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.983240 loss:        0.054707
Test - acc:         0.928200 loss:        0.243964
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.983980 loss:        0.053323
Test - acc:         0.926900 loss:        0.242195
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.984000 loss:        0.051518
Test - acc:         0.928700 loss:        0.239507
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.985300 loss:        0.049165
Test - acc:         0.928200 loss:        0.242291
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.985180 loss:        0.048785
Test - acc:         0.930700 loss:        0.241613
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.985140 loss:        0.047938
Test - acc:         0.930100 loss:        0.239391
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.985420 loss:        0.048738
Test - acc:         0.929700 loss:        0.237526
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.986100 loss:        0.045765
Test - acc:         0.929800 loss:        0.238897
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.987240 loss:        0.044474
Test - acc:         0.929100 loss:        0.242764
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.986780 loss:        0.044825
Test - acc:         0.931000 loss:        0.237452
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.986860 loss:        0.044509
Test - acc:         0.930900 loss:        0.239069
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.986680 loss:        0.043997
Test - acc:         0.930600 loss:        0.239015
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.988300 loss:        0.041263
Test - acc:         0.929200 loss:        0.241372
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.987560 loss:        0.043073
Test - acc:         0.929900 loss:        0.240463
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.988200 loss:        0.041417
Test - acc:         0.931100 loss:        0.243383
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.988080 loss:        0.041230
Test - acc:         0.930500 loss:        0.242733
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.988220 loss:        0.040791
Test - acc:         0.929700 loss:        0.245389
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.988160 loss:        0.040603
Test - acc:         0.929800 loss:        0.244763
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.987960 loss:        0.039583
Test - acc:         0.928900 loss:        0.245001
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.988840 loss:        0.038807
Test - acc:         0.931100 loss:        0.243150
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.988660 loss:        0.039263
Test - acc:         0.930800 loss:        0.242487
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.989440 loss:        0.037699
Test - acc:         0.932400 loss:        0.242884
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.989140 loss:        0.037692
Test - acc:         0.931300 loss:        0.250521
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.988780 loss:        0.038532
Test - acc:         0.931000 loss:        0.246842
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.989700 loss:        0.035462
Test - acc:         0.931800 loss:        0.244609
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.989060 loss:        0.037092
Test - acc:         0.931400 loss:        0.244250
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.989700 loss:        0.036145
Test - acc:         0.930300 loss:        0.245052
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.989700 loss:        0.035492
Test - acc:         0.929600 loss:        0.246759
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.990040 loss:        0.034634
Test - acc:         0.930900 loss:        0.246312
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.888220 loss:        0.330732
Test - acc:         0.885800 loss:        0.347124
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.917780 loss:        0.241711
Test - acc:         0.894000 loss:        0.322715
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.923220 loss:        0.221694
Test - acc:         0.897800 loss:        0.308304
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.927860 loss:        0.209329
Test - acc:         0.902100 loss:        0.299271
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.932460 loss:        0.197748
Test - acc:         0.904200 loss:        0.294767
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.935840 loss:        0.188278
Test - acc:         0.902600 loss:        0.294515
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.937140 loss:        0.182018
Test - acc:         0.905300 loss:        0.288773
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.939800 loss:        0.176570
Test - acc:         0.908100 loss:        0.284617
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.940180 loss:        0.173200
Test - acc:         0.905800 loss:        0.287627
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.940560 loss:        0.170546
Test - acc:         0.909900 loss:        0.278071
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.941940 loss:        0.167781
Test - acc:         0.909400 loss:        0.282012
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.943160 loss:        0.165919
Test - acc:         0.909400 loss:        0.279913
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.943980 loss:        0.162949
Test - acc:         0.908500 loss:        0.279436
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.945760 loss:        0.156214
Test - acc:         0.910600 loss:        0.273621
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.945500 loss:        0.156698
Test - acc:         0.911200 loss:        0.278792
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.946540 loss:        0.155971
Test - acc:         0.910800 loss:        0.273382
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.946880 loss:        0.154374
Test - acc:         0.913000 loss:        0.272094
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.946600 loss:        0.153429
Test - acc:         0.915100 loss:        0.268506
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.947640 loss:        0.150662
Test - acc:         0.911500 loss:        0.273681
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.950380 loss:        0.145526
Test - acc:         0.912400 loss:        0.267161
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.950500 loss:        0.143719
Test - acc:         0.915500 loss:        0.265999
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.951420 loss:        0.142514
Test - acc:         0.915600 loss:        0.264392
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.950100 loss:        0.145251
Test - acc:         0.913300 loss:        0.269883
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.951940 loss:        0.142136
Test - acc:         0.913800 loss:        0.268773
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.952780 loss:        0.139679
Test - acc:         0.913800 loss:        0.269660
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.952040 loss:        0.139219
Test - acc:         0.912300 loss:        0.272425
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.952540 loss:        0.139112
Test - acc:         0.916100 loss:        0.266201
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.952060 loss:        0.139555
Test - acc:         0.912200 loss:        0.268663
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.952960 loss:        0.136645
Test - acc:         0.914900 loss:        0.270087
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.953840 loss:        0.135778
Test - acc:         0.915300 loss:        0.267399
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.953700 loss:        0.135879
Test - acc:         0.918000 loss:        0.262537
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.955080 loss:        0.131983
Test - acc:         0.915700 loss:        0.265730
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.954180 loss:        0.133534
Test - acc:         0.915900 loss:        0.270137
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.953600 loss:        0.133274
Test - acc:         0.916200 loss:        0.269270
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.954260 loss:        0.133823
Test - acc:         0.917900 loss:        0.266677
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.953520 loss:        0.134419
Test - acc:         0.916900 loss:        0.267797
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.956560 loss:        0.128831
Test - acc:         0.917700 loss:        0.265710
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.955520 loss:        0.131107
Test - acc:         0.918100 loss:        0.267198
Sparsity :          0.9961
Wdecay :        0.000500
