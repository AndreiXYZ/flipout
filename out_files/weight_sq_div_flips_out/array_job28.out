Running --model vgg19 --noise --prune_criterion weight_squared_div_flips --seed 44 --prune_freq 50 --prune_rate 0.5 --comment=vgg19_crit=weight_squared_div_flips_pf=50_seed=44 --save_model=pre-finetune/vgg19_weight_squared_div_flips_pf50_s44 --logdir=criterion_experiment_no_bias/vgg19
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_weight_squared_div_flips_pf50_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
num.prunable=20024000
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.142280 loss:        2.601396
Test - acc:         0.195100 loss:        2.206085
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.246620 loss:        1.929788
Test - acc:         0.236800 loss:        2.038829
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.317560 loss:        1.723914
Test - acc:         0.351700 loss:        1.641593
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.425080 loss:        1.509369
Test - acc:         0.464300 loss:        1.391149
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.541380 loss:        1.253156
Test - acc:         0.522800 loss:        1.493001
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.633180 loss:        1.041143
Test - acc:         0.596700 loss:        1.259192
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.678500 loss:        0.936769
Test - acc:         0.640400 loss:        1.105356
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.714680 loss:        0.849851
Test - acc:         0.644600 loss:        1.120985
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.737920 loss:        0.792265
Test - acc:         0.681900 loss:        0.964537
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.752860 loss:        0.748902
Test - acc:         0.563600 loss:        1.665525
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.767840 loss:        0.712795
Test - acc:         0.691600 loss:        0.983145
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.770600 loss:        0.698706
Test - acc:         0.638800 loss:        1.232823
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.781260 loss:        0.673360
Test - acc:         0.745400 loss:        0.827440
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.788280 loss:        0.654842
Test - acc:         0.754700 loss:        0.740684
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.795360 loss:        0.629470
Test - acc:         0.758100 loss:        0.771187
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.795060 loss:        0.629044
Test - acc:         0.725700 loss:        0.844025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.797820 loss:        0.619835
Test - acc:         0.711800 loss:        0.991450
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.802960 loss:        0.607546
Test - acc:         0.760700 loss:        0.740478
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.807800 loss:        0.589372
Test - acc:         0.766400 loss:        0.679525
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.808960 loss:        0.584498
Test - acc:         0.768200 loss:        0.704599
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.813640 loss:        0.575247
Test - acc:         0.781300 loss:        0.677502
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.815120 loss:        0.567870
Test - acc:         0.657500 loss:        1.206960
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.817880 loss:        0.565544
Test - acc:         0.748500 loss:        0.781382
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.816420 loss:        0.563773
Test - acc:         0.747700 loss:        0.820916
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.821880 loss:        0.550806
Test - acc:         0.784600 loss:        0.671629
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.822420 loss:        0.550200
Test - acc:         0.720700 loss:        0.864449
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.824940 loss:        0.540110
Test - acc:         0.777900 loss:        0.700156
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.825000 loss:        0.535996
Test - acc:         0.753200 loss:        0.851256
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.823200 loss:        0.541764
Test - acc:         0.794300 loss:        0.637654
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.825820 loss:        0.535093
Test - acc:         0.807500 loss:        0.606167
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.827760 loss:        0.530152
Test - acc:         0.766000 loss:        0.738118
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.828760 loss:        0.526739
Test - acc:         0.706400 loss:        0.947048
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.830080 loss:        0.521866
Test - acc:         0.806100 loss:        0.597559
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.828940 loss:        0.526150
Test - acc:         0.775400 loss:        0.670340
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.834500 loss:        0.507103
Test - acc:         0.767200 loss:        0.753308
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.832060 loss:        0.512132
Test - acc:         0.770600 loss:        0.721919
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.833820 loss:        0.511304
Test - acc:         0.778100 loss:        0.707310
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.833820 loss:        0.508718
Test - acc:         0.771000 loss:        0.770115
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.832700 loss:        0.512441
Test - acc:         0.787900 loss:        0.664319
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.834660 loss:        0.504084
Test - acc:         0.760100 loss:        0.795909
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.832100 loss:        0.510878
Test - acc:         0.798500 loss:        0.622620
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.835640 loss:        0.506822
Test - acc:         0.722400 loss:        0.936751
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.836760 loss:        0.500486
Test - acc:         0.794400 loss:        0.644790
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.836280 loss:        0.500193
Test - acc:         0.815700 loss:        0.576709
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.839780 loss:        0.494932
Test - acc:         0.824400 loss:        0.527701
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.839120 loss:        0.491536
Test - acc:         0.811300 loss:        0.575493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.839280 loss:        0.492528
Test - acc:         0.806200 loss:        0.582704
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.835840 loss:        0.499516
Test - acc:         0.826600 loss:        0.538733
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.838400 loss:        0.495237
Test - acc:         0.827300 loss:        0.543576
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.838480 loss:        0.497662
Test - acc:         0.715500 loss:        0.962318
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.843840 loss:        0.476397
Test - acc:         0.763300 loss:        0.745961
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.842720 loss:        0.472092
Test - acc:         0.774800 loss:        0.721277
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.845360 loss:        0.468965
Test - acc:         0.764300 loss:        0.735576
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.841680 loss:        0.482309
Test - acc:         0.782100 loss:        0.690119
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.842800 loss:        0.478456
Test - acc:         0.670300 loss:        1.227393
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.844240 loss:        0.472085
Test - acc:         0.808300 loss:        0.575837
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.842280 loss:        0.481034
Test - acc:         0.788900 loss:        0.624363
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.844120 loss:        0.479078
Test - acc:         0.800600 loss:        0.603173
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.847760 loss:        0.466571
Test - acc:         0.769500 loss:        0.727823
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.845080 loss:        0.466267
Test - acc:         0.764900 loss:        0.750453
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.847480 loss:        0.464850
Test - acc:         0.812500 loss:        0.560795
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.845120 loss:        0.467784
Test - acc:         0.757600 loss:        0.774064
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.844220 loss:        0.472180
Test - acc:         0.754300 loss:        0.763372
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.847820 loss:        0.464139
Test - acc:         0.805800 loss:        0.624184
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.843000 loss:        0.473608
Test - acc:         0.814300 loss:        0.545122
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.843960 loss:        0.473752
Test - acc:         0.821700 loss:        0.544013
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.844080 loss:        0.474388
Test - acc:         0.803100 loss:        0.602248
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.845160 loss:        0.463860
Test - acc:         0.810800 loss:        0.582268
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.844420 loss:        0.470247
Test - acc:         0.786900 loss:        0.693772
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.850200 loss:        0.455930
Test - acc:         0.741300 loss:        0.872585
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.844940 loss:        0.469951
Test - acc:         0.722400 loss:        0.937662
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.848520 loss:        0.460814
Test - acc:         0.811600 loss:        0.567609
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.845520 loss:        0.466762
Test - acc:         0.792700 loss:        0.661844
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.847780 loss:        0.462915
Test - acc:         0.802600 loss:        0.618093
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.849340 loss:        0.455135
Test - acc:         0.790400 loss:        0.689621
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.848040 loss:        0.458185
Test - acc:         0.824200 loss:        0.536135
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.846420 loss:        0.465396
Test - acc:         0.836800 loss:        0.489462
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.850420 loss:        0.457030
Test - acc:         0.796200 loss:        0.611649
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.849040 loss:        0.455613
Test - acc:         0.771900 loss:        0.743215
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.845040 loss:        0.468585
Test - acc:         0.734900 loss:        0.847619
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.850460 loss:        0.455241
Test - acc:         0.827100 loss:        0.542318
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.846220 loss:        0.467814
Test - acc:         0.802500 loss:        0.629997
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.846400 loss:        0.466731
Test - acc:         0.806400 loss:        0.627012
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.849520 loss:        0.456495
Test - acc:         0.754100 loss:        0.806422
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.847060 loss:        0.461043
Test - acc:         0.774700 loss:        0.709144
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.847120 loss:        0.462089
Test - acc:         0.810300 loss:        0.633445
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.847660 loss:        0.460250
Test - acc:         0.748700 loss:        0.787028
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.851440 loss:        0.452055
Test - acc:         0.841400 loss:        0.486821
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.848560 loss:        0.462842
Test - acc:         0.801200 loss:        0.627452
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.848100 loss:        0.459859
Test - acc:         0.778500 loss:        0.740084
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.844080 loss:        0.462725
Test - acc:         0.805900 loss:        0.584653
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.850300 loss:        0.454755
Test - acc:         0.785000 loss:        0.711910
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.848300 loss:        0.459422
Test - acc:         0.745700 loss:        0.874871
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.847160 loss:        0.467682
Test - acc:         0.783900 loss:        0.655215
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.845500 loss:        0.461030
Test - acc:         0.786300 loss:        0.712807
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.847380 loss:        0.467103
Test - acc:         0.822600 loss:        0.553946
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.849280 loss:        0.455950
Test - acc:         0.789400 loss:        0.664732
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.849500 loss:        0.461557
Test - acc:         0.824900 loss:        0.520187
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.849100 loss:        0.454917
Test - acc:         0.791200 loss:        0.679000
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.848560 loss:        0.456224
Test - acc:         0.818100 loss:        0.570431
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.858000 loss:        0.431105
Test - acc:         0.818500 loss:        0.522305
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.853060 loss:        0.442093
Test - acc:         0.758400 loss:        0.720832
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.853080 loss:        0.444320
Test - acc:         0.793300 loss:        0.642997
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.852760 loss:        0.441603
Test - acc:         0.845400 loss:        0.480944
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.853560 loss:        0.443437
Test - acc:         0.805100 loss:        0.604989
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.852660 loss:        0.443719
Test - acc:         0.821400 loss:        0.546061
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.853520 loss:        0.443593
Test - acc:         0.822900 loss:        0.532685
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.852040 loss:        0.445708
Test - acc:         0.824400 loss:        0.535171
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.852580 loss:        0.448981
Test - acc:         0.811600 loss:        0.568878
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.851820 loss:        0.445727
Test - acc:         0.740800 loss:        0.809955
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.849700 loss:        0.453522
Test - acc:         0.781500 loss:        0.658239
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.851580 loss:        0.450868
Test - acc:         0.747100 loss:        0.891963
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.854560 loss:        0.439811
Test - acc:         0.783100 loss:        0.729884
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.851860 loss:        0.449177
Test - acc:         0.769600 loss:        0.721728
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.853540 loss:        0.436659
Test - acc:         0.783100 loss:        0.727312
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.851920 loss:        0.450335
Test - acc:         0.806100 loss:        0.594975
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.855640 loss:        0.436507
Test - acc:         0.822200 loss:        0.572470
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.853460 loss:        0.443101
Test - acc:         0.802700 loss:        0.609468
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.853180 loss:        0.441770
Test - acc:         0.844000 loss:        0.469974
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.855700 loss:        0.434660
Test - acc:         0.801800 loss:        0.598484
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.855980 loss:        0.435340
Test - acc:         0.789500 loss:        0.658341
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.851980 loss:        0.442705
Test - acc:         0.802100 loss:        0.578481
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.852600 loss:        0.444825
Test - acc:         0.706700 loss:        0.973021
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.852260 loss:        0.447922
Test - acc:         0.826000 loss:        0.536334
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.853700 loss:        0.440030
Test - acc:         0.800800 loss:        0.607330
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.855100 loss:        0.436625
Test - acc:         0.817800 loss:        0.553530
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.855500 loss:        0.434451
Test - acc:         0.785900 loss:        0.651134
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.855020 loss:        0.441059
Test - acc:         0.811700 loss:        0.562447
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.852980 loss:        0.444695
Test - acc:         0.781900 loss:        0.661499
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.853820 loss:        0.438799
Test - acc:         0.765000 loss:        0.788475
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.852020 loss:        0.443931
Test - acc:         0.784100 loss:        0.666946
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.852980 loss:        0.437846
Test - acc:         0.776900 loss:        0.708806
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.852840 loss:        0.440903
Test - acc:         0.794800 loss:        0.638657
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.855620 loss:        0.435369
Test - acc:         0.794600 loss:        0.640871
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.856300 loss:        0.437781
Test - acc:         0.821500 loss:        0.536764
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.854500 loss:        0.437617
Test - acc:         0.810500 loss:        0.612279
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.854800 loss:        0.436060
Test - acc:         0.826800 loss:        0.535239
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.853080 loss:        0.441120
Test - acc:         0.792900 loss:        0.642951
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.853720 loss:        0.442320
Test - acc:         0.794200 loss:        0.651470
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.851260 loss:        0.443238
Test - acc:         0.746800 loss:        0.806193
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.852740 loss:        0.442561
Test - acc:         0.800700 loss:        0.609672
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.854180 loss:        0.442628
Test - acc:         0.827700 loss:        0.537979
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.853760 loss:        0.437324
Test - acc:         0.840800 loss:        0.491444
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.854820 loss:        0.437444
Test - acc:         0.802400 loss:        0.594592
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.853920 loss:        0.442684
Test - acc:         0.788400 loss:        0.663073
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.855060 loss:        0.433132
Test - acc:         0.815000 loss:        0.598132
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.852760 loss:        0.442162
Test - acc:         0.824300 loss:        0.524531
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.853140 loss:        0.437080
Test - acc:         0.791500 loss:        0.682174
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.855140 loss:        0.435477
Test - acc:         0.718900 loss:        0.929412
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.856680 loss:        0.427561
Test - acc:         0.828300 loss:        0.521224
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.913180 loss:        0.261096
Test - acc:         0.905300 loss:        0.288433
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.930540 loss:        0.206214
Test - acc:         0.910500 loss:        0.273561
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.937320 loss:        0.186638
Test - acc:         0.912900 loss:        0.267047
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.942460 loss:        0.170749
Test - acc:         0.915000 loss:        0.264871
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.945440 loss:        0.159415
Test - acc:         0.911700 loss:        0.267225
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.947720 loss:        0.151447
Test - acc:         0.912300 loss:        0.275296
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.951200 loss:        0.143222
Test - acc:         0.909800 loss:        0.281212
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.953760 loss:        0.132530
Test - acc:         0.912000 loss:        0.274361
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.956520 loss:        0.126495
Test - acc:         0.909800 loss:        0.280519
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.958620 loss:        0.120732
Test - acc:         0.912700 loss:        0.279221
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.959940 loss:        0.114047
Test - acc:         0.913500 loss:        0.279786
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.961680 loss:        0.110841
Test - acc:         0.913100 loss:        0.278369
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.963440 loss:        0.109032
Test - acc:         0.909000 loss:        0.303153
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.964240 loss:        0.106701
Test - acc:         0.911200 loss:        0.292746
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.965100 loss:        0.099199
Test - acc:         0.912200 loss:        0.289886
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.965260 loss:        0.100142
Test - acc:         0.913500 loss:        0.293318
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.965280 loss:        0.098817
Test - acc:         0.908500 loss:        0.303224
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.966860 loss:        0.096801
Test - acc:         0.912400 loss:        0.293253
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.966980 loss:        0.093976
Test - acc:         0.915800 loss:        0.291065
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.968960 loss:        0.092413
Test - acc:         0.907100 loss:        0.311648
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.970360 loss:        0.087475
Test - acc:         0.914800 loss:        0.298373
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.967260 loss:        0.093244
Test - acc:         0.912100 loss:        0.297417
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.969800 loss:        0.087923
Test - acc:         0.910000 loss:        0.309066
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.968620 loss:        0.090870
Test - acc:         0.909900 loss:        0.321668
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.967840 loss:        0.093434
Test - acc:         0.907500 loss:        0.329520
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.968420 loss:        0.090350
Test - acc:         0.909600 loss:        0.321203
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.969220 loss:        0.086727
Test - acc:         0.909200 loss:        0.321533
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.968000 loss:        0.093895
Test - acc:         0.908700 loss:        0.318312
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.968680 loss:        0.091142
Test - acc:         0.908700 loss:        0.313994
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.969100 loss:        0.088950
Test - acc:         0.904400 loss:        0.337439
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.967580 loss:        0.094739
Test - acc:         0.895500 loss:        0.375530
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.968180 loss:        0.092214
Test - acc:         0.906000 loss:        0.334174
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.967920 loss:        0.091866
Test - acc:         0.907800 loss:        0.323849
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.968260 loss:        0.093489
Test - acc:         0.905000 loss:        0.350946
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.966980 loss:        0.094809
Test - acc:         0.909500 loss:        0.308838
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.968600 loss:        0.090075
Test - acc:         0.895500 loss:        0.375683
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.966400 loss:        0.098140
Test - acc:         0.907200 loss:        0.311478
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.967940 loss:        0.094332
Test - acc:         0.905100 loss:        0.330057
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.968520 loss:        0.091606
Test - acc:         0.906300 loss:        0.335977
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.968100 loss:        0.093652
Test - acc:         0.904300 loss:        0.349990
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.968440 loss:        0.091395
Test - acc:         0.908500 loss:        0.321689
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.967660 loss:        0.094305
Test - acc:         0.898700 loss:        0.339195
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.967560 loss:        0.091473
Test - acc:         0.905600 loss:        0.351991
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.967000 loss:        0.095617
Test - acc:         0.900800 loss:        0.356509
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.968960 loss:        0.088306
Test - acc:         0.896200 loss:        0.367461
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.967820 loss:        0.094616
Test - acc:         0.900500 loss:        0.354264
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.968600 loss:        0.092551
Test - acc:         0.894200 loss:        0.380842
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.967800 loss:        0.094280
Test - acc:         0.899700 loss:        0.356435
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.967960 loss:        0.093025
Test - acc:         0.904200 loss:        0.348830
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.967360 loss:        0.094455
Test - acc:         0.896300 loss:        0.372403
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.971760 loss:        0.083367
Test - acc:         0.905400 loss:        0.325088
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.974780 loss:        0.075120
Test - acc:         0.908900 loss:        0.329394
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.973220 loss:        0.075930
Test - acc:         0.896400 loss:        0.398942
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.972140 loss:        0.081038
Test - acc:         0.904700 loss:        0.339935
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.973100 loss:        0.076664
Test - acc:         0.907400 loss:        0.321447
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.972880 loss:        0.081715
Test - acc:         0.907300 loss:        0.324979
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.973940 loss:        0.076578
Test - acc:         0.899200 loss:        0.372582
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.972000 loss:        0.080495
Test - acc:         0.905500 loss:        0.347326
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.971360 loss:        0.082351
Test - acc:         0.901900 loss:        0.355904
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.972060 loss:        0.081295
Test - acc:         0.902700 loss:        0.349757
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.971600 loss:        0.082304
Test - acc:         0.893300 loss:        0.396304
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.972080 loss:        0.081138
Test - acc:         0.901600 loss:        0.366321
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.970880 loss:        0.085116
Test - acc:         0.902300 loss:        0.347638
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.972120 loss:        0.081817
Test - acc:         0.901000 loss:        0.352960
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.972300 loss:        0.079716
Test - acc:         0.895900 loss:        0.388434
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.971520 loss:        0.080130
Test - acc:         0.907700 loss:        0.335053
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.971660 loss:        0.082821
Test - acc:         0.900300 loss:        0.365157
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.971000 loss:        0.084240
Test - acc:         0.902100 loss:        0.358441
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.971180 loss:        0.083605
Test - acc:         0.905100 loss:        0.345838
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.969820 loss:        0.086795
Test - acc:         0.900600 loss:        0.355693
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.972500 loss:        0.080456
Test - acc:         0.903400 loss:        0.364523
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.970140 loss:        0.085888
Test - acc:         0.893400 loss:        0.388660
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.970720 loss:        0.086408
Test - acc:         0.902200 loss:        0.364226
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.971660 loss:        0.083763
Test - acc:         0.905300 loss:        0.328924
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.970480 loss:        0.086580
Test - acc:         0.899300 loss:        0.364574
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.973540 loss:        0.078014
Test - acc:         0.901800 loss:        0.372476
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.969720 loss:        0.087292
Test - acc:         0.895500 loss:        0.369915
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.972120 loss:        0.080657
Test - acc:         0.901300 loss:        0.365929
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.971460 loss:        0.082654
Test - acc:         0.901300 loss:        0.359635
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.971960 loss:        0.081815
Test - acc:         0.900500 loss:        0.358472
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.971160 loss:        0.083374
Test - acc:         0.899500 loss:        0.362463
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.973080 loss:        0.081425
Test - acc:         0.898300 loss:        0.376503
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.970340 loss:        0.087411
Test - acc:         0.902900 loss:        0.357357
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.971420 loss:        0.081828
Test - acc:         0.905700 loss:        0.339794
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.970860 loss:        0.083619
Test - acc:         0.904700 loss:        0.348179
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.973580 loss:        0.078191
Test - acc:         0.913300 loss:        0.323580
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.970620 loss:        0.083942
Test - acc:         0.902100 loss:        0.342126
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.971440 loss:        0.083039
Test - acc:         0.902600 loss:        0.353999
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.971740 loss:        0.080499
Test - acc:         0.905700 loss:        0.349474
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.971800 loss:        0.081648
Test - acc:         0.898500 loss:        0.380524
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.970780 loss:        0.085427
Test - acc:         0.901000 loss:        0.359914
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.971020 loss:        0.083612
Test - acc:         0.906100 loss:        0.333312
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.973040 loss:        0.079183
Test - acc:         0.905400 loss:        0.354916
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.970200 loss:        0.086316
Test - acc:         0.907800 loss:        0.329637
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.972600 loss:        0.082203
Test - acc:         0.904900 loss:        0.345203
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.972120 loss:        0.082190
Test - acc:         0.900200 loss:        0.366568
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.970900 loss:        0.084547
Test - acc:         0.901600 loss:        0.353313
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.974080 loss:        0.077382
Test - acc:         0.902500 loss:        0.359215
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.972640 loss:        0.080357
Test - acc:         0.890400 loss:        0.403991
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.970460 loss:        0.084867
Test - acc:         0.893000 loss:        0.387417
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.983460 loss:        0.052677
Test - acc:         0.920400 loss:        0.274104
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.989380 loss:        0.035818
Test - acc:         0.922400 loss:        0.273339
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.991240 loss:        0.029261
Test - acc:         0.922700 loss:        0.272814
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.991800 loss:        0.026643
Test - acc:         0.923000 loss:        0.275633
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.993080 loss:        0.023190
Test - acc:         0.923100 loss:        0.279550
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.994100 loss:        0.021276
Test - acc:         0.923100 loss:        0.280419
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.994600 loss:        0.018993
Test - acc:         0.924900 loss:        0.284695
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.994580 loss:        0.018133
Test - acc:         0.924400 loss:        0.288682
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.994620 loss:        0.018057
Test - acc:         0.926500 loss:        0.291516
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.995260 loss:        0.015989
Test - acc:         0.924300 loss:        0.297891
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.995120 loss:        0.016047
Test - acc:         0.925800 loss:        0.295953
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.995980 loss:        0.013884
Test - acc:         0.923600 loss:        0.296057
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.996300 loss:        0.013595
Test - acc:         0.923600 loss:        0.299462
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.995980 loss:        0.013785
Test - acc:         0.924600 loss:        0.303664
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.996440 loss:        0.012422
Test - acc:         0.924000 loss:        0.304869
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.996320 loss:        0.012548
Test - acc:         0.926500 loss:        0.305339
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.996920 loss:        0.011253
Test - acc:         0.926100 loss:        0.303629
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.996560 loss:        0.011625
Test - acc:         0.927100 loss:        0.308218
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.997120 loss:        0.010853
Test - acc:         0.926300 loss:        0.305959
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.997300 loss:        0.009919
Test - acc:         0.925600 loss:        0.310590
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.997240 loss:        0.010107
Test - acc:         0.925100 loss:        0.309140
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.997260 loss:        0.009961
Test - acc:         0.925300 loss:        0.314927
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.997080 loss:        0.009822
Test - acc:         0.925900 loss:        0.313381
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.997060 loss:        0.009561
Test - acc:         0.926000 loss:        0.314227
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.997680 loss:        0.008702
Test - acc:         0.927000 loss:        0.316533
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.997800 loss:        0.008448
Test - acc:         0.927600 loss:        0.316710
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.997500 loss:        0.008577
Test - acc:         0.927800 loss:        0.316206
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.997560 loss:        0.008408
Test - acc:         0.927100 loss:        0.316699
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.997740 loss:        0.008074
Test - acc:         0.927700 loss:        0.316176
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.997920 loss:        0.007741
Test - acc:         0.927300 loss:        0.318910
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.997940 loss:        0.007785
Test - acc:         0.927500 loss:        0.321507
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.998100 loss:        0.007596
Test - acc:         0.927600 loss:        0.321880
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.998020 loss:        0.007243
Test - acc:         0.926700 loss:        0.323493
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.998300 loss:        0.006596
Test - acc:         0.928000 loss:        0.324748
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.998240 loss:        0.006737
Test - acc:         0.927200 loss:        0.322585
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.998520 loss:        0.006273
Test - acc:         0.927000 loss:        0.325746
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.998260 loss:        0.007198
Test - acc:         0.927100 loss:        0.324354
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.998540 loss:        0.006062
Test - acc:         0.927600 loss:        0.328054
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.998480 loss:        0.006049
Test - acc:         0.928100 loss:        0.329587
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.998380 loss:        0.005992
Test - acc:         0.925800 loss:        0.333711
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.998420 loss:        0.005885
Test - acc:         0.926300 loss:        0.335835
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.998640 loss:        0.005759
Test - acc:         0.927100 loss:        0.336263
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.998620 loss:        0.005786
Test - acc:         0.929300 loss:        0.332847
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.998580 loss:        0.005760
Test - acc:         0.928100 loss:        0.331750
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.998400 loss:        0.005979
Test - acc:         0.928200 loss:        0.338535
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.998620 loss:        0.005478
Test - acc:         0.928300 loss:        0.338925
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.004801
Test - acc:         0.927100 loss:        0.338213
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.998140 loss:        0.006431
Test - acc:         0.923700 loss:        0.338587
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.998560 loss:        0.005493
Test - acc:         0.926300 loss:        0.337031
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.998860 loss:        0.004947
Test - acc:         0.926800 loss:        0.335725
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.989840 loss:        0.032818
Test - acc:         0.921300 loss:        0.321579
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.992760 loss:        0.023451
Test - acc:         0.924000 loss:        0.314447
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.993880 loss:        0.020511
Test - acc:         0.922900 loss:        0.320484
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.994160 loss:        0.019872
Test - acc:         0.921900 loss:        0.315420
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.995020 loss:        0.016912
Test - acc:         0.925300 loss:        0.318328
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.994860 loss:        0.017383
Test - acc:         0.922900 loss:        0.320104
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.995480 loss:        0.015789
Test - acc:         0.923800 loss:        0.323748
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.995200 loss:        0.016012
Test - acc:         0.923600 loss:        0.322070
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.995220 loss:        0.014946
Test - acc:         0.923200 loss:        0.322247
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.996480 loss:        0.012468
Test - acc:         0.925600 loss:        0.324609
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.996680 loss:        0.012585
Test - acc:         0.923600 loss:        0.330906
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.995720 loss:        0.013701
Test - acc:         0.924200 loss:        0.331314
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.995960 loss:        0.013275
Test - acc:         0.923900 loss:        0.338277
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.996860 loss:        0.011312
Test - acc:         0.924300 loss:        0.334213
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.996260 loss:        0.012365
Test - acc:         0.924400 loss:        0.333942
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.996840 loss:        0.010809
Test - acc:         0.924800 loss:        0.341398
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.997120 loss:        0.010309
Test - acc:         0.923600 loss:        0.338871
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.996880 loss:        0.010858
Test - acc:         0.924900 loss:        0.338959
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.997500 loss:        0.009648
Test - acc:         0.925800 loss:        0.342829
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.997080 loss:        0.009613
Test - acc:         0.926900 loss:        0.345794
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.997640 loss:        0.009143
Test - acc:         0.924500 loss:        0.350228
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.997080 loss:        0.009473
Test - acc:         0.924900 loss:        0.346988
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.997400 loss:        0.008888
Test - acc:         0.926500 loss:        0.343151
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.997220 loss:        0.009746
Test - acc:         0.924600 loss:        0.346562
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.997280 loss:        0.009102
Test - acc:         0.924400 loss:        0.346228
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.997440 loss:        0.009002
Test - acc:         0.924400 loss:        0.349863
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.997660 loss:        0.008662
Test - acc:         0.924100 loss:        0.346506
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.997620 loss:        0.008302
Test - acc:         0.923700 loss:        0.346906
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.997780 loss:        0.007719
Test - acc:         0.924100 loss:        0.349519
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.997420 loss:        0.008796
Test - acc:         0.922500 loss:        0.355661
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.997960 loss:        0.007807
Test - acc:         0.924100 loss:        0.345781
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.997620 loss:        0.007888
Test - acc:         0.925500 loss:        0.349192
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.997760 loss:        0.008151
Test - acc:         0.924500 loss:        0.353479
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.997800 loss:        0.008265
Test - acc:         0.924200 loss:        0.352671
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.997640 loss:        0.007795
Test - acc:         0.922800 loss:        0.351198
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.997840 loss:        0.008326
Test - acc:         0.923700 loss:        0.347133
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.998380 loss:        0.006624
Test - acc:         0.924700 loss:        0.351279
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.997520 loss:        0.007339
Test - acc:         0.924600 loss:        0.350974
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.997760 loss:        0.007888
Test - acc:         0.923600 loss:        0.356322
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.997920 loss:        0.008166
Test - acc:         0.923900 loss:        0.357506
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.998180 loss:        0.006983
Test - acc:         0.924900 loss:        0.348317
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.997880 loss:        0.007485
Test - acc:         0.923700 loss:        0.353678
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.998100 loss:        0.007455
Test - acc:         0.924400 loss:        0.352974
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.998060 loss:        0.006893
Test - acc:         0.925000 loss:        0.352857
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.997860 loss:        0.007482
Test - acc:         0.928900 loss:        0.348979
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.998420 loss:        0.006534
Test - acc:         0.924500 loss:        0.357527
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.998080 loss:        0.006928
Test - acc:         0.925600 loss:        0.355865
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.998140 loss:        0.006640
Test - acc:         0.927100 loss:        0.354315
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.998160 loss:        0.006320
Test - acc:         0.926000 loss:        0.357173
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.998180 loss:        0.006896
Test - acc:         0.925500 loss:        0.355078
Sparsity :          0.9844
Wdecay :        0.000500
