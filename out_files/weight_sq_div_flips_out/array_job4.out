Running --model resnet18 --noise --prune_criterion weight_squared_div_flips --seed 42 --prune_freq 39 --prune_rate 0.5 --comment=resnet18_crit=weight_squared_div_flips_pf=39_seed=42 --save_model=pre-finetune/resnet18_weight_squared_div_flips_pf39_s42 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_squared_div_flips_pf39_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11164352
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.319760 loss:        2.027701
Test - acc:         0.373600 loss:        1.709656
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.483520 loss:        1.421054
Test - acc:         0.526700 loss:        1.263736
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.596320 loss:        1.129661
Test - acc:         0.595600 loss:        1.171379
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.673280 loss:        0.927933
Test - acc:         0.610100 loss:        1.134739
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.723460 loss:        0.790148
Test - acc:         0.705200 loss:        0.872062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.766400 loss:        0.675023
Test - acc:         0.746900 loss:        0.720768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.790000 loss:        0.608257
Test - acc:         0.766300 loss:        0.692052
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.805140 loss:        0.568890
Test - acc:         0.777000 loss:        0.661023
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.812440 loss:        0.540974
Test - acc:         0.748500 loss:        0.765813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.823580 loss:        0.513555
Test - acc:         0.742700 loss:        0.756514
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.831620 loss:        0.490783
Test - acc:         0.778800 loss:        0.671311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.834560 loss:        0.478143
Test - acc:         0.793000 loss:        0.626211
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.841480 loss:        0.462775
Test - acc:         0.792800 loss:        0.604948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838760 loss:        0.468699
Test - acc:         0.789100 loss:        0.624176
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844780 loss:        0.450396
Test - acc:         0.816100 loss:        0.559841
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.846280 loss:        0.448807
Test - acc:         0.764600 loss:        0.719807
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.850540 loss:        0.438978
Test - acc:         0.818300 loss:        0.530640
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.850840 loss:        0.433470
Test - acc:         0.796600 loss:        0.604748
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.853620 loss:        0.425959
Test - acc:         0.841700 loss:        0.488544
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.854620 loss:        0.421329
Test - acc:         0.795500 loss:        0.623324
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.859220 loss:        0.417579
Test - acc:         0.823000 loss:        0.516899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.856440 loss:        0.420470
Test - acc:         0.758600 loss:        0.712926
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.858640 loss:        0.411732
Test - acc:         0.793800 loss:        0.625225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.858100 loss:        0.413444
Test - acc:         0.793100 loss:        0.628817
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.862000 loss:        0.407803
Test - acc:         0.819100 loss:        0.573117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.861660 loss:        0.407108
Test - acc:         0.805500 loss:        0.599522
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.864180 loss:        0.397832
Test - acc:         0.836000 loss:        0.468893
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.863120 loss:        0.402166
Test - acc:         0.833700 loss:        0.486745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.863140 loss:        0.396906
Test - acc:         0.855500 loss:        0.423829
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.867600 loss:        0.394886
Test - acc:         0.810100 loss:        0.579276
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.865620 loss:        0.394207
Test - acc:         0.853400 loss:        0.434255
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.866820 loss:        0.393427
Test - acc:         0.698200 loss:        1.116456
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.866560 loss:        0.390132
Test - acc:         0.833300 loss:        0.506263
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.866820 loss:        0.389891
Test - acc:         0.838100 loss:        0.489114
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.868660 loss:        0.383678
Test - acc:         0.817400 loss:        0.559010
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.384254
Test - acc:         0.822300 loss:        0.562526
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.866740 loss:        0.387204
Test - acc:         0.820700 loss:        0.539781
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.868140 loss:        0.390223
Test - acc:         0.839300 loss:        0.510206
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.866480 loss:        0.388159
Test - acc:         0.846800 loss:        0.458624
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.879000 loss:        0.350591
Test - acc:         0.815900 loss:        0.558461
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.876200 loss:        0.361401
Test - acc:         0.802100 loss:        0.617512
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.877000 loss:        0.358484
Test - acc:         0.819000 loss:        0.537731
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.878040 loss:        0.360805
Test - acc:         0.807900 loss:        0.594469
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.874960 loss:        0.364547
Test - acc:         0.784800 loss:        0.681064
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.873720 loss:        0.369573
Test - acc:         0.815000 loss:        0.553531
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.872780 loss:        0.366659
Test - acc:         0.783800 loss:        0.678289
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.873940 loss:        0.368321
Test - acc:         0.825500 loss:        0.523897
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.878520 loss:        0.361379
Test - acc:         0.802400 loss:        0.599055
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.876580 loss:        0.366287
Test - acc:         0.771900 loss:        0.763725
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.876160 loss:        0.365261
Test - acc:         0.832100 loss:        0.510568
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.876460 loss:        0.363288
Test - acc:         0.828700 loss:        0.534667
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.874300 loss:        0.366862
Test - acc:         0.837900 loss:        0.494807
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.875800 loss:        0.365069
Test - acc:         0.778300 loss:        0.709194
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.877880 loss:        0.358199
Test - acc:         0.806300 loss:        0.627821
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.876300 loss:        0.362311
Test - acc:         0.819500 loss:        0.562000
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.876040 loss:        0.364468
Test - acc:         0.814800 loss:        0.571240
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.876380 loss:        0.359651
Test - acc:         0.842500 loss:        0.474710
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.874520 loss:        0.362030
Test - acc:         0.837400 loss:        0.487392
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.877420 loss:        0.364077
Test - acc:         0.832300 loss:        0.488122
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.876580 loss:        0.364442
Test - acc:         0.797400 loss:        0.619070
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.875440 loss:        0.365653
Test - acc:         0.829100 loss:        0.525394
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.873060 loss:        0.370014
Test - acc:         0.754900 loss:        0.834760
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.878160 loss:        0.358952
Test - acc:         0.805100 loss:        0.617437
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.875680 loss:        0.363444
Test - acc:         0.819800 loss:        0.563872
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.873840 loss:        0.367009
Test - acc:         0.836500 loss:        0.492251
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.878880 loss:        0.354727
Test - acc:         0.830600 loss:        0.527034
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.875300 loss:        0.360327
Test - acc:         0.850800 loss:        0.447722
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.876900 loss:        0.361804
Test - acc:         0.826000 loss:        0.550399
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.876660 loss:        0.360826
Test - acc:         0.873300 loss:        0.392009
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.879060 loss:        0.358971
Test - acc:         0.836100 loss:        0.493075
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.876620 loss:        0.361183
Test - acc:         0.835100 loss:        0.496601
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.873920 loss:        0.362530
Test - acc:         0.773800 loss:        0.731939
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.876640 loss:        0.362536
Test - acc:         0.810800 loss:        0.561269
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.877860 loss:        0.361526
Test - acc:         0.841800 loss:        0.469641
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.876920 loss:        0.359818
Test - acc:         0.822900 loss:        0.558111
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.878580 loss:        0.362274
Test - acc:         0.829300 loss:        0.532747
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.876860 loss:        0.363713
Test - acc:         0.840700 loss:        0.481071
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.878040 loss:        0.357867
Test - acc:         0.824600 loss:        0.516613
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.887880 loss:        0.325845
Test - acc:         0.844300 loss:        0.474720
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.887880 loss:        0.332063
Test - acc:         0.856900 loss:        0.430529
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.883220 loss:        0.342126
Test - acc:         0.845800 loss:        0.474771
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.882280 loss:        0.342621
Test - acc:         0.792000 loss:        0.642315
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.883840 loss:        0.340378
Test - acc:         0.843600 loss:        0.477579
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.884240 loss:        0.338837
Test - acc:         0.827600 loss:        0.530708
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.881560 loss:        0.343816
Test - acc:         0.850500 loss:        0.434032
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.882140 loss:        0.340996
Test - acc:         0.867900 loss:        0.397125
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.881720 loss:        0.345595
Test - acc:         0.835300 loss:        0.496482
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.886440 loss:        0.333914
Test - acc:         0.801700 loss:        0.599648
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.880720 loss:        0.351350
Test - acc:         0.807300 loss:        0.624491
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.884060 loss:        0.340878
Test - acc:         0.845000 loss:        0.474210
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.881760 loss:        0.343285
Test - acc:         0.855500 loss:        0.426618
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.884900 loss:        0.338474
Test - acc:         0.830900 loss:        0.510801
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.882380 loss:        0.341236
Test - acc:         0.829500 loss:        0.555720
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.880920 loss:        0.344263
Test - acc:         0.854000 loss:        0.435572
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.880480 loss:        0.346031
Test - acc:         0.804800 loss:        0.594597
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.882580 loss:        0.343145
Test - acc:         0.791700 loss:        0.649901
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.883620 loss:        0.339470
Test - acc:         0.853800 loss:        0.447784
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.882780 loss:        0.341879
Test - acc:         0.839700 loss:        0.488088
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.881160 loss:        0.346186
Test - acc:         0.855900 loss:        0.436876
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.882680 loss:        0.339411
Test - acc:         0.858600 loss:        0.428310
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.884280 loss:        0.339920
Test - acc:         0.835200 loss:        0.515460
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.883600 loss:        0.338469
Test - acc:         0.827800 loss:        0.534256
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.883140 loss:        0.341081
Test - acc:         0.832100 loss:        0.511436
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.881660 loss:        0.344715
Test - acc:         0.837900 loss:        0.503965
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.882740 loss:        0.342041
Test - acc:         0.841400 loss:        0.461635
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.882920 loss:        0.343381
Test - acc:         0.816100 loss:        0.595119
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.882720 loss:        0.343255
Test - acc:         0.836700 loss:        0.500614
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.882300 loss:        0.345673
Test - acc:         0.823200 loss:        0.526241
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.884720 loss:        0.342589
Test - acc:         0.839500 loss:        0.495775
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.883700 loss:        0.340490
Test - acc:         0.841800 loss:        0.490301
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.882300 loss:        0.342343
Test - acc:         0.794000 loss:        0.639903
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.883360 loss:        0.342256
Test - acc:         0.813100 loss:        0.564996
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.881520 loss:        0.346838
Test - acc:         0.836500 loss:        0.513972
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.884080 loss:        0.342089
Test - acc:         0.856300 loss:        0.424530
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.883560 loss:        0.341343
Test - acc:         0.846500 loss:        0.456333
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.883900 loss:        0.342985
Test - acc:         0.807400 loss:        0.617392
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.882160 loss:        0.339062
Test - acc:         0.855000 loss:        0.439577
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.893240 loss:        0.309590
Test - acc:         0.795400 loss:        0.624126
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.894880 loss:        0.306947
Test - acc:         0.825800 loss:        0.535056
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.891960 loss:        0.317734
Test - acc:         0.853300 loss:        0.443863
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.891220 loss:        0.315510
Test - acc:         0.853000 loss:        0.444802
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.889220 loss:        0.323887
Test - acc:         0.806600 loss:        0.595185
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.888840 loss:        0.322036
Test - acc:         0.863000 loss:        0.420859
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.889000 loss:        0.319558
Test - acc:         0.815400 loss:        0.577169
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.889480 loss:        0.321792
Test - acc:         0.853100 loss:        0.449241
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.891340 loss:        0.318095
Test - acc:         0.865600 loss:        0.397444
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.891660 loss:        0.318544
Test - acc:         0.801100 loss:        0.676060
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.888760 loss:        0.323556
Test - acc:         0.843100 loss:        0.460257
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.888420 loss:        0.320921
Test - acc:         0.831900 loss:        0.529785
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.888780 loss:        0.322952
Test - acc:         0.842600 loss:        0.470462
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.891440 loss:        0.321157
Test - acc:         0.840900 loss:        0.479823
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.888860 loss:        0.325357
Test - acc:         0.871300 loss:        0.377440
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.889740 loss:        0.322056
Test - acc:         0.836700 loss:        0.491077
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.889740 loss:        0.319921
Test - acc:         0.801400 loss:        0.636393
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.887200 loss:        0.326071
Test - acc:         0.859400 loss:        0.429409
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.891640 loss:        0.315953
Test - acc:         0.829600 loss:        0.535269
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.889540 loss:        0.323073
Test - acc:         0.849100 loss:        0.458538
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.890800 loss:        0.316967
Test - acc:         0.823000 loss:        0.540847
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.887460 loss:        0.325783
Test - acc:         0.778600 loss:        0.708250
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.888640 loss:        0.323445
Test - acc:         0.829300 loss:        0.532066
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.889940 loss:        0.320970
Test - acc:         0.853000 loss:        0.466922
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.890020 loss:        0.320511
Test - acc:         0.863400 loss:        0.419946
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.887920 loss:        0.327599
Test - acc:         0.862900 loss:        0.406707
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.892520 loss:        0.316989
Test - acc:         0.843800 loss:        0.509825
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.889660 loss:        0.319160
Test - acc:         0.846700 loss:        0.482768
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.891160 loss:        0.321121
Test - acc:         0.821900 loss:        0.563016
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.890880 loss:        0.321686
Test - acc:         0.854300 loss:        0.439803
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.890440 loss:        0.320398
Test - acc:         0.832500 loss:        0.510217
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.888380 loss:        0.324031
Test - acc:         0.834700 loss:        0.515081
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.890440 loss:        0.320034
Test - acc:         0.854500 loss:        0.461845
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.936400 loss:        0.187959
Test - acc:         0.923700 loss:        0.225283
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.952600 loss:        0.142982
Test - acc:         0.928500 loss:        0.215735
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.956140 loss:        0.128328
Test - acc:         0.928000 loss:        0.211975
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.961320 loss:        0.114655
Test - acc:         0.931400 loss:        0.210946
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.964400 loss:        0.106692
Test - acc:         0.930100 loss:        0.208916
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.967620 loss:        0.095664
Test - acc:         0.931700 loss:        0.210156
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.969820 loss:        0.091396
Test - acc:         0.934500 loss:        0.204938
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.972020 loss:        0.085082
Test - acc:         0.935200 loss:        0.206787
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.973080 loss:        0.082125
Test - acc:         0.932600 loss:        0.212554
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.975040 loss:        0.076198
Test - acc:         0.931500 loss:        0.212721
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.974720 loss:        0.074914
Test - acc:         0.935800 loss:        0.214244
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.976400 loss:        0.070886
Test - acc:         0.930200 loss:        0.220994
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.978340 loss:        0.065552
Test - acc:         0.931900 loss:        0.220310
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.977920 loss:        0.064395
Test - acc:         0.932600 loss:        0.230333
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.978840 loss:        0.061735
Test - acc:         0.932800 loss:        0.228596
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.979180 loss:        0.062154
Test - acc:         0.934200 loss:        0.227093
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.060470
Test - acc:         0.930300 loss:        0.236023
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.980500 loss:        0.059363
Test - acc:         0.932000 loss:        0.228571
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.981380 loss:        0.057707
Test - acc:         0.930300 loss:        0.237090
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.056995
Test - acc:         0.930200 loss:        0.243628
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.981400 loss:        0.057098
Test - acc:         0.930300 loss:        0.240447
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.980600 loss:        0.056147
Test - acc:         0.929800 loss:        0.234414
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.981320 loss:        0.056028
Test - acc:         0.927700 loss:        0.244965
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.980980 loss:        0.055986
Test - acc:         0.928800 loss:        0.243195
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.982460 loss:        0.054509
Test - acc:         0.926400 loss:        0.250278
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.981520 loss:        0.055354
Test - acc:         0.922600 loss:        0.272675
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.982020 loss:        0.055155
Test - acc:         0.922700 loss:        0.274094
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.981120 loss:        0.055584
Test - acc:         0.929200 loss:        0.237195
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.981560 loss:        0.054803
Test - acc:         0.927200 loss:        0.249391
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.056407
Test - acc:         0.926600 loss:        0.255938
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.982000 loss:        0.054249
Test - acc:         0.924200 loss:        0.259991
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.980740 loss:        0.057147
Test - acc:         0.925000 loss:        0.268850
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.979260 loss:        0.060946
Test - acc:         0.923700 loss:        0.255807
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.980980 loss:        0.056687
Test - acc:         0.921800 loss:        0.266308
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.979780 loss:        0.058672
Test - acc:         0.926900 loss:        0.260934
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.979660 loss:        0.059480
Test - acc:         0.923000 loss:        0.261239
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.979640 loss:        0.061081
Test - acc:         0.926300 loss:        0.258802
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.979920 loss:        0.060322
Test - acc:         0.925100 loss:        0.256176
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.979000 loss:        0.062632
Test - acc:         0.925000 loss:        0.264948
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.978700 loss:        0.062219
Test - acc:         0.912400 loss:        0.310590
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.979540 loss:        0.061784
Test - acc:         0.920500 loss:        0.276804
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.978480 loss:        0.062646
Test - acc:         0.921100 loss:        0.277568
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.978080 loss:        0.065097
Test - acc:         0.919800 loss:        0.277212
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.977640 loss:        0.064010
Test - acc:         0.925600 loss:        0.261190
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.978000 loss:        0.065203
Test - acc:         0.922300 loss:        0.273230
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.977940 loss:        0.066167
Test - acc:         0.930100 loss:        0.246345
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.980420 loss:        0.057315
Test - acc:         0.926200 loss:        0.246215
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.981500 loss:        0.055819
Test - acc:         0.921600 loss:        0.276096
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.979100 loss:        0.060657
Test - acc:         0.924200 loss:        0.266996
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.980040 loss:        0.059140
Test - acc:         0.922800 loss:        0.269061
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.981600 loss:        0.055576
Test - acc:         0.923300 loss:        0.267086
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.981680 loss:        0.054461
Test - acc:         0.918200 loss:        0.292729
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.982200 loss:        0.053186
Test - acc:         0.924900 loss:        0.269872
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.980780 loss:        0.059239
Test - acc:         0.914200 loss:        0.314445
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.981060 loss:        0.057207
Test - acc:         0.922400 loss:        0.279579
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.058590
Test - acc:         0.924300 loss:        0.260940
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.979220 loss:        0.059838
Test - acc:         0.921800 loss:        0.267306
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.980520 loss:        0.058678
Test - acc:         0.913700 loss:        0.316915
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.977620 loss:        0.064837
Test - acc:         0.921600 loss:        0.285960
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.979420 loss:        0.060951
Test - acc:         0.919100 loss:        0.297068
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.978680 loss:        0.064117
Test - acc:         0.923500 loss:        0.266095
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.979820 loss:        0.058209
Test - acc:         0.921100 loss:        0.271989
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.978120 loss:        0.063056
Test - acc:         0.923000 loss:        0.277164
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.978720 loss:        0.064255
Test - acc:         0.922600 loss:        0.272294
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.979800 loss:        0.060506
Test - acc:         0.927200 loss:        0.270634
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.980200 loss:        0.058911
Test - acc:         0.920000 loss:        0.287992
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.979500 loss:        0.060781
Test - acc:         0.923000 loss:        0.260796
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.979080 loss:        0.062312
Test - acc:         0.924000 loss:        0.259330
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.978700 loss:        0.061975
Test - acc:         0.920000 loss:        0.284205
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.979560 loss:        0.059612
Test - acc:         0.919200 loss:        0.297661
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.978560 loss:        0.063359
Test - acc:         0.923100 loss:        0.272082
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.979580 loss:        0.061567
Test - acc:         0.920900 loss:        0.292514
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.979920 loss:        0.061351
Test - acc:         0.923700 loss:        0.260757
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.977880 loss:        0.063434
Test - acc:         0.921900 loss:        0.260351
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.980140 loss:        0.059584
Test - acc:         0.922600 loss:        0.276324
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.978080 loss:        0.064189
Test - acc:         0.917800 loss:        0.295324
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.977380 loss:        0.066033
Test - acc:         0.912900 loss:        0.303658
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.979080 loss:        0.062849
Test - acc:         0.925100 loss:        0.272497
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.980400 loss:        0.059887
Test - acc:         0.918900 loss:        0.294669
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.977400 loss:        0.065128
Test - acc:         0.918700 loss:        0.290599
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.979480 loss:        0.060855
Test - acc:         0.920300 loss:        0.284254
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.979180 loss:        0.059769
Test - acc:         0.916100 loss:        0.312882
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.978240 loss:        0.063949
Test - acc:         0.914500 loss:        0.303320
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.978280 loss:        0.063691
Test - acc:         0.926000 loss:        0.266788
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.970920 loss:        0.086114
Test - acc:         0.921500 loss:        0.260888
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.973320 loss:        0.078464
Test - acc:         0.919300 loss:        0.273082
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.974580 loss:        0.075393
Test - acc:         0.917000 loss:        0.293118
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.975280 loss:        0.073276
Test - acc:         0.925200 loss:        0.256982
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.976920 loss:        0.067837
Test - acc:         0.916500 loss:        0.292461
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.976640 loss:        0.068272
Test - acc:         0.916700 loss:        0.287081
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.977160 loss:        0.067626
Test - acc:         0.920900 loss:        0.274212
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.975380 loss:        0.070454
Test - acc:         0.920100 loss:        0.290083
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.977720 loss:        0.067312
Test - acc:         0.922200 loss:        0.279364
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.977460 loss:        0.068115
Test - acc:         0.923600 loss:        0.270134
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.977380 loss:        0.067521
Test - acc:         0.924800 loss:        0.266824
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.976800 loss:        0.067344
Test - acc:         0.918300 loss:        0.291306
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.976240 loss:        0.069668
Test - acc:         0.920000 loss:        0.276914
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.976160 loss:        0.068402
Test - acc:         0.918300 loss:        0.283968
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.977260 loss:        0.067998
Test - acc:         0.915000 loss:        0.302619
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.977160 loss:        0.068999
Test - acc:         0.919600 loss:        0.277565
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.986460 loss:        0.044840
Test - acc:         0.934900 loss:        0.221920
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.990260 loss:        0.033133
Test - acc:         0.935000 loss:        0.220343
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.991620 loss:        0.030603
Test - acc:         0.936500 loss:        0.216771
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.993280 loss:        0.025919
Test - acc:         0.937500 loss:        0.215359
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.993120 loss:        0.025065
Test - acc:         0.938100 loss:        0.216187
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.994720 loss:        0.022833
Test - acc:         0.938300 loss:        0.217639
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.994880 loss:        0.021929
Test - acc:         0.937800 loss:        0.218965
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.994080 loss:        0.022742
Test - acc:         0.938200 loss:        0.217663
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.995140 loss:        0.020329
Test - acc:         0.937600 loss:        0.220609
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.995420 loss:        0.019388
Test - acc:         0.937300 loss:        0.223169
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.995420 loss:        0.019153
Test - acc:         0.937800 loss:        0.219303
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.995860 loss:        0.018216
Test - acc:         0.936800 loss:        0.221659
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.995880 loss:        0.017534
Test - acc:         0.937300 loss:        0.221446
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.996000 loss:        0.017629
Test - acc:         0.938000 loss:        0.222314
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.996080 loss:        0.017289
Test - acc:         0.937100 loss:        0.220984
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.995820 loss:        0.017670
Test - acc:         0.938600 loss:        0.223445
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.997040 loss:        0.016226
Test - acc:         0.938700 loss:        0.222901
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.996400 loss:        0.016299
Test - acc:         0.939300 loss:        0.223991
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.996480 loss:        0.015768
Test - acc:         0.937700 loss:        0.224028
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.996960 loss:        0.014854
Test - acc:         0.939400 loss:        0.222982
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.996920 loss:        0.014495
Test - acc:         0.939700 loss:        0.224545
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.996600 loss:        0.015170
Test - acc:         0.939100 loss:        0.222302
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.996440 loss:        0.015178
Test - acc:         0.938900 loss:        0.224376
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.964820 loss:        0.109109
Test - acc:         0.918600 loss:        0.264304
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.972580 loss:        0.084973
Test - acc:         0.920300 loss:        0.259993
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.974960 loss:        0.076948
Test - acc:         0.924000 loss:        0.253624
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.978040 loss:        0.069610
Test - acc:         0.924100 loss:        0.251447
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.977900 loss:        0.067780
Test - acc:         0.923000 loss:        0.250065
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.979600 loss:        0.063979
Test - acc:         0.925600 loss:        0.250759
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.981220 loss:        0.061351
Test - acc:         0.924700 loss:        0.253365
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.980940 loss:        0.057966
Test - acc:         0.925600 loss:        0.250471
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.982300 loss:        0.056030
Test - acc:         0.924800 loss:        0.248820
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.983020 loss:        0.055250
Test - acc:         0.927000 loss:        0.248672
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.984500 loss:        0.050557
Test - acc:         0.926500 loss:        0.247850
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.984160 loss:        0.052781
Test - acc:         0.927900 loss:        0.247796
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.984200 loss:        0.051480
Test - acc:         0.926500 loss:        0.248266
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.984240 loss:        0.050561
Test - acc:         0.926300 loss:        0.251321
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.985220 loss:        0.047668
Test - acc:         0.927200 loss:        0.249479
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.986000 loss:        0.046250
Test - acc:         0.926100 loss:        0.247966
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.985760 loss:        0.046208
Test - acc:         0.925900 loss:        0.251095
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.986420 loss:        0.045376
Test - acc:         0.927200 loss:        0.248918
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.986740 loss:        0.044528
Test - acc:         0.927000 loss:        0.249676
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.986220 loss:        0.045219
Test - acc:         0.927500 loss:        0.251368
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.986340 loss:        0.043897
Test - acc:         0.929600 loss:        0.248853
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.988100 loss:        0.041854
Test - acc:         0.928700 loss:        0.245667
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.988020 loss:        0.040964
Test - acc:         0.927900 loss:        0.248150
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.987380 loss:        0.042207
Test - acc:         0.928100 loss:        0.248317
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.988260 loss:        0.040391
Test - acc:         0.928100 loss:        0.248816
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.988280 loss:        0.039243
Test - acc:         0.929300 loss:        0.249125
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.988860 loss:        0.037473
Test - acc:         0.929700 loss:        0.247716
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.988220 loss:        0.037974
Test - acc:         0.926500 loss:        0.253556
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.989500 loss:        0.037131
Test - acc:         0.927600 loss:        0.252955
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.988980 loss:        0.037000
Test - acc:         0.927200 loss:        0.254519
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.989780 loss:        0.035940
Test - acc:         0.928400 loss:        0.253216
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.989100 loss:        0.037684
Test - acc:         0.928700 loss:        0.256382
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.989800 loss:        0.035644
Test - acc:         0.928000 loss:        0.255858
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.989140 loss:        0.036603
Test - acc:         0.928500 loss:        0.255713
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.989780 loss:        0.035709
Test - acc:         0.928500 loss:        0.254708
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.990160 loss:        0.034149
Test - acc:         0.926400 loss:        0.254432
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.990420 loss:        0.034205
Test - acc:         0.928600 loss:        0.254123
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.990180 loss:        0.034408
Test - acc:         0.928300 loss:        0.256100
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.990500 loss:        0.033012
Test - acc:         0.926900 loss:        0.255806
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.887920 loss:        0.331068
Test - acc:         0.886700 loss:        0.344028
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.916180 loss:        0.242889
Test - acc:         0.899800 loss:        0.315484
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.926060 loss:        0.215812
Test - acc:         0.902300 loss:        0.306861
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.929940 loss:        0.204175
Test - acc:         0.904200 loss:        0.294531
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.934440 loss:        0.190951
Test - acc:         0.905700 loss:        0.289727
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.935260 loss:        0.186698
Test - acc:         0.907900 loss:        0.288404
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.940320 loss:        0.175410
Test - acc:         0.905800 loss:        0.286768
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.940780 loss:        0.173334
Test - acc:         0.907500 loss:        0.284106
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.941700 loss:        0.168395
Test - acc:         0.910000 loss:        0.280698
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.942100 loss:        0.168063
Test - acc:         0.909500 loss:        0.279426
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.944260 loss:        0.162734
Test - acc:         0.911300 loss:        0.276573
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.945880 loss:        0.157149
Test - acc:         0.912700 loss:        0.275965
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.945940 loss:        0.156441
Test - acc:         0.911700 loss:        0.277321
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.947920 loss:        0.152913
Test - acc:         0.910300 loss:        0.275759
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.947500 loss:        0.150653
Test - acc:         0.911700 loss:        0.275849
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.949500 loss:        0.147879
Test - acc:         0.914500 loss:        0.271225
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.948540 loss:        0.149789
Test - acc:         0.913000 loss:        0.276835
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.950000 loss:        0.146629
Test - acc:         0.911600 loss:        0.279236
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.948940 loss:        0.146122
Test - acc:         0.912500 loss:        0.277865
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.950340 loss:        0.143704
Test - acc:         0.913100 loss:        0.277273
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.951080 loss:        0.143744
Test - acc:         0.915000 loss:        0.275439
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.953200 loss:        0.138213
Test - acc:         0.914600 loss:        0.274610
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.952920 loss:        0.135641
Test - acc:         0.911700 loss:        0.274121
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.952940 loss:        0.135714
Test - acc:         0.913800 loss:        0.272557
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.952640 loss:        0.137720
Test - acc:         0.912500 loss:        0.271785
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.954120 loss:        0.135239
Test - acc:         0.911600 loss:        0.275600
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.953620 loss:        0.136479
Test - acc:         0.913000 loss:        0.273516
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.953680 loss:        0.135099
Test - acc:         0.912000 loss:        0.272363
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.956040 loss:        0.129693
Test - acc:         0.914200 loss:        0.274112
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.954080 loss:        0.131449
Test - acc:         0.912400 loss:        0.272090
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.954140 loss:        0.132778
Test - acc:         0.913000 loss:        0.278837
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.955260 loss:        0.129857
Test - acc:         0.914600 loss:        0.275878
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.955420 loss:        0.127686
Test - acc:         0.912600 loss:        0.271922
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.956840 loss:        0.126918
Test - acc:         0.913200 loss:        0.276392
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.956900 loss:        0.127498
Test - acc:         0.909900 loss:        0.279323
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.955440 loss:        0.129892
Test - acc:         0.914500 loss:        0.271880
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.955720 loss:        0.126834
Test - acc:         0.912900 loss:        0.278806
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.957120 loss:        0.126024
Test - acc:         0.913600 loss:        0.274653
Sparsity :          0.9961
Wdecay :        0.000500
