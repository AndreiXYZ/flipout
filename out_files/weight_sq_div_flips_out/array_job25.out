Running --model vgg19 --noise --prune_criterion weight_squared_div_flips --seed 43 --prune_freq 32 --prune_rate 0.5 --comment=vgg19_crit=weight_squared_div_flips_pf=32_seed=43 --save_model=pre-finetune/vgg19_weight_squared_div_flips_pf32_s43 --logdir=criterion_experiment_no_bias/vgg19
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_weight_squared_div_flips_pf32_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
num.prunable=20024000
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.105080 loss:        3.010980
Test - acc:         0.123000 loss:        2.359551
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.174860 loss:        2.110594
Test - acc:         0.220300 loss:        1.926493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.228320 loss:        1.910866
Test - acc:         0.260400 loss:        1.855760
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.280100 loss:        1.830354
Test - acc:         0.280900 loss:        1.853899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.337380 loss:        1.708202
Test - acc:         0.334500 loss:        1.776725
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.462520 loss:        1.424452
Test - acc:         0.521400 loss:        1.340117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.580400 loss:        1.178112
Test - acc:         0.606200 loss:        1.185553
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.652880 loss:        1.000811
Test - acc:         0.627500 loss:        1.159720
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.695780 loss:        0.890398
Test - acc:         0.423400 loss:        2.100009
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.726660 loss:        0.822717
Test - acc:         0.596000 loss:        1.244230
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.742400 loss:        0.778924
Test - acc:         0.633200 loss:        1.214685
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.756740 loss:        0.737486
Test - acc:         0.703600 loss:        0.920415
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.765240 loss:        0.717637
Test - acc:         0.699700 loss:        0.949451
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.769120 loss:        0.701513
Test - acc:         0.711200 loss:        0.881064
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.779120 loss:        0.674703
Test - acc:         0.733500 loss:        0.862860
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.783200 loss:        0.666881
Test - acc:         0.703900 loss:        0.881886
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.789080 loss:        0.649335
Test - acc:         0.656300 loss:        1.096627
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.790460 loss:        0.640201
Test - acc:         0.726200 loss:        0.868548
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.793520 loss:        0.634689
Test - acc:         0.695700 loss:        0.978369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.799100 loss:        0.619804
Test - acc:         0.743000 loss:        0.818328
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.798620 loss:        0.618211
Test - acc:         0.759600 loss:        0.749610
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.801900 loss:        0.610053
Test - acc:         0.771800 loss:        0.715111
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.803940 loss:        0.601588
Test - acc:         0.746400 loss:        0.797954
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.806880 loss:        0.595910
Test - acc:         0.749800 loss:        0.763987
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.806680 loss:        0.596324
Test - acc:         0.764100 loss:        0.743477
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.811680 loss:        0.583949
Test - acc:         0.740500 loss:        0.875080
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.813120 loss:        0.581808
Test - acc:         0.771900 loss:        0.738781
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.810200 loss:        0.591224
Test - acc:         0.796400 loss:        0.653327
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.815840 loss:        0.575764
Test - acc:         0.768500 loss:        0.731990
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.818460 loss:        0.565293
Test - acc:         0.714600 loss:        1.062118
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.815740 loss:        0.568747
Test - acc:         0.728600 loss:        0.906000
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.820140 loss:        0.556274
Test - acc:         0.751500 loss:        0.763809
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.829160 loss:        0.526836
Test - acc:         0.759800 loss:        0.734195
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.830220 loss:        0.523180
Test - acc:         0.793200 loss:        0.634783
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.824680 loss:        0.534172
Test - acc:         0.735100 loss:        0.814379
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.827900 loss:        0.524633
Test - acc:         0.756800 loss:        0.771207
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.828460 loss:        0.523718
Test - acc:         0.787500 loss:        0.641243
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.826560 loss:        0.529690
Test - acc:         0.717900 loss:        0.990391
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.830340 loss:        0.523291
Test - acc:         0.745600 loss:        0.797464
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.831760 loss:        0.513043
Test - acc:         0.766400 loss:        0.720864
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.829020 loss:        0.523819
Test - acc:         0.777200 loss:        0.708021
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.831540 loss:        0.513969
Test - acc:         0.759700 loss:        0.820794
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.832060 loss:        0.514245
Test - acc:         0.794200 loss:        0.659396
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.833520 loss:        0.510264
Test - acc:         0.690200 loss:        1.122565
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.835520 loss:        0.504341
Test - acc:         0.749500 loss:        0.821331
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.832620 loss:        0.513704
Test - acc:         0.714600 loss:        0.964263
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.836760 loss:        0.504327
Test - acc:         0.769800 loss:        0.717654
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.835460 loss:        0.505083
Test - acc:         0.742600 loss:        0.804012
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.832280 loss:        0.505034
Test - acc:         0.794200 loss:        0.657209
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.831640 loss:        0.514729
Test - acc:         0.790000 loss:        0.664315
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.835700 loss:        0.501288
Test - acc:         0.801800 loss:        0.641483
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.836480 loss:        0.501576
Test - acc:         0.720100 loss:        0.952626
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.838420 loss:        0.498700
Test - acc:         0.823000 loss:        0.555373
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.840720 loss:        0.490184
Test - acc:         0.809700 loss:        0.590624
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.841360 loss:        0.485378
Test - acc:         0.773100 loss:        0.753562
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.836380 loss:        0.499343
Test - acc:         0.770700 loss:        0.722814
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.838180 loss:        0.493115
Test - acc:         0.776400 loss:        0.715152
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.837620 loss:        0.495505
Test - acc:         0.780900 loss:        0.679857
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.839100 loss:        0.490989
Test - acc:         0.786100 loss:        0.681415
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.838120 loss:        0.492799
Test - acc:         0.788400 loss:        0.662827
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.839680 loss:        0.490136
Test - acc:         0.782200 loss:        0.693620
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.841480 loss:        0.483401
Test - acc:         0.825100 loss:        0.519460
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.838320 loss:        0.491924
Test - acc:         0.738300 loss:        0.886233
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.838140 loss:        0.491485
Test - acc:         0.773900 loss:        0.678504
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.845360 loss:        0.470273
Test - acc:         0.783300 loss:        0.676551
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.844520 loss:        0.470325
Test - acc:         0.782500 loss:        0.697097
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.844380 loss:        0.474345
Test - acc:         0.787300 loss:        0.671329
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.844240 loss:        0.471668
Test - acc:         0.769800 loss:        0.782052
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.842040 loss:        0.477406
Test - acc:         0.789900 loss:        0.661711
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.843380 loss:        0.478700
Test - acc:         0.807400 loss:        0.633461
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.840360 loss:        0.480937
Test - acc:         0.805400 loss:        0.620227
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.845700 loss:        0.470898
Test - acc:         0.813400 loss:        0.544005
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.844760 loss:        0.469487
Test - acc:         0.720800 loss:        0.969729
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.846660 loss:        0.467401
Test - acc:         0.721800 loss:        0.982339
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.845000 loss:        0.473055
Test - acc:         0.745400 loss:        0.921296
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.844620 loss:        0.472170
Test - acc:         0.772000 loss:        0.759060
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.846660 loss:        0.468438
Test - acc:         0.795200 loss:        0.629912
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.844780 loss:        0.470432
Test - acc:         0.799900 loss:        0.638351
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.845480 loss:        0.471536
Test - acc:         0.754800 loss:        0.777342
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.842800 loss:        0.475134
Test - acc:         0.780400 loss:        0.662935
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.844220 loss:        0.472899
Test - acc:         0.717800 loss:        0.942994
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.847480 loss:        0.464755
Test - acc:         0.789500 loss:        0.679946
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.846060 loss:        0.466338
Test - acc:         0.771000 loss:        0.734676
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.845720 loss:        0.467921
Test - acc:         0.752100 loss:        0.785144
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.845420 loss:        0.468194
Test - acc:         0.794200 loss:        0.635939
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.847740 loss:        0.464056
Test - acc:         0.755100 loss:        0.753614
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.845440 loss:        0.469978
Test - acc:         0.797900 loss:        0.638922
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.848380 loss:        0.462632
Test - acc:         0.773000 loss:        0.703859
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.844600 loss:        0.466213
Test - acc:         0.772100 loss:        0.736608
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.847560 loss:        0.467040
Test - acc:         0.769600 loss:        0.762337
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.844680 loss:        0.473031
Test - acc:         0.806900 loss:        0.612513
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.848480 loss:        0.464347
Test - acc:         0.813600 loss:        0.577360
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.846000 loss:        0.463717
Test - acc:         0.792400 loss:        0.617804
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.845760 loss:        0.470905
Test - acc:         0.821900 loss:        0.542186
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.846960 loss:        0.464048
Test - acc:         0.706200 loss:        1.017913
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.847260 loss:        0.462125
Test - acc:         0.818700 loss:        0.572145
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.851340 loss:        0.444136
Test - acc:         0.821500 loss:        0.559959
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.850340 loss:        0.450961
Test - acc:         0.806400 loss:        0.615911
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.851600 loss:        0.446996
Test - acc:         0.742800 loss:        0.881634
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.851880 loss:        0.449397
Test - acc:         0.765200 loss:        0.767688
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.851080 loss:        0.456010
Test - acc:         0.806800 loss:        0.614302
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.850640 loss:        0.451139
Test - acc:         0.775000 loss:        0.720774
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.849900 loss:        0.453404
Test - acc:         0.796600 loss:        0.617496
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.851040 loss:        0.449310
Test - acc:         0.809200 loss:        0.584727
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.849480 loss:        0.453109
Test - acc:         0.818100 loss:        0.555319
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.849800 loss:        0.454562
Test - acc:         0.797400 loss:        0.624559
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.849880 loss:        0.453262
Test - acc:         0.835700 loss:        0.490447
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.851220 loss:        0.450180
Test - acc:         0.728700 loss:        0.947360
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.851200 loss:        0.447318
Test - acc:         0.792600 loss:        0.634964
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.851780 loss:        0.453088
Test - acc:         0.806500 loss:        0.596771
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.849400 loss:        0.452701
Test - acc:         0.780400 loss:        0.704763
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.852660 loss:        0.447974
Test - acc:         0.827600 loss:        0.522665
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.852560 loss:        0.444801
Test - acc:         0.699000 loss:        1.075809
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.848760 loss:        0.454147
Test - acc:         0.768900 loss:        0.760956
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.851240 loss:        0.446868
Test - acc:         0.792100 loss:        0.629150
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.852700 loss:        0.444361
Test - acc:         0.826300 loss:        0.517274
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.850720 loss:        0.445365
Test - acc:         0.799100 loss:        0.626859
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.850980 loss:        0.449602
Test - acc:         0.816400 loss:        0.571508
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.855000 loss:        0.436494
Test - acc:         0.747600 loss:        0.800143
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.851600 loss:        0.448807
Test - acc:         0.801600 loss:        0.650096
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.852320 loss:        0.443004
Test - acc:         0.815900 loss:        0.566114
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.851720 loss:        0.443358
Test - acc:         0.818500 loss:        0.535377
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.851620 loss:        0.443943
Test - acc:         0.818800 loss:        0.558437
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.854500 loss:        0.441227
Test - acc:         0.728700 loss:        0.968221
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.848780 loss:        0.452284
Test - acc:         0.803900 loss:        0.622882
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.853160 loss:        0.441214
Test - acc:         0.727600 loss:        0.941833
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.850940 loss:        0.449137
Test - acc:         0.761000 loss:        0.746384
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.854960 loss:        0.441921
Test - acc:         0.737700 loss:        0.910202
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.858120 loss:        0.421919
Test - acc:         0.691000 loss:        1.191619
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.860380 loss:        0.418705
Test - acc:         0.804200 loss:        0.608960
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.858680 loss:        0.423009
Test - acc:         0.813000 loss:        0.576219
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.853020 loss:        0.434120
Test - acc:         0.821600 loss:        0.557248
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.856440 loss:        0.427631
Test - acc:         0.793600 loss:        0.656982
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.858500 loss:        0.424571
Test - acc:         0.793000 loss:        0.678558
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.861520 loss:        0.415888
Test - acc:         0.802500 loss:        0.641636
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.857340 loss:        0.426802
Test - acc:         0.773000 loss:        0.711681
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.857660 loss:        0.427708
Test - acc:         0.707000 loss:        0.938678
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.855940 loss:        0.427603
Test - acc:         0.827700 loss:        0.518601
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.859400 loss:        0.420980
Test - acc:         0.785500 loss:        0.680604
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.856620 loss:        0.426708
Test - acc:         0.774000 loss:        0.734268
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.859200 loss:        0.421821
Test - acc:         0.789900 loss:        0.641187
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.857120 loss:        0.425415
Test - acc:         0.764100 loss:        0.768266
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.858280 loss:        0.428286
Test - acc:         0.817600 loss:        0.567293
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.859660 loss:        0.423331
Test - acc:         0.773100 loss:        0.687310
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.859280 loss:        0.422909
Test - acc:         0.804100 loss:        0.608742
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.858340 loss:        0.427803
Test - acc:         0.798100 loss:        0.623592
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.858540 loss:        0.427737
Test - acc:         0.798900 loss:        0.637400
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.858720 loss:        0.426489
Test - acc:         0.796600 loss:        0.646129
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.860740 loss:        0.420434
Test - acc:         0.814600 loss:        0.541793
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.857680 loss:        0.426664
Test - acc:         0.817600 loss:        0.555662
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.911400 loss:        0.261578
Test - acc:         0.902500 loss:        0.294501
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.928880 loss:        0.209423
Test - acc:         0.905400 loss:        0.281664
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.936100 loss:        0.189634
Test - acc:         0.910100 loss:        0.276555
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.941000 loss:        0.174688
Test - acc:         0.909900 loss:        0.274275
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.944640 loss:        0.165176
Test - acc:         0.910400 loss:        0.281179
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.947800 loss:        0.153123
Test - acc:         0.913600 loss:        0.274033
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.950120 loss:        0.145990
Test - acc:         0.911000 loss:        0.276471
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.954040 loss:        0.136809
Test - acc:         0.910200 loss:        0.274927
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.954620 loss:        0.132783
Test - acc:         0.911900 loss:        0.281210
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.956880 loss:        0.126765
Test - acc:         0.913100 loss:        0.284979
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.959580 loss:        0.117182
Test - acc:         0.912100 loss:        0.285292
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.960780 loss:        0.112798
Test - acc:         0.912100 loss:        0.290423
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.960940 loss:        0.112306
Test - acc:         0.912900 loss:        0.292840
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.963760 loss:        0.108375
Test - acc:         0.914600 loss:        0.282722
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.965580 loss:        0.099959
Test - acc:         0.912800 loss:        0.286967
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.964980 loss:        0.102221
Test - acc:         0.910600 loss:        0.311792
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.966320 loss:        0.099173
Test - acc:         0.908600 loss:        0.313693
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.964800 loss:        0.101092
Test - acc:         0.913800 loss:        0.298052
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.966680 loss:        0.095863
Test - acc:         0.913000 loss:        0.293809
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.966400 loss:        0.096909
Test - acc:         0.903000 loss:        0.333030
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.967460 loss:        0.096462
Test - acc:         0.899600 loss:        0.342698
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.966100 loss:        0.097554
Test - acc:         0.909500 loss:        0.324241
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.965840 loss:        0.096789
Test - acc:         0.909400 loss:        0.318340
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.967340 loss:        0.095400
Test - acc:         0.906200 loss:        0.325961
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.967560 loss:        0.092083
Test - acc:         0.914500 loss:        0.303646
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.966800 loss:        0.096846
Test - acc:         0.905300 loss:        0.332879
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.966640 loss:        0.094508
Test - acc:         0.906300 loss:        0.333891
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.965860 loss:        0.097841
Test - acc:         0.904000 loss:        0.321990
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.968000 loss:        0.092124
Test - acc:         0.907400 loss:        0.327545
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.967280 loss:        0.094235
Test - acc:         0.906000 loss:        0.325472
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.967200 loss:        0.094225
Test - acc:         0.903100 loss:        0.357734
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.964900 loss:        0.100378
Test - acc:         0.912200 loss:        0.303510
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.967360 loss:        0.095663
Test - acc:         0.901500 loss:        0.339689
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.964800 loss:        0.100143
Test - acc:         0.897300 loss:        0.348228
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.965400 loss:        0.099443
Test - acc:         0.904100 loss:        0.338464
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.964920 loss:        0.098918
Test - acc:         0.904200 loss:        0.331592
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.964440 loss:        0.101179
Test - acc:         0.904100 loss:        0.328634
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.966480 loss:        0.096957
Test - acc:         0.909200 loss:        0.320509
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.966420 loss:        0.097093
Test - acc:         0.904700 loss:        0.331397
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.965060 loss:        0.100017
Test - acc:         0.903700 loss:        0.338298
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.965800 loss:        0.097417
Test - acc:         0.907900 loss:        0.336151
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.963940 loss:        0.101431
Test - acc:         0.901000 loss:        0.345701
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.965560 loss:        0.101113
Test - acc:         0.899500 loss:        0.356950
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.967560 loss:        0.091566
Test - acc:         0.908500 loss:        0.322310
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.968120 loss:        0.091843
Test - acc:         0.901100 loss:        0.373909
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.968480 loss:        0.090610
Test - acc:         0.897900 loss:        0.368734
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.967880 loss:        0.088860
Test - acc:         0.911000 loss:        0.323354
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.968380 loss:        0.090958
Test - acc:         0.905400 loss:        0.346931
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.970320 loss:        0.086627
Test - acc:         0.902100 loss:        0.351148
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.967520 loss:        0.091975
Test - acc:         0.901100 loss:        0.352934
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.968720 loss:        0.093147
Test - acc:         0.908500 loss:        0.329852
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.967840 loss:        0.091083
Test - acc:         0.906900 loss:        0.340908
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.967840 loss:        0.091271
Test - acc:         0.906600 loss:        0.339669
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.967780 loss:        0.093820
Test - acc:         0.907200 loss:        0.326592
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.969400 loss:        0.089621
Test - acc:         0.901300 loss:        0.365654
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.968460 loss:        0.090945
Test - acc:         0.903500 loss:        0.345523
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.969360 loss:        0.089126
Test - acc:         0.904900 loss:        0.333595
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.970040 loss:        0.085748
Test - acc:         0.899200 loss:        0.368242
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.969620 loss:        0.089172
Test - acc:         0.908200 loss:        0.325443
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.968420 loss:        0.092287
Test - acc:         0.901100 loss:        0.340870
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.966120 loss:        0.097423
Test - acc:         0.902600 loss:        0.341865
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.969340 loss:        0.091369
Test - acc:         0.894900 loss:        0.361737
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.968700 loss:        0.092464
Test - acc:         0.901700 loss:        0.344222
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.967300 loss:        0.095448
Test - acc:         0.900500 loss:        0.355928
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.967560 loss:        0.094979
Test - acc:         0.893200 loss:        0.374416
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.966680 loss:        0.095915
Test - acc:         0.906000 loss:        0.325511
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.965680 loss:        0.099655
Test - acc:         0.887900 loss:        0.405655
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.966280 loss:        0.097535
Test - acc:         0.904100 loss:        0.341183
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.967080 loss:        0.095279
Test - acc:         0.899800 loss:        0.366024
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.966020 loss:        0.096266
Test - acc:         0.905400 loss:        0.336259
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.965940 loss:        0.097500
Test - acc:         0.896500 loss:        0.377630
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.968400 loss:        0.092353
Test - acc:         0.900000 loss:        0.344031
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.966680 loss:        0.096463
Test - acc:         0.899200 loss:        0.362124
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.966320 loss:        0.097664
Test - acc:         0.902500 loss:        0.331865
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.956800 loss:        0.124492
Test - acc:         0.899600 loss:        0.341219
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.962260 loss:        0.109513
Test - acc:         0.902100 loss:        0.344174
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.964000 loss:        0.102588
Test - acc:         0.893500 loss:        0.363119
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.964660 loss:        0.101720
Test - acc:         0.901800 loss:        0.357547
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.964900 loss:        0.101080
Test - acc:         0.891300 loss:        0.386179
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.965420 loss:        0.099997
Test - acc:         0.895300 loss:        0.362020
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.966040 loss:        0.097635
Test - acc:         0.901500 loss:        0.348112
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.966760 loss:        0.096495
Test - acc:         0.895600 loss:        0.372300
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.965240 loss:        0.102186
Test - acc:         0.903400 loss:        0.337211
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.966720 loss:        0.096587
Test - acc:         0.898100 loss:        0.351613
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.965400 loss:        0.098858
Test - acc:         0.903800 loss:        0.336122
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.964500 loss:        0.101768
Test - acc:         0.898500 loss:        0.341646
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.966080 loss:        0.099589
Test - acc:         0.907100 loss:        0.322703
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.967360 loss:        0.097807
Test - acc:         0.907300 loss:        0.320697
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.965680 loss:        0.097280
Test - acc:         0.903800 loss:        0.342622
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.964240 loss:        0.102163
Test - acc:         0.900600 loss:        0.349301
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.966160 loss:        0.099420
Test - acc:         0.905100 loss:        0.326796
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.965880 loss:        0.097055
Test - acc:         0.908300 loss:        0.327442
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.965200 loss:        0.099820
Test - acc:         0.897800 loss:        0.354154
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.965680 loss:        0.097100
Test - acc:         0.899100 loss:        0.364586
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.966100 loss:        0.095753
Test - acc:         0.895700 loss:        0.364938
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.965580 loss:        0.098218
Test - acc:         0.902000 loss:        0.352600
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.966860 loss:        0.097723
Test - acc:         0.897800 loss:        0.353488
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.965940 loss:        0.098914
Test - acc:         0.900000 loss:        0.348613
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.965600 loss:        0.098115
Test - acc:         0.884800 loss:        0.441459
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.966120 loss:        0.098583
Test - acc:         0.897500 loss:        0.376055
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.978840 loss:        0.063805
Test - acc:         0.919900 loss:        0.292411
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.983540 loss:        0.048908
Test - acc:         0.921400 loss:        0.289614
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.986080 loss:        0.044012
Test - acc:         0.921400 loss:        0.293632
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.986580 loss:        0.041734
Test - acc:         0.923300 loss:        0.293666
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.989260 loss:        0.035100
Test - acc:         0.922300 loss:        0.298607
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.989060 loss:        0.034437
Test - acc:         0.922500 loss:        0.302585
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.954040 loss:        0.131294
Test - acc:         0.906400 loss:        0.336966
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.961900 loss:        0.109024
Test - acc:         0.909300 loss:        0.327779
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.966400 loss:        0.098558
Test - acc:         0.909500 loss:        0.323510
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.968260 loss:        0.090955
Test - acc:         0.910500 loss:        0.317509
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.970560 loss:        0.085764
Test - acc:         0.913900 loss:        0.320174
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.971280 loss:        0.084302
Test - acc:         0.911600 loss:        0.319861
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.971060 loss:        0.081390
Test - acc:         0.912000 loss:        0.317263
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.972260 loss:        0.078763
Test - acc:         0.911900 loss:        0.319311
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.974080 loss:        0.074219
Test - acc:         0.913100 loss:        0.322243
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.974460 loss:        0.074634
Test - acc:         0.912900 loss:        0.318383
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.976180 loss:        0.069777
Test - acc:         0.912800 loss:        0.319783
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.976060 loss:        0.068440
Test - acc:         0.912300 loss:        0.327021
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.977220 loss:        0.067094
Test - acc:         0.913600 loss:        0.319984
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.978100 loss:        0.064198
Test - acc:         0.913500 loss:        0.324138
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.978140 loss:        0.064994
Test - acc:         0.914000 loss:        0.320728
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.979460 loss:        0.060987
Test - acc:         0.915400 loss:        0.329171
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.979300 loss:        0.061573
Test - acc:         0.914400 loss:        0.326570
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.979980 loss:        0.059662
Test - acc:         0.914900 loss:        0.327411
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.979640 loss:        0.060485
Test - acc:         0.914900 loss:        0.329516
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.980060 loss:        0.058945
Test - acc:         0.913700 loss:        0.328784
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.980100 loss:        0.058215
Test - acc:         0.913600 loss:        0.326109
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.981500 loss:        0.055574
Test - acc:         0.914400 loss:        0.333916
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.981660 loss:        0.053755
Test - acc:         0.916100 loss:        0.332653
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.980420 loss:        0.056106
Test - acc:         0.915600 loss:        0.329935
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.981420 loss:        0.054398
Test - acc:         0.914700 loss:        0.330517
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.981940 loss:        0.052757
Test - acc:         0.915800 loss:        0.331129
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.982620 loss:        0.050608
Test - acc:         0.915300 loss:        0.338525
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.982400 loss:        0.051177
Test - acc:         0.915400 loss:        0.335636
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.982840 loss:        0.050630
Test - acc:         0.917200 loss:        0.339264
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.982720 loss:        0.050516
Test - acc:         0.915000 loss:        0.341036
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.983360 loss:        0.050218
Test - acc:         0.915600 loss:        0.337347
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.982920 loss:        0.049876
Test - acc:         0.915600 loss:        0.342258
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.873160 loss:        0.395960
Test - acc:         0.868900 loss:        0.415741
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.902500 loss:        0.285079
Test - acc:         0.875700 loss:        0.393374
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.910980 loss:        0.260286
Test - acc:         0.876700 loss:        0.381430
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.916100 loss:        0.242251
Test - acc:         0.885900 loss:        0.367644
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.921020 loss:        0.228471
Test - acc:         0.881900 loss:        0.368761
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.923880 loss:        0.218766
Test - acc:         0.882200 loss:        0.363735
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.925700 loss:        0.212509
Test - acc:         0.889000 loss:        0.354856
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.928260 loss:        0.205987
Test - acc:         0.891400 loss:        0.349441
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.929760 loss:        0.202220
Test - acc:         0.888300 loss:        0.353073
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.930820 loss:        0.199101
Test - acc:         0.887900 loss:        0.354625
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.931880 loss:        0.194991
Test - acc:         0.890500 loss:        0.348912
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.934020 loss:        0.189317
Test - acc:         0.892000 loss:        0.345453
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.934760 loss:        0.186184
Test - acc:         0.892200 loss:        0.344561
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.935220 loss:        0.184988
Test - acc:         0.893100 loss:        0.342156
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.937460 loss:        0.181721
Test - acc:         0.892200 loss:        0.338514
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.938280 loss:        0.177795
Test - acc:         0.893400 loss:        0.343014
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.938020 loss:        0.177996
Test - acc:         0.894600 loss:        0.336213
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.940440 loss:        0.173563
Test - acc:         0.895700 loss:        0.335657
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.939660 loss:        0.168910
Test - acc:         0.896300 loss:        0.339485
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.939320 loss:        0.173583
Test - acc:         0.896300 loss:        0.336394
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.942200 loss:        0.166746
Test - acc:         0.896500 loss:        0.338028
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.941600 loss:        0.166659
Test - acc:         0.898700 loss:        0.338873
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.941520 loss:        0.165116
Test - acc:         0.895300 loss:        0.337175
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.943940 loss:        0.159823
Test - acc:         0.897200 loss:        0.340824
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.942640 loss:        0.161599
Test - acc:         0.895300 loss:        0.341420
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.943380 loss:        0.162221
Test - acc:         0.896800 loss:        0.338485
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.944420 loss:        0.158846
Test - acc:         0.897300 loss:        0.333473
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.944440 loss:        0.161417
Test - acc:         0.897400 loss:        0.328949
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.942760 loss:        0.160837
Test - acc:         0.898200 loss:        0.330115
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.946540 loss:        0.154280
Test - acc:         0.898600 loss:        0.334863
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.946080 loss:        0.155624
Test - acc:         0.896800 loss:        0.337784
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.946420 loss:        0.153799
Test - acc:         0.895900 loss:        0.342113
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.794140 loss:        0.624260
Test - acc:         0.828500 loss:        0.529697
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.839940 loss:        0.477167
Test - acc:         0.842700 loss:        0.488605
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.852740 loss:        0.434249
Test - acc:         0.849100 loss:        0.462256
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.862460 loss:        0.408861
Test - acc:         0.853900 loss:        0.447852
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.864820 loss:        0.397995
Test - acc:         0.855100 loss:        0.435190
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.869920 loss:        0.385949
Test - acc:         0.860500 loss:        0.430733
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.873820 loss:        0.375284
Test - acc:         0.860400 loss:        0.423583
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.874080 loss:        0.367301
Test - acc:         0.860700 loss:        0.420514
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.877500 loss:        0.359516
Test - acc:         0.863600 loss:        0.423165
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.878680 loss:        0.354564
Test - acc:         0.863600 loss:        0.415550
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.881220 loss:        0.347884
Test - acc:         0.867400 loss:        0.411301
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.882900 loss:        0.343095
Test - acc:         0.866500 loss:        0.405599
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.883260 loss:        0.342206
Test - acc:         0.864800 loss:        0.413341
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.885300 loss:        0.337128
Test - acc:         0.868300 loss:        0.405179
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.886100 loss:        0.334518
Test - acc:         0.865900 loss:        0.403065
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.885400 loss:        0.335716
Test - acc:         0.866500 loss:        0.405324
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.887640 loss:        0.326056
Test - acc:         0.868200 loss:        0.400519
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.888260 loss:        0.324546
Test - acc:         0.867800 loss:        0.407749
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.888940 loss:        0.327469
Test - acc:         0.868700 loss:        0.403678
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.889740 loss:        0.321216
Test - acc:         0.867700 loss:        0.403775
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.890240 loss:        0.319263
Test - acc:         0.870900 loss:        0.397477
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.890980 loss:        0.318778
Test - acc:         0.869100 loss:        0.400206
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.889980 loss:        0.320784
Test - acc:         0.865100 loss:        0.404864
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.891980 loss:        0.315993
Test - acc:         0.873400 loss:        0.393032
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.893100 loss:        0.312860
Test - acc:         0.870200 loss:        0.394224
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.889520 loss:        0.317532
Test - acc:         0.872000 loss:        0.386888
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.892260 loss:        0.313657
Test - acc:         0.874200 loss:        0.385590
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.894760 loss:        0.309705
Test - acc:         0.871000 loss:        0.391369
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.894440 loss:        0.309482
Test - acc:         0.875900 loss:        0.385978
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.895980 loss:        0.305909
Test - acc:         0.872300 loss:        0.394391
Sparsity :          0.9990
Wdecay :        0.000500
