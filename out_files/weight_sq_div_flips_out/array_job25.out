Running --model vgg19 --noise --prune_criterion weight_squared_div_flips --seed 43 --prune_freq 32 --prune_rate 0.5 --comment=vgg19_crit=weight_squared_div_flips_pf=32_seed=43 --save_model=pre-finetune/vgg19_weight_squared_div_flips_pf32_s43 --logdir=criterion_experiment_no_bias/vgg19
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_weight_squared_div_flips_pf32_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
num.prunable=20024000
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.105080 loss:        3.010980
Test - acc:         0.123000 loss:        2.359551
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.174860 loss:        2.110594
Test - acc:         0.220300 loss:        1.926493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.228320 loss:        1.910866
Test - acc:         0.260400 loss:        1.855760
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.280100 loss:        1.830354
Test - acc:         0.280900 loss:        1.853899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.337380 loss:        1.708202
Test - acc:         0.334500 loss:        1.776725
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.462520 loss:        1.424452
Test - acc:         0.521400 loss:        1.340117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.580400 loss:        1.178112
Test - acc:         0.606200 loss:        1.185553
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.652880 loss:        1.000811
Test - acc:         0.627500 loss:        1.159720
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.695780 loss:        0.890398
Test - acc:         0.423400 loss:        2.100009
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.726660 loss:        0.822717
Test - acc:         0.596000 loss:        1.244230
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.742400 loss:        0.778924
Test - acc:         0.633200 loss:        1.214685
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.756740 loss:        0.737486
Test - acc:         0.703600 loss:        0.920415
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.765240 loss:        0.717637
Test - acc:         0.699700 loss:        0.949451
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.769120 loss:        0.701513
Test - acc:         0.711200 loss:        0.881064
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.779120 loss:        0.674703
Test - acc:         0.733500 loss:        0.862860
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.783200 loss:        0.666881
Test - acc:         0.703900 loss:        0.881886
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.789080 loss:        0.649335
Test - acc:         0.656300 loss:        1.096627
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.790460 loss:        0.640201
Test - acc:         0.726200 loss:        0.868548
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.793520 loss:        0.634689
Test - acc:         0.695700 loss:        0.978369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.799100 loss:        0.619804
Test - acc:         0.743000 loss:        0.818328
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.798620 loss:        0.618211
Test - acc:         0.759600 loss:        0.749610
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.801900 loss:        0.610053
Test - acc:         0.771800 loss:        0.715111
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.803940 loss:        0.601588
Test - acc:         0.746400 loss:        0.797954
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.806880 loss:        0.595910
Test - acc:         0.749800 loss:        0.763987
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.806680 loss:        0.596324
Test - acc:         0.764100 loss:        0.743477
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.811680 loss:        0.583949
Test - acc:         0.740500 loss:        0.875080
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.813120 loss:        0.581808
Test - acc:         0.771900 loss:        0.738781
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.810200 loss:        0.591224
Test - acc:         0.796400 loss:        0.653327
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.815840 loss:        0.575764
Test - acc:         0.768500 loss:        0.731990
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.818460 loss:        0.565293
Test - acc:         0.714600 loss:        1.062118
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.815740 loss:        0.568747
Test - acc:         0.728600 loss:        0.906000
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.820140 loss:        0.556274
Test - acc:         0.751500 loss:        0.763809
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.829160 loss:        0.526836
Test - acc:         0.759800 loss:        0.734195
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.830220 loss:        0.523180
Test - acc:         0.793200 loss:        0.634783
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.824680 loss:        0.534172
Test - acc:         0.735100 loss:        0.814379
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.827900 loss:        0.524633
Test - acc:         0.756800 loss:        0.771207
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.828460 loss:        0.523718
Test - acc:         0.787500 loss:        0.641243
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.826560 loss:        0.529690
Test - acc:         0.717900 loss:        0.990391
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.830340 loss:        0.523291
Test - acc:         0.745600 loss:        0.797464
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.831760 loss:        0.513043
Test - acc:         0.766400 loss:        0.720864
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.829020 loss:        0.523819
Test - acc:         0.777200 loss:        0.708021
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.831540 loss:        0.513969
Test - acc:         0.759700 loss:        0.820794
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.832060 loss:        0.514245
Test - acc:         0.794200 loss:        0.659396
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.833520 loss:        0.510264
Test - acc:         0.690200 loss:        1.122565
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.835520 loss:        0.504341
Test - acc:         0.749500 loss:        0.821331
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.832620 loss:        0.513704
Test - acc:         0.714600 loss:        0.964263
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.836760 loss:        0.504327
Test - acc:         0.769800 loss:        0.717654
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.835460 loss:        0.505083
Test - acc:         0.742600 loss:        0.804012
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.832280 loss:        0.505034
Test - acc:         0.794200 loss:        0.657209
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.831640 loss:        0.514729
Test - acc:         0.790000 loss:        0.664315
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.835700 loss:        0.501288
Test - acc:         0.801800 loss:        0.641483
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.836480 loss:        0.501576
Test - acc:         0.720100 loss:        0.952626
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.838420 loss:        0.498700
Test - acc:         0.823000 loss:        0.555373
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.840720 loss:        0.490184
Test - acc:         0.809700 loss:        0.590624
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.841360 loss:        0.485378
Test - acc:         0.773100 loss:        0.753562
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.836380 loss:        0.499343
Test - acc:         0.770700 loss:        0.722814
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.838180 loss:        0.493115
Test - acc:         0.776400 loss:        0.715152
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.837620 loss:        0.495505
Test - acc:         0.780900 loss:        0.679857
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.839100 loss:        0.490989
Test - acc:         0.786100 loss:        0.681415
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.838120 loss:        0.492799
Test - acc:         0.788400 loss:        0.662827
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.839680 loss:        0.490136
Test - acc:         0.782200 loss:        0.693620
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.841480 loss:        0.483401
Test - acc:         0.825100 loss:        0.519460
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.838320 loss:        0.491924
Test - acc:         0.738300 loss:        0.886233
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.838140 loss:        0.491485
Test - acc:         0.773900 loss:        0.678504
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.845360 loss:        0.470273
Test - acc:         0.783300 loss:        0.676551
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.844520 loss:        0.470325
Test - acc:         0.782500 loss:        0.697097
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.844380 loss:        0.474345
Test - acc:         0.787300 loss:        0.671329
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.844240 loss:        0.471668
Test - acc:         0.769800 loss:        0.782052
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.842040 loss:        0.477406
Test - acc:         0.789900 loss:        0.661711
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.843380 loss:        0.478700
Test - acc:         0.807400 loss:        0.633461
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.840360 loss:        0.480937
Test - acc:         0.805400 loss:        0.620227
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.845700 loss:        0.470898
Test - acc:         0.813400 loss:        0.544005
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.844760 loss:        0.469487
Test - acc:         0.720800 loss:        0.969729
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.846660 loss:        0.467401
Test - acc:         0.721800 loss:        0.982339
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.845000 loss:        0.473055
Test - acc:         0.745400 loss:        0.921296
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.844620 loss:        0.472170
Test - acc:         0.772000 loss:        0.759060
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.846660 loss:        0.468438
Test - acc:         0.795200 loss:        0.629912
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.844780 loss:        0.470432
Test - acc:         0.799900 loss:        0.638351
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.845480 loss:        0.471536
Test - acc:         0.754800 loss:        0.777342
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.842800 loss:        0.475134
Test - acc:         0.780400 loss:        0.662935
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.844220 loss:        0.472899
Test - acc:         0.717800 loss:        0.942994
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.847480 loss:        0.464755
Test - acc:         0.789500 loss:        0.679946
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.846060 loss:        0.466338
Test - acc:         0.771000 loss:        0.734676
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.845720 loss:        0.467921
Test - acc:         0.752100 loss:        0.785144
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.845420 loss:        0.468194
Test - acc:         0.794200 loss:        0.635939
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.847740 loss:        0.464056
Test - acc:         0.755100 loss:        0.753614
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.845440 loss:        0.469978
Test - acc:         0.797900 loss:        0.638922
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.848380 loss:        0.462632
Test - acc:         0.773000 loss:        0.703859
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.844600 loss:        0.466213
Test - acc:         0.772100 loss:        0.736608
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.847560 loss:        0.467040
Test - acc:         0.769600 loss:        0.762337
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.844680 loss:        0.473031
Test - acc:         0.806900 loss:        0.612513
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.848480 loss:        0.464347
Test - acc:         0.813600 loss:        0.577360
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.846000 loss:        0.463717
Test - acc:         0.792400 loss:        0.617804
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.845760 loss:        0.470905
Test - acc:         0.821900 loss:        0.542186
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.846960 loss:        0.464048
Test - acc:         0.706200 loss:        1.017913
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.847260 loss:        0.462125
Test - acc:         0.818700 loss:        0.572145
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.851340 loss:        0.444136
Test - acc:         0.821500 loss:        0.559959
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.850340 loss:        0.450961
Test - acc:         0.806400 loss:        0.615911
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.851600 loss:        0.446996
Test - acc:         0.742800 loss:        0.881634
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.851880 loss:        0.449397
Test - acc:         0.765200 loss:        0.767688
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.851080 loss:        0.456010
Test - acc:         0.806800 loss:        0.614302
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.850640 loss:        0.451139
Test - acc:         0.775000 loss:        0.720774
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.849900 loss:        0.453404
Test - acc:         0.796600 loss:        0.617496
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.851040 loss:        0.449310
Test - acc:         0.809200 loss:        0.584727
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.849480 loss:        0.453109
Test - acc:         0.818100 loss:        0.555319
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.849800 loss:        0.454562
Test - acc:         0.797400 loss:        0.624559
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.849880 loss:        0.453262
Test - acc:         0.835700 loss:        0.490447
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.851220 loss:        0.450180
Test - acc:         0.728700 loss:        0.947360
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.851200 loss:        0.447318
Test - acc:         0.792600 loss:        0.634964
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.851780 loss:        0.453088
Test - acc:         0.806500 loss:        0.596771
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.849400 loss:        0.452701
Test - acc:         0.780400 loss:        0.704763
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.852660 loss:        0.447974
Test - acc:         0.827600 loss:        0.522665
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.852560 loss:        0.444801
Test - acc:         0.699000 loss:        1.075809
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.848760 loss:        0.454147
Test - acc:         0.768900 loss:        0.760956
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.851240 loss:        0.446868
Test - acc:         0.792100 loss:        0.629150
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.852700 loss:        0.444361
Test - acc:         0.826300 loss:        0.517274
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.850720 loss:        0.445365
Test - acc:         0.799100 loss:        0.626859
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.850980 loss:        0.449602
Test - acc:         0.816400 loss:        0.571508
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.855000 loss:        0.436494
Test - acc:         0.747600 loss:        0.800143
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.851600 loss:        0.448807
Test - acc:         0.801600 loss:        0.650096
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.852320 loss:        0.443004
Test - acc:         0.815900 loss:        0.566114
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.851720 loss:        0.443358
Test - acc:         0.818500 loss:        0.535377
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.851620 loss:        0.443943
Test - acc:         0.818800 loss:        0.558437
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.854500 loss:        0.441227
Test - acc:         0.728700 loss:        0.968221
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
