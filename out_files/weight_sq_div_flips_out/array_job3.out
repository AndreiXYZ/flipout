Running --model resnet18 --noise --prune_criterion weight_squared_div_flips --seed 42 --prune_freq 50 --prune_rate 0.5 --comment=resnet18_crit=weight_squared_div_flips_pf=50_seed=42 --save_model=pre-finetune/resnet18_weight_squared_div_flips_pf50_s42 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_squared_div_flips_pf50_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11164352
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.319760 loss:        2.027701
Test - acc:         0.373600 loss:        1.709656
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.483520 loss:        1.421054
Test - acc:         0.526700 loss:        1.263736
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.596320 loss:        1.129661
Test - acc:         0.595600 loss:        1.171379
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.673280 loss:        0.927933
Test - acc:         0.610100 loss:        1.134739
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.723460 loss:        0.790148
Test - acc:         0.705200 loss:        0.872062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.766400 loss:        0.675023
Test - acc:         0.746900 loss:        0.720768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.790000 loss:        0.608257
Test - acc:         0.766300 loss:        0.692052
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.805140 loss:        0.568890
Test - acc:         0.777000 loss:        0.661023
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.812440 loss:        0.540974
Test - acc:         0.748500 loss:        0.765813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.823580 loss:        0.513555
Test - acc:         0.742700 loss:        0.756514
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.831620 loss:        0.490783
Test - acc:         0.778800 loss:        0.671311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.834560 loss:        0.478143
Test - acc:         0.793000 loss:        0.626211
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.841480 loss:        0.462775
Test - acc:         0.792800 loss:        0.604948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838760 loss:        0.468699
Test - acc:         0.789100 loss:        0.624176
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844780 loss:        0.450396
Test - acc:         0.816100 loss:        0.559841
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.846280 loss:        0.448807
Test - acc:         0.764600 loss:        0.719807
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.850540 loss:        0.438978
Test - acc:         0.818300 loss:        0.530640
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.850840 loss:        0.433470
Test - acc:         0.796600 loss:        0.604748
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.853620 loss:        0.425959
Test - acc:         0.841700 loss:        0.488544
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.854620 loss:        0.421329
Test - acc:         0.795500 loss:        0.623324
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.859220 loss:        0.417579
Test - acc:         0.823000 loss:        0.516899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.856440 loss:        0.420470
Test - acc:         0.758600 loss:        0.712926
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.858640 loss:        0.411732
Test - acc:         0.793800 loss:        0.625225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.858100 loss:        0.413444
Test - acc:         0.793100 loss:        0.628817
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.862000 loss:        0.407803
Test - acc:         0.819100 loss:        0.573117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.861660 loss:        0.407108
Test - acc:         0.805500 loss:        0.599522
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.864180 loss:        0.397832
Test - acc:         0.836000 loss:        0.468893
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.863120 loss:        0.402166
Test - acc:         0.833700 loss:        0.486745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.863140 loss:        0.396906
Test - acc:         0.855500 loss:        0.423829
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.867600 loss:        0.394886
Test - acc:         0.810100 loss:        0.579276
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.865620 loss:        0.394207
Test - acc:         0.853400 loss:        0.434255
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.866820 loss:        0.393427
Test - acc:         0.698200 loss:        1.116456
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.866560 loss:        0.390132
Test - acc:         0.833300 loss:        0.506263
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.866820 loss:        0.389891
Test - acc:         0.838100 loss:        0.489114
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.868660 loss:        0.383678
Test - acc:         0.817400 loss:        0.559010
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.384254
Test - acc:         0.822300 loss:        0.562526
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.866740 loss:        0.387204
Test - acc:         0.820700 loss:        0.539781
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.868140 loss:        0.390223
Test - acc:         0.839300 loss:        0.510206
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.866480 loss:        0.388159
Test - acc:         0.846800 loss:        0.458624
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.870960 loss:        0.379702
Test - acc:         0.817800 loss:        0.564097
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.387867
Test - acc:         0.835000 loss:        0.494873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.872200 loss:        0.374784
Test - acc:         0.814500 loss:        0.557098
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.873960 loss:        0.370231
Test - acc:         0.826900 loss:        0.516385
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.869500 loss:        0.379596
Test - acc:         0.837700 loss:        0.498690
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.868400 loss:        0.381868
Test - acc:         0.857200 loss:        0.420949
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.871940 loss:        0.377637
Test - acc:         0.825900 loss:        0.547199
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.870680 loss:        0.378822
Test - acc:         0.831400 loss:        0.497769
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.872980 loss:        0.375904
Test - acc:         0.809000 loss:        0.558676
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.872880 loss:        0.375953
Test - acc:         0.809400 loss:        0.604101
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.870100 loss:        0.383866
Test - acc:         0.825900 loss:        0.523767
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.880460 loss:        0.350919
Test - acc:         0.845400 loss:        0.484995
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.879080 loss:        0.355678
Test - acc:         0.855200 loss:        0.437104
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.879600 loss:        0.353747
Test - acc:         0.819600 loss:        0.551945
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.878120 loss:        0.356764
Test - acc:         0.794600 loss:        0.652052
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.876440 loss:        0.359237
Test - acc:         0.846100 loss:        0.475322
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.877240 loss:        0.360321
Test - acc:         0.823400 loss:        0.552719
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.876240 loss:        0.361426
Test - acc:         0.846600 loss:        0.452148
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.878280 loss:        0.355761
Test - acc:         0.853800 loss:        0.435343
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.877740 loss:        0.358662
Test - acc:         0.831500 loss:        0.517644
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.875100 loss:        0.365391
Test - acc:         0.836300 loss:        0.500484
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.876980 loss:        0.362381
Test - acc:         0.837800 loss:        0.480613
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.878900 loss:        0.356206
Test - acc:         0.749400 loss:        0.839846
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.874860 loss:        0.362504
Test - acc:         0.822300 loss:        0.533745
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.877040 loss:        0.362574
Test - acc:         0.782800 loss:        0.685615
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.874940 loss:        0.367209
Test - acc:         0.820500 loss:        0.546850
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.879420 loss:        0.354924
Test - acc:         0.844100 loss:        0.460000
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.877000 loss:        0.358408
Test - acc:         0.836800 loss:        0.485322
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.875300 loss:        0.365128
Test - acc:         0.804200 loss:        0.594919
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.875140 loss:        0.365128
Test - acc:         0.850200 loss:        0.459963
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.878700 loss:        0.359150
Test - acc:         0.827600 loss:        0.536998
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.879040 loss:        0.352930
Test - acc:         0.821600 loss:        0.554552
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.874600 loss:        0.360598
Test - acc:         0.836500 loss:        0.486041
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.877880 loss:        0.357797
Test - acc:         0.834400 loss:        0.504517
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.877020 loss:        0.361089
Test - acc:         0.799000 loss:        0.601627
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.880180 loss:        0.353270
Test - acc:         0.815300 loss:        0.607806
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.876840 loss:        0.361482
Test - acc:         0.784400 loss:        0.728237
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.876580 loss:        0.357359
Test - acc:         0.828000 loss:        0.527030
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.874860 loss:        0.361539
Test - acc:         0.813600 loss:        0.540782
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.878260 loss:        0.361228
Test - acc:         0.836100 loss:        0.478585
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.878580 loss:        0.354334
Test - acc:         0.791000 loss:        0.631292
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.877660 loss:        0.358741
Test - acc:         0.827500 loss:        0.540270
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.879760 loss:        0.356812
Test - acc:         0.840500 loss:        0.488570
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.877280 loss:        0.358305
Test - acc:         0.859000 loss:        0.417570
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.880160 loss:        0.350730
Test - acc:         0.801800 loss:        0.662831
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.880480 loss:        0.351874
Test - acc:         0.798300 loss:        0.628104
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.876040 loss:        0.358117
Test - acc:         0.843400 loss:        0.489248
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.879680 loss:        0.353368
Test - acc:         0.831300 loss:        0.508180
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.877960 loss:        0.359292
Test - acc:         0.857300 loss:        0.422958
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.878920 loss:        0.356240
Test - acc:         0.840800 loss:        0.478802
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.880580 loss:        0.352324
Test - acc:         0.840400 loss:        0.486614
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.877020 loss:        0.358741
Test - acc:         0.838600 loss:        0.488801
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.879720 loss:        0.354414
Test - acc:         0.850200 loss:        0.453669
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.877720 loss:        0.354043
Test - acc:         0.818400 loss:        0.550735
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.876620 loss:        0.360140
Test - acc:         0.800400 loss:        0.601080
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.877840 loss:        0.357157
Test - acc:         0.795700 loss:        0.656371
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.877520 loss:        0.358307
Test - acc:         0.830200 loss:        0.522753
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.879420 loss:        0.355861
Test - acc:         0.843200 loss:        0.491376
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.879840 loss:        0.355239
Test - acc:         0.830900 loss:        0.506213
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.877320 loss:        0.357641
Test - acc:         0.857200 loss:        0.445176
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.878220 loss:        0.355597
Test - acc:         0.844200 loss:        0.466304
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.889140 loss:        0.324016
Test - acc:         0.859400 loss:        0.423963
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.887180 loss:        0.330513
Test - acc:         0.838800 loss:        0.493517
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.887260 loss:        0.329695
Test - acc:         0.836900 loss:        0.498373
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.882360 loss:        0.341551
Test - acc:         0.859900 loss:        0.415736
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.883900 loss:        0.338238
Test - acc:         0.839100 loss:        0.477240
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.883540 loss:        0.341898
Test - acc:         0.819300 loss:        0.587377
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.885980 loss:        0.333413
Test - acc:         0.824900 loss:        0.570586
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.884860 loss:        0.338567
Test - acc:         0.852800 loss:        0.465117
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.883540 loss:        0.341813
Test - acc:         0.826900 loss:        0.540655
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.883320 loss:        0.337884
Test - acc:         0.817200 loss:        0.564163
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.883240 loss:        0.339075
Test - acc:         0.855200 loss:        0.433812
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.884720 loss:        0.337335
Test - acc:         0.839800 loss:        0.482780
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.882660 loss:        0.343127
Test - acc:         0.851600 loss:        0.437280
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.882000 loss:        0.339950
Test - acc:         0.851400 loss:        0.428277
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.888640 loss:        0.331757
Test - acc:         0.844300 loss:        0.462289
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.883400 loss:        0.340543
Test - acc:         0.834300 loss:        0.535551
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.885340 loss:        0.334807
Test - acc:         0.842800 loss:        0.487474
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.882500 loss:        0.341923
Test - acc:         0.772700 loss:        0.704672
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.886460 loss:        0.331759
Test - acc:         0.812500 loss:        0.583793
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.882020 loss:        0.341547
Test - acc:         0.842600 loss:        0.473735
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.886560 loss:        0.334737
Test - acc:         0.839300 loss:        0.476760
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.883700 loss:        0.340234
Test - acc:         0.802200 loss:        0.604051
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.885980 loss:        0.335970
Test - acc:         0.855200 loss:        0.438659
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.882140 loss:        0.341925
Test - acc:         0.846100 loss:        0.473380
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.883480 loss:        0.341966
Test - acc:         0.835600 loss:        0.497621
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.884960 loss:        0.338763
Test - acc:         0.848200 loss:        0.455810
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.883740 loss:        0.341458
Test - acc:         0.860700 loss:        0.422843
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.883500 loss:        0.340070
Test - acc:         0.857300 loss:        0.428803
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.882860 loss:        0.340015
Test - acc:         0.849800 loss:        0.463538
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.884820 loss:        0.340168
Test - acc:         0.755400 loss:        0.785194
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.885920 loss:        0.335297
Test - acc:         0.792500 loss:        0.640623
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.879800 loss:        0.348344
Test - acc:         0.861200 loss:        0.410272
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.883760 loss:        0.341215
Test - acc:         0.846100 loss:        0.473933
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.883780 loss:        0.341745
Test - acc:         0.843100 loss:        0.480987
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.885760 loss:        0.339928
Test - acc:         0.855900 loss:        0.447841
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.885480 loss:        0.334470
Test - acc:         0.809100 loss:        0.585662
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.883260 loss:        0.341499
Test - acc:         0.849200 loss:        0.445126
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.883300 loss:        0.341427
Test - acc:         0.830500 loss:        0.501097
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.881180 loss:        0.346238
Test - acc:         0.798300 loss:        0.633575
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.882840 loss:        0.340934
Test - acc:         0.748300 loss:        0.863156
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.881900 loss:        0.344558
Test - acc:         0.838600 loss:        0.473728
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.887180 loss:        0.333372
Test - acc:         0.850600 loss:        0.472344
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.882860 loss:        0.339743
Test - acc:         0.821200 loss:        0.534559
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.885320 loss:        0.334527
Test - acc:         0.843400 loss:        0.473992
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.886920 loss:        0.334951
Test - acc:         0.834000 loss:        0.528976
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.884940 loss:        0.341703
Test - acc:         0.836300 loss:        0.494272
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.882740 loss:        0.343207
Test - acc:         0.853200 loss:        0.455608
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.883000 loss:        0.339807
Test - acc:         0.810800 loss:        0.596459
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.886640 loss:        0.333123
Test - acc:         0.819200 loss:        0.545776
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.885120 loss:        0.336833
Test - acc:         0.856000 loss:        0.454179
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.932300 loss:        0.202253
Test - acc:         0.922200 loss:        0.230709
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.947980 loss:        0.155642
Test - acc:         0.926000 loss:        0.218950
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.952720 loss:        0.138227
Test - acc:         0.927900 loss:        0.213161
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.957940 loss:        0.123434
Test - acc:         0.929000 loss:        0.211046
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.960420 loss:        0.117113
Test - acc:         0.932200 loss:        0.211640
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.963640 loss:        0.106516
Test - acc:         0.933000 loss:        0.211969
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.965940 loss:        0.101295
Test - acc:         0.930900 loss:        0.211051
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.966840 loss:        0.096112
Test - acc:         0.932300 loss:        0.208519
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.969380 loss:        0.091575
Test - acc:         0.931900 loss:        0.217141
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.971260 loss:        0.084128
Test - acc:         0.932500 loss:        0.211942
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.973700 loss:        0.079092
Test - acc:         0.933700 loss:        0.212033
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.974420 loss:        0.075431
Test - acc:         0.933300 loss:        0.219636
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.975500 loss:        0.073762
Test - acc:         0.930800 loss:        0.228033
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.976820 loss:        0.067810
Test - acc:         0.932700 loss:        0.223664
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.977940 loss:        0.064346
Test - acc:         0.933700 loss:        0.222168
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.978060 loss:        0.065939
Test - acc:         0.932500 loss:        0.219481
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.980180 loss:        0.060374
Test - acc:         0.932100 loss:        0.226459
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.979380 loss:        0.061516
Test - acc:         0.929200 loss:        0.239649
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.980180 loss:        0.058616
Test - acc:         0.924000 loss:        0.251532
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.980760 loss:        0.058825
Test - acc:         0.923500 loss:        0.260499
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.979560 loss:        0.060408
Test - acc:         0.928100 loss:        0.240982
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.981740 loss:        0.055578
Test - acc:         0.931100 loss:        0.237541
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.982100 loss:        0.053952
Test - acc:         0.924000 loss:        0.274231
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.980740 loss:        0.057303
Test - acc:         0.929000 loss:        0.255972
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.980780 loss:        0.056726
Test - acc:         0.926300 loss:        0.260390
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.980660 loss:        0.057425
Test - acc:         0.927200 loss:        0.258835
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.981120 loss:        0.055677
Test - acc:         0.926100 loss:        0.268838
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.981960 loss:        0.054432
Test - acc:         0.923100 loss:        0.270225
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.980860 loss:        0.057743
Test - acc:         0.923600 loss:        0.266647
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.981580 loss:        0.055922
Test - acc:         0.926900 loss:        0.265355
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.056675
Test - acc:         0.929100 loss:        0.241812
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.979880 loss:        0.059257
Test - acc:         0.917100 loss:        0.277639
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.978360 loss:        0.062284
Test - acc:         0.919800 loss:        0.286216
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.979580 loss:        0.060001
Test - acc:         0.926300 loss:        0.261440
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.979920 loss:        0.059738
Test - acc:         0.923300 loss:        0.264970
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.979140 loss:        0.060612
Test - acc:         0.927700 loss:        0.250469
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.980800 loss:        0.058181
Test - acc:         0.921600 loss:        0.275255
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.059691
Test - acc:         0.920000 loss:        0.280075
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.979340 loss:        0.061454
Test - acc:         0.922200 loss:        0.267091
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.977480 loss:        0.065849
Test - acc:         0.918100 loss:        0.287221
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.978760 loss:        0.063362
Test - acc:         0.923600 loss:        0.275011
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.978780 loss:        0.060245
Test - acc:         0.911700 loss:        0.302497
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.977780 loss:        0.064147
Test - acc:         0.923600 loss:        0.269163
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.978340 loss:        0.063075
Test - acc:         0.924000 loss:        0.258339
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.978200 loss:        0.064785
Test - acc:         0.917900 loss:        0.294937
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.976240 loss:        0.069644
Test - acc:         0.919400 loss:        0.272061
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.977580 loss:        0.065935
Test - acc:         0.919000 loss:        0.269834
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.977840 loss:        0.064110
Test - acc:         0.917600 loss:        0.285979
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.977120 loss:        0.067285
Test - acc:         0.908900 loss:        0.318817
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.977400 loss:        0.065935
Test - acc:         0.922400 loss:        0.274437
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.983140 loss:        0.052519
Test - acc:         0.926800 loss:        0.249682
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.984640 loss:        0.048012
Test - acc:         0.923700 loss:        0.273779
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.985420 loss:        0.045120
Test - acc:         0.927700 loss:        0.258470
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.983500 loss:        0.049649
Test - acc:         0.925100 loss:        0.253653
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.983820 loss:        0.050338
Test - acc:         0.925300 loss:        0.258925
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.983240 loss:        0.050668
Test - acc:         0.924000 loss:        0.274789
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.982680 loss:        0.052831
Test - acc:         0.920400 loss:        0.274350
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.982860 loss:        0.052303
Test - acc:         0.919500 loss:        0.284453
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.982080 loss:        0.053532
Test - acc:         0.923100 loss:        0.284314
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.982520 loss:        0.051535
Test - acc:         0.919400 loss:        0.287318
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.980480 loss:        0.057667
Test - acc:         0.922700 loss:        0.276241
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.981940 loss:        0.054402
Test - acc:         0.922000 loss:        0.274701
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.982620 loss:        0.053774
Test - acc:         0.916900 loss:        0.303441
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.981140 loss:        0.056148
Test - acc:         0.925100 loss:        0.258865
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.981100 loss:        0.055812
Test - acc:         0.918100 loss:        0.288883
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.980140 loss:        0.058946
Test - acc:         0.920700 loss:        0.288773
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.979880 loss:        0.060659
Test - acc:         0.926900 loss:        0.252653
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.981100 loss:        0.056652
Test - acc:         0.921200 loss:        0.280931
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.982520 loss:        0.053682
Test - acc:         0.919900 loss:        0.292535
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.981540 loss:        0.055549
Test - acc:         0.915800 loss:        0.305314
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.981340 loss:        0.055422
Test - acc:         0.921900 loss:        0.280806
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.981340 loss:        0.055893
Test - acc:         0.923100 loss:        0.282578
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.979940 loss:        0.058645
Test - acc:         0.922600 loss:        0.272560
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.981160 loss:        0.056099
Test - acc:         0.922800 loss:        0.277510
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.982000 loss:        0.053624
Test - acc:         0.925700 loss:        0.264798
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.981440 loss:        0.056474
Test - acc:         0.924400 loss:        0.274252
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.980440 loss:        0.056376
Test - acc:         0.918300 loss:        0.283315
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.980120 loss:        0.059371
Test - acc:         0.923800 loss:        0.272007
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.982080 loss:        0.053494
Test - acc:         0.921600 loss:        0.277256
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.979320 loss:        0.059974
Test - acc:         0.922800 loss:        0.272258
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.980060 loss:        0.060060
Test - acc:         0.923100 loss:        0.269706
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.981740 loss:        0.054842
Test - acc:         0.916000 loss:        0.308673
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.980780 loss:        0.055708
Test - acc:         0.921700 loss:        0.276715
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.979960 loss:        0.058894
Test - acc:         0.920400 loss:        0.284656
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.978920 loss:        0.061716
Test - acc:         0.919300 loss:        0.287739
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.979440 loss:        0.061510
Test - acc:         0.921400 loss:        0.282198
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.982100 loss:        0.053360
Test - acc:         0.918000 loss:        0.301239
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.978340 loss:        0.062738
Test - acc:         0.925400 loss:        0.259789
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.980860 loss:        0.058414
Test - acc:         0.923200 loss:        0.275976
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.979320 loss:        0.060924
Test - acc:         0.921000 loss:        0.278832
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.978820 loss:        0.061590
Test - acc:         0.925100 loss:        0.265440
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.981460 loss:        0.055737
Test - acc:         0.923800 loss:        0.272043
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.980100 loss:        0.059268
Test - acc:         0.921900 loss:        0.272952
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.057842
Test - acc:         0.916200 loss:        0.305027
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.981220 loss:        0.055032
Test - acc:         0.918700 loss:        0.302457
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.981160 loss:        0.056779
Test - acc:         0.919400 loss:        0.290315
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.979220 loss:        0.061892
Test - acc:         0.917400 loss:        0.290659
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.978220 loss:        0.062241
Test - acc:         0.921300 loss:        0.280114
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.978160 loss:        0.063105
Test - acc:         0.922000 loss:        0.276245
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.980520 loss:        0.058151
Test - acc:         0.918500 loss:        0.279210
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.984400 loss:        0.048778
Test - acc:         0.933800 loss:        0.218405
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.990720 loss:        0.032641
Test - acc:         0.937100 loss:        0.211370
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.992220 loss:        0.028118
Test - acc:         0.937900 loss:        0.208757
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.994320 loss:        0.023468
Test - acc:         0.939500 loss:        0.206091
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.994420 loss:        0.022658
Test - acc:         0.939100 loss:        0.206324
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.995140 loss:        0.020665
Test - acc:         0.938900 loss:        0.205820
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.995320 loss:        0.019813
Test - acc:         0.939600 loss:        0.204987
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.995760 loss:        0.018713
Test - acc:         0.940200 loss:        0.203896
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.995720 loss:        0.017640
Test - acc:         0.941500 loss:        0.207529
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.996480 loss:        0.016218
Test - acc:         0.940700 loss:        0.205938
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.996280 loss:        0.016376
Test - acc:         0.940800 loss:        0.206809
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.996860 loss:        0.015087
Test - acc:         0.941000 loss:        0.206434
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.997320 loss:        0.014455
Test - acc:         0.940800 loss:        0.207480
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.996980 loss:        0.014151
Test - acc:         0.941600 loss:        0.208349
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.997020 loss:        0.014286
Test - acc:         0.940200 loss:        0.207740
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.997120 loss:        0.013914
Test - acc:         0.940600 loss:        0.206989
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.997760 loss:        0.012556
Test - acc:         0.941100 loss:        0.209988
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.997580 loss:        0.012488
Test - acc:         0.941700 loss:        0.209150
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.997660 loss:        0.012135
Test - acc:         0.942000 loss:        0.207138
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.997380 loss:        0.012598
Test - acc:         0.942600 loss:        0.206906
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.998140 loss:        0.011597
Test - acc:         0.942100 loss:        0.208354
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.997900 loss:        0.011280
Test - acc:         0.940800 loss:        0.207903
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.998180 loss:        0.010916
Test - acc:         0.942100 loss:        0.208624
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.997840 loss:        0.011395
Test - acc:         0.942400 loss:        0.208689
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.997980 loss:        0.010909
Test - acc:         0.942700 loss:        0.208404
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.997980 loss:        0.010736
Test - acc:         0.941600 loss:        0.207108
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.998020 loss:        0.010357
Test - acc:         0.942900 loss:        0.207270
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.998340 loss:        0.009872
Test - acc:         0.942600 loss:        0.207123
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.997880 loss:        0.010534
Test - acc:         0.942500 loss:        0.207169
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.998560 loss:        0.009346
Test - acc:         0.942700 loss:        0.208103
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.998500 loss:        0.008751
Test - acc:         0.942600 loss:        0.207552
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.998080 loss:        0.009873
Test - acc:         0.942200 loss:        0.206310
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.998280 loss:        0.009219
Test - acc:         0.942600 loss:        0.207639
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.998600 loss:        0.008658
Test - acc:         0.942000 loss:        0.208462
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.998760 loss:        0.008671
Test - acc:         0.943400 loss:        0.210147
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.998700 loss:        0.008975
Test - acc:         0.941200 loss:        0.209281
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.998580 loss:        0.008510
Test - acc:         0.942100 loss:        0.208952
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.998640 loss:        0.008239
Test - acc:         0.941800 loss:        0.207638
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.998580 loss:        0.008288
Test - acc:         0.943800 loss:        0.206112
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.998600 loss:        0.008424
Test - acc:         0.943300 loss:        0.208815
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.998640 loss:        0.008277
Test - acc:         0.942500 loss:        0.208171
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.998900 loss:        0.007940
Test - acc:         0.943400 loss:        0.208203
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.999080 loss:        0.007338
Test - acc:         0.942500 loss:        0.209652
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.998940 loss:        0.007799
Test - acc:         0.943100 loss:        0.212123
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.998740 loss:        0.007942
Test - acc:         0.942600 loss:        0.211850
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.998640 loss:        0.008086
Test - acc:         0.944500 loss:        0.212039
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.998720 loss:        0.007708
Test - acc:         0.943000 loss:        0.210262
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.998980 loss:        0.007407
Test - acc:         0.942500 loss:        0.210173
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.998960 loss:        0.007363
Test - acc:         0.943000 loss:        0.210730
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.998860 loss:        0.007251
Test - acc:         0.942500 loss:        0.211910
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.983140 loss:        0.058909
Test - acc:         0.930200 loss:        0.238100
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.988300 loss:        0.042444
Test - acc:         0.931200 loss:        0.233838
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.990120 loss:        0.036778
Test - acc:         0.933500 loss:        0.234420
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.991100 loss:        0.033717
Test - acc:         0.934300 loss:        0.233847
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.991180 loss:        0.032470
Test - acc:         0.933200 loss:        0.234313
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.993000 loss:        0.028834
Test - acc:         0.935500 loss:        0.232168
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.992680 loss:        0.028202
Test - acc:         0.936400 loss:        0.231733
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.993300 loss:        0.026710
Test - acc:         0.935600 loss:        0.231195
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.992560 loss:        0.027770
Test - acc:         0.935400 loss:        0.232249
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.994160 loss:        0.024879
Test - acc:         0.935300 loss:        0.230505
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.994380 loss:        0.023643
Test - acc:         0.934300 loss:        0.232425
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.994320 loss:        0.022887
Test - acc:         0.934300 loss:        0.232454
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.993760 loss:        0.023984
Test - acc:         0.936200 loss:        0.230201
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.994680 loss:        0.022240
Test - acc:         0.935100 loss:        0.232275
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.995280 loss:        0.020842
Test - acc:         0.935300 loss:        0.233034
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.994940 loss:        0.020353
Test - acc:         0.935300 loss:        0.233519
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.994840 loss:        0.020235
Test - acc:         0.936400 loss:        0.231984
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.995660 loss:        0.019234
Test - acc:         0.934800 loss:        0.233624
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.995480 loss:        0.019144
Test - acc:         0.935100 loss:        0.232269
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.995660 loss:        0.018679
Test - acc:         0.936200 loss:        0.236541
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.995560 loss:        0.018367
Test - acc:         0.936000 loss:        0.235466
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.995840 loss:        0.018277
Test - acc:         0.936800 loss:        0.238052
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.995800 loss:        0.018316
Test - acc:         0.936300 loss:        0.235547
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.996560 loss:        0.016440
Test - acc:         0.935600 loss:        0.235771
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.996020 loss:        0.016793
Test - acc:         0.937700 loss:        0.234129
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.996360 loss:        0.016630
Test - acc:         0.936000 loss:        0.237392
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.996540 loss:        0.015995
Test - acc:         0.936200 loss:        0.235450
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.996060 loss:        0.016424
Test - acc:         0.938800 loss:        0.231864
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.996560 loss:        0.015456
Test - acc:         0.937300 loss:        0.235185
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.996140 loss:        0.016532
Test - acc:         0.936700 loss:        0.239020
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.996760 loss:        0.015574
Test - acc:         0.937000 loss:        0.235298
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.996880 loss:        0.014986
Test - acc:         0.936600 loss:        0.238158
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.997180 loss:        0.014559
Test - acc:         0.936700 loss:        0.238074
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.996840 loss:        0.014500
Test - acc:         0.937400 loss:        0.243058
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.997380 loss:        0.013322
Test - acc:         0.936900 loss:        0.242297
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.996940 loss:        0.014331
Test - acc:         0.937100 loss:        0.241980
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.997120 loss:        0.013777
Test - acc:         0.936700 loss:        0.241648
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.997220 loss:        0.013630
Test - acc:         0.936700 loss:        0.239091
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.996760 loss:        0.013894
Test - acc:         0.937000 loss:        0.238911
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.997000 loss:        0.014007
Test - acc:         0.937400 loss:        0.239861
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.997120 loss:        0.013654
Test - acc:         0.936300 loss:        0.239074
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.997460 loss:        0.012820
Test - acc:         0.935400 loss:        0.241238
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.997260 loss:        0.013399
Test - acc:         0.935900 loss:        0.240907
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.996920 loss:        0.013548
Test - acc:         0.936000 loss:        0.241606
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.997220 loss:        0.012699
Test - acc:         0.934700 loss:        0.244454
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.997500 loss:        0.011796
Test - acc:         0.937800 loss:        0.240590
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.997740 loss:        0.012154
Test - acc:         0.936300 loss:        0.242719
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.997420 loss:        0.012709
Test - acc:         0.934900 loss:        0.242413
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.997380 loss:        0.012587
Test - acc:         0.936000 loss:        0.241801
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.997400 loss:        0.012228
Test - acc:         0.935200 loss:        0.241794
Sparsity :          0.9844
Wdecay :        0.000500
