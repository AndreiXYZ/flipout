Running --prune_bnorm --model resnet18 --prune_criterion global_magnitude --seed 43 --prune_freq 117 --prune_rate 0.5 --comment=resnet18_crit=global_magnitude_pf=117_seed=43 --save_model=pre-finetune/resnet18_global_magnitude_pf117_s43 --logdir=criterion_experiment_prune_bnorm/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "global_magnitude",
    "prune_freq": 117,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": true,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_prune_bnorm/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_global_magnitude_pf117_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11169152
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11169152
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.348320 loss:        1.810961
Test - acc:         0.439400 loss:        1.484592
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.526380 loss:        1.296187
Test - acc:         0.587300 loss:        1.162469
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.635500 loss:        1.021202
Test - acc:         0.626700 loss:        1.042242
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.702640 loss:        0.843171
Test - acc:         0.711600 loss:        0.860480
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.752060 loss:        0.713263
Test - acc:         0.752700 loss:        0.721025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.783660 loss:        0.623389
Test - acc:         0.733700 loss:        0.786503
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.801860 loss:        0.570900
Test - acc:         0.757000 loss:        0.762860
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.814640 loss:        0.539573
Test - acc:         0.704900 loss:        0.941443
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.825480 loss:        0.505140
Test - acc:         0.784800 loss:        0.633475
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.830340 loss:        0.493756
Test - acc:         0.779500 loss:        0.657745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.838340 loss:        0.474250
Test - acc:         0.804200 loss:        0.593576
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.844280 loss:        0.458721
Test - acc:         0.834400 loss:        0.502315
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.846500 loss:        0.449426
Test - acc:         0.817100 loss:        0.554424
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.847180 loss:        0.443512
Test - acc:         0.812100 loss:        0.573417
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.853420 loss:        0.425940
Test - acc:         0.804500 loss:        0.581330
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.857080 loss:        0.415865
Test - acc:         0.827800 loss:        0.517806
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.861940 loss:        0.404304
Test - acc:         0.815300 loss:        0.557021
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.863300 loss:        0.400604
Test - acc:         0.797600 loss:        0.647381
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.864700 loss:        0.398190
Test - acc:         0.820300 loss:        0.561645
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.869280 loss:        0.387392
Test - acc:         0.815000 loss:        0.584442
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.870620 loss:        0.382326
Test - acc:         0.818900 loss:        0.536924
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870120 loss:        0.380016
Test - acc:         0.783000 loss:        0.719661
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.869920 loss:        0.379140
Test - acc:         0.822700 loss:        0.551593
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.874520 loss:        0.374324
Test - acc:         0.836900 loss:        0.490705
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.874720 loss:        0.370254
Test - acc:         0.827900 loss:        0.513918
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.876980 loss:        0.362354
Test - acc:         0.850800 loss:        0.456385
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.873500 loss:        0.368452
Test - acc:         0.823200 loss:        0.540426
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.876640 loss:        0.360789
Test - acc:         0.848400 loss:        0.446691
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.876560 loss:        0.360781
Test - acc:         0.803700 loss:        0.617252
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.879380 loss:        0.354437
Test - acc:         0.703300 loss:        1.070219
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.879920 loss:        0.350535
Test - acc:         0.835700 loss:        0.514332
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.877980 loss:        0.357668
Test - acc:         0.852200 loss:        0.417868
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.880080 loss:        0.353408
Test - acc:         0.832300 loss:        0.508966
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.881440 loss:        0.346138
Test - acc:         0.838300 loss:        0.485547
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.881720 loss:        0.348059
Test - acc:         0.816300 loss:        0.577212
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.882320 loss:        0.345955
Test - acc:         0.811300 loss:        0.593824
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.881720 loss:        0.347067
Test - acc:         0.747800 loss:        0.780011
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.882480 loss:        0.344012
Test - acc:         0.844400 loss:        0.473060
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.884840 loss:        0.337756
Test - acc:         0.833300 loss:        0.514843
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.885000 loss:        0.339157
Test - acc:         0.854200 loss:        0.426642
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.885940 loss:        0.336634
Test - acc:         0.836300 loss:        0.491133
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.886120 loss:        0.339098
Test - acc:         0.830000 loss:        0.525768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.883640 loss:        0.338832
Test - acc:         0.814100 loss:        0.564862
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.884660 loss:        0.337910
Test - acc:         0.829100 loss:        0.533913
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.886640 loss:        0.334556
Test - acc:         0.818900 loss:        0.554093
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.890480 loss:        0.326737
Test - acc:         0.806500 loss:        0.570581
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.885980 loss:        0.331932
Test - acc:         0.846200 loss:        0.474265
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.884680 loss:        0.336937
Test - acc:         0.839600 loss:        0.486112
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.884980 loss:        0.331095
Test - acc:         0.828700 loss:        0.547677
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.890180 loss:        0.326304
Test - acc:         0.827700 loss:        0.516710
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.886640 loss:        0.331757
Test - acc:         0.812200 loss:        0.614143
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.890140 loss:        0.324573
Test - acc:         0.859100 loss:        0.426430
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.887580 loss:        0.329810
Test - acc:         0.842400 loss:        0.494844
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.891320 loss:        0.319971
Test - acc:         0.824100 loss:        0.530216
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.887600 loss:        0.330502
Test - acc:         0.825600 loss:        0.537351
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.889020 loss:        0.326788
Test - acc:         0.835200 loss:        0.516938
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.887480 loss:        0.329787
Test - acc:         0.807100 loss:        0.601139
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.888260 loss:        0.329031
Test - acc:         0.848900 loss:        0.453850
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.892020 loss:        0.318753
Test - acc:         0.839600 loss:        0.512315
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.888740 loss:        0.326566
Test - acc:         0.808100 loss:        0.592465
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.887040 loss:        0.328475
Test - acc:         0.849300 loss:        0.449541
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.890980 loss:        0.322665
Test - acc:         0.857600 loss:        0.428949
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.889880 loss:        0.320743
Test - acc:         0.818000 loss:        0.573634
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.889000 loss:        0.327076
Test - acc:         0.837500 loss:        0.517986
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.892480 loss:        0.319330
Test - acc:         0.848700 loss:        0.479477
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.892680 loss:        0.319000
Test - acc:         0.829700 loss:        0.531207
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.892220 loss:        0.318582
Test - acc:         0.845700 loss:        0.454661
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.891320 loss:        0.319381
Test - acc:         0.841400 loss:        0.478651
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.888180 loss:        0.324136
Test - acc:         0.856900 loss:        0.427462
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.890640 loss:        0.321718
Test - acc:         0.835400 loss:        0.526046
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.889920 loss:        0.323294
Test - acc:         0.846200 loss:        0.492419
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.891640 loss:        0.316768
Test - acc:         0.849900 loss:        0.458620
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.892840 loss:        0.314266
Test - acc:         0.858200 loss:        0.441614
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.892140 loss:        0.318381
Test - acc:         0.848100 loss:        0.455647
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.891700 loss:        0.318076
Test - acc:         0.838300 loss:        0.503477
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.892620 loss:        0.317811
Test - acc:         0.816700 loss:        0.564494
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.895000 loss:        0.310169
Test - acc:         0.855600 loss:        0.436198
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.894580 loss:        0.311985
Test - acc:         0.864300 loss:        0.406897
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.891780 loss:        0.317743
Test - acc:         0.843400 loss:        0.495719
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.892380 loss:        0.317269
Test - acc:         0.865600 loss:        0.412980
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.893020 loss:        0.314492
Test - acc:         0.846300 loss:        0.447643
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.892580 loss:        0.317376
Test - acc:         0.790400 loss:        0.665067
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.891000 loss:        0.319906
Test - acc:         0.851200 loss:        0.455092
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.891900 loss:        0.319623
Test - acc:         0.851300 loss:        0.439389
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.892100 loss:        0.318777
Test - acc:         0.846500 loss:        0.469813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.892600 loss:        0.316523
Test - acc:         0.841800 loss:        0.500816
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.893620 loss:        0.311710
Test - acc:         0.851200 loss:        0.452665
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.892720 loss:        0.315482
Test - acc:         0.851200 loss:        0.434452
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.891440 loss:        0.318979
Test - acc:         0.827300 loss:        0.539899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.893700 loss:        0.313235
Test - acc:         0.836000 loss:        0.499143
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.894600 loss:        0.308747
Test - acc:         0.806700 loss:        0.620177
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.894280 loss:        0.314004
Test - acc:         0.836100 loss:        0.490656
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.893120 loss:        0.314930
Test - acc:         0.831800 loss:        0.513320
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.893160 loss:        0.314206
Test - acc:         0.810900 loss:        0.583824
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.894460 loss:        0.313130
Test - acc:         0.805700 loss:        0.633219
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.895180 loss:        0.312284
Test - acc:         0.814100 loss:        0.571790
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.894260 loss:        0.312728
Test - acc:         0.790700 loss:        0.662448
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.895100 loss:        0.309831
Test - acc:         0.774200 loss:        0.792613
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.895660 loss:        0.310572
Test - acc:         0.854300 loss:        0.437531
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.893560 loss:        0.310187
Test - acc:         0.806900 loss:        0.593057
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.892740 loss:        0.310590
Test - acc:         0.841500 loss:        0.475282
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.893860 loss:        0.312416
Test - acc:         0.861400 loss:        0.418170
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.893540 loss:        0.309348
Test - acc:         0.816700 loss:        0.565096
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.894760 loss:        0.312122
Test - acc:         0.826700 loss:        0.541952
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.897240 loss:        0.307112
Test - acc:         0.862400 loss:        0.420506
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.894800 loss:        0.311027
Test - acc:         0.866900 loss:        0.398001
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.893160 loss:        0.314269
Test - acc:         0.806100 loss:        0.612473
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.895460 loss:        0.307874
Test - acc:         0.845500 loss:        0.475903
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.894000 loss:        0.313167
Test - acc:         0.793900 loss:        0.649441
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.894300 loss:        0.308211
Test - acc:         0.851800 loss:        0.469013
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.896720 loss:        0.306801
Test - acc:         0.864400 loss:        0.412888
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.895040 loss:        0.310464
Test - acc:         0.857600 loss:        0.430434
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.893300 loss:        0.313156
Test - acc:         0.797900 loss:        0.679293
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.895080 loss:        0.308161
Test - acc:         0.855900 loss:        0.465189
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.895620 loss:        0.309594
Test - acc:         0.872400 loss:        0.377811
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.894820 loss:        0.309258
Test - acc:         0.877500 loss:        0.361174
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.894600 loss:        0.308879
Test - acc:         0.839200 loss:        0.471187
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.903660 loss:        0.282942
Test - acc:         0.883800 loss:        0.334248
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.901800 loss:        0.286571
Test - acc:         0.811200 loss:        0.610923
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.900600 loss:        0.292122
Test - acc:         0.861400 loss:        0.419745
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.900200 loss:        0.293480
Test - acc:         0.849200 loss:        0.456077
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.901060 loss:        0.291766
Test - acc:         0.842600 loss:        0.489603
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.901580 loss:        0.289691
Test - acc:         0.863500 loss:        0.412149
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.900420 loss:        0.292436
Test - acc:         0.841600 loss:        0.494083
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.901380 loss:        0.290053
Test - acc:         0.862800 loss:        0.413043
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.901300 loss:        0.292418
Test - acc:         0.822200 loss:        0.564345
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.901100 loss:        0.290706
Test - acc:         0.852100 loss:        0.449451
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.898900 loss:        0.295714
Test - acc:         0.835500 loss:        0.506401
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.901400 loss:        0.289777
Test - acc:         0.827200 loss:        0.540874
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.900220 loss:        0.292756
Test - acc:         0.856900 loss:        0.423456
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.903220 loss:        0.284257
Test - acc:         0.866800 loss:        0.409253
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.900780 loss:        0.289936
Test - acc:         0.831000 loss:        0.549601
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.901760 loss:        0.289835
Test - acc:         0.792400 loss:        0.713599
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.903000 loss:        0.287169
Test - acc:         0.835700 loss:        0.509255
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.901280 loss:        0.291266
Test - acc:         0.857900 loss:        0.423981
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.901980 loss:        0.290669
Test - acc:         0.851200 loss:        0.460241
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.902100 loss:        0.287207
Test - acc:         0.864500 loss:        0.410593
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.900500 loss:        0.292429
Test - acc:         0.850500 loss:        0.447537
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.902980 loss:        0.283617
Test - acc:         0.854000 loss:        0.445122
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.901120 loss:        0.290738
Test - acc:         0.848700 loss:        0.457154
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.900700 loss:        0.290571
Test - acc:         0.834500 loss:        0.511534
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.899540 loss:        0.292751
Test - acc:         0.882200 loss:        0.365464
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.901060 loss:        0.288600
Test - acc:         0.872800 loss:        0.383608
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.900240 loss:        0.292412
Test - acc:         0.858000 loss:        0.437724
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.903700 loss:        0.286536
Test - acc:         0.852000 loss:        0.449470
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.902520 loss:        0.286095
Test - acc:         0.866400 loss:        0.403742
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.901240 loss:        0.291072
Test - acc:         0.820700 loss:        0.558464
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.900400 loss:        0.288386
Test - acc:         0.854000 loss:        0.467219
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.902180 loss:        0.289029
Test - acc:         0.863200 loss:        0.421237
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.901680 loss:        0.287632
Test - acc:         0.831700 loss:        0.508119
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.950780 loss:        0.146725
Test - acc:         0.934700 loss:        0.194052
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.966980 loss:        0.101230
Test - acc:         0.939000 loss:        0.184390
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.971480 loss:        0.084019
Test - acc:         0.939800 loss:        0.184118
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.975420 loss:        0.073221
Test - acc:         0.940100 loss:        0.181132
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.979920 loss:        0.064446
Test - acc:         0.942400 loss:        0.175484
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.981800 loss:        0.055551
Test - acc:         0.943000 loss:        0.181560
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.984720 loss:        0.048699
Test - acc:         0.943200 loss:        0.185609
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.985620 loss:        0.044900
Test - acc:         0.941700 loss:        0.192547
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.986540 loss:        0.041835
Test - acc:         0.941600 loss:        0.188409
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.988800 loss:        0.035747
Test - acc:         0.943400 loss:        0.189644
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.989740 loss:        0.033420
Test - acc:         0.942900 loss:        0.197668
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.990700 loss:        0.030243
Test - acc:         0.940600 loss:        0.207852
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.990920 loss:        0.028806
Test - acc:         0.942400 loss:        0.203471
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.991000 loss:        0.028623
Test - acc:         0.939000 loss:        0.207769
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.991920 loss:        0.026750
Test - acc:         0.943500 loss:        0.210291
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.992020 loss:        0.025735
Test - acc:         0.943400 loss:        0.202046
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.991560 loss:        0.026886
Test - acc:         0.943100 loss:        0.211482
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.992520 loss:        0.025470
Test - acc:         0.938900 loss:        0.221751
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.992120 loss:        0.024940
Test - acc:         0.939200 loss:        0.223684
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.992680 loss:        0.024128
Test - acc:         0.938300 loss:        0.227677
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.992180 loss:        0.025293
Test - acc:         0.940200 loss:        0.223045
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.991420 loss:        0.026838
Test - acc:         0.941800 loss:        0.209290
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.991620 loss:        0.026910
Test - acc:         0.938300 loss:        0.227928
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.991860 loss:        0.025836
Test - acc:         0.938600 loss:        0.221811
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.990820 loss:        0.027530
Test - acc:         0.939300 loss:        0.219500
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.991920 loss:        0.025664
Test - acc:         0.939400 loss:        0.217785
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.990720 loss:        0.028862
Test - acc:         0.937400 loss:        0.231592
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.990820 loss:        0.028729
Test - acc:         0.938700 loss:        0.226818
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.990720 loss:        0.030392
Test - acc:         0.936000 loss:        0.227693
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.991380 loss:        0.027466
Test - acc:         0.931700 loss:        0.236559
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.990660 loss:        0.029384
Test - acc:         0.937600 loss:        0.236038
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.988540 loss:        0.035313
Test - acc:         0.932300 loss:        0.251344
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.989700 loss:        0.033243
Test - acc:         0.933500 loss:        0.246089
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.989200 loss:        0.035381
Test - acc:         0.929100 loss:        0.262088
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.989460 loss:        0.033793
Test - acc:         0.932900 loss:        0.242196
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.989120 loss:        0.033676
Test - acc:         0.932900 loss:        0.234711
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.987700 loss:        0.036957
Test - acc:         0.933300 loss:        0.238211
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.988160 loss:        0.036035
Test - acc:         0.926300 loss:        0.266341
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.987540 loss:        0.039423
Test - acc:         0.929000 loss:        0.250154
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.987520 loss:        0.038019
Test - acc:         0.930900 loss:        0.248375
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.986220 loss:        0.040755
Test - acc:         0.923300 loss:        0.268020
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.986280 loss:        0.041632
Test - acc:         0.921800 loss:        0.267031
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.986180 loss:        0.042741
Test - acc:         0.931300 loss:        0.251952
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.986260 loss:        0.039764
Test - acc:         0.928700 loss:        0.262074
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.986100 loss:        0.042466
Test - acc:         0.930400 loss:        0.260139
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.985740 loss:        0.043786
Test - acc:         0.932300 loss:        0.254489
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.988600 loss:        0.036408
Test - acc:         0.933100 loss:        0.248773
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.985940 loss:        0.043453
Test - acc:         0.922000 loss:        0.278191
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.985940 loss:        0.042049
Test - acc:         0.929300 loss:        0.265345
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.986320 loss:        0.040958
Test - acc:         0.927400 loss:        0.265764
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.984960 loss:        0.045379
Test - acc:         0.932800 loss:        0.231223
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.986420 loss:        0.042514
Test - acc:         0.937600 loss:        0.225535
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.985820 loss:        0.042440
Test - acc:         0.932300 loss:        0.247935
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.986200 loss:        0.043107
Test - acc:         0.933400 loss:        0.242571
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.985180 loss:        0.045713
Test - acc:         0.922600 loss:        0.286220
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.985580 loss:        0.044043
Test - acc:         0.933100 loss:        0.236023
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.986900 loss:        0.039570
Test - acc:         0.925600 loss:        0.262114
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.984900 loss:        0.045581
Test - acc:         0.928300 loss:        0.248834
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.988360 loss:        0.036307
Test - acc:         0.925800 loss:        0.263174
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.984100 loss:        0.046497
Test - acc:         0.926900 loss:        0.261292
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.986380 loss:        0.040882
Test - acc:         0.927600 loss:        0.258130
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.985120 loss:        0.043938
Test - acc:         0.931400 loss:        0.236566
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.986400 loss:        0.042595
Test - acc:         0.927600 loss:        0.268295
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.988520 loss:        0.035627
Test - acc:         0.934500 loss:        0.239396
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.986820 loss:        0.042045
Test - acc:         0.919900 loss:        0.298079
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.985980 loss:        0.043708
Test - acc:         0.923400 loss:        0.279037
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.984800 loss:        0.045965
Test - acc:         0.924600 loss:        0.280681
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.985800 loss:        0.043057
Test - acc:         0.928100 loss:        0.241518
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.986940 loss:        0.041123
Test - acc:         0.919500 loss:        0.284114
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.986840 loss:        0.040580
Test - acc:         0.925500 loss:        0.266680
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.986920 loss:        0.041240
Test - acc:         0.932300 loss:        0.255044
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.985460 loss:        0.043649
Test - acc:         0.931900 loss:        0.246015
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.984340 loss:        0.047401
Test - acc:         0.933400 loss:        0.239820
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.985280 loss:        0.045708
Test - acc:         0.931500 loss:        0.239384
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.987000 loss:        0.040725
Test - acc:         0.928100 loss:        0.250953
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.986400 loss:        0.041682
Test - acc:         0.927900 loss:        0.263019
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.986640 loss:        0.041201
Test - acc:         0.923500 loss:        0.290032
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.985560 loss:        0.043865
Test - acc:         0.928700 loss:        0.259124
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.985440 loss:        0.043209
Test - acc:         0.928500 loss:        0.264532
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.986080 loss:        0.041647
Test - acc:         0.932800 loss:        0.246495
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.986040 loss:        0.042075
Test - acc:         0.920100 loss:        0.297842
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.986780 loss:        0.041377
Test - acc:         0.929000 loss:        0.258035
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.986600 loss:        0.040600
Test - acc:         0.932000 loss:        0.245923
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.988200 loss:        0.035340
Test - acc:         0.932500 loss:        0.245947
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.993520 loss:        0.024643
Test - acc:         0.938700 loss:        0.225585
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.994080 loss:        0.019906
Test - acc:         0.938000 loss:        0.217479
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.994780 loss:        0.018945
Test - acc:         0.939600 loss:        0.230146
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.994860 loss:        0.018757
Test - acc:         0.936000 loss:        0.249659
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.992260 loss:        0.025533
Test - acc:         0.928600 loss:        0.270895
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.993020 loss:        0.023729
Test - acc:         0.932200 loss:        0.243592
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.992560 loss:        0.023677
Test - acc:         0.938000 loss:        0.226270
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.991040 loss:        0.028498
Test - acc:         0.923700 loss:        0.284650
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.989800 loss:        0.030950
Test - acc:         0.931900 loss:        0.244958
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.992120 loss:        0.027065
Test - acc:         0.928400 loss:        0.270491
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.991660 loss:        0.026810
Test - acc:         0.929100 loss:        0.268486
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.990900 loss:        0.029669
Test - acc:         0.930300 loss:        0.258918
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.990900 loss:        0.028382
Test - acc:         0.933300 loss:        0.249897
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.990180 loss:        0.031693
Test - acc:         0.925600 loss:        0.274822
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.986800 loss:        0.039539
Test - acc:         0.921200 loss:        0.287345
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.989180 loss:        0.033531
Test - acc:         0.935800 loss:        0.236883
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.996380 loss:        0.013901
Test - acc:         0.943200 loss:        0.201711
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.998460 loss:        0.008140
Test - acc:         0.946700 loss:        0.195949
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.998840 loss:        0.006980
Test - acc:         0.947400 loss:        0.192206
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.999080 loss:        0.005737
Test - acc:         0.948900 loss:        0.190043
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.005455
Test - acc:         0.947900 loss:        0.190195
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.004548
Test - acc:         0.947400 loss:        0.188914
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.004257
Test - acc:         0.949100 loss:        0.188014
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.004074
Test - acc:         0.948500 loss:        0.187795
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.003818
Test - acc:         0.949100 loss:        0.186068
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.003678
Test - acc:         0.948000 loss:        0.187679
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.003365
Test - acc:         0.948900 loss:        0.186182
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003419
Test - acc:         0.948600 loss:        0.184052
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.003282
Test - acc:         0.949500 loss:        0.183151
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.003116
Test - acc:         0.949600 loss:        0.184577
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.003148
Test - acc:         0.950000 loss:        0.182699
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002698
Test - acc:         0.950700 loss:        0.181738
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.002881
Test - acc:         0.950900 loss:        0.181135
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002740
Test - acc:         0.951200 loss:        0.179687
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.002879
Test - acc:         0.951200 loss:        0.181906
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003002
Test - acc:         0.950500 loss:        0.181139
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002566
Test - acc:         0.950200 loss:        0.181839
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002587
Test - acc:         0.950100 loss:        0.182018
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.002606
Test - acc:         0.949500 loss:        0.179399
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.002608
Test - acc:         0.951200 loss:        0.179718
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.002588
Test - acc:         0.951400 loss:        0.181420
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002618
Test - acc:         0.951600 loss:        0.179271
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002579
Test - acc:         0.952100 loss:        0.178377
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002335
Test - acc:         0.950600 loss:        0.180112
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002349
Test - acc:         0.951400 loss:        0.178465
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002231
Test - acc:         0.951200 loss:        0.178424
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002232
Test - acc:         0.952700 loss:        0.177457
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002291
Test - acc:         0.952000 loss:        0.176943
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002180
Test - acc:         0.951800 loss:        0.176932
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002250
Test - acc:         0.952300 loss:        0.177441
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002254
Test - acc:         0.950500 loss:        0.176465
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002143
Test - acc:         0.951800 loss:        0.176888
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002375
Test - acc:         0.950900 loss:        0.178018
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002227
Test - acc:         0.951300 loss:        0.176641
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002199
Test - acc:         0.951600 loss:        0.176568
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002038
Test - acc:         0.951100 loss:        0.174856
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.002045
Test - acc:         0.951700 loss:        0.175248
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002245
Test - acc:         0.951800 loss:        0.175717
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002080
Test - acc:         0.952500 loss:        0.174889
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002124
Test - acc:         0.953000 loss:        0.174690
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002084
Test - acc:         0.951300 loss:        0.175068
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002031
Test - acc:         0.952800 loss:        0.174404
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002158
Test - acc:         0.951400 loss:        0.174447
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002060
Test - acc:         0.952800 loss:        0.174190
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002042
Test - acc:         0.952400 loss:        0.174436
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002057
Test - acc:         0.952400 loss:        0.173230
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001956
Test - acc:         0.951900 loss:        0.173771
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002101
Test - acc:         0.952700 loss:        0.172401
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002067
Test - acc:         0.953700 loss:        0.171801
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002018
Test - acc:         0.953100 loss:        0.172407
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.001946
Test - acc:         0.952300 loss:        0.171520
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.001961
Test - acc:         0.953600 loss:        0.172241
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001957
Test - acc:         0.953100 loss:        0.173459
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002099
Test - acc:         0.952900 loss:        0.172203
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.001933
Test - acc:         0.952200 loss:        0.171579
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.001999
Test - acc:         0.952600 loss:        0.172015
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001915
Test - acc:         0.951600 loss:        0.171725
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.001918
Test - acc:         0.952600 loss:        0.171813
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.001962
Test - acc:         0.952600 loss:        0.171081
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002096
Test - acc:         0.953500 loss:        0.170015
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.001977
Test - acc:         0.953100 loss:        0.170199
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002011
Test - acc:         0.952800 loss:        0.171716
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.001936
Test - acc:         0.953100 loss:        0.171181
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001974
Test - acc:         0.952200 loss:        0.170804
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.001953
Test - acc:         0.952000 loss:        0.170392
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001936
Test - acc:         0.953200 loss:        0.170841
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.001903
Test - acc:         0.952500 loss:        0.169672
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002098
Test - acc:         0.952800 loss:        0.171148
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.001973
Test - acc:         0.953900 loss:        0.169173
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.001997
Test - acc:         0.953400 loss:        0.169282
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.001912
Test - acc:         0.953600 loss:        0.169992
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001939
Test - acc:         0.954100 loss:        0.168477
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.001920
Test - acc:         0.953600 loss:        0.170505
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.001948
Test - acc:         0.952900 loss:        0.169956
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001913
Test - acc:         0.953800 loss:        0.168993
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001986
Test - acc:         0.953600 loss:        0.169068
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001867
Test - acc:         0.954100 loss:        0.169345
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001895
Test - acc:         0.953200 loss:        0.169182
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002033
Test - acc:         0.953300 loss:        0.169720
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002048
Test - acc:         0.954800 loss:        0.169122
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001854
Test - acc:         0.954400 loss:        0.169641
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.001961
Test - acc:         0.953700 loss:        0.171722
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001961
Test - acc:         0.952900 loss:        0.169732
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.001975
Test - acc:         0.953700 loss:        0.169044
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.001875
Test - acc:         0.954400 loss:        0.169176
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.001975
Test - acc:         0.953900 loss:        0.169362
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001877
Test - acc:         0.952600 loss:        0.169780
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001897
Test - acc:         0.952900 loss:        0.169493
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.002086
Test - acc:         0.953700 loss:        0.169815
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        1.000000 loss:        0.002028
Test - acc:         0.954300 loss:        0.169284
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001952
Test - acc:         0.953400 loss:        0.169441
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001990
Test - acc:         0.953000 loss:        0.169671
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001972
Test - acc:         0.952700 loss:        0.171169
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001888
Test - acc:         0.953000 loss:        0.171015
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.999980 loss:        0.001918
Test - acc:         0.952400 loss:        0.170271
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999960 loss:        0.001984
Test - acc:         0.953800 loss:        0.168252
Sparsity :          0.7500
Wdecay :        0.000500
