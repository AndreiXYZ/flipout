Running --prune_bnorm --model resnet18 --noise --prune_criterion weight_div_flips --seed 44 --prune_freq 70 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=70_seed=44 --save_model=pre-finetune/resnet18_weight_div_flips_pf70_s44 --logdir=criterion_experiment_prune_bnorm/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 70,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": true,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_prune_bnorm/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf70_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11169152
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11169152
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.303340 loss:        2.045872
Test - acc:         0.449200 loss:        1.518474
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.486940 loss:        1.408749
Test - acc:         0.553600 loss:        1.216369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.595040 loss:        1.130263
Test - acc:         0.627400 loss:        1.061316
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.662820 loss:        0.954220
Test - acc:         0.666000 loss:        0.963036
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.710980 loss:        0.823103
Test - acc:         0.700400 loss:        0.900422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.754080 loss:        0.701555
Test - acc:         0.719300 loss:        0.843699
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.784640 loss:        0.626890
Test - acc:         0.762900 loss:        0.702303
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.800120 loss:        0.580976
Test - acc:         0.678300 loss:        1.056559
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.809040 loss:        0.553956
Test - acc:         0.750400 loss:        0.725537
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.817400 loss:        0.527065
Test - acc:         0.750100 loss:        0.776029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.828080 loss:        0.504549
Test - acc:         0.806100 loss:        0.560775
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.827040 loss:        0.498283
Test - acc:         0.787800 loss:        0.637411
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.835700 loss:        0.482435
Test - acc:         0.794000 loss:        0.615528
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838900 loss:        0.468737
Test - acc:         0.820400 loss:        0.537681
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844200 loss:        0.459359
Test - acc:         0.804800 loss:        0.583139
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845740 loss:        0.448696
Test - acc:         0.757400 loss:        0.755301
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.845840 loss:        0.444416
Test - acc:         0.784900 loss:        0.670693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.849560 loss:        0.437153
Test - acc:         0.836300 loss:        0.472858
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.852000 loss:        0.431503
Test - acc:         0.805300 loss:        0.564962
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.853780 loss:        0.428139
Test - acc:         0.822000 loss:        0.534250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.417603
Test - acc:         0.776100 loss:        0.709650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.857220 loss:        0.416609
Test - acc:         0.822800 loss:        0.534768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.418318
Test - acc:         0.835400 loss:        0.503948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.857700 loss:        0.416371
Test - acc:         0.797500 loss:        0.599861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.861780 loss:        0.406843
Test - acc:         0.789200 loss:        0.638827
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.858420 loss:        0.412091
Test - acc:         0.812200 loss:        0.574223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.861500 loss:        0.402603
Test - acc:         0.834200 loss:        0.487177
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.861100 loss:        0.403876
Test - acc:         0.788300 loss:        0.694953
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.862260 loss:        0.397948
Test - acc:         0.834000 loss:        0.496371
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.861600 loss:        0.404324
Test - acc:         0.767800 loss:        0.741355
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.863460 loss:        0.399084
Test - acc:         0.813800 loss:        0.548094
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.862380 loss:        0.399397
Test - acc:         0.821100 loss:        0.548047
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.866720 loss:        0.392388
Test - acc:         0.831100 loss:        0.522133
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.863960 loss:        0.395103
Test - acc:         0.791300 loss:        0.647549
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.865880 loss:        0.393367
Test - acc:         0.748300 loss:        0.861032
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.867420 loss:        0.392783
Test - acc:         0.793100 loss:        0.629516
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.867160 loss:        0.387712
Test - acc:         0.855000 loss:        0.438008
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.387555
Test - acc:         0.817900 loss:        0.557149
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.867240 loss:        0.389858
Test - acc:         0.841200 loss:        0.478896
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.869000 loss:        0.387413
Test - acc:         0.832700 loss:        0.490588
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.864980 loss:        0.394477
Test - acc:         0.841800 loss:        0.463607
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.869720 loss:        0.383677
Test - acc:         0.843400 loss:        0.466556
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.870200 loss:        0.381420
Test - acc:         0.781100 loss:        0.700399
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.869540 loss:        0.383390
Test - acc:         0.801500 loss:        0.623825
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.870180 loss:        0.382817
Test - acc:         0.783300 loss:        0.685600
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.870820 loss:        0.376388
Test - acc:         0.813000 loss:        0.573551
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.869700 loss:        0.380625
Test - acc:         0.832900 loss:        0.502137
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.869460 loss:        0.382523
Test - acc:         0.762700 loss:        0.762488
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.869520 loss:        0.381779
Test - acc:         0.829900 loss:        0.524733
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.867960 loss:        0.379790
Test - acc:         0.834400 loss:        0.510441
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.872280 loss:        0.373012
Test - acc:         0.811000 loss:        0.582797
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.873420 loss:        0.372036
Test - acc:         0.843100 loss:        0.475100
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.870400 loss:        0.378922
Test - acc:         0.856800 loss:        0.411110
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.874820 loss:        0.371978
Test - acc:         0.797100 loss:        0.629058
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.870640 loss:        0.378161
Test - acc:         0.767800 loss:        0.727634
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.872540 loss:        0.372711
Test - acc:         0.797500 loss:        0.611661
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.871460 loss:        0.373897
Test - acc:         0.854300 loss:        0.436976
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.872720 loss:        0.374363
Test - acc:         0.833100 loss:        0.507876
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.872780 loss:        0.371153
Test - acc:         0.867000 loss:        0.394288
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.874740 loss:        0.367565
Test - acc:         0.849200 loss:        0.475920
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.874780 loss:        0.369434
Test - acc:         0.845800 loss:        0.448344
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.871860 loss:        0.376186
Test - acc:         0.781900 loss:        0.695914
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.873440 loss:        0.371171
Test - acc:         0.834900 loss:        0.500511
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.871740 loss:        0.371933
Test - acc:         0.852100 loss:        0.444918
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.871780 loss:        0.371990
Test - acc:         0.841500 loss:        0.489206
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.873980 loss:        0.370594
Test - acc:         0.855200 loss:        0.438623
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.872280 loss:        0.372062
Test - acc:         0.828900 loss:        0.518650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.873000 loss:        0.370841
Test - acc:         0.805500 loss:        0.613012
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.874360 loss:        0.368795
Test - acc:         0.826500 loss:        0.525628
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.877540 loss:        0.359595
Test - acc:         0.854300 loss:        0.434711
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.884060 loss:        0.338920
Test - acc:         0.849400 loss:        0.474978
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.882260 loss:        0.345226
Test - acc:         0.840700 loss:        0.474592
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.879360 loss:        0.352819
Test - acc:         0.870500 loss:        0.402650
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.882380 loss:        0.348061
Test - acc:         0.849800 loss:        0.452050
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.878540 loss:        0.357921
Test - acc:         0.831000 loss:        0.524249
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.880460 loss:        0.348323
Test - acc:         0.845800 loss:        0.461867
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.881260 loss:        0.347700
Test - acc:         0.814900 loss:        0.572868
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.879360 loss:        0.353253
Test - acc:         0.814900 loss:        0.548538
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.879240 loss:        0.353568
Test - acc:         0.812600 loss:        0.566189
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.878540 loss:        0.353944
Test - acc:         0.842000 loss:        0.505409
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.879620 loss:        0.352961
Test - acc:         0.845000 loss:        0.449844
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.881260 loss:        0.345344
Test - acc:         0.781000 loss:        0.679762
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.878640 loss:        0.355829
Test - acc:         0.828800 loss:        0.537729
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.878860 loss:        0.355589
Test - acc:         0.827200 loss:        0.535056
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.881120 loss:        0.350067
Test - acc:         0.837800 loss:        0.476653
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.879580 loss:        0.351018
Test - acc:         0.864800 loss:        0.413981
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.879480 loss:        0.350079
Test - acc:         0.843900 loss:        0.469955
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.878920 loss:        0.347736
Test - acc:         0.834500 loss:        0.506679
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.880220 loss:        0.350494
Test - acc:         0.809200 loss:        0.574903
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.879860 loss:        0.348843
Test - acc:         0.854200 loss:        0.434577
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.879080 loss:        0.349086
Test - acc:         0.838300 loss:        0.484748
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.879000 loss:        0.355543
Test - acc:         0.821900 loss:        0.555947
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.882020 loss:        0.343541
Test - acc:         0.838400 loss:        0.490169
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.881220 loss:        0.348123
Test - acc:         0.852600 loss:        0.429450
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.881220 loss:        0.347982
Test - acc:         0.832500 loss:        0.496400
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.880040 loss:        0.351706
Test - acc:         0.843100 loss:        0.486243
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.878620 loss:        0.357225
Test - acc:         0.843400 loss:        0.467699
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.878780 loss:        0.353955
Test - acc:         0.795600 loss:        0.654334
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.882520 loss:        0.345877
Test - acc:         0.837100 loss:        0.513565
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.878900 loss:        0.353663
Test - acc:         0.775700 loss:        0.703046
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.880080 loss:        0.349134
Test - acc:         0.825400 loss:        0.564518
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.879580 loss:        0.352930
Test - acc:         0.752000 loss:        0.831871
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.880420 loss:        0.350706
Test - acc:         0.861500 loss:        0.426667
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.879460 loss:        0.351585
Test - acc:         0.861600 loss:        0.418914
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.881760 loss:        0.345226
Test - acc:         0.833300 loss:        0.521131
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.882360 loss:        0.344193
Test - acc:         0.814700 loss:        0.590639
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.878140 loss:        0.356996
Test - acc:         0.843200 loss:        0.472486
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.879540 loss:        0.351706
Test - acc:         0.781100 loss:        0.744460
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.879460 loss:        0.349127
Test - acc:         0.849900 loss:        0.446218
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.880260 loss:        0.350209
Test - acc:         0.846800 loss:        0.459204
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.879920 loss:        0.350484
Test - acc:         0.788600 loss:        0.625889
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.879800 loss:        0.351746
Test - acc:         0.810600 loss:        0.554783
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.879940 loss:        0.351082
Test - acc:         0.825500 loss:        0.535655
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.881200 loss:        0.347141
Test - acc:         0.796700 loss:        0.658509
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.880400 loss:        0.348147
Test - acc:         0.833100 loss:        0.508482
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.880680 loss:        0.347463
Test - acc:         0.816200 loss:        0.582684
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.881360 loss:        0.348842
Test - acc:         0.844700 loss:        0.478798
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.880660 loss:        0.350975
Test - acc:         0.843100 loss:        0.487550
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.879540 loss:        0.352401
Test - acc:         0.814400 loss:        0.572296
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.883380 loss:        0.346276
Test - acc:         0.850400 loss:        0.449784
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.880040 loss:        0.350935
Test - acc:         0.822600 loss:        0.549384
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.880780 loss:        0.348941
Test - acc:         0.864300 loss:        0.412874
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.878600 loss:        0.355887
Test - acc:         0.870000 loss:        0.400287
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.882020 loss:        0.349102
Test - acc:         0.839400 loss:        0.496645
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.879920 loss:        0.351522
Test - acc:         0.854000 loss:        0.453821
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.881020 loss:        0.348497
Test - acc:         0.830100 loss:        0.496461
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.880380 loss:        0.348771
Test - acc:         0.834200 loss:        0.513688
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.879660 loss:        0.350753
Test - acc:         0.829200 loss:        0.523956
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.879860 loss:        0.349783
Test - acc:         0.809600 loss:        0.589362
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.882440 loss:        0.348282
Test - acc:         0.831400 loss:        0.514065
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.881760 loss:        0.348556
Test - acc:         0.813100 loss:        0.604383
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.880740 loss:        0.345291
Test - acc:         0.858700 loss:        0.435495
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.880260 loss:        0.351380
Test - acc:         0.863500 loss:        0.411809
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.880200 loss:        0.349505
Test - acc:         0.819000 loss:        0.531644
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.881900 loss:        0.345091
Test - acc:         0.789600 loss:        0.677466
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.882880 loss:        0.345893
Test - acc:         0.848900 loss:        0.452273
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.881920 loss:        0.347748
Test - acc:         0.827600 loss:        0.542605
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.879320 loss:        0.351992
Test - acc:         0.827300 loss:        0.527210
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.881280 loss:        0.349331
Test - acc:         0.820100 loss:        0.559607
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.879580 loss:        0.351211
Test - acc:         0.782700 loss:        0.717626
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.890420 loss:        0.319164
Test - acc:         0.850700 loss:        0.451633
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.889100 loss:        0.322447
Test - acc:         0.867300 loss:        0.399618
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.889000 loss:        0.327168
Test - acc:         0.833800 loss:        0.503204
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.887400 loss:        0.328768
Test - acc:         0.822800 loss:        0.537018
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.886340 loss:        0.331076
Test - acc:         0.831100 loss:        0.517449
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.885420 loss:        0.334492
Test - acc:         0.837700 loss:        0.496625
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.889360 loss:        0.325293
Test - acc:         0.834700 loss:        0.499666
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.888300 loss:        0.325352
Test - acc:         0.797500 loss:        0.646397
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.884500 loss:        0.336066
Test - acc:         0.806000 loss:        0.634106
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.886240 loss:        0.333068
Test - acc:         0.823600 loss:        0.538608
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.932880 loss:        0.197521
Test - acc:         0.925000 loss:        0.220908
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.948340 loss:        0.151320
Test - acc:         0.927300 loss:        0.206690
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.955760 loss:        0.130448
Test - acc:         0.929900 loss:        0.203176
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.960380 loss:        0.118181
Test - acc:         0.930800 loss:        0.205184
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.963280 loss:        0.109735
Test - acc:         0.931300 loss:        0.206242
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.966780 loss:        0.099014
Test - acc:         0.930700 loss:        0.203265
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.969480 loss:        0.091873
Test - acc:         0.931700 loss:        0.199664
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.972120 loss:        0.084094
Test - acc:         0.933000 loss:        0.208250
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.973400 loss:        0.079270
Test - acc:         0.932300 loss:        0.209289
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.973720 loss:        0.075504
Test - acc:         0.931800 loss:        0.215916
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.976040 loss:        0.073065
Test - acc:         0.931300 loss:        0.223191
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.976860 loss:        0.069785
Test - acc:         0.932200 loss:        0.216712
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.978520 loss:        0.064741
Test - acc:         0.933300 loss:        0.214714
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.977580 loss:        0.064648
Test - acc:         0.929500 loss:        0.226544
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.979820 loss:        0.061097
Test - acc:         0.932700 loss:        0.219696
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.979500 loss:        0.060376
Test - acc:         0.930200 loss:        0.227910
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.058318
Test - acc:         0.934500 loss:        0.220165
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.980880 loss:        0.056840
Test - acc:         0.928900 loss:        0.237437
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.980800 loss:        0.056148
Test - acc:         0.924200 loss:        0.252155
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.981600 loss:        0.054572
Test - acc:         0.929200 loss:        0.245419
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.980900 loss:        0.056867
Test - acc:         0.931600 loss:        0.226757
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.980440 loss:        0.055552
Test - acc:         0.930800 loss:        0.234353
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.981780 loss:        0.054011
Test - acc:         0.924300 loss:        0.249999
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.980820 loss:        0.054673
Test - acc:         0.927000 loss:        0.248203
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.981460 loss:        0.053087
Test - acc:         0.927100 loss:        0.246685
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.982400 loss:        0.053015
Test - acc:         0.927600 loss:        0.259181
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.982140 loss:        0.052810
Test - acc:         0.926200 loss:        0.249580
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.981540 loss:        0.055429
Test - acc:         0.923900 loss:        0.260619
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.980560 loss:        0.057476
Test - acc:         0.931700 loss:        0.234848
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.980380 loss:        0.057074
Test - acc:         0.930000 loss:        0.247204
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.979080 loss:        0.060387
Test - acc:         0.925100 loss:        0.252361
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.981440 loss:        0.055163
Test - acc:         0.928900 loss:        0.242544
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.980540 loss:        0.056907
Test - acc:         0.926300 loss:        0.259441
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.982360 loss:        0.054026
Test - acc:         0.930200 loss:        0.241176
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.980240 loss:        0.058690
Test - acc:         0.917300 loss:        0.295705
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.979560 loss:        0.058367
Test - acc:         0.924800 loss:        0.256058
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.980040 loss:        0.058344
Test - acc:         0.927600 loss:        0.247027
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.979280 loss:        0.059731
Test - acc:         0.925000 loss:        0.258601
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.979240 loss:        0.061598
Test - acc:         0.921600 loss:        0.269091
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.979120 loss:        0.063880
Test - acc:         0.921700 loss:        0.266962
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.979700 loss:        0.059383
Test - acc:         0.914500 loss:        0.287607
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.978180 loss:        0.064143
Test - acc:         0.912700 loss:        0.307820
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.978660 loss:        0.063414
Test - acc:         0.915200 loss:        0.301782
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.977420 loss:        0.065994
Test - acc:         0.922800 loss:        0.264785
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.977920 loss:        0.063848
Test - acc:         0.914300 loss:        0.306313
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.977400 loss:        0.066135
Test - acc:         0.919100 loss:        0.275337
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.978860 loss:        0.063983
Test - acc:         0.921500 loss:        0.261494
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.978820 loss:        0.062553
Test - acc:         0.920800 loss:        0.279214
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.976560 loss:        0.067867
Test - acc:         0.920500 loss:        0.282362
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.976660 loss:        0.066712
Test - acc:         0.915800 loss:        0.293307
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.977900 loss:        0.064385
Test - acc:         0.922100 loss:        0.267994
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.978360 loss:        0.063120
Test - acc:         0.917800 loss:        0.285679
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.977600 loss:        0.064759
Test - acc:         0.919200 loss:        0.287809
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.977320 loss:        0.067056
Test - acc:         0.922100 loss:        0.268671
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.978760 loss:        0.062218
Test - acc:         0.922700 loss:        0.273486
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.978620 loss:        0.064687
Test - acc:         0.923300 loss:        0.272093
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.977840 loss:        0.064299
Test - acc:         0.917200 loss:        0.303081
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.978380 loss:        0.066887
Test - acc:         0.921400 loss:        0.269911
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.976840 loss:        0.067182
Test - acc:         0.917700 loss:        0.283330
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.978920 loss:        0.063665
Test - acc:         0.921400 loss:        0.272191
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.983980 loss:        0.049255
Test - acc:         0.921300 loss:        0.265793
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.985760 loss:        0.044296
Test - acc:         0.925600 loss:        0.259678
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.985440 loss:        0.044327
Test - acc:         0.925900 loss:        0.259501
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.986040 loss:        0.042004
Test - acc:         0.925400 loss:        0.264653
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.984660 loss:        0.046804
Test - acc:         0.924900 loss:        0.269292
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.985380 loss:        0.044541
Test - acc:         0.920700 loss:        0.288153
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.984720 loss:        0.045649
Test - acc:         0.924900 loss:        0.268314
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.983800 loss:        0.049043
Test - acc:         0.926900 loss:        0.256885
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.984880 loss:        0.047750
Test - acc:         0.923300 loss:        0.261927
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.985300 loss:        0.044947
Test - acc:         0.929100 loss:        0.259431
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.984780 loss:        0.045977
Test - acc:         0.919700 loss:        0.280343
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.982120 loss:        0.054211
Test - acc:         0.921100 loss:        0.284290
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.984040 loss:        0.047878
Test - acc:         0.925500 loss:        0.269370
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.983140 loss:        0.051018
Test - acc:         0.924600 loss:        0.274837
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.982460 loss:        0.050967
Test - acc:         0.922200 loss:        0.276873
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.980520 loss:        0.056319
Test - acc:         0.921600 loss:        0.291470
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.981200 loss:        0.054207
Test - acc:         0.918300 loss:        0.278284
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.982520 loss:        0.052854
Test - acc:         0.925000 loss:        0.261640
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.982120 loss:        0.053883
Test - acc:         0.922100 loss:        0.272141
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.981280 loss:        0.056474
Test - acc:         0.922900 loss:        0.266421
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.983500 loss:        0.049865
Test - acc:         0.915800 loss:        0.303941
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.983100 loss:        0.049677
Test - acc:         0.918800 loss:        0.298932
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.980480 loss:        0.057713
Test - acc:         0.920000 loss:        0.289475
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.983260 loss:        0.050794
Test - acc:         0.917500 loss:        0.294232
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.981540 loss:        0.055744
Test - acc:         0.923900 loss:        0.279075
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.983140 loss:        0.050981
Test - acc:         0.923500 loss:        0.273979
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.981700 loss:        0.053423
Test - acc:         0.923100 loss:        0.268242
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.983160 loss:        0.051556
Test - acc:         0.920600 loss:        0.288653
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.983160 loss:        0.050902
Test - acc:         0.919100 loss:        0.286957
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.981540 loss:        0.054106
Test - acc:         0.918400 loss:        0.283592
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.981480 loss:        0.055592
Test - acc:         0.917600 loss:        0.299694
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.981640 loss:        0.054327
Test - acc:         0.928600 loss:        0.255109
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.980200 loss:        0.057971
Test - acc:         0.923600 loss:        0.267353
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.984000 loss:        0.048680
Test - acc:         0.923300 loss:        0.283706
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.981760 loss:        0.054809
Test - acc:         0.917800 loss:        0.288489
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.981500 loss:        0.055579
Test - acc:         0.921900 loss:        0.277237
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.981520 loss:        0.056072
Test - acc:         0.922300 loss:        0.277830
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.983080 loss:        0.051041
Test - acc:         0.914900 loss:        0.311665
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.982180 loss:        0.054171
Test - acc:         0.922700 loss:        0.274312
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.982400 loss:        0.052701
Test - acc:         0.919400 loss:        0.274559
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.991980 loss:        0.028490
Test - acc:         0.937600 loss:        0.216924
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.995220 loss:        0.018635
Test - acc:         0.939700 loss:        0.213716
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.995940 loss:        0.016238
Test - acc:         0.938800 loss:        0.209170
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.996920 loss:        0.013582
Test - acc:         0.940200 loss:        0.209993
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.997280 loss:        0.011943
Test - acc:         0.939400 loss:        0.211089
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.997740 loss:        0.011246
Test - acc:         0.940600 loss:        0.208696
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.998220 loss:        0.009804
Test - acc:         0.939900 loss:        0.206755
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.998140 loss:        0.009839
Test - acc:         0.939800 loss:        0.208540
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.998600 loss:        0.008684
Test - acc:         0.941800 loss:        0.209685
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.998180 loss:        0.009321
Test - acc:         0.942500 loss:        0.208037
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.998400 loss:        0.008389
Test - acc:         0.941800 loss:        0.205887
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.998540 loss:        0.008252
Test - acc:         0.942300 loss:        0.205856
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.998740 loss:        0.007461
Test - acc:         0.941700 loss:        0.206200
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.998740 loss:        0.007451
Test - acc:         0.941900 loss:        0.207145
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.006936
Test - acc:         0.942700 loss:        0.204859
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.999000 loss:        0.006680
Test - acc:         0.943500 loss:        0.204733
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.006678
Test - acc:         0.943400 loss:        0.204764
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.006140
Test - acc:         0.944000 loss:        0.204236
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.006186
Test - acc:         0.943300 loss:        0.204265
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999120 loss:        0.006067
Test - acc:         0.943600 loss:        0.204488
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.006029
Test - acc:         0.944200 loss:        0.203438
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.005679
Test - acc:         0.943800 loss:        0.202312
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999140 loss:        0.005895
Test - acc:         0.942800 loss:        0.204137
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.005549
Test - acc:         0.943400 loss:        0.204155
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.005501
Test - acc:         0.943400 loss:        0.201420
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.005420
Test - acc:         0.943400 loss:        0.205885
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999380 loss:        0.005222
Test - acc:         0.945100 loss:        0.203479
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999220 loss:        0.005436
Test - acc:         0.944800 loss:        0.203833
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.005343
Test - acc:         0.943800 loss:        0.205335
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.004818
Test - acc:         0.945400 loss:        0.202465
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.998640 loss:        0.008562
Test - acc:         0.942200 loss:        0.203332
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.998820 loss:        0.007966
Test - acc:         0.942600 loss:        0.205057
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.998600 loss:        0.007988
Test - acc:         0.943600 loss:        0.205559
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.007345
Test - acc:         0.942900 loss:        0.205406
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.998880 loss:        0.007249
Test - acc:         0.942500 loss:        0.205824
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.006707
Test - acc:         0.942700 loss:        0.205046
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.006192
Test - acc:         0.941500 loss:        0.206352
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.006676
Test - acc:         0.943200 loss:        0.203157
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.006204
Test - acc:         0.943300 loss:        0.203361
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.999180 loss:        0.006271
Test - acc:         0.943400 loss:        0.206109
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.006022
Test - acc:         0.943600 loss:        0.203739
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.999260 loss:        0.005739
Test - acc:         0.943300 loss:        0.205642
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.005848
Test - acc:         0.942400 loss:        0.205313
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.005586
Test - acc:         0.942800 loss:        0.205004
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.005969
Test - acc:         0.943600 loss:        0.203892
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.999380 loss:        0.005630
Test - acc:         0.944100 loss:        0.205344
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.999280 loss:        0.005808
Test - acc:         0.943200 loss:        0.201995
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.005729
Test - acc:         0.944300 loss:        0.203663
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.999080 loss:        0.005727
Test - acc:         0.943800 loss:        0.206932
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.005373
Test - acc:         0.942800 loss:        0.203954
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.005575
Test - acc:         0.943400 loss:        0.204528
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.005538
Test - acc:         0.943600 loss:        0.204178
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.005250
Test - acc:         0.943100 loss:        0.205264
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.005284
Test - acc:         0.942200 loss:        0.205135
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.005396
Test - acc:         0.943400 loss:        0.206452
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.005121
Test - acc:         0.943000 loss:        0.205090
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999280 loss:        0.005550
Test - acc:         0.944200 loss:        0.204515
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.005055
Test - acc:         0.944300 loss:        0.202885
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.004619
Test - acc:         0.943600 loss:        0.204924
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.004960
Test - acc:         0.943700 loss:        0.202963
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.004876
Test - acc:         0.943600 loss:        0.203562
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.004704
Test - acc:         0.943800 loss:        0.204320
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.004863
Test - acc:         0.943400 loss:        0.204841
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.004576
Test - acc:         0.943000 loss:        0.204848
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004590
Test - acc:         0.943300 loss:        0.205495
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.004360
Test - acc:         0.943200 loss:        0.203095
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.004476
Test - acc:         0.943100 loss:        0.203657
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.004411
Test - acc:         0.943400 loss:        0.203404
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.004285
Test - acc:         0.943100 loss:        0.205162
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.004350
Test - acc:         0.943600 loss:        0.206819
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.004305
Test - acc:         0.943200 loss:        0.204776
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.004704
Test - acc:         0.942600 loss:        0.205926
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.004973
Test - acc:         0.942300 loss:        0.205350
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.004428
Test - acc:         0.942900 loss:        0.205602
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.004668
Test - acc:         0.943900 loss:        0.205029
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.004385
Test - acc:         0.942200 loss:        0.207261
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.004589
Test - acc:         0.943300 loss:        0.204837
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.004528
Test - acc:         0.943700 loss:        0.203510
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.004152
Test - acc:         0.944100 loss:        0.203465
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.004273
Test - acc:         0.944100 loss:        0.203708
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.004158
Test - acc:         0.943200 loss:        0.204531
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.004162
Test - acc:         0.944100 loss:        0.203992
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.003898
Test - acc:         0.943900 loss:        0.204033
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.004041
Test - acc:         0.943800 loss:        0.204656
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.004347
Test - acc:         0.942800 loss:        0.205428
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.004270
Test - acc:         0.943400 loss:        0.204097
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.003872
Test - acc:         0.943300 loss:        0.204932
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.003890
Test - acc:         0.943000 loss:        0.205758
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.004071
Test - acc:         0.944300 loss:        0.203713
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.004172
Test - acc:         0.943900 loss:        0.205884
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.004044
Test - acc:         0.944600 loss:        0.206577
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.004098
Test - acc:         0.944100 loss:        0.206941
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.003947
Test - acc:         0.944700 loss:        0.203810
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.004013
Test - acc:         0.944300 loss:        0.205890
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.003999
Test - acc:         0.943100 loss:        0.205240
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.003760
Test - acc:         0.943300 loss:        0.204882
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004038
Test - acc:         0.944900 loss:        0.201421
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.003827
Test - acc:         0.943700 loss:        0.204258
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.003804
Test - acc:         0.944300 loss:        0.204200
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.003828
Test - acc:         0.944300 loss:        0.204673
Sparsity :          0.9375
Wdecay :        0.000500
