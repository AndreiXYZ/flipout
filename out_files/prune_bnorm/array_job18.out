Running --prune_bnorm --model resnet18 --prune_criterion global_magnitude --seed 43 --prune_freq 39 --prune_rate 0.5 --comment=resnet18_crit=global_magnitude_pf=39_seed=43 --save_model=pre-finetune/resnet18_global_magnitude_pf39_s43 --logdir=criterion_experiment_prune_bnorm/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "global_magnitude",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": true,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_prune_bnorm/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_global_magnitude_pf39_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11169152
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11169152
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.348320 loss:        1.810961
Test - acc:         0.439400 loss:        1.484592
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.526380 loss:        1.296187
Test - acc:         0.587300 loss:        1.162469
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.635500 loss:        1.021202
Test - acc:         0.626700 loss:        1.042242
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.702640 loss:        0.843171
Test - acc:         0.711600 loss:        0.860480
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.752060 loss:        0.713263
Test - acc:         0.752700 loss:        0.721025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.783660 loss:        0.623389
Test - acc:         0.733700 loss:        0.786503
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.801860 loss:        0.570900
Test - acc:         0.757000 loss:        0.762860
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.814640 loss:        0.539573
Test - acc:         0.704900 loss:        0.941443
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.825480 loss:        0.505140
Test - acc:         0.784800 loss:        0.633475
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.830340 loss:        0.493756
Test - acc:         0.779500 loss:        0.657745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.838340 loss:        0.474250
Test - acc:         0.804200 loss:        0.593576
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.844280 loss:        0.458721
Test - acc:         0.834400 loss:        0.502315
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.846500 loss:        0.449426
Test - acc:         0.817100 loss:        0.554424
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.847180 loss:        0.443512
Test - acc:         0.812100 loss:        0.573417
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.853420 loss:        0.425940
Test - acc:         0.804500 loss:        0.581330
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.857080 loss:        0.415865
Test - acc:         0.827800 loss:        0.517806
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.861940 loss:        0.404304
Test - acc:         0.815300 loss:        0.557021
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.863300 loss:        0.400604
Test - acc:         0.797600 loss:        0.647381
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.864700 loss:        0.398190
Test - acc:         0.820300 loss:        0.561645
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.869280 loss:        0.387392
Test - acc:         0.815000 loss:        0.584442
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.870620 loss:        0.382326
Test - acc:         0.818900 loss:        0.536924
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870120 loss:        0.380016
Test - acc:         0.783000 loss:        0.719661
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.869920 loss:        0.379140
Test - acc:         0.822700 loss:        0.551593
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.874520 loss:        0.374324
Test - acc:         0.836900 loss:        0.490705
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.874720 loss:        0.370254
Test - acc:         0.827900 loss:        0.513918
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.876980 loss:        0.362354
Test - acc:         0.850800 loss:        0.456385
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.873500 loss:        0.368452
Test - acc:         0.823200 loss:        0.540426
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.876640 loss:        0.360789
Test - acc:         0.848400 loss:        0.446691
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.876560 loss:        0.360781
Test - acc:         0.803700 loss:        0.617252
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.879380 loss:        0.354437
Test - acc:         0.703300 loss:        1.070219
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.879920 loss:        0.350535
Test - acc:         0.835700 loss:        0.514332
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.877980 loss:        0.357668
Test - acc:         0.852200 loss:        0.417868
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.880080 loss:        0.353408
Test - acc:         0.832300 loss:        0.508966
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.881440 loss:        0.346138
Test - acc:         0.838300 loss:        0.485547
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.881720 loss:        0.348059
Test - acc:         0.816300 loss:        0.577212
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.882320 loss:        0.345955
Test - acc:         0.811300 loss:        0.593824
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.881720 loss:        0.347067
Test - acc:         0.747800 loss:        0.780011
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.882480 loss:        0.344012
Test - acc:         0.844400 loss:        0.473060
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.884840 loss:        0.337756
Test - acc:         0.833300 loss:        0.514843
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.893180 loss:        0.314835
Test - acc:         0.851000 loss:        0.442271
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.893700 loss:        0.314331
Test - acc:         0.867100 loss:        0.400995
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.890800 loss:        0.317224
Test - acc:         0.808700 loss:        0.606625
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.890880 loss:        0.320333
Test - acc:         0.829900 loss:        0.541439
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.891600 loss:        0.316710
Test - acc:         0.857600 loss:        0.422880
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.893240 loss:        0.312863
Test - acc:         0.834300 loss:        0.500009
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.891300 loss:        0.316374
Test - acc:         0.802800 loss:        0.586419
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.889240 loss:        0.320183
Test - acc:         0.847000 loss:        0.467929
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.891640 loss:        0.317737
Test - acc:         0.873600 loss:        0.389679
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.893640 loss:        0.311846
Test - acc:         0.849300 loss:        0.460027
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.892300 loss:        0.312528
Test - acc:         0.830100 loss:        0.530015
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.893220 loss:        0.310138
Test - acc:         0.839500 loss:        0.482862
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.896000 loss:        0.305627
Test - acc:         0.799400 loss:        0.640870
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.893820 loss:        0.308973
Test - acc:         0.848600 loss:        0.470634
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.897520 loss:        0.301957
Test - acc:         0.844400 loss:        0.477223
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.892200 loss:        0.314946
Test - acc:         0.829500 loss:        0.521404
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.895820 loss:        0.304149
Test - acc:         0.842600 loss:        0.499524
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.894020 loss:        0.310519
Test - acc:         0.868300 loss:        0.386339
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.895060 loss:        0.304604
Test - acc:         0.849100 loss:        0.460860
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.895680 loss:        0.306013
Test - acc:         0.818100 loss:        0.582802
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.893440 loss:        0.309638
Test - acc:         0.818600 loss:        0.539831
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.895180 loss:        0.305700
Test - acc:         0.855000 loss:        0.432997
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.895020 loss:        0.307613
Test - acc:         0.868700 loss:        0.373836
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.897020 loss:        0.300347
Test - acc:         0.821900 loss:        0.571132
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.894800 loss:        0.308590
Test - acc:         0.857500 loss:        0.424362
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.897220 loss:        0.299200
Test - acc:         0.852700 loss:        0.456941
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.897220 loss:        0.302215
Test - acc:         0.836700 loss:        0.535507
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.897060 loss:        0.298886
Test - acc:         0.858700 loss:        0.425410
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.898680 loss:        0.300526
Test - acc:         0.839300 loss:        0.524378
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.896400 loss:        0.302851
Test - acc:         0.843900 loss:        0.466627
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.893820 loss:        0.309055
Test - acc:         0.836100 loss:        0.515341
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.897020 loss:        0.301760
Test - acc:         0.855700 loss:        0.442925
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.897400 loss:        0.298867
Test - acc:         0.840400 loss:        0.481601
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.896000 loss:        0.303592
Test - acc:         0.874300 loss:        0.362063
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.896960 loss:        0.300108
Test - acc:         0.854500 loss:        0.435910
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.897300 loss:        0.301328
Test - acc:         0.860800 loss:        0.435571
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.897680 loss:        0.296914
Test - acc:         0.798800 loss:        0.717289
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.898360 loss:        0.298370
Test - acc:         0.868400 loss:        0.400235
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.897500 loss:        0.298072
Test - acc:         0.854600 loss:        0.421620
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.910760 loss:        0.259605
Test - acc:         0.866700 loss:        0.393787
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.907860 loss:        0.270628
Test - acc:         0.832300 loss:        0.514750
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.905260 loss:        0.276558
Test - acc:         0.863300 loss:        0.417939
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.905280 loss:        0.274158
Test - acc:         0.850800 loss:        0.447754
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.905920 loss:        0.276776
Test - acc:         0.857300 loss:        0.435562
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.906600 loss:        0.274532
Test - acc:         0.851800 loss:        0.440861
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.905580 loss:        0.276601
Test - acc:         0.864400 loss:        0.427819
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.908800 loss:        0.266819
Test - acc:         0.867200 loss:        0.391935
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.906140 loss:        0.274418
Test - acc:         0.875900 loss:        0.374058
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.905320 loss:        0.275594
Test - acc:         0.846700 loss:        0.476115
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.904620 loss:        0.277336
Test - acc:         0.829800 loss:        0.540102
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.905720 loss:        0.275389
Test - acc:         0.842900 loss:        0.486447
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.907980 loss:        0.267578
Test - acc:         0.799500 loss:        0.697372
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.907060 loss:        0.272492
Test - acc:         0.873800 loss:        0.395813
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.907760 loss:        0.271459
Test - acc:         0.848700 loss:        0.475043
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.906620 loss:        0.274207
Test - acc:         0.834700 loss:        0.534748
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.905900 loss:        0.274835
Test - acc:         0.828400 loss:        0.529900
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.906660 loss:        0.270739
Test - acc:         0.851400 loss:        0.441761
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.906640 loss:        0.272568
Test - acc:         0.855400 loss:        0.446708
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.907740 loss:        0.273083
Test - acc:         0.829900 loss:        0.527275
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.909460 loss:        0.265692
Test - acc:         0.828600 loss:        0.563454
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.906620 loss:        0.270020
Test - acc:         0.868200 loss:        0.383223
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.906440 loss:        0.272656
Test - acc:         0.876600 loss:        0.382797
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.907720 loss:        0.270773
Test - acc:         0.856600 loss:        0.422129
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.910440 loss:        0.265150
Test - acc:         0.829400 loss:        0.550594
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.906820 loss:        0.273415
Test - acc:         0.865600 loss:        0.413388
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.904660 loss:        0.276035
Test - acc:         0.865900 loss:        0.411613
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.908320 loss:        0.267782
Test - acc:         0.856000 loss:        0.438708
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.905320 loss:        0.274062
Test - acc:         0.863400 loss:        0.405386
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.905700 loss:        0.274108
Test - acc:         0.684500 loss:        1.312513
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.908320 loss:        0.264601
Test - acc:         0.816600 loss:        0.577259
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.907940 loss:        0.269961
Test - acc:         0.813800 loss:        0.591614
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.906420 loss:        0.272508
Test - acc:         0.846800 loss:        0.471704
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.905520 loss:        0.272542
Test - acc:         0.830800 loss:        0.528793
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.905360 loss:        0.273021
Test - acc:         0.863000 loss:        0.430797
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.906180 loss:        0.272358
Test - acc:         0.863300 loss:        0.410104
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.907360 loss:        0.270858
Test - acc:         0.844400 loss:        0.474115
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.907180 loss:        0.273501
Test - acc:         0.856300 loss:        0.452751
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.906560 loss:        0.273456
Test - acc:         0.830100 loss:        0.495088
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.919960 loss:        0.231955
Test - acc:         0.873300 loss:        0.402266
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.917060 loss:        0.238743
Test - acc:         0.875700 loss:        0.391034
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.916560 loss:        0.243334
Test - acc:         0.855700 loss:        0.447790
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.914060 loss:        0.246461
Test - acc:         0.860700 loss:        0.418335
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.914420 loss:        0.250510
Test - acc:         0.872900 loss:        0.387256
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.913200 loss:        0.250776
Test - acc:         0.872400 loss:        0.375223
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.916520 loss:        0.242228
Test - acc:         0.857100 loss:        0.434898
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.915540 loss:        0.245671
Test - acc:         0.860100 loss:        0.423231
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.913440 loss:        0.248714
Test - acc:         0.813900 loss:        0.607045
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.914480 loss:        0.245218
Test - acc:         0.855200 loss:        0.450940
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.913520 loss:        0.251568
Test - acc:         0.833500 loss:        0.528017
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.915000 loss:        0.246213
Test - acc:         0.865200 loss:        0.403432
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.916800 loss:        0.246121
Test - acc:         0.858400 loss:        0.426235
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.913220 loss:        0.249563
Test - acc:         0.864700 loss:        0.398782
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.915680 loss:        0.245678
Test - acc:         0.846700 loss:        0.497585
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.914080 loss:        0.248783
Test - acc:         0.855900 loss:        0.448189
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.915460 loss:        0.246369
Test - acc:         0.834100 loss:        0.526730
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.915240 loss:        0.246719
Test - acc:         0.847500 loss:        0.477099
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.917140 loss:        0.241759
Test - acc:         0.838000 loss:        0.519680
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.916220 loss:        0.243085
Test - acc:         0.824600 loss:        0.622668
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.914940 loss:        0.245330
Test - acc:         0.851300 loss:        0.452180
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.915780 loss:        0.246644
Test - acc:         0.875000 loss:        0.370467
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.913260 loss:        0.251976
Test - acc:         0.871000 loss:        0.389406
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.913940 loss:        0.246605
Test - acc:         0.873200 loss:        0.395592
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.915020 loss:        0.247687
Test - acc:         0.857400 loss:        0.447840
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.916640 loss:        0.246222
Test - acc:         0.865800 loss:        0.406379
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.915080 loss:        0.248127
Test - acc:         0.882400 loss:        0.372905
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.915200 loss:        0.246120
Test - acc:         0.873400 loss:        0.382755
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.915440 loss:        0.243222
Test - acc:         0.888000 loss:        0.321898
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.913720 loss:        0.248651
Test - acc:         0.876900 loss:        0.374215
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.914880 loss:        0.244201
Test - acc:         0.869900 loss:        0.407550
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.913180 loss:        0.250731
Test - acc:         0.853700 loss:        0.450312
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.911680 loss:        0.249832
Test - acc:         0.870900 loss:        0.398234
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.957580 loss:        0.127519
Test - acc:         0.937000 loss:        0.182902
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.970920 loss:        0.088900
Test - acc:         0.940700 loss:        0.179685
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.975560 loss:        0.074587
Test - acc:         0.940200 loss:        0.178238
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.978360 loss:        0.066590
Test - acc:         0.940300 loss:        0.178598
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.981240 loss:        0.058142
Test - acc:         0.942700 loss:        0.173742
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.983840 loss:        0.051529
Test - acc:         0.943300 loss:        0.176552
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.985100 loss:        0.050719
Test - acc:         0.942000 loss:        0.180519
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.986960 loss:        0.044969
Test - acc:         0.941600 loss:        0.178847
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.987640 loss:        0.041946
Test - acc:         0.946000 loss:        0.177759
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.988460 loss:        0.038583
Test - acc:         0.942400 loss:        0.187461
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.989300 loss:        0.036976
Test - acc:         0.943100 loss:        0.186837
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.989640 loss:        0.034148
Test - acc:         0.943400 loss:        0.188052
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.990520 loss:        0.032178
Test - acc:         0.944300 loss:        0.188586
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.991680 loss:        0.029261
Test - acc:         0.943400 loss:        0.190738
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.991240 loss:        0.029605
Test - acc:         0.942200 loss:        0.196971
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.991920 loss:        0.028316
Test - acc:         0.942200 loss:        0.188062
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.992360 loss:        0.027074
Test - acc:         0.943400 loss:        0.193774
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.992520 loss:        0.026246
Test - acc:         0.945100 loss:        0.196149
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.993920 loss:        0.023546
Test - acc:         0.940500 loss:        0.204542
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.992880 loss:        0.024455
Test - acc:         0.942300 loss:        0.200734
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.993380 loss:        0.023569
Test - acc:         0.943700 loss:        0.198860
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.993420 loss:        0.024449
Test - acc:         0.939700 loss:        0.212948
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.993260 loss:        0.024021
Test - acc:         0.941100 loss:        0.212411
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.994320 loss:        0.021642
Test - acc:         0.942300 loss:        0.209079
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.994000 loss:        0.021498
Test - acc:         0.944200 loss:        0.204493
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.993980 loss:        0.022117
Test - acc:         0.941600 loss:        0.211872
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.993680 loss:        0.022839
Test - acc:         0.943000 loss:        0.211343
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.992980 loss:        0.022731
Test - acc:         0.939600 loss:        0.219983
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.994180 loss:        0.021029
Test - acc:         0.944100 loss:        0.215146
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.994320 loss:        0.021840
Test - acc:         0.942500 loss:        0.209136
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.993500 loss:        0.022841
Test - acc:         0.940100 loss:        0.216738
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.991980 loss:        0.026556
Test - acc:         0.941100 loss:        0.217662
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.991980 loss:        0.027118
Test - acc:         0.938600 loss:        0.225043
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.991600 loss:        0.028036
Test - acc:         0.939200 loss:        0.221311
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.990260 loss:        0.031811
Test - acc:         0.936800 loss:        0.231446
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.991240 loss:        0.029670
Test - acc:         0.937900 loss:        0.219729
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.991100 loss:        0.029090
Test - acc:         0.933800 loss:        0.229096
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.991240 loss:        0.028798
Test - acc:         0.933000 loss:        0.233067
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.990940 loss:        0.030683
Test - acc:         0.927000 loss:        0.257102
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.989380 loss:        0.034002
Test - acc:         0.934500 loss:        0.223266
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.990960 loss:        0.029751
Test - acc:         0.937100 loss:        0.219784
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.990100 loss:        0.032926
Test - acc:         0.929200 loss:        0.258979
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.989800 loss:        0.033943
Test - acc:         0.938500 loss:        0.221918
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.990600 loss:        0.032349
Test - acc:         0.936300 loss:        0.222567
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.989240 loss:        0.034269
Test - acc:         0.932800 loss:        0.237945
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.980240 loss:        0.061839
Test - acc:         0.934100 loss:        0.229825
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.983560 loss:        0.052825
Test - acc:         0.928900 loss:        0.246066
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.986020 loss:        0.045431
Test - acc:         0.926400 loss:        0.252591
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.986440 loss:        0.043646
Test - acc:         0.931400 loss:        0.248184
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.987140 loss:        0.042604
Test - acc:         0.929200 loss:        0.255285
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.986180 loss:        0.042442
Test - acc:         0.932500 loss:        0.246302
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.986760 loss:        0.040275
Test - acc:         0.924000 loss:        0.278119
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.987440 loss:        0.041178
Test - acc:         0.929800 loss:        0.259880
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.985900 loss:        0.043521
Test - acc:         0.920000 loss:        0.294228
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.987080 loss:        0.039574
Test - acc:         0.927300 loss:        0.269722
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.987140 loss:        0.040432
Test - acc:         0.929700 loss:        0.252437
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.987480 loss:        0.039857
Test - acc:         0.928300 loss:        0.248958
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.987980 loss:        0.038180
Test - acc:         0.931100 loss:        0.246934
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.988360 loss:        0.037601
Test - acc:         0.929400 loss:        0.255893
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.985400 loss:        0.045195
Test - acc:         0.919400 loss:        0.302967
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.986220 loss:        0.042821
Test - acc:         0.928100 loss:        0.258086
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.986480 loss:        0.041831
Test - acc:         0.927300 loss:        0.262740
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.987200 loss:        0.039710
Test - acc:         0.926700 loss:        0.258410
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.988560 loss:        0.037182
Test - acc:         0.930000 loss:        0.251706
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.988460 loss:        0.036977
Test - acc:         0.932200 loss:        0.244381
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.986840 loss:        0.040572
Test - acc:         0.929000 loss:        0.249396
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.987120 loss:        0.039839
Test - acc:         0.929100 loss:        0.263288
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.987200 loss:        0.039628
Test - acc:         0.925600 loss:        0.268839
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.986620 loss:        0.041357
Test - acc:         0.920300 loss:        0.289855
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.988380 loss:        0.037202
Test - acc:         0.926800 loss:        0.260217
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.985120 loss:        0.044628
Test - acc:         0.934600 loss:        0.227617
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.986580 loss:        0.042073
Test - acc:         0.931100 loss:        0.247975
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.986800 loss:        0.041207
Test - acc:         0.928500 loss:        0.251866
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.985600 loss:        0.043271
Test - acc:         0.924300 loss:        0.269115
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.987900 loss:        0.038161
Test - acc:         0.929600 loss:        0.246724
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.987160 loss:        0.039082
Test - acc:         0.925700 loss:        0.270908
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.986580 loss:        0.041288
Test - acc:         0.931000 loss:        0.252192
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.987220 loss:        0.040444
Test - acc:         0.929600 loss:        0.258423
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.987560 loss:        0.040171
Test - acc:         0.931000 loss:        0.246870
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.987900 loss:        0.038248
Test - acc:         0.927200 loss:        0.266917
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.986820 loss:        0.041988
Test - acc:         0.925900 loss:        0.277561
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.987060 loss:        0.041600
Test - acc:         0.933300 loss:        0.233614
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.985820 loss:        0.043726
Test - acc:         0.929300 loss:        0.245945
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.986940 loss:        0.041587
Test - acc:         0.932900 loss:        0.242579
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.970580 loss:        0.091843
Test - acc:         0.924000 loss:        0.255121
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.977740 loss:        0.069195
Test - acc:         0.920400 loss:        0.254824
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.978540 loss:        0.065748
Test - acc:         0.922000 loss:        0.272448
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.976560 loss:        0.069323
Test - acc:         0.921100 loss:        0.259344
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.978180 loss:        0.065942
Test - acc:         0.925200 loss:        0.262704
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.978640 loss:        0.064750
Test - acc:         0.925700 loss:        0.252044
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.980140 loss:        0.061171
Test - acc:         0.921400 loss:        0.265766
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.060159
Test - acc:         0.928200 loss:        0.236508
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.979460 loss:        0.060523
Test - acc:         0.924000 loss:        0.268307
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.980860 loss:        0.056880
Test - acc:         0.923400 loss:        0.270503
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.981460 loss:        0.055948
Test - acc:         0.922700 loss:        0.280758
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.981320 loss:        0.055712
Test - acc:         0.931600 loss:        0.244569
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.983120 loss:        0.053425
Test - acc:         0.927800 loss:        0.254321
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.981040 loss:        0.057165
Test - acc:         0.920600 loss:        0.282378
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.980840 loss:        0.057568
Test - acc:         0.928200 loss:        0.251698
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.982140 loss:        0.052961
Test - acc:         0.926000 loss:        0.265883
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.989780 loss:        0.034782
Test - acc:         0.940300 loss:        0.204977
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.994160 loss:        0.023791
Test - acc:         0.942700 loss:        0.200438
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.995080 loss:        0.021504
Test - acc:         0.944500 loss:        0.200243
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.995720 loss:        0.018943
Test - acc:         0.943100 loss:        0.199854
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.995840 loss:        0.018448
Test - acc:         0.944200 loss:        0.200593
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.996800 loss:        0.016144
Test - acc:         0.943800 loss:        0.199400
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.996680 loss:        0.016044
Test - acc:         0.944300 loss:        0.199756
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.996400 loss:        0.015996
Test - acc:         0.946100 loss:        0.197141
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.997240 loss:        0.014024
Test - acc:         0.944500 loss:        0.198848
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.996880 loss:        0.014271
Test - acc:         0.944300 loss:        0.200395
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.997360 loss:        0.013215
Test - acc:         0.944400 loss:        0.200147
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.997540 loss:        0.012747
Test - acc:         0.943300 loss:        0.199294
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.997400 loss:        0.013271
Test - acc:         0.944100 loss:        0.200740
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.997460 loss:        0.013069
Test - acc:         0.944300 loss:        0.200801
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.998120 loss:        0.011511
Test - acc:         0.944300 loss:        0.198873
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.998400 loss:        0.010681
Test - acc:         0.943700 loss:        0.199702
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.997940 loss:        0.011672
Test - acc:         0.944900 loss:        0.201455
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.998140 loss:        0.011058
Test - acc:         0.944200 loss:        0.201206
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.998040 loss:        0.011232
Test - acc:         0.944300 loss:        0.201690
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.997820 loss:        0.011207
Test - acc:         0.945600 loss:        0.200814
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.998020 loss:        0.010334
Test - acc:         0.945700 loss:        0.201362
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.998120 loss:        0.010641
Test - acc:         0.944800 loss:        0.201210
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.998240 loss:        0.010437
Test - acc:         0.946100 loss:        0.200516
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.961420 loss:        0.132979
Test - acc:         0.918600 loss:        0.262050
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.973000 loss:        0.096540
Test - acc:         0.924400 loss:        0.249402
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.975100 loss:        0.086019
Test - acc:         0.925900 loss:        0.239312
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.978520 loss:        0.076818
Test - acc:         0.927400 loss:        0.238825
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.978900 loss:        0.072749
Test - acc:         0.928200 loss:        0.236623
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.980980 loss:        0.067691
Test - acc:         0.926300 loss:        0.235008
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.982180 loss:        0.062525
Test - acc:         0.929500 loss:        0.232359
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.982420 loss:        0.060656
Test - acc:         0.929700 loss:        0.233306
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.983280 loss:        0.059007
Test - acc:         0.929600 loss:        0.238013
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.984900 loss:        0.056628
Test - acc:         0.931500 loss:        0.231517
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.983780 loss:        0.056396
Test - acc:         0.932600 loss:        0.228144
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.984520 loss:        0.054237
Test - acc:         0.929400 loss:        0.230889
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.985140 loss:        0.051513
Test - acc:         0.931700 loss:        0.230003
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.985640 loss:        0.051147
Test - acc:         0.930800 loss:        0.234893
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.986920 loss:        0.048745
Test - acc:         0.933400 loss:        0.227235
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.986960 loss:        0.047302
Test - acc:         0.933200 loss:        0.229373
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.986460 loss:        0.047425
Test - acc:         0.931900 loss:        0.229209
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.987960 loss:        0.044598
Test - acc:         0.930700 loss:        0.232533
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.988020 loss:        0.043820
Test - acc:         0.931600 loss:        0.230442
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.988040 loss:        0.043256
Test - acc:         0.933200 loss:        0.229113
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.987500 loss:        0.042948
Test - acc:         0.933500 loss:        0.233758
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.988660 loss:        0.040624
Test - acc:         0.932700 loss:        0.232416
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.988820 loss:        0.041333
Test - acc:         0.934600 loss:        0.227723
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.989340 loss:        0.039834
Test - acc:         0.933500 loss:        0.226990
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.989340 loss:        0.038807
Test - acc:         0.932200 loss:        0.231357
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.989240 loss:        0.039606
Test - acc:         0.933500 loss:        0.230717
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.989160 loss:        0.038642
Test - acc:         0.935200 loss:        0.230230
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.989900 loss:        0.037217
Test - acc:         0.933400 loss:        0.229280
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.990500 loss:        0.035874
Test - acc:         0.935100 loss:        0.232328
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.991040 loss:        0.035262
Test - acc:         0.933600 loss:        0.234309
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.990840 loss:        0.035987
Test - acc:         0.932000 loss:        0.235914
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.990280 loss:        0.036498
Test - acc:         0.933700 loss:        0.236272
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.990500 loss:        0.035080
Test - acc:         0.932100 loss:        0.239531
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.990900 loss:        0.034833
Test - acc:         0.933200 loss:        0.237409
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.990780 loss:        0.034854
Test - acc:         0.933300 loss:        0.238389
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
