Running --prune_bnorm --model resnet18 --noise --prune_criterion weight_div_flips --seed 43 --prune_freq 117 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=117_seed=43 --save_model=pre-finetune/resnet18_weight_div_flips_pf117_s43 --logdir=criterion_experiment_prune_bnorm/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 117,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": true,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_prune_bnorm/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf117_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11169152
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11169152
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.275580 loss:        2.206162
Test - acc:         0.379300 loss:        1.773930
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.448080 loss:        1.503151
Test - acc:         0.529700 loss:        1.350102
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.561660 loss:        1.220748
Test - acc:         0.589900 loss:        1.153149
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.644920 loss:        1.002910
Test - acc:         0.641300 loss:        1.060353
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.705420 loss:        0.837933
Test - acc:         0.662400 loss:        0.967185
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.753280 loss:        0.707195
Test - acc:         0.719800 loss:        0.820684
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.778140 loss:        0.637992
Test - acc:         0.746700 loss:        0.735908
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.797340 loss:        0.584231
Test - acc:         0.764400 loss:        0.731834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.810600 loss:        0.549598
Test - acc:         0.739700 loss:        0.764843
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.816060 loss:        0.533603
Test - acc:         0.747800 loss:        0.758131
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.822860 loss:        0.513958
Test - acc:         0.768500 loss:        0.701588
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.827900 loss:        0.501282
Test - acc:         0.816000 loss:        0.537637
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.833580 loss:        0.483668
Test - acc:         0.810200 loss:        0.568396
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.836280 loss:        0.476076
Test - acc:         0.768400 loss:        0.720073
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.839420 loss:        0.464692
Test - acc:         0.759300 loss:        0.735929
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845000 loss:        0.453000
Test - acc:         0.824500 loss:        0.526457
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.849680 loss:        0.444501
Test - acc:         0.826500 loss:        0.520368
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.849380 loss:        0.438538
Test - acc:         0.798100 loss:        0.637503
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.850040 loss:        0.440032
Test - acc:         0.829700 loss:        0.507356
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.854180 loss:        0.426594
Test - acc:         0.802500 loss:        0.589427
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.855560 loss:        0.423533
Test - acc:         0.786400 loss:        0.632288
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.855960 loss:        0.420786
Test - acc:         0.789600 loss:        0.646576
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.858360 loss:        0.416946
Test - acc:         0.752800 loss:        0.756219
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.860040 loss:        0.415164
Test - acc:         0.812200 loss:        0.558783
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.858620 loss:        0.414178
Test - acc:         0.814800 loss:        0.577834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.859180 loss:        0.410590
Test - acc:         0.813800 loss:        0.559850
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.860320 loss:        0.407069
Test - acc:         0.851000 loss:        0.442927
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.857900 loss:        0.414354
Test - acc:         0.838400 loss:        0.479358
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.861760 loss:        0.403122
Test - acc:         0.813600 loss:        0.552473
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.864300 loss:        0.400279
Test - acc:         0.798500 loss:        0.615302
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.864260 loss:        0.395054
Test - acc:         0.802800 loss:        0.596695
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.862920 loss:        0.397425
Test - acc:         0.813300 loss:        0.539908
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.863160 loss:        0.393080
Test - acc:         0.815100 loss:        0.580103
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.865860 loss:        0.394840
Test - acc:         0.824300 loss:        0.534873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.864560 loss:        0.393687
Test - acc:         0.828400 loss:        0.520443
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.867500 loss:        0.385755
Test - acc:         0.811500 loss:        0.577941
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.864660 loss:        0.392658
Test - acc:         0.724200 loss:        0.831120
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.866840 loss:        0.389900
Test - acc:         0.847400 loss:        0.461297
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.869240 loss:        0.384570
Test - acc:         0.804400 loss:        0.615804
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.868220 loss:        0.392120
Test - acc:         0.835200 loss:        0.483585
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.868400 loss:        0.386378
Test - acc:         0.832600 loss:        0.516400
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.871260 loss:        0.379740
Test - acc:         0.803600 loss:        0.591817
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.868820 loss:        0.384890
Test - acc:         0.824800 loss:        0.553375
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.867840 loss:        0.387072
Test - acc:         0.832100 loss:        0.534749
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.869020 loss:        0.387383
Test - acc:         0.812300 loss:        0.552859
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.868060 loss:        0.383151
Test - acc:         0.788200 loss:        0.640874
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.870100 loss:        0.382418
Test - acc:         0.809300 loss:        0.578768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.871020 loss:        0.381197
Test - acc:         0.852600 loss:        0.426795
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.869460 loss:        0.380910
Test - acc:         0.806300 loss:        0.583816
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.871240 loss:        0.376807
Test - acc:         0.826200 loss:        0.532606
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.870840 loss:        0.376128
Test - acc:         0.741300 loss:        0.833599
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.871800 loss:        0.376930
Test - acc:         0.803300 loss:        0.625899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.872840 loss:        0.372406
Test - acc:         0.805600 loss:        0.615994
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.872480 loss:        0.377339
Test - acc:         0.808300 loss:        0.567164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.872680 loss:        0.372489
Test - acc:         0.817000 loss:        0.557201
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.872240 loss:        0.375717
Test - acc:         0.820600 loss:        0.541011
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.871320 loss:        0.375434
Test - acc:         0.817900 loss:        0.565509
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.871360 loss:        0.374007
Test - acc:         0.827800 loss:        0.531900
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.873820 loss:        0.373272
Test - acc:         0.821500 loss:        0.544774
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.872140 loss:        0.377352
Test - acc:         0.797900 loss:        0.611422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.870440 loss:        0.378227
Test - acc:         0.837500 loss:        0.477486
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.871900 loss:        0.375845
Test - acc:         0.833100 loss:        0.488383
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.874100 loss:        0.367354
Test - acc:         0.826500 loss:        0.539295
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.871080 loss:        0.375922
Test - acc:         0.841800 loss:        0.473066
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.874420 loss:        0.369489
Test - acc:         0.840400 loss:        0.472459
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.874260 loss:        0.373805
Test - acc:         0.819500 loss:        0.551145
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.874260 loss:        0.371657
Test - acc:         0.854800 loss:        0.430678
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.874440 loss:        0.364968
Test - acc:         0.855400 loss:        0.423692
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.870380 loss:        0.378505
Test - acc:         0.794900 loss:        0.651736
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.871280 loss:        0.374141
Test - acc:         0.820000 loss:        0.559046
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.873240 loss:        0.370397
Test - acc:         0.806100 loss:        0.632268
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.874620 loss:        0.367881
Test - acc:         0.857500 loss:        0.429875
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.874280 loss:        0.370817
Test - acc:         0.826000 loss:        0.523220
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.874020 loss:        0.368452
Test - acc:         0.835100 loss:        0.523028
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.873600 loss:        0.374590
Test - acc:         0.826800 loss:        0.531651
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.874400 loss:        0.370043
Test - acc:         0.807200 loss:        0.664722
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.874180 loss:        0.370537
Test - acc:         0.822800 loss:        0.571619
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.874600 loss:        0.366761
Test - acc:         0.836000 loss:        0.495049
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.873540 loss:        0.372187
Test - acc:         0.826100 loss:        0.527888
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.872460 loss:        0.373549
Test - acc:         0.766800 loss:        0.795014
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.874640 loss:        0.366928
Test - acc:         0.828800 loss:        0.519656
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.874720 loss:        0.367976
Test - acc:         0.820000 loss:        0.568690
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.874220 loss:        0.370904
Test - acc:         0.820300 loss:        0.575675
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.874780 loss:        0.367725
Test - acc:         0.844600 loss:        0.476037
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.875320 loss:        0.367132
Test - acc:         0.834100 loss:        0.508926
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.873220 loss:        0.369282
Test - acc:         0.792800 loss:        0.655586
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.876020 loss:        0.363000
Test - acc:         0.809100 loss:        0.580211
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.874320 loss:        0.370175
Test - acc:         0.829300 loss:        0.512752
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.872960 loss:        0.370879
Test - acc:         0.865500 loss:        0.396209
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.873320 loss:        0.371317
Test - acc:         0.835700 loss:        0.485132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.876060 loss:        0.364766
Test - acc:         0.812000 loss:        0.566341
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.873920 loss:        0.370142
Test - acc:         0.821300 loss:        0.544474
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.875960 loss:        0.366758
Test - acc:         0.813800 loss:        0.569448
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.875080 loss:        0.367920
Test - acc:         0.831400 loss:        0.516796
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.873840 loss:        0.368995
Test - acc:         0.762900 loss:        0.744116
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.876640 loss:        0.364468
Test - acc:         0.788400 loss:        0.622598
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.874340 loss:        0.369125
Test - acc:         0.792900 loss:        0.647570
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.873940 loss:        0.369864
Test - acc:         0.741700 loss:        0.853622
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.874500 loss:        0.367637
Test - acc:         0.840600 loss:        0.503573
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.876660 loss:        0.362519
Test - acc:         0.784900 loss:        0.649397
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.875220 loss:        0.365134
Test - acc:         0.849800 loss:        0.460079
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.877000 loss:        0.363290
Test - acc:         0.843100 loss:        0.459065
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.876500 loss:        0.362516
Test - acc:         0.798200 loss:        0.621677
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.876840 loss:        0.367939
Test - acc:         0.783400 loss:        0.719071
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.875520 loss:        0.365693
Test - acc:         0.821600 loss:        0.536567
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.875060 loss:        0.366684
Test - acc:         0.849600 loss:        0.438157
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.875340 loss:        0.364328
Test - acc:         0.821600 loss:        0.533868
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.878300 loss:        0.358434
Test - acc:         0.837800 loss:        0.494301
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.878940 loss:        0.358265
Test - acc:         0.685600 loss:        1.170237
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.874260 loss:        0.366177
Test - acc:         0.845500 loss:        0.471626
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.874340 loss:        0.364436
Test - acc:         0.807000 loss:        0.590794
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.876520 loss:        0.362383
Test - acc:         0.849900 loss:        0.454454
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.876160 loss:        0.367129
Test - acc:         0.717600 loss:        0.997870
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.875200 loss:        0.364394
Test - acc:         0.819600 loss:        0.558280
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.876060 loss:        0.363108
Test - acc:         0.847200 loss:        0.453666
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.876240 loss:        0.363968
Test - acc:         0.833100 loss:        0.505149
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.875420 loss:        0.364747
Test - acc:         0.767000 loss:        0.785326
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.883420 loss:        0.340556
Test - acc:         0.841700 loss:        0.471917
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.884620 loss:        0.337716
Test - acc:         0.813000 loss:        0.572297
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.880600 loss:        0.350107
Test - acc:         0.799900 loss:        0.604181
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.882060 loss:        0.346872
Test - acc:         0.835200 loss:        0.504727
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.881760 loss:        0.344850
Test - acc:         0.843600 loss:        0.505588
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.880460 loss:        0.349921
Test - acc:         0.820400 loss:        0.549165
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.881340 loss:        0.345828
Test - acc:         0.813200 loss:        0.586520
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.880860 loss:        0.348592
Test - acc:         0.854400 loss:        0.431304
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.881440 loss:        0.348892
Test - acc:         0.812700 loss:        0.597898
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.883720 loss:        0.342723
Test - acc:         0.832600 loss:        0.511618
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.880360 loss:        0.347595
Test - acc:         0.851400 loss:        0.440655
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.883400 loss:        0.345083
Test - acc:         0.858300 loss:        0.427513
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.882400 loss:        0.346262
Test - acc:         0.854500 loss:        0.437079
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.880760 loss:        0.348087
Test - acc:         0.834300 loss:        0.499099
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.878940 loss:        0.350155
Test - acc:         0.842800 loss:        0.505304
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.882040 loss:        0.345034
Test - acc:         0.829800 loss:        0.509091
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.881400 loss:        0.346683
Test - acc:         0.847700 loss:        0.459088
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.882560 loss:        0.347756
Test - acc:         0.811100 loss:        0.586390
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.878760 loss:        0.353809
Test - acc:         0.840100 loss:        0.483812
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.880480 loss:        0.350853
Test - acc:         0.854100 loss:        0.434249
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.883460 loss:        0.343280
Test - acc:         0.856000 loss:        0.447414
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.881520 loss:        0.349567
Test - acc:         0.859200 loss:        0.423104
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.880520 loss:        0.350114
Test - acc:         0.846000 loss:        0.463041
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.880300 loss:        0.348513
Test - acc:         0.853700 loss:        0.441033
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.880140 loss:        0.349319
Test - acc:         0.792100 loss:        0.702633
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.880660 loss:        0.349398
Test - acc:         0.844200 loss:        0.468309
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.880000 loss:        0.348513
Test - acc:         0.860400 loss:        0.418063
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.881360 loss:        0.348496
Test - acc:         0.810800 loss:        0.569043
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.881360 loss:        0.344525
Test - acc:         0.838000 loss:        0.468698
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.880260 loss:        0.351859
Test - acc:         0.837000 loss:        0.506210
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.878300 loss:        0.356628
Test - acc:         0.822100 loss:        0.552808
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.881040 loss:        0.347157
Test - acc:         0.820100 loss:        0.559748
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.879040 loss:        0.351196
Test - acc:         0.778900 loss:        0.667475
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.933040 loss:        0.200954
Test - acc:         0.919200 loss:        0.231958
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.949200 loss:        0.151471
Test - acc:         0.922200 loss:        0.228379
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.954340 loss:        0.134136
Test - acc:         0.926900 loss:        0.212295
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.958220 loss:        0.121857
Test - acc:         0.927700 loss:        0.211247
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.961480 loss:        0.114169
Test - acc:         0.928100 loss:        0.213596
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.965240 loss:        0.103249
Test - acc:         0.930700 loss:        0.212377
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.967740 loss:        0.094614
Test - acc:         0.926100 loss:        0.223258
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.971160 loss:        0.086931
Test - acc:         0.929600 loss:        0.222043
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.971940 loss:        0.082828
Test - acc:         0.929400 loss:        0.223715
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.973660 loss:        0.076990
Test - acc:         0.929500 loss:        0.219932
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.974480 loss:        0.074210
Test - acc:         0.931600 loss:        0.224128
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.977000 loss:        0.068645
Test - acc:         0.931200 loss:        0.228654
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.978240 loss:        0.066415
Test - acc:         0.931100 loss:        0.222704
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.979440 loss:        0.059970
Test - acc:         0.927200 loss:        0.250961
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.978540 loss:        0.062808
Test - acc:         0.932400 loss:        0.234548
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.978580 loss:        0.062992
Test - acc:         0.927600 loss:        0.253740
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.978960 loss:        0.060574
Test - acc:         0.928200 loss:        0.251325
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.981220 loss:        0.056053
Test - acc:         0.931400 loss:        0.242077
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.981540 loss:        0.054810
Test - acc:         0.927700 loss:        0.246665
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.981360 loss:        0.055111
Test - acc:         0.930000 loss:        0.256558
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.979720 loss:        0.058465
Test - acc:         0.925700 loss:        0.260165
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.980600 loss:        0.057288
Test - acc:         0.924800 loss:        0.255855
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.981400 loss:        0.056898
Test - acc:         0.926200 loss:        0.264242
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.982560 loss:        0.053203
Test - acc:         0.926300 loss:        0.268157
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.980820 loss:        0.056291
Test - acc:         0.927400 loss:        0.257061
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.056246
Test - acc:         0.924600 loss:        0.260514
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.978860 loss:        0.059419
Test - acc:         0.917800 loss:        0.301491
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.980720 loss:        0.057343
Test - acc:         0.925500 loss:        0.255470
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.983120 loss:        0.050892
Test - acc:         0.928100 loss:        0.255722
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.980220 loss:        0.057643
Test - acc:         0.923100 loss:        0.271044
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.980200 loss:        0.058012
Test - acc:         0.925100 loss:        0.264732
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.979520 loss:        0.060337
Test - acc:         0.925000 loss:        0.275258
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.979240 loss:        0.059655
Test - acc:         0.920600 loss:        0.287040
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.978840 loss:        0.063632
Test - acc:         0.924500 loss:        0.260169
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.979540 loss:        0.061696
Test - acc:         0.920900 loss:        0.270681
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.979860 loss:        0.059362
Test - acc:         0.924200 loss:        0.267010
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.978880 loss:        0.062566
Test - acc:         0.921900 loss:        0.273830
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.978840 loss:        0.062269
Test - acc:         0.921400 loss:        0.278936
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.979980 loss:        0.059805
Test - acc:         0.921700 loss:        0.273437
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.979540 loss:        0.060339
Test - acc:         0.926700 loss:        0.260002
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.977800 loss:        0.065236
Test - acc:         0.916600 loss:        0.289451
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.978780 loss:        0.063544
Test - acc:         0.923300 loss:        0.270173
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.979480 loss:        0.061947
Test - acc:         0.922600 loss:        0.264037
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.978900 loss:        0.063629
Test - acc:         0.923200 loss:        0.264759
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.977800 loss:        0.065459
Test - acc:         0.917800 loss:        0.291398
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.978440 loss:        0.065677
Test - acc:         0.914200 loss:        0.306628
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.977600 loss:        0.066105
Test - acc:         0.916400 loss:        0.296087
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.976960 loss:        0.069385
Test - acc:         0.912900 loss:        0.298590
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.978400 loss:        0.063606
Test - acc:         0.918800 loss:        0.280324
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.977420 loss:        0.066090
Test - acc:         0.923800 loss:        0.266571
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.977820 loss:        0.067637
Test - acc:         0.919500 loss:        0.285208
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.977420 loss:        0.065571
Test - acc:         0.917900 loss:        0.304781
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.977460 loss:        0.065981
Test - acc:         0.919100 loss:        0.293972
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.977680 loss:        0.065944
Test - acc:         0.921100 loss:        0.280035
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.978100 loss:        0.065918
Test - acc:         0.921200 loss:        0.291911
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.976000 loss:        0.069179
Test - acc:         0.916600 loss:        0.281940
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.980020 loss:        0.059462
Test - acc:         0.907000 loss:        0.342770
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.977860 loss:        0.065224
Test - acc:         0.918300 loss:        0.304380
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.979880 loss:        0.059948
Test - acc:         0.922100 loss:        0.281149
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.974220 loss:        0.073947
Test - acc:         0.917300 loss:        0.284322
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.976680 loss:        0.069244
Test - acc:         0.921400 loss:        0.274831
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.977700 loss:        0.065875
Test - acc:         0.917000 loss:        0.299770
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.979500 loss:        0.061796
Test - acc:         0.918100 loss:        0.301631
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.977600 loss:        0.066523
Test - acc:         0.917100 loss:        0.306737
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.975760 loss:        0.070331
Test - acc:         0.921300 loss:        0.271236
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.978640 loss:        0.064440
Test - acc:         0.919500 loss:        0.289716
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.979880 loss:        0.059961
Test - acc:         0.923100 loss:        0.280932
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.980100 loss:        0.059133
Test - acc:         0.921300 loss:        0.294436
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.977740 loss:        0.065827
Test - acc:         0.915000 loss:        0.310540
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.978600 loss:        0.060830
Test - acc:         0.918800 loss:        0.299791
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.976040 loss:        0.069938
Test - acc:         0.919100 loss:        0.284237
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.977140 loss:        0.067192
Test - acc:         0.923200 loss:        0.278770
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.979460 loss:        0.062333
Test - acc:         0.912000 loss:        0.318642
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.976440 loss:        0.069117
Test - acc:         0.916900 loss:        0.296556
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.978420 loss:        0.065214
Test - acc:         0.915400 loss:        0.300835
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.979020 loss:        0.061921
Test - acc:         0.912200 loss:        0.310817
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.979220 loss:        0.061292
Test - acc:         0.917700 loss:        0.302214
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.979080 loss:        0.062777
Test - acc:         0.919200 loss:        0.288947
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.978060 loss:        0.064645
Test - acc:         0.919900 loss:        0.278074
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.980180 loss:        0.058829
Test - acc:         0.924500 loss:        0.281946
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.977880 loss:        0.066071
Test - acc:         0.911100 loss:        0.319406
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.977840 loss:        0.066556
Test - acc:         0.927800 loss:        0.265667
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.979500 loss:        0.061238
Test - acc:         0.922500 loss:        0.266675
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.979400 loss:        0.060426
Test - acc:         0.920000 loss:        0.281856
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.986080 loss:        0.044206
Test - acc:         0.930400 loss:        0.248613
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.985740 loss:        0.042416
Test - acc:         0.931500 loss:        0.255199
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.984900 loss:        0.045589
Test - acc:         0.916200 loss:        0.307528
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.985980 loss:        0.041884
Test - acc:         0.929200 loss:        0.263027
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.985460 loss:        0.045098
Test - acc:         0.923400 loss:        0.277902
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.983160 loss:        0.051371
Test - acc:         0.920900 loss:        0.287005
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.984860 loss:        0.046873
Test - acc:         0.926200 loss:        0.257860
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.984120 loss:        0.048205
Test - acc:         0.925300 loss:        0.277718
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.984560 loss:        0.045816
Test - acc:         0.927400 loss:        0.273730
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.984280 loss:        0.048239
Test - acc:         0.923900 loss:        0.285910
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.984600 loss:        0.047496
Test - acc:         0.922900 loss:        0.281114
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.984000 loss:        0.049310
Test - acc:         0.922100 loss:        0.279263
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.982380 loss:        0.050191
Test - acc:         0.915600 loss:        0.303993
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.981140 loss:        0.056187
Test - acc:         0.923200 loss:        0.270850
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.981240 loss:        0.055397
Test - acc:         0.918000 loss:        0.295344
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.984140 loss:        0.048113
Test - acc:         0.920200 loss:        0.282306
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.991700 loss:        0.027394
Test - acc:         0.936300 loss:        0.225256
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.995520 loss:        0.016910
Test - acc:         0.938600 loss:        0.216979
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.996800 loss:        0.013247
Test - acc:         0.941400 loss:        0.216605
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.997320 loss:        0.012411
Test - acc:         0.942500 loss:        0.214694
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.997620 loss:        0.011005
Test - acc:         0.943500 loss:        0.213316
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.998120 loss:        0.009431
Test - acc:         0.942000 loss:        0.214934
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.998380 loss:        0.008637
Test - acc:         0.943200 loss:        0.214425
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.998700 loss:        0.008130
Test - acc:         0.942500 loss:        0.214041
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.998500 loss:        0.007553
Test - acc:         0.943000 loss:        0.213542
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.998660 loss:        0.007675
Test - acc:         0.942700 loss:        0.214226
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.998720 loss:        0.007126
Test - acc:         0.943400 loss:        0.211724
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.999180 loss:        0.006293
Test - acc:         0.943200 loss:        0.212539
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.998780 loss:        0.006394
Test - acc:         0.942600 loss:        0.213042
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.999100 loss:        0.006212
Test - acc:         0.943600 loss:        0.212246
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.999120 loss:        0.005852
Test - acc:         0.944700 loss:        0.215197
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.999260 loss:        0.005644
Test - acc:         0.945400 loss:        0.210597
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999160 loss:        0.005541
Test - acc:         0.943300 loss:        0.214455
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999180 loss:        0.005391
Test - acc:         0.944000 loss:        0.211911
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.005421
Test - acc:         0.944000 loss:        0.215012
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999180 loss:        0.005277
Test - acc:         0.945100 loss:        0.213394
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.004806
Test - acc:         0.944700 loss:        0.212555
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.004720
Test - acc:         0.945300 loss:        0.212182
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.004389
Test - acc:         0.944900 loss:        0.210830
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.004859
Test - acc:         0.946200 loss:        0.210861
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.004557
Test - acc:         0.945500 loss:        0.213526
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.004542
Test - acc:         0.945300 loss:        0.212592
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.004420
Test - acc:         0.946600 loss:        0.211732
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.003947
Test - acc:         0.945200 loss:        0.213474
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.004348
Test - acc:         0.945100 loss:        0.212148
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.003762
Test - acc:         0.945900 loss:        0.212801
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.004028
Test - acc:         0.946900 loss:        0.211016
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.004090
Test - acc:         0.946100 loss:        0.211218
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.003804
Test - acc:         0.946500 loss:        0.210644
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.004002
Test - acc:         0.946500 loss:        0.210877
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.003748
Test - acc:         0.945500 loss:        0.210407
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.004014
Test - acc:         0.945300 loss:        0.210523
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.004169
Test - acc:         0.946100 loss:        0.210477
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003697
Test - acc:         0.945900 loss:        0.210653
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.003728
Test - acc:         0.946200 loss:        0.210568
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.003644
Test - acc:         0.945400 loss:        0.210228
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.003416
Test - acc:         0.945000 loss:        0.211076
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003557
Test - acc:         0.945200 loss:        0.210993
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.003520
Test - acc:         0.947400 loss:        0.208809
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.003536
Test - acc:         0.946100 loss:        0.211301
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.003415
Test - acc:         0.946700 loss:        0.209314
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.003436
Test - acc:         0.946700 loss:        0.209359
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003502
Test - acc:         0.946900 loss:        0.208863
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.003260
Test - acc:         0.945700 loss:        0.211651
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.003651
Test - acc:         0.946100 loss:        0.209492
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.003440
Test - acc:         0.945500 loss:        0.209720
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.003154
Test - acc:         0.945800 loss:        0.209157
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.003234
Test - acc:         0.946300 loss:        0.208582
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.003108
Test - acc:         0.945700 loss:        0.208704
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.003250
Test - acc:         0.945900 loss:        0.208756
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.003101
Test - acc:         0.947200 loss:        0.206872
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002948
Test - acc:         0.946800 loss:        0.210077
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.003274
Test - acc:         0.945900 loss:        0.209035
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.003198
Test - acc:         0.945300 loss:        0.208901
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.003163
Test - acc:         0.946600 loss:        0.207617
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.003038
Test - acc:         0.946500 loss:        0.207090
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002785
Test - acc:         0.947400 loss:        0.206688
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.003040
Test - acc:         0.946600 loss:        0.208341
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.002932
Test - acc:         0.946400 loss:        0.207614
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003147
Test - acc:         0.946300 loss:        0.207309
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002982
Test - acc:         0.947000 loss:        0.206797
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002910
Test - acc:         0.946300 loss:        0.207591
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002807
Test - acc:         0.944900 loss:        0.209347
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002821
Test - acc:         0.946000 loss:        0.207284
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002873
Test - acc:         0.947100 loss:        0.205830
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002644
Test - acc:         0.947500 loss:        0.207011
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002928
Test - acc:         0.947000 loss:        0.206091
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002749
Test - acc:         0.946900 loss:        0.207162
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002661
Test - acc:         0.946700 loss:        0.205577
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002649
Test - acc:         0.948000 loss:        0.205561
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002729
Test - acc:         0.947000 loss:        0.205547
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002727
Test - acc:         0.947500 loss:        0.205783
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002749
Test - acc:         0.946700 loss:        0.206869
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002605
Test - acc:         0.947200 loss:        0.206245
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002776
Test - acc:         0.946300 loss:        0.206165
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002764
Test - acc:         0.947100 loss:        0.206670
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002654
Test - acc:         0.947100 loss:        0.205560
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002620
Test - acc:         0.947200 loss:        0.206083
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.002976
Test - acc:         0.945500 loss:        0.207675
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.002991
Test - acc:         0.946900 loss:        0.204730
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002706
Test - acc:         0.945400 loss:        0.207314
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999940 loss:        0.002578
Test - acc:         0.948300 loss:        0.204724
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.002801
Test - acc:         0.947200 loss:        0.206156
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002787
Test - acc:         0.948100 loss:        0.203505
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002549
Test - acc:         0.947500 loss:        0.204242
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002706
Test - acc:         0.947300 loss:        0.204092
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002555
Test - acc:         0.947700 loss:        0.203331
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002584
Test - acc:         0.948600 loss:        0.203003
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002597
Test - acc:         0.945900 loss:        0.204573
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002635
Test - acc:         0.947000 loss:        0.204147
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002644
Test - acc:         0.947500 loss:        0.204828
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002610
Test - acc:         0.947100 loss:        0.202763
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999900 loss:        0.002595
Test - acc:         0.946300 loss:        0.205942
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002713
Test - acc:         0.947600 loss:        0.204892
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002601
Test - acc:         0.947600 loss:        0.202986
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002520
Test - acc:         0.948600 loss:        0.202649
Sparsity :          0.7500
Wdecay :        0.000500
