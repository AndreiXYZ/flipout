Running --prune_bnorm --model resnet18 --noise --prune_criterion weight_div_flips --seed 43 --prune_freq 50 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=50_seed=43 --save_model=pre-finetune/resnet18_weight_div_flips_pf50_s43 --logdir=criterion_experiment_prune_bnorm/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": true,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_prune_bnorm/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf50_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11169152
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11169152
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.275580 loss:        2.206162
Test - acc:         0.379300 loss:        1.773930
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.448080 loss:        1.503151
Test - acc:         0.529700 loss:        1.350102
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.561660 loss:        1.220748
Test - acc:         0.589900 loss:        1.153149
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.644920 loss:        1.002910
Test - acc:         0.641300 loss:        1.060353
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.705420 loss:        0.837933
Test - acc:         0.662400 loss:        0.967185
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.753280 loss:        0.707195
Test - acc:         0.719800 loss:        0.820684
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.778140 loss:        0.637992
Test - acc:         0.746700 loss:        0.735908
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.797340 loss:        0.584231
Test - acc:         0.764400 loss:        0.731834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.810600 loss:        0.549598
Test - acc:         0.739700 loss:        0.764843
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.816060 loss:        0.533603
Test - acc:         0.747800 loss:        0.758131
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.822860 loss:        0.513958
Test - acc:         0.768500 loss:        0.701588
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.827900 loss:        0.501282
Test - acc:         0.816000 loss:        0.537637
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.833580 loss:        0.483668
Test - acc:         0.810200 loss:        0.568396
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.836280 loss:        0.476076
Test - acc:         0.768400 loss:        0.720073
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.839420 loss:        0.464692
Test - acc:         0.759300 loss:        0.735929
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845000 loss:        0.453000
Test - acc:         0.824500 loss:        0.526457
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.849680 loss:        0.444501
Test - acc:         0.826500 loss:        0.520368
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.849380 loss:        0.438538
Test - acc:         0.798100 loss:        0.637503
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.850040 loss:        0.440032
Test - acc:         0.829700 loss:        0.507356
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.854180 loss:        0.426594
Test - acc:         0.802500 loss:        0.589427
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.855560 loss:        0.423533
Test - acc:         0.786400 loss:        0.632288
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.855960 loss:        0.420786
Test - acc:         0.789600 loss:        0.646576
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.858360 loss:        0.416946
Test - acc:         0.752800 loss:        0.756219
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.860040 loss:        0.415164
Test - acc:         0.812200 loss:        0.558783
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.858620 loss:        0.414178
Test - acc:         0.814800 loss:        0.577834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.859180 loss:        0.410590
Test - acc:         0.813800 loss:        0.559850
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.860320 loss:        0.407069
Test - acc:         0.851000 loss:        0.442927
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.857900 loss:        0.414354
Test - acc:         0.838400 loss:        0.479358
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.861760 loss:        0.403122
Test - acc:         0.813600 loss:        0.552473
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.864300 loss:        0.400279
Test - acc:         0.798500 loss:        0.615302
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.864260 loss:        0.395054
Test - acc:         0.802800 loss:        0.596695
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.862920 loss:        0.397425
Test - acc:         0.813300 loss:        0.539908
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.863160 loss:        0.393080
Test - acc:         0.815100 loss:        0.580103
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.865860 loss:        0.394840
Test - acc:         0.824300 loss:        0.534873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.864560 loss:        0.393687
Test - acc:         0.828400 loss:        0.520443
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.867500 loss:        0.385755
Test - acc:         0.811500 loss:        0.577941
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.864660 loss:        0.392658
Test - acc:         0.724200 loss:        0.831120
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.866840 loss:        0.389900
Test - acc:         0.847400 loss:        0.461297
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.869240 loss:        0.384570
Test - acc:         0.804400 loss:        0.615804
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.868220 loss:        0.392120
Test - acc:         0.835200 loss:        0.483585
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.868400 loss:        0.386378
Test - acc:         0.832600 loss:        0.516400
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.871260 loss:        0.379740
Test - acc:         0.803600 loss:        0.591817
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.868820 loss:        0.384890
Test - acc:         0.824800 loss:        0.553375
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.867840 loss:        0.387072
Test - acc:         0.832100 loss:        0.534749
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.869020 loss:        0.387383
Test - acc:         0.812300 loss:        0.552859
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.868060 loss:        0.383151
Test - acc:         0.788200 loss:        0.640874
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.870100 loss:        0.382418
Test - acc:         0.809300 loss:        0.578768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.871020 loss:        0.381197
Test - acc:         0.852600 loss:        0.426795
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.869460 loss:        0.380910
Test - acc:         0.806300 loss:        0.583816
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.871240 loss:        0.376807
Test - acc:         0.826200 loss:        0.532606
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.884020 loss:        0.342371
Test - acc:         0.800800 loss:        0.652818
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.880140 loss:        0.349581
Test - acc:         0.797400 loss:        0.659870
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.878740 loss:        0.355571
Test - acc:         0.785200 loss:        0.687007
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.880360 loss:        0.351620
Test - acc:         0.847200 loss:        0.465945
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.876320 loss:        0.360418
Test - acc:         0.813100 loss:        0.557891
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.877840 loss:        0.355264
Test - acc:         0.854600 loss:        0.441978
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.876640 loss:        0.360650
Test - acc:         0.814800 loss:        0.566016
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.876440 loss:        0.360398
Test - acc:         0.835300 loss:        0.503754
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.877920 loss:        0.357403
Test - acc:         0.833400 loss:        0.541043
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.876400 loss:        0.360601
Test - acc:         0.818300 loss:        0.535045
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.876940 loss:        0.361186
Test - acc:         0.825600 loss:        0.528368
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.879320 loss:        0.356275
Test - acc:         0.836700 loss:        0.495692
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.881560 loss:        0.346317
Test - acc:         0.832800 loss:        0.504342
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.878020 loss:        0.360855
Test - acc:         0.810900 loss:        0.618037
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.879160 loss:        0.354941
Test - acc:         0.845800 loss:        0.484606
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.877040 loss:        0.358306
Test - acc:         0.829600 loss:        0.541088
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.875260 loss:        0.360771
Test - acc:         0.847700 loss:        0.475660
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.880220 loss:        0.349577
Test - acc:         0.854700 loss:        0.416530
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.877280 loss:        0.354655
Test - acc:         0.784700 loss:        0.691386
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.876160 loss:        0.362090
Test - acc:         0.803100 loss:        0.616563
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.877880 loss:        0.359163
Test - acc:         0.817400 loss:        0.568024
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.878100 loss:        0.355922
Test - acc:         0.812600 loss:        0.563595
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.879960 loss:        0.356827
Test - acc:         0.823500 loss:        0.539507
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.879020 loss:        0.354926
Test - acc:         0.812400 loss:        0.576939
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.879800 loss:        0.354815
Test - acc:         0.835500 loss:        0.502198
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.877120 loss:        0.355522
Test - acc:         0.823800 loss:        0.586864
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.877700 loss:        0.358974
Test - acc:         0.852000 loss:        0.430075
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.880360 loss:        0.353035
Test - acc:         0.858500 loss:        0.421617
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.878320 loss:        0.357190
Test - acc:         0.825800 loss:        0.545334
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.878280 loss:        0.356549
Test - acc:         0.818900 loss:        0.574073
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.879300 loss:        0.351703
Test - acc:         0.850000 loss:        0.440696
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.877000 loss:        0.359416
Test - acc:         0.805300 loss:        0.623769
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.878240 loss:        0.358516
Test - acc:         0.827400 loss:        0.532736
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.879600 loss:        0.354731
Test - acc:         0.843100 loss:        0.487948
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.876900 loss:        0.359348
Test - acc:         0.806800 loss:        0.626340
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.880380 loss:        0.352432
Test - acc:         0.833800 loss:        0.510100
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.877940 loss:        0.354902
Test - acc:         0.829800 loss:        0.498694
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.876640 loss:        0.360526
Test - acc:         0.821400 loss:        0.541759
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.876440 loss:        0.359311
Test - acc:         0.825900 loss:        0.561633
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.878180 loss:        0.356320
Test - acc:         0.812000 loss:        0.569389
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.877900 loss:        0.354658
Test - acc:         0.797200 loss:        0.601245
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.881020 loss:        0.352637
Test - acc:         0.829100 loss:        0.517266
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.879680 loss:        0.351646
Test - acc:         0.803400 loss:        0.630951
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.878860 loss:        0.354103
Test - acc:         0.763700 loss:        0.794683
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.876820 loss:        0.355922
Test - acc:         0.793200 loss:        0.677792
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.880060 loss:        0.351388
Test - acc:         0.731900 loss:        0.808255
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.878680 loss:        0.356362
Test - acc:         0.828300 loss:        0.511167
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.878060 loss:        0.356330
Test - acc:         0.709900 loss:        1.021386
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.877460 loss:        0.356206
Test - acc:         0.826000 loss:        0.542756
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.881640 loss:        0.348925
Test - acc:         0.768300 loss:        0.701493
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.892320 loss:        0.316952
Test - acc:         0.841100 loss:        0.517243
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.889780 loss:        0.322479
Test - acc:         0.829400 loss:        0.511602
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.888660 loss:        0.323767
Test - acc:         0.809600 loss:        0.590451
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.885780 loss:        0.335585
Test - acc:         0.823700 loss:        0.561507
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.885520 loss:        0.333736
Test - acc:         0.822500 loss:        0.544689
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.885040 loss:        0.335341
Test - acc:         0.853200 loss:        0.442107
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.882940 loss:        0.336692
Test - acc:         0.832400 loss:        0.507156
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.885200 loss:        0.331375
Test - acc:         0.811800 loss:        0.565919
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.887660 loss:        0.328395
Test - acc:         0.759300 loss:        0.839197
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.886280 loss:        0.332888
Test - acc:         0.811200 loss:        0.569893
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.883220 loss:        0.339631
Test - acc:         0.865500 loss:        0.412350
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.886980 loss:        0.335296
Test - acc:         0.861700 loss:        0.413401
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.886040 loss:        0.333238
Test - acc:         0.846800 loss:        0.492465
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.886700 loss:        0.331871
Test - acc:         0.850000 loss:        0.462042
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.884540 loss:        0.334060
Test - acc:         0.831200 loss:        0.496463
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.882480 loss:        0.339178
Test - acc:         0.844500 loss:        0.466016
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.888720 loss:        0.329275
Test - acc:         0.833600 loss:        0.512884
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.884860 loss:        0.334534
Test - acc:         0.863300 loss:        0.411917
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.888100 loss:        0.328967
Test - acc:         0.808500 loss:        0.590617
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.885420 loss:        0.332973
Test - acc:         0.832600 loss:        0.540130
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.885240 loss:        0.335931
Test - acc:         0.843000 loss:        0.473707
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.888600 loss:        0.330833
Test - acc:         0.833200 loss:        0.538096
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.885320 loss:        0.332426
Test - acc:         0.816200 loss:        0.565244
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.885200 loss:        0.333007
Test - acc:         0.804100 loss:        0.627530
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.888240 loss:        0.329039
Test - acc:         0.847600 loss:        0.462171
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.885360 loss:        0.335581
Test - acc:         0.835200 loss:        0.517955
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.887720 loss:        0.328303
Test - acc:         0.839700 loss:        0.516256
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.885820 loss:        0.332388
Test - acc:         0.838800 loss:        0.495099
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.885760 loss:        0.333808
Test - acc:         0.823700 loss:        0.553021
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.885960 loss:        0.332211
Test - acc:         0.835200 loss:        0.525567
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.887180 loss:        0.332071
Test - acc:         0.847400 loss:        0.447193
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.884560 loss:        0.334527
Test - acc:         0.821400 loss:        0.588315
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.884280 loss:        0.332128
Test - acc:         0.809400 loss:        0.596219
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.886140 loss:        0.330552
Test - acc:         0.858400 loss:        0.443551
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.886280 loss:        0.330545
Test - acc:         0.826000 loss:        0.521970
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.884860 loss:        0.337009
Test - acc:         0.850400 loss:        0.455883
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.886000 loss:        0.335505
Test - acc:         0.844500 loss:        0.473563
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.886680 loss:        0.329722
Test - acc:         0.832800 loss:        0.522432
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.886440 loss:        0.334417
Test - acc:         0.824900 loss:        0.542911
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.886360 loss:        0.332678
Test - acc:         0.817300 loss:        0.553892
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.888240 loss:        0.327464
Test - acc:         0.847300 loss:        0.462099
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.885180 loss:        0.334511
Test - acc:         0.862300 loss:        0.418992
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.885600 loss:        0.332143
Test - acc:         0.834400 loss:        0.513825
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.886200 loss:        0.333413
Test - acc:         0.847600 loss:        0.458635
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.885440 loss:        0.335190
Test - acc:         0.818100 loss:        0.569663
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.885580 loss:        0.332241
Test - acc:         0.864400 loss:        0.402306
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.885900 loss:        0.331773
Test - acc:         0.829500 loss:        0.537456
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.884940 loss:        0.333697
Test - acc:         0.840100 loss:        0.499759
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.884780 loss:        0.337909
Test - acc:         0.758200 loss:        0.825989
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.885380 loss:        0.332288
Test - acc:         0.829500 loss:        0.523070
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.936080 loss:        0.193031
Test - acc:         0.920300 loss:        0.229602
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.948300 loss:        0.153158
Test - acc:         0.924500 loss:        0.221716
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.953620 loss:        0.134711
Test - acc:         0.927100 loss:        0.215881
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.957880 loss:        0.125041
Test - acc:         0.929200 loss:        0.213366
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.960060 loss:        0.117385
Test - acc:         0.927300 loss:        0.212666
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.964520 loss:        0.106713
Test - acc:         0.926500 loss:        0.214631
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.965780 loss:        0.099614
Test - acc:         0.926300 loss:        0.217825
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.967200 loss:        0.095094
Test - acc:         0.930400 loss:        0.218108
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.968820 loss:        0.090455
Test - acc:         0.930700 loss:        0.217431
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.971720 loss:        0.083800
Test - acc:         0.930600 loss:        0.215840
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.972400 loss:        0.080567
Test - acc:         0.929100 loss:        0.219288
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.974360 loss:        0.075256
Test - acc:         0.930400 loss:        0.219301
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.976180 loss:        0.070581
Test - acc:         0.931500 loss:        0.221003
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.977740 loss:        0.068258
Test - acc:         0.929400 loss:        0.230413
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.977040 loss:        0.068794
Test - acc:         0.932800 loss:        0.225211
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.977720 loss:        0.064604
Test - acc:         0.929300 loss:        0.230580
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.979860 loss:        0.061783
Test - acc:         0.926900 loss:        0.234575
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.978700 loss:        0.061965
Test - acc:         0.928600 loss:        0.235802
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.980380 loss:        0.059098
Test - acc:         0.929900 loss:        0.237661
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.981120 loss:        0.056555
Test - acc:         0.932500 loss:        0.235860
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.980520 loss:        0.057612
Test - acc:         0.930000 loss:        0.238569
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.980720 loss:        0.057760
Test - acc:         0.927700 loss:        0.244015
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.980620 loss:        0.056786
Test - acc:         0.924500 loss:        0.252047
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.982700 loss:        0.051423
Test - acc:         0.929800 loss:        0.255382
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.982640 loss:        0.051811
Test - acc:         0.926000 loss:        0.258290
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.981220 loss:        0.055113
Test - acc:         0.923200 loss:        0.264855
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.979800 loss:        0.058955
Test - acc:         0.924200 loss:        0.258487
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.981140 loss:        0.056053
Test - acc:         0.929200 loss:        0.241976
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.982860 loss:        0.051821
Test - acc:         0.930600 loss:        0.244192
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.982180 loss:        0.053238
Test - acc:         0.925600 loss:        0.255295
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.981180 loss:        0.055650
Test - acc:         0.923300 loss:        0.274286
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.980100 loss:        0.058811
Test - acc:         0.927100 loss:        0.256170
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.982160 loss:        0.054364
Test - acc:         0.924100 loss:        0.266520
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.980420 loss:        0.057335
Test - acc:         0.925400 loss:        0.266921
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.978860 loss:        0.061463
Test - acc:         0.925100 loss:        0.265037
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.979580 loss:        0.059402
Test - acc:         0.919300 loss:        0.286118
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.979140 loss:        0.060291
Test - acc:         0.927200 loss:        0.253781
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.980080 loss:        0.060268
Test - acc:         0.920700 loss:        0.282674
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.979700 loss:        0.059295
Test - acc:         0.918700 loss:        0.296766
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.058054
Test - acc:         0.926100 loss:        0.255175
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.979640 loss:        0.060960
Test - acc:         0.925600 loss:        0.251395
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.978320 loss:        0.062875
Test - acc:         0.923100 loss:        0.269618
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.977640 loss:        0.064736
Test - acc:         0.927500 loss:        0.253871
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.976740 loss:        0.066281
Test - acc:         0.926400 loss:        0.257693
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.980420 loss:        0.058647
Test - acc:         0.928000 loss:        0.250890
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.978280 loss:        0.062769
Test - acc:         0.917200 loss:        0.295622
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.978480 loss:        0.063247
Test - acc:         0.923800 loss:        0.271517
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.978940 loss:        0.063912
Test - acc:         0.922300 loss:        0.273935
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.978240 loss:        0.063505
Test - acc:         0.924600 loss:        0.262865
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.976660 loss:        0.066532
Test - acc:         0.921500 loss:        0.269263
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.982400 loss:        0.054410
Test - acc:         0.930900 loss:        0.244238
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.984520 loss:        0.047437
Test - acc:         0.928700 loss:        0.244911
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.984920 loss:        0.046283
Test - acc:         0.927200 loss:        0.258822
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.982980 loss:        0.052408
Test - acc:         0.930600 loss:        0.250417
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.984140 loss:        0.047613
Test - acc:         0.923700 loss:        0.269255
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.984440 loss:        0.046290
Test - acc:         0.925200 loss:        0.272486
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.984100 loss:        0.048917
Test - acc:         0.921300 loss:        0.275724
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.983100 loss:        0.050464
Test - acc:         0.923100 loss:        0.290089
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.983120 loss:        0.050375
Test - acc:         0.927100 loss:        0.262287
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.981680 loss:        0.055213
Test - acc:         0.922800 loss:        0.268117
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.983160 loss:        0.051354
Test - acc:         0.924800 loss:        0.272116
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.980860 loss:        0.055577
Test - acc:         0.922500 loss:        0.272256
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.981440 loss:        0.056464
Test - acc:         0.922400 loss:        0.274154
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.980500 loss:        0.058425
Test - acc:         0.922600 loss:        0.279661
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.981480 loss:        0.054609
Test - acc:         0.921900 loss:        0.294395
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.983180 loss:        0.051449
Test - acc:         0.919800 loss:        0.307492
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.981140 loss:        0.055880
Test - acc:         0.914400 loss:        0.313804
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.980780 loss:        0.057591
Test - acc:         0.926000 loss:        0.259928
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.981060 loss:        0.055738
Test - acc:         0.925100 loss:        0.267805
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.983760 loss:        0.050940
Test - acc:         0.930100 loss:        0.251349
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.055319
Test - acc:         0.924700 loss:        0.271017
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.980660 loss:        0.058231
Test - acc:         0.923200 loss:        0.273728
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.980480 loss:        0.057911
Test - acc:         0.924400 loss:        0.280326
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.979460 loss:        0.060828
Test - acc:         0.922500 loss:        0.276217
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.981760 loss:        0.055033
Test - acc:         0.923200 loss:        0.268732
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.982920 loss:        0.051788
Test - acc:         0.913800 loss:        0.308205
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.982160 loss:        0.054962
Test - acc:         0.923500 loss:        0.277479
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.982220 loss:        0.053962
Test - acc:         0.929900 loss:        0.250887
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.981540 loss:        0.054765
Test - acc:         0.923400 loss:        0.276270
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.979340 loss:        0.058885
Test - acc:         0.922900 loss:        0.280108
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.979560 loss:        0.061322
Test - acc:         0.924500 loss:        0.264719
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.980980 loss:        0.057056
Test - acc:         0.928500 loss:        0.256935
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.981540 loss:        0.055971
Test - acc:         0.924100 loss:        0.267452
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.981660 loss:        0.053904
Test - acc:         0.922000 loss:        0.276020
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.981400 loss:        0.056356
Test - acc:         0.925900 loss:        0.259941
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.981500 loss:        0.055958
Test - acc:         0.920700 loss:        0.275737
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.978920 loss:        0.062107
Test - acc:         0.918300 loss:        0.299194
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.981280 loss:        0.057427
Test - acc:         0.922900 loss:        0.266083
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.981480 loss:        0.054721
Test - acc:         0.919900 loss:        0.286417
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.981580 loss:        0.054884
Test - acc:         0.922000 loss:        0.276568
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.979940 loss:        0.060167
Test - acc:         0.922900 loss:        0.267864
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.059448
Test - acc:         0.924800 loss:        0.255489
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.978920 loss:        0.061608
Test - acc:         0.922500 loss:        0.275007
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.981400 loss:        0.055899
Test - acc:         0.920300 loss:        0.297938
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.979800 loss:        0.059567
Test - acc:         0.924300 loss:        0.272668
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.982120 loss:        0.053756
Test - acc:         0.922500 loss:        0.268604
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.982820 loss:        0.051436
Test - acc:         0.920900 loss:        0.289158
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.980020 loss:        0.058957
Test - acc:         0.919500 loss:        0.289795
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.979860 loss:        0.060501
Test - acc:         0.905900 loss:        0.351209
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.980740 loss:        0.057778
Test - acc:         0.922900 loss:        0.269115
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.985460 loss:        0.046281
Test - acc:         0.935300 loss:        0.218921
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.991480 loss:        0.031649
Test - acc:         0.939400 loss:        0.212873
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.991880 loss:        0.028590
Test - acc:         0.938700 loss:        0.215867
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.993100 loss:        0.025810
Test - acc:         0.939700 loss:        0.214464
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.994340 loss:        0.023371
Test - acc:         0.940300 loss:        0.212459
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.994940 loss:        0.020988
Test - acc:         0.939900 loss:        0.211768
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.994960 loss:        0.020010
Test - acc:         0.941100 loss:        0.212520
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.995620 loss:        0.019253
Test - acc:         0.941000 loss:        0.213218
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.995780 loss:        0.017802
Test - acc:         0.941400 loss:        0.213489
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.995800 loss:        0.018130
Test - acc:         0.940300 loss:        0.213344
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.996280 loss:        0.016252
Test - acc:         0.942300 loss:        0.214048
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.996400 loss:        0.016174
Test - acc:         0.941600 loss:        0.214763
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.996640 loss:        0.015051
Test - acc:         0.941600 loss:        0.215221
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.996780 loss:        0.015032
Test - acc:         0.942900 loss:        0.212493
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.996660 loss:        0.014797
Test - acc:         0.941600 loss:        0.214254
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.997200 loss:        0.014110
Test - acc:         0.941900 loss:        0.212850
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.997180 loss:        0.013294
Test - acc:         0.942000 loss:        0.214490
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.997460 loss:        0.013138
Test - acc:         0.941900 loss:        0.216025
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.997380 loss:        0.012661
Test - acc:         0.942000 loss:        0.214408
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.997540 loss:        0.012779
Test - acc:         0.942300 loss:        0.215456
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.997440 loss:        0.012596
Test - acc:         0.942300 loss:        0.214878
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.998020 loss:        0.011660
Test - acc:         0.942400 loss:        0.213681
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.998020 loss:        0.011253
Test - acc:         0.942500 loss:        0.214566
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.998000 loss:        0.011212
Test - acc:         0.942600 loss:        0.213954
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.997680 loss:        0.011669
Test - acc:         0.942300 loss:        0.215161
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.998100 loss:        0.011303
Test - acc:         0.941800 loss:        0.217057
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.997520 loss:        0.011565
Test - acc:         0.943900 loss:        0.212660
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.998100 loss:        0.010771
Test - acc:         0.943100 loss:        0.214674
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.998020 loss:        0.010652
Test - acc:         0.942600 loss:        0.216223
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.998440 loss:        0.009744
Test - acc:         0.942000 loss:        0.216566
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.998300 loss:        0.009948
Test - acc:         0.942500 loss:        0.216210
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.998320 loss:        0.009802
Test - acc:         0.942800 loss:        0.218277
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.998920 loss:        0.008668
Test - acc:         0.942500 loss:        0.215705
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.998260 loss:        0.009982
Test - acc:         0.943200 loss:        0.217839
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.998480 loss:        0.009565
Test - acc:         0.942600 loss:        0.215382
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.998420 loss:        0.009450
Test - acc:         0.942900 loss:        0.215278
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.998400 loss:        0.009170
Test - acc:         0.941100 loss:        0.218134
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.998180 loss:        0.009267
Test - acc:         0.943100 loss:        0.218618
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.998340 loss:        0.009423
Test - acc:         0.943800 loss:        0.218755
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.998520 loss:        0.008722
Test - acc:         0.943600 loss:        0.215606
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.998720 loss:        0.008568
Test - acc:         0.942200 loss:        0.216085
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.998680 loss:        0.008436
Test - acc:         0.942300 loss:        0.216936
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.998920 loss:        0.007950
Test - acc:         0.943700 loss:        0.217540
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.998620 loss:        0.008699
Test - acc:         0.943100 loss:        0.219243
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.008084
Test - acc:         0.942600 loss:        0.217752
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.998520 loss:        0.008381
Test - acc:         0.943300 loss:        0.217583
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.998580 loss:        0.008352
Test - acc:         0.943200 loss:        0.219029
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.998820 loss:        0.008211
Test - acc:         0.944000 loss:        0.221367
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.998320 loss:        0.008621
Test - acc:         0.944100 loss:        0.218978
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.998720 loss:        0.008029
Test - acc:         0.943600 loss:        0.216020
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.974980 loss:        0.075040
Test - acc:         0.928400 loss:        0.237536
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.982540 loss:        0.055544
Test - acc:         0.930700 loss:        0.229208
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.985560 loss:        0.048294
Test - acc:         0.932500 loss:        0.224445
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.986500 loss:        0.045211
Test - acc:         0.934500 loss:        0.224549
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.987600 loss:        0.042200
Test - acc:         0.933600 loss:        0.228669
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.987620 loss:        0.040633
Test - acc:         0.932600 loss:        0.228349
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.989420 loss:        0.036987
Test - acc:         0.933600 loss:        0.229488
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.989260 loss:        0.036969
Test - acc:         0.936400 loss:        0.227783
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.989580 loss:        0.035402
Test - acc:         0.935000 loss:        0.230394
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.991000 loss:        0.032657
Test - acc:         0.935800 loss:        0.227641
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.990840 loss:        0.032244
Test - acc:         0.937200 loss:        0.226381
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.990960 loss:        0.031558
Test - acc:         0.936100 loss:        0.227711
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.991360 loss:        0.030882
Test - acc:         0.936300 loss:        0.228711
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.992000 loss:        0.029793
Test - acc:         0.935900 loss:        0.230502
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.992400 loss:        0.028297
Test - acc:         0.937500 loss:        0.229902
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.992560 loss:        0.028378
Test - acc:         0.937000 loss:        0.231443
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.993180 loss:        0.025644
Test - acc:         0.936600 loss:        0.231598
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.993660 loss:        0.026045
Test - acc:         0.937000 loss:        0.232050
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.993780 loss:        0.025034
Test - acc:         0.937000 loss:        0.232386
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.994240 loss:        0.024435
Test - acc:         0.937400 loss:        0.229202
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.993880 loss:        0.023689
Test - acc:         0.937600 loss:        0.230927
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.993560 loss:        0.024375
Test - acc:         0.935200 loss:        0.236499
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.994560 loss:        0.022583
Test - acc:         0.937500 loss:        0.232476
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.994140 loss:        0.022876
Test - acc:         0.935900 loss:        0.234683
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.994460 loss:        0.021933
Test - acc:         0.935500 loss:        0.234469
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.994460 loss:        0.021982
Test - acc:         0.936400 loss:        0.235569
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.994160 loss:        0.022886
Test - acc:         0.936100 loss:        0.236906
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.994880 loss:        0.021414
Test - acc:         0.936400 loss:        0.238143
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.995020 loss:        0.020831
Test - acc:         0.936600 loss:        0.238836
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.994460 loss:        0.021604
Test - acc:         0.934400 loss:        0.242719
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.994960 loss:        0.020004
Test - acc:         0.936000 loss:        0.241034
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.995060 loss:        0.020083
Test - acc:         0.936800 loss:        0.237839
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.994560 loss:        0.020800
Test - acc:         0.934600 loss:        0.242016
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.994880 loss:        0.020468
Test - acc:         0.936100 loss:        0.240485
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.994940 loss:        0.019317
Test - acc:         0.935900 loss:        0.240795
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.995360 loss:        0.019054
Test - acc:         0.936800 loss:        0.237919
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.995440 loss:        0.018506
Test - acc:         0.936100 loss:        0.238538
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.995720 loss:        0.018724
Test - acc:         0.936400 loss:        0.239505
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.995620 loss:        0.018274
Test - acc:         0.937700 loss:        0.238623
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.995360 loss:        0.018242
Test - acc:         0.938500 loss:        0.239392
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.996300 loss:        0.017266
Test - acc:         0.938200 loss:        0.239758
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.996060 loss:        0.017135
Test - acc:         0.935200 loss:        0.240186
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.995720 loss:        0.018269
Test - acc:         0.938000 loss:        0.239759
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.995680 loss:        0.017127
Test - acc:         0.938300 loss:        0.239414
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.995820 loss:        0.017133
Test - acc:         0.938100 loss:        0.238780
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.995620 loss:        0.017085
Test - acc:         0.937800 loss:        0.239668
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.996460 loss:        0.016338
Test - acc:         0.938200 loss:        0.242738
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.995880 loss:        0.017180
Test - acc:         0.936700 loss:        0.243032
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.996340 loss:        0.016109
Test - acc:         0.937800 loss:        0.241439
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.996380 loss:        0.015767
Test - acc:         0.937600 loss:        0.238631
Sparsity :          0.9844
Wdecay :        0.000500
