Running --prune_bnorm --model resnet18 --noise --prune_criterion weight_div_flips --seed 44 --prune_freq 32 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=32_seed=44 --save_model=pre-finetune/resnet18_weight_div_flips_pf32_s44 --logdir=criterion_experiment_prune_bnorm/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": true,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_prune_bnorm/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf32_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11169152
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11169152
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.303340 loss:        2.045872
Test - acc:         0.449200 loss:        1.518474
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.486940 loss:        1.408749
Test - acc:         0.553600 loss:        1.216369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.595040 loss:        1.130263
Test - acc:         0.627400 loss:        1.061316
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.662820 loss:        0.954220
Test - acc:         0.666000 loss:        0.963036
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.710980 loss:        0.823103
Test - acc:         0.700400 loss:        0.900422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.754080 loss:        0.701555
Test - acc:         0.719300 loss:        0.843699
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.784640 loss:        0.626890
Test - acc:         0.762900 loss:        0.702303
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.800120 loss:        0.580976
Test - acc:         0.678300 loss:        1.056559
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.809040 loss:        0.553956
Test - acc:         0.750400 loss:        0.725537
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.817400 loss:        0.527065
Test - acc:         0.750100 loss:        0.776029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.828080 loss:        0.504549
Test - acc:         0.806100 loss:        0.560775
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.827040 loss:        0.498283
Test - acc:         0.787800 loss:        0.637411
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.835700 loss:        0.482435
Test - acc:         0.794000 loss:        0.615528
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838900 loss:        0.468737
Test - acc:         0.820400 loss:        0.537681
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844200 loss:        0.459359
Test - acc:         0.804800 loss:        0.583139
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845740 loss:        0.448696
Test - acc:         0.757400 loss:        0.755301
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.845840 loss:        0.444416
Test - acc:         0.784900 loss:        0.670693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.849560 loss:        0.437153
Test - acc:         0.836300 loss:        0.472858
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.852000 loss:        0.431503
Test - acc:         0.805300 loss:        0.564962
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.853780 loss:        0.428139
Test - acc:         0.822000 loss:        0.534250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.417603
Test - acc:         0.776100 loss:        0.709650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.857220 loss:        0.416609
Test - acc:         0.822800 loss:        0.534768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.418318
Test - acc:         0.835400 loss:        0.503948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.857700 loss:        0.416371
Test - acc:         0.797500 loss:        0.599861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.861780 loss:        0.406843
Test - acc:         0.789200 loss:        0.638827
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.858420 loss:        0.412091
Test - acc:         0.812200 loss:        0.574223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.861500 loss:        0.402603
Test - acc:         0.834200 loss:        0.487177
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.861100 loss:        0.403876
Test - acc:         0.788300 loss:        0.694953
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.862260 loss:        0.397948
Test - acc:         0.834000 loss:        0.496371
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.861600 loss:        0.404324
Test - acc:         0.767800 loss:        0.741355
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.863460 loss:        0.399084
Test - acc:         0.813800 loss:        0.548094
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.862380 loss:        0.399397
Test - acc:         0.821100 loss:        0.548047
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.877580 loss:        0.355628
Test - acc:         0.822800 loss:        0.566291
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.874980 loss:        0.364481
Test - acc:         0.809000 loss:        0.579537
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.873460 loss:        0.369397
Test - acc:         0.779300 loss:        0.701801
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.872700 loss:        0.373467
Test - acc:         0.830800 loss:        0.515757
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.873680 loss:        0.369815
Test - acc:         0.837500 loss:        0.492747
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.873480 loss:        0.370571
Test - acc:         0.855400 loss:        0.446039
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.872660 loss:        0.372917
Test - acc:         0.848600 loss:        0.467407
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.871040 loss:        0.375923
Test - acc:         0.847700 loss:        0.446145
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.871060 loss:        0.376509
Test - acc:         0.828500 loss:        0.521068
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.875420 loss:        0.366449
Test - acc:         0.841700 loss:        0.460231
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.875980 loss:        0.364344
Test - acc:         0.832600 loss:        0.508112
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.872540 loss:        0.371452
Test - acc:         0.808700 loss:        0.607496
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.874420 loss:        0.364327
Test - acc:         0.849100 loss:        0.454777
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.876620 loss:        0.361036
Test - acc:         0.835400 loss:        0.499938
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.873380 loss:        0.370522
Test - acc:         0.843500 loss:        0.476657
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.874720 loss:        0.365813
Test - acc:         0.764500 loss:        0.765666
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.875000 loss:        0.365610
Test - acc:         0.808600 loss:        0.582667
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.873440 loss:        0.365900
Test - acc:         0.854100 loss:        0.445437
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.876380 loss:        0.362245
Test - acc:         0.856300 loss:        0.436301
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.878560 loss:        0.359085
Test - acc:         0.833600 loss:        0.506940
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.873560 loss:        0.366494
Test - acc:         0.844700 loss:        0.466073
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.875800 loss:        0.362732
Test - acc:         0.848000 loss:        0.448229
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.876340 loss:        0.361359
Test - acc:         0.818900 loss:        0.542369
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.876960 loss:        0.364260
Test - acc:         0.832500 loss:        0.482162
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.874960 loss:        0.361397
Test - acc:         0.828400 loss:        0.509624
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.874040 loss:        0.364729
Test - acc:         0.821200 loss:        0.536164
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.874980 loss:        0.361554
Test - acc:         0.854700 loss:        0.416595
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.877280 loss:        0.359903
Test - acc:         0.825000 loss:        0.561927
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.877820 loss:        0.358684
Test - acc:         0.865800 loss:        0.407315
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.874280 loss:        0.366400
Test - acc:         0.850600 loss:        0.448771
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.876580 loss:        0.362576
Test - acc:         0.800400 loss:        0.647353
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.875900 loss:        0.359037
Test - acc:         0.839900 loss:        0.471263
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.888340 loss:        0.325728
Test - acc:         0.854700 loss:        0.438430
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.887200 loss:        0.328083
Test - acc:         0.834200 loss:        0.522114
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.886580 loss:        0.333052
Test - acc:         0.824800 loss:        0.548678
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.884080 loss:        0.339518
Test - acc:         0.836200 loss:        0.500507
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.882760 loss:        0.341799
Test - acc:         0.782300 loss:        0.699261
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.885840 loss:        0.333714
Test - acc:         0.812700 loss:        0.562325
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.883400 loss:        0.334974
Test - acc:         0.830800 loss:        0.506647
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.883460 loss:        0.337729
Test - acc:         0.814400 loss:        0.577665
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.883040 loss:        0.338716
Test - acc:         0.861600 loss:        0.423478
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.883660 loss:        0.340236
Test - acc:         0.844800 loss:        0.464989
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.882660 loss:        0.343560
Test - acc:         0.810100 loss:        0.574874
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.886940 loss:        0.329802
Test - acc:         0.836500 loss:        0.493380
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.884100 loss:        0.338694
Test - acc:         0.832500 loss:        0.503526
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.881000 loss:        0.345449
Test - acc:         0.781200 loss:        0.687895
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.884140 loss:        0.337129
Test - acc:         0.767900 loss:        0.694240
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.885800 loss:        0.333677
Test - acc:         0.829900 loss:        0.530009
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.884180 loss:        0.336716
Test - acc:         0.841200 loss:        0.473972
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.886320 loss:        0.337008
Test - acc:         0.804900 loss:        0.617953
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.884020 loss:        0.338739
Test - acc:         0.868200 loss:        0.389496
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.883220 loss:        0.342840
Test - acc:         0.857700 loss:        0.424835
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.885200 loss:        0.333443
Test - acc:         0.838400 loss:        0.482401
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.882920 loss:        0.340475
Test - acc:         0.845200 loss:        0.476198
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.885080 loss:        0.336274
Test - acc:         0.837100 loss:        0.481292
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.883560 loss:        0.339718
Test - acc:         0.857000 loss:        0.431497
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.883300 loss:        0.340904
Test - acc:         0.820600 loss:        0.562142
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.883280 loss:        0.340559
Test - acc:         0.844100 loss:        0.484813
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.886500 loss:        0.336937
Test - acc:         0.781400 loss:        0.697229
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.885680 loss:        0.337043
Test - acc:         0.843200 loss:        0.487397
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.885300 loss:        0.336255
Test - acc:         0.857700 loss:        0.420788
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.884500 loss:        0.337838
Test - acc:         0.846400 loss:        0.464999
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.884160 loss:        0.339997
Test - acc:         0.863500 loss:        0.400787
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.882420 loss:        0.341675
Test - acc:         0.836300 loss:        0.481242
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.898980 loss:        0.295300
Test - acc:         0.833800 loss:        0.504933
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.893060 loss:        0.309664
Test - acc:         0.792900 loss:        0.659095
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.892960 loss:        0.311509
Test - acc:         0.835400 loss:        0.507683
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.892860 loss:        0.315522
Test - acc:         0.828800 loss:        0.531014
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.892400 loss:        0.315543
Test - acc:         0.815800 loss:        0.575995
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.895340 loss:        0.309363
Test - acc:         0.862800 loss:        0.425214
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.892600 loss:        0.315319
Test - acc:         0.828300 loss:        0.540188
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.889540 loss:        0.322398
Test - acc:         0.828300 loss:        0.567089
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.893240 loss:        0.310370
Test - acc:         0.856300 loss:        0.439244
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.893560 loss:        0.309886
Test - acc:         0.863500 loss:        0.424603
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.889240 loss:        0.318533
Test - acc:         0.851900 loss:        0.451242
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.889640 loss:        0.317678
Test - acc:         0.850500 loss:        0.448196
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.891200 loss:        0.314175
Test - acc:         0.853900 loss:        0.447376
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.892040 loss:        0.313248
Test - acc:         0.858500 loss:        0.425783
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.891500 loss:        0.313799
Test - acc:         0.802300 loss:        0.607190
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.890740 loss:        0.319067
Test - acc:         0.825500 loss:        0.529057
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.888700 loss:        0.322704
Test - acc:         0.804200 loss:        0.595015
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.892820 loss:        0.313964
Test - acc:         0.830400 loss:        0.523178
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.892260 loss:        0.313159
Test - acc:         0.841800 loss:        0.480917
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.893420 loss:        0.313573
Test - acc:         0.847500 loss:        0.468051
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.892620 loss:        0.315196
Test - acc:         0.828300 loss:        0.525869
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.891200 loss:        0.317644
Test - acc:         0.845600 loss:        0.477477
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.893580 loss:        0.312379
Test - acc:         0.866200 loss:        0.395409
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.890420 loss:        0.313357
Test - acc:         0.857300 loss:        0.433848
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.890120 loss:        0.317889
Test - acc:         0.831300 loss:        0.516739
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.892060 loss:        0.317807
Test - acc:         0.825800 loss:        0.524753
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.890140 loss:        0.318345
Test - acc:         0.872500 loss:        0.382756
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.890060 loss:        0.321031
Test - acc:         0.799800 loss:        0.627448
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.893100 loss:        0.313918
Test - acc:         0.831800 loss:        0.517092
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.890500 loss:        0.318722
Test - acc:         0.845800 loss:        0.459864
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.892220 loss:        0.312713
Test - acc:         0.845400 loss:        0.471929
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.891440 loss:        0.314547
Test - acc:         0.865700 loss:        0.417267
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.905180 loss:        0.277574
Test - acc:         0.858100 loss:        0.440451
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.902060 loss:        0.287243
Test - acc:         0.834700 loss:        0.510448
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.898880 loss:        0.292419
Test - acc:         0.860000 loss:        0.428838
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.901080 loss:        0.286068
Test - acc:         0.865700 loss:        0.408122
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.899720 loss:        0.292935
Test - acc:         0.873600 loss:        0.368664
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.897560 loss:        0.296892
Test - acc:         0.861700 loss:        0.416402
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.897740 loss:        0.294945
Test - acc:         0.843400 loss:        0.503594
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.898080 loss:        0.292620
Test - acc:         0.859800 loss:        0.433765
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.899560 loss:        0.294206
Test - acc:         0.842200 loss:        0.499542
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.898680 loss:        0.292202
Test - acc:         0.853100 loss:        0.450578
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.900480 loss:        0.291131
Test - acc:         0.859900 loss:        0.434323
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.898520 loss:        0.293895
Test - acc:         0.828200 loss:        0.546170
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.898580 loss:        0.296880
Test - acc:         0.822500 loss:        0.561295
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.900600 loss:        0.289780
Test - acc:         0.853500 loss:        0.468118
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.897780 loss:        0.295509
Test - acc:         0.866200 loss:        0.386487
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.898400 loss:        0.295200
Test - acc:         0.873700 loss:        0.378248
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.898980 loss:        0.294073
Test - acc:         0.846900 loss:        0.465640
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.897720 loss:        0.299451
Test - acc:         0.820900 loss:        0.561184
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.897500 loss:        0.294355
Test - acc:         0.861100 loss:        0.426674
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.898180 loss:        0.292517
Test - acc:         0.858500 loss:        0.427543
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.897420 loss:        0.297269
Test - acc:         0.838200 loss:        0.507912
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.899060 loss:        0.293890
Test - acc:         0.834000 loss:        0.527832
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.938720 loss:        0.181681
Test - acc:         0.925100 loss:        0.224844
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.952560 loss:        0.142093
Test - acc:         0.926900 loss:        0.214766
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.959380 loss:        0.123265
Test - acc:         0.928800 loss:        0.211065
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.961560 loss:        0.114060
Test - acc:         0.929300 loss:        0.212255
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.964480 loss:        0.107785
Test - acc:         0.929300 loss:        0.214752
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.966540 loss:        0.098856
Test - acc:         0.931700 loss:        0.205937
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.969680 loss:        0.091073
Test - acc:         0.929700 loss:        0.210731
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.970660 loss:        0.086703
Test - acc:         0.933200 loss:        0.211120
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.972000 loss:        0.082683
Test - acc:         0.932700 loss:        0.214527
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.972800 loss:        0.079608
Test - acc:         0.933800 loss:        0.209320
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.973840 loss:        0.079387
Test - acc:         0.932700 loss:        0.211671
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.975720 loss:        0.074172
Test - acc:         0.933100 loss:        0.208679
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.976100 loss:        0.071774
Test - acc:         0.933800 loss:        0.207363
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.976820 loss:        0.069324
Test - acc:         0.930400 loss:        0.217318
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.977780 loss:        0.067872
Test - acc:         0.931400 loss:        0.217705
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.976600 loss:        0.067651
Test - acc:         0.933900 loss:        0.217654
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.977820 loss:        0.064255
Test - acc:         0.933400 loss:        0.217331
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.979640 loss:        0.060785
Test - acc:         0.931200 loss:        0.230687
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.980520 loss:        0.058017
Test - acc:         0.930000 loss:        0.245895
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.059986
Test - acc:         0.932300 loss:        0.227245
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.978920 loss:        0.061755
Test - acc:         0.932600 loss:        0.230063
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.980560 loss:        0.058071
Test - acc:         0.932000 loss:        0.229091
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.981100 loss:        0.056362
Test - acc:         0.930800 loss:        0.243358
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.980400 loss:        0.058595
Test - acc:         0.928900 loss:        0.238132
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.981300 loss:        0.057054
Test - acc:         0.931400 loss:        0.228464
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.978940 loss:        0.062210
Test - acc:         0.927300 loss:        0.251447
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.980200 loss:        0.059384
Test - acc:         0.932100 loss:        0.242959
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.980140 loss:        0.058979
Test - acc:         0.926200 loss:        0.248866
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.980780 loss:        0.057979
Test - acc:         0.929600 loss:        0.242015
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.979480 loss:        0.061851
Test - acc:         0.925600 loss:        0.256281
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.980480 loss:        0.059758
Test - acc:         0.926500 loss:        0.244587
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.979940 loss:        0.060205
Test - acc:         0.926600 loss:        0.259023
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.979040 loss:        0.060606
Test - acc:         0.924700 loss:        0.263156
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.980460 loss:        0.059140
Test - acc:         0.929300 loss:        0.250580
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.978620 loss:        0.063289
Test - acc:         0.924400 loss:        0.266332
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.978940 loss:        0.062928
Test - acc:         0.926000 loss:        0.259878
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.979380 loss:        0.061403
Test - acc:         0.925200 loss:        0.264273
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.978620 loss:        0.062512
Test - acc:         0.917900 loss:        0.279351
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.979140 loss:        0.062132
Test - acc:         0.926900 loss:        0.263169
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.977180 loss:        0.066826
Test - acc:         0.923400 loss:        0.265880
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.976980 loss:        0.066512
Test - acc:         0.927200 loss:        0.248398
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.977340 loss:        0.065897
Test - acc:         0.921800 loss:        0.260650
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.963700 loss:        0.103586
Test - acc:         0.914600 loss:        0.280885
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.969780 loss:        0.088171
Test - acc:         0.922100 loss:        0.252122
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.971200 loss:        0.084762
Test - acc:         0.920600 loss:        0.265044
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.972040 loss:        0.081255
Test - acc:         0.923900 loss:        0.252703
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.971720 loss:        0.081860
Test - acc:         0.920800 loss:        0.268923
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.973120 loss:        0.079058
Test - acc:         0.920500 loss:        0.258797
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.970980 loss:        0.084723
Test - acc:         0.923500 loss:        0.257528
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.973060 loss:        0.079077
Test - acc:         0.917100 loss:        0.286834
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.972300 loss:        0.078832
Test - acc:         0.923600 loss:        0.263236
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.972840 loss:        0.078621
Test - acc:         0.920100 loss:        0.279417
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.972560 loss:        0.080017
Test - acc:         0.924500 loss:        0.266284
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.972860 loss:        0.078183
Test - acc:         0.921200 loss:        0.267408
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.973860 loss:        0.077441
Test - acc:         0.919900 loss:        0.272756
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.974220 loss:        0.076285
Test - acc:         0.917500 loss:        0.276190
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.971820 loss:        0.081325
Test - acc:         0.916800 loss:        0.288439
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.972800 loss:        0.079110
Test - acc:         0.921300 loss:        0.262067
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.972260 loss:        0.079415
Test - acc:         0.921700 loss:        0.264139
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.972640 loss:        0.080176
Test - acc:         0.923400 loss:        0.261445
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.973560 loss:        0.076546
Test - acc:         0.920100 loss:        0.274304
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.972000 loss:        0.079670
Test - acc:         0.920600 loss:        0.267497
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.972660 loss:        0.077812
Test - acc:         0.921000 loss:        0.275222
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.971440 loss:        0.080001
Test - acc:         0.918000 loss:        0.273746
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.972620 loss:        0.078954
Test - acc:         0.917900 loss:        0.284349
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.972600 loss:        0.079977
Test - acc:         0.920300 loss:        0.270390
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.975320 loss:        0.075108
Test - acc:         0.922000 loss:        0.273690
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.973660 loss:        0.077112
Test - acc:         0.922000 loss:        0.270504
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.973780 loss:        0.077063
Test - acc:         0.920800 loss:        0.267483
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.975300 loss:        0.071669
Test - acc:         0.915100 loss:        0.294567
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.974720 loss:        0.075400
Test - acc:         0.922200 loss:        0.261565
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.972280 loss:        0.079530
Test - acc:         0.922700 loss:        0.261230
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.973020 loss:        0.079352
Test - acc:         0.923900 loss:        0.261462
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.972700 loss:        0.081230
Test - acc:         0.921900 loss:        0.270260
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.936600 loss:        0.181872
Test - acc:         0.902600 loss:        0.296529
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.944760 loss:        0.158179
Test - acc:         0.911300 loss:        0.288003
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.948440 loss:        0.148271
Test - acc:         0.907600 loss:        0.296419
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.948920 loss:        0.144552
Test - acc:         0.906100 loss:        0.304291
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.954020 loss:        0.135792
Test - acc:         0.914100 loss:        0.275194
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.952560 loss:        0.137030
Test - acc:         0.906900 loss:        0.302565
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.952880 loss:        0.133264
Test - acc:         0.907900 loss:        0.286247
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.954140 loss:        0.131781
Test - acc:         0.900800 loss:        0.317590
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.953520 loss:        0.132963
Test - acc:         0.915000 loss:        0.280978
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.954520 loss:        0.129419
Test - acc:         0.909000 loss:        0.300435
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.955700 loss:        0.127459
Test - acc:         0.910000 loss:        0.288413
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.956620 loss:        0.124439
Test - acc:         0.910100 loss:        0.288526
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.956560 loss:        0.124983
Test - acc:         0.910000 loss:        0.281526
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.956340 loss:        0.125260
Test - acc:         0.912000 loss:        0.283715
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.958880 loss:        0.120309
Test - acc:         0.912000 loss:        0.281111
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.958820 loss:        0.119740
Test - acc:         0.911200 loss:        0.283403
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.958240 loss:        0.121151
Test - acc:         0.913100 loss:        0.283617
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.958440 loss:        0.121374
Test - acc:         0.909500 loss:        0.297663
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.956620 loss:        0.123618
Test - acc:         0.912500 loss:        0.276532
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.957600 loss:        0.121004
Test - acc:         0.906700 loss:        0.303327
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.957360 loss:        0.121658
Test - acc:         0.913700 loss:        0.272851
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.958480 loss:        0.118738
Test - acc:         0.911100 loss:        0.286270
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.955980 loss:        0.122293
Test - acc:         0.906900 loss:        0.293472
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.959500 loss:        0.116276
Test - acc:         0.913500 loss:        0.285285
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.958920 loss:        0.116865
Test - acc:         0.910400 loss:        0.295271
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.958840 loss:        0.118516
Test - acc:         0.918600 loss:        0.269866
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.971000 loss:        0.087272
Test - acc:         0.926500 loss:        0.230078
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.975660 loss:        0.074148
Test - acc:         0.927700 loss:        0.228577
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.976480 loss:        0.071819
Test - acc:         0.928700 loss:        0.226834
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.978260 loss:        0.068159
Test - acc:         0.929100 loss:        0.228120
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.980140 loss:        0.064802
Test - acc:         0.929200 loss:        0.226733
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.980100 loss:        0.063529
Test - acc:         0.929500 loss:        0.227834
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.884320 loss:        0.335247
Test - acc:         0.883100 loss:        0.350133
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.909780 loss:        0.259215
Test - acc:         0.892800 loss:        0.325660
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.917540 loss:        0.237558
Test - acc:         0.896000 loss:        0.318071
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.923180 loss:        0.223113
Test - acc:         0.897500 loss:        0.309188
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.925600 loss:        0.214729
Test - acc:         0.901200 loss:        0.301873
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.926900 loss:        0.209016
Test - acc:         0.903900 loss:        0.297485
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.929400 loss:        0.202357
Test - acc:         0.903100 loss:        0.295553
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.932320 loss:        0.195502
Test - acc:         0.905100 loss:        0.294031
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.933560 loss:        0.190955
Test - acc:         0.905200 loss:        0.290650
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.933720 loss:        0.189591
Test - acc:         0.906900 loss:        0.286158
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.935360 loss:        0.185140
Test - acc:         0.907000 loss:        0.287881
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.937120 loss:        0.181629
Test - acc:         0.908100 loss:        0.289527
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.938880 loss:        0.178318
Test - acc:         0.907600 loss:        0.282085
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.939440 loss:        0.175015
Test - acc:         0.909400 loss:        0.282212
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.939000 loss:        0.174552
Test - acc:         0.910300 loss:        0.287700
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.942340 loss:        0.169361
Test - acc:         0.910600 loss:        0.280612
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.939860 loss:        0.171631
Test - acc:         0.911500 loss:        0.277245
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.941300 loss:        0.167835
Test - acc:         0.909900 loss:        0.278190
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.943400 loss:        0.165622
Test - acc:         0.910100 loss:        0.279903
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.942440 loss:        0.163889
Test - acc:         0.908200 loss:        0.280150
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.946000 loss:        0.161328
Test - acc:         0.910200 loss:        0.280406
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.943560 loss:        0.163723
Test - acc:         0.911300 loss:        0.278974
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.942900 loss:        0.161657
Test - acc:         0.909000 loss:        0.277719
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.945260 loss:        0.156141
Test - acc:         0.909600 loss:        0.278067
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.946180 loss:        0.155400
Test - acc:         0.911300 loss:        0.276682
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.946560 loss:        0.155270
Test - acc:         0.910300 loss:        0.275732
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.944520 loss:        0.156498
Test - acc:         0.911300 loss:        0.279180
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.946920 loss:        0.156007
Test - acc:         0.911700 loss:        0.276134
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.945940 loss:        0.156956
Test - acc:         0.910600 loss:        0.273182
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.947000 loss:        0.153236
Test - acc:         0.913200 loss:        0.274492
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.947920 loss:        0.150389
Test - acc:         0.910500 loss:        0.277255
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.948380 loss:        0.151168
Test - acc:         0.911900 loss:        0.276163
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.735580 loss:        0.772144
Test - acc:         0.789300 loss:        0.600970
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.800240 loss:        0.585171
Test - acc:         0.810600 loss:        0.540675
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.818380 loss:        0.534025
Test - acc:         0.824100 loss:        0.508571
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.828680 loss:        0.506896
Test - acc:         0.832800 loss:        0.490939
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.834380 loss:        0.485942
Test - acc:         0.836100 loss:        0.477713
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.840580 loss:        0.470124
Test - acc:         0.836600 loss:        0.465814
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.843140 loss:        0.457928
Test - acc:         0.843300 loss:        0.459820
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.845660 loss:        0.446475
Test - acc:         0.841400 loss:        0.448905
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.848140 loss:        0.440098
Test - acc:         0.847200 loss:        0.448540
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.851400 loss:        0.432701
Test - acc:         0.848000 loss:        0.443035
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.853040 loss:        0.427005
Test - acc:         0.844900 loss:        0.445622
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.856160 loss:        0.421850
Test - acc:         0.848700 loss:        0.438767
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.857040 loss:        0.415722
Test - acc:         0.850700 loss:        0.432096
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.858140 loss:        0.415244
Test - acc:         0.851200 loss:        0.426507
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.859680 loss:        0.410084
Test - acc:         0.851400 loss:        0.429158
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.858920 loss:        0.408616
Test - acc:         0.854300 loss:        0.427963
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.862360 loss:        0.402868
Test - acc:         0.850800 loss:        0.429086
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.862160 loss:        0.399580
Test - acc:         0.854700 loss:        0.423100
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.862760 loss:        0.400201
Test - acc:         0.851900 loss:        0.424547
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.864040 loss:        0.396091
Test - acc:         0.855500 loss:        0.420144
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.865060 loss:        0.392005
Test - acc:         0.858000 loss:        0.419305
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.864340 loss:        0.391131
Test - acc:         0.857400 loss:        0.415820
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.868040 loss:        0.385640
Test - acc:         0.856700 loss:        0.414929
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.866740 loss:        0.389312
Test - acc:         0.857900 loss:        0.409908
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.868800 loss:        0.384207
Test - acc:         0.856200 loss:        0.414671
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.868260 loss:        0.383494
Test - acc:         0.857000 loss:        0.407757
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.867780 loss:        0.381325
Test - acc:         0.859200 loss:        0.411100
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.869140 loss:        0.382468
Test - acc:         0.857500 loss:        0.412879
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.869980 loss:        0.377624
Test - acc:         0.859000 loss:        0.408144
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.870120 loss:        0.378132
Test - acc:         0.856200 loss:        0.407371
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.870500 loss:        0.376218
Test - acc:         0.859200 loss:        0.406810
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.871240 loss:        0.375685
Test - acc:         0.857300 loss:        0.407523
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.552380 loss:        1.261581
Test - acc:         0.635300 loss:        1.031558
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.644980 loss:        1.019517
Test - acc:         0.675700 loss:        0.927569
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.673000 loss:        0.945937
Test - acc:         0.690200 loss:        0.878460
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.686560 loss:        0.903592
Test - acc:         0.696900 loss:        0.853058
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.698320 loss:        0.874174
Test - acc:         0.710200 loss:        0.823355
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.703400 loss:        0.855110
Test - acc:         0.716400 loss:        0.812321
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.709400 loss:        0.836496
Test - acc:         0.725400 loss:        0.788447
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.713940 loss:        0.827350
Test - acc:         0.722000 loss:        0.779387
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.717800 loss:        0.813378
Test - acc:         0.731700 loss:        0.772252
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.721420 loss:        0.801186
Test - acc:         0.729700 loss:        0.769550
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.725660 loss:        0.794136
Test - acc:         0.731600 loss:        0.767807
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.729560 loss:        0.780548
Test - acc:         0.733600 loss:        0.755564
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.729020 loss:        0.781525
Test - acc:         0.740600 loss:        0.745247
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.734480 loss:        0.769957
Test - acc:         0.734000 loss:        0.745243
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.732520 loss:        0.772814
Test - acc:         0.739400 loss:        0.742962
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.736320 loss:        0.762342
Test - acc:         0.736900 loss:        0.747615
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.736460 loss:        0.759231
Test - acc:         0.744700 loss:        0.731979
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.739340 loss:        0.753474
Test - acc:         0.747000 loss:        0.729079
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.741560 loss:        0.748983
Test - acc:         0.746900 loss:        0.722376
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.743180 loss:        0.743324
Test - acc:         0.749900 loss:        0.726138
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.744960 loss:        0.741572
Test - acc:         0.749600 loss:        0.723847
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.745360 loss:        0.734485
Test - acc:         0.749700 loss:        0.710902
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.746200 loss:        0.731032
Test - acc:         0.752800 loss:        0.708297
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.746280 loss:        0.731423
Test - acc:         0.752000 loss:        0.706708
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.751160 loss:        0.724491
Test - acc:         0.751600 loss:        0.715315
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.748420 loss:        0.726793
Test - acc:         0.756000 loss:        0.699736
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.750720 loss:        0.723849
Test - acc:         0.757200 loss:        0.699153
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.751980 loss:        0.720500
Test - acc:         0.755600 loss:        0.705570
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.753200 loss:        0.714829
Test - acc:         0.754200 loss:        0.707416
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.754000 loss:        0.714971
Test - acc:         0.750700 loss:        0.717558
Sparsity :          0.9990
Wdecay :        0.000500
