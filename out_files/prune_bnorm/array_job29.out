Running --prune_bnorm --model resnet18 --noise --prune_criterion weight_div_flips --seed 44 --prune_freq 32 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=32_seed=44 --save_model=pre-finetune/resnet18_weight_div_flips_pf32_s44 --logdir=criterion_experiment_prune_bnorm/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": true,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_prune_bnorm/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf32_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11169152
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11169152
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.303340 loss:        2.045872
Test - acc:         0.449200 loss:        1.518474
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.486940 loss:        1.408749
Test - acc:         0.553600 loss:        1.216369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.595040 loss:        1.130263
Test - acc:         0.627400 loss:        1.061316
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.662820 loss:        0.954220
Test - acc:         0.666000 loss:        0.963036
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.710980 loss:        0.823103
Test - acc:         0.700400 loss:        0.900422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.754080 loss:        0.701555
Test - acc:         0.719300 loss:        0.843699
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.784640 loss:        0.626890
Test - acc:         0.762900 loss:        0.702303
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.800120 loss:        0.580976
Test - acc:         0.678300 loss:        1.056559
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.809040 loss:        0.553956
Test - acc:         0.750400 loss:        0.725537
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.817400 loss:        0.527065
Test - acc:         0.750100 loss:        0.776029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.828080 loss:        0.504549
Test - acc:         0.806100 loss:        0.560775
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.827040 loss:        0.498283
Test - acc:         0.787800 loss:        0.637411
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.835700 loss:        0.482435
Test - acc:         0.794000 loss:        0.615528
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838900 loss:        0.468737
Test - acc:         0.820400 loss:        0.537681
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844200 loss:        0.459359
Test - acc:         0.804800 loss:        0.583139
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845740 loss:        0.448696
Test - acc:         0.757400 loss:        0.755301
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.845840 loss:        0.444416
Test - acc:         0.784900 loss:        0.670693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.849560 loss:        0.437153
Test - acc:         0.836300 loss:        0.472858
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.852000 loss:        0.431503
Test - acc:         0.805300 loss:        0.564962
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.853780 loss:        0.428139
Test - acc:         0.822000 loss:        0.534250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.417603
Test - acc:         0.776100 loss:        0.709650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.857220 loss:        0.416609
Test - acc:         0.822800 loss:        0.534768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.418318
Test - acc:         0.835400 loss:        0.503948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.857700 loss:        0.416371
Test - acc:         0.797500 loss:        0.599861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.861780 loss:        0.406843
Test - acc:         0.789200 loss:        0.638827
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.858420 loss:        0.412091
Test - acc:         0.812200 loss:        0.574223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.861500 loss:        0.402603
Test - acc:         0.834200 loss:        0.487177
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.861100 loss:        0.403876
Test - acc:         0.788300 loss:        0.694953
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.862260 loss:        0.397948
Test - acc:         0.834000 loss:        0.496371
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.861600 loss:        0.404324
Test - acc:         0.767800 loss:        0.741355
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.863460 loss:        0.399084
Test - acc:         0.813800 loss:        0.548094
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.862380 loss:        0.399397
Test - acc:         0.821100 loss:        0.548047
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.877580 loss:        0.355628
Test - acc:         0.822800 loss:        0.566291
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.874980 loss:        0.364481
Test - acc:         0.809000 loss:        0.579537
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.873460 loss:        0.369397
Test - acc:         0.779300 loss:        0.701801
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.872700 loss:        0.373467
Test - acc:         0.830800 loss:        0.515757
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.873680 loss:        0.369815
Test - acc:         0.837500 loss:        0.492747
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.873480 loss:        0.370571
Test - acc:         0.855400 loss:        0.446039
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.872660 loss:        0.372917
Test - acc:         0.848600 loss:        0.467407
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.871040 loss:        0.375923
Test - acc:         0.847700 loss:        0.446145
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.871060 loss:        0.376509
Test - acc:         0.828500 loss:        0.521068
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.875420 loss:        0.366449
Test - acc:         0.841700 loss:        0.460231
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.875980 loss:        0.364344
Test - acc:         0.832600 loss:        0.508112
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.872540 loss:        0.371452
Test - acc:         0.808700 loss:        0.607496
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.874420 loss:        0.364327
Test - acc:         0.849100 loss:        0.454777
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.876620 loss:        0.361036
Test - acc:         0.835400 loss:        0.499938
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.873380 loss:        0.370522
Test - acc:         0.843500 loss:        0.476657
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.874720 loss:        0.365813
Test - acc:         0.764500 loss:        0.765666
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.875000 loss:        0.365610
Test - acc:         0.808600 loss:        0.582667
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.873440 loss:        0.365900
Test - acc:         0.854100 loss:        0.445437
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.876380 loss:        0.362245
Test - acc:         0.856300 loss:        0.436301
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.878560 loss:        0.359085
Test - acc:         0.833600 loss:        0.506940
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.873560 loss:        0.366494
Test - acc:         0.844700 loss:        0.466073
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.875800 loss:        0.362732
Test - acc:         0.848000 loss:        0.448229
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.876340 loss:        0.361359
Test - acc:         0.818900 loss:        0.542369
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.876960 loss:        0.364260
Test - acc:         0.832500 loss:        0.482162
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.874960 loss:        0.361397
Test - acc:         0.828400 loss:        0.509624
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.874040 loss:        0.364729
Test - acc:         0.821200 loss:        0.536164
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.874980 loss:        0.361554
Test - acc:         0.854700 loss:        0.416595
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.877280 loss:        0.359903
Test - acc:         0.825000 loss:        0.561927
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.877820 loss:        0.358684
Test - acc:         0.865800 loss:        0.407315
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.874280 loss:        0.366400
Test - acc:         0.850600 loss:        0.448771
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.876580 loss:        0.362576
Test - acc:         0.800400 loss:        0.647353
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.875900 loss:        0.359037
Test - acc:         0.839900 loss:        0.471263
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.888340 loss:        0.325728
Test - acc:         0.854700 loss:        0.438430
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.887200 loss:        0.328083
Test - acc:         0.834200 loss:        0.522114
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.886580 loss:        0.333052
Test - acc:         0.824800 loss:        0.548678
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.884080 loss:        0.339518
Test - acc:         0.836200 loss:        0.500507
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.882760 loss:        0.341799
Test - acc:         0.782300 loss:        0.699261
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.885840 loss:        0.333714
Test - acc:         0.812700 loss:        0.562325
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.883400 loss:        0.334974
Test - acc:         0.830800 loss:        0.506647
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.883460 loss:        0.337729
Test - acc:         0.814400 loss:        0.577665
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.883040 loss:        0.338716
Test - acc:         0.861600 loss:        0.423478
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.883660 loss:        0.340236
Test - acc:         0.844800 loss:        0.464989
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.882660 loss:        0.343560
Test - acc:         0.810100 loss:        0.574874
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.886940 loss:        0.329802
Test - acc:         0.836500 loss:        0.493380
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.884100 loss:        0.338694
Test - acc:         0.832500 loss:        0.503526
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.881000 loss:        0.345449
Test - acc:         0.781200 loss:        0.687895
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.884140 loss:        0.337129
Test - acc:         0.767900 loss:        0.694240
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.885800 loss:        0.333677
Test - acc:         0.829900 loss:        0.530009
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.884180 loss:        0.336716
Test - acc:         0.841200 loss:        0.473972
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.886320 loss:        0.337008
Test - acc:         0.804900 loss:        0.617953
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.884020 loss:        0.338739
Test - acc:         0.868200 loss:        0.389496
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.883220 loss:        0.342840
Test - acc:         0.857700 loss:        0.424835
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.885200 loss:        0.333443
Test - acc:         0.838400 loss:        0.482401
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.882920 loss:        0.340475
Test - acc:         0.845200 loss:        0.476198
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.885080 loss:        0.336274
Test - acc:         0.837100 loss:        0.481292
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.883560 loss:        0.339718
Test - acc:         0.857000 loss:        0.431497
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.883300 loss:        0.340904
Test - acc:         0.820600 loss:        0.562142
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.883280 loss:        0.340559
Test - acc:         0.844100 loss:        0.484813
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.886500 loss:        0.336937
Test - acc:         0.781400 loss:        0.697229
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.885680 loss:        0.337043
Test - acc:         0.843200 loss:        0.487397
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.885300 loss:        0.336255
Test - acc:         0.857700 loss:        0.420788
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.884500 loss:        0.337838
Test - acc:         0.846400 loss:        0.464999
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.884160 loss:        0.339997
Test - acc:         0.863500 loss:        0.400787
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.882420 loss:        0.341675
Test - acc:         0.836300 loss:        0.481242
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.898980 loss:        0.295300
Test - acc:         0.833800 loss:        0.504933
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.893060 loss:        0.309664
Test - acc:         0.792900 loss:        0.659095
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.892960 loss:        0.311509
Test - acc:         0.835400 loss:        0.507683
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.892860 loss:        0.315522
Test - acc:         0.828800 loss:        0.531014
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.892400 loss:        0.315543
Test - acc:         0.815800 loss:        0.575995
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.895340 loss:        0.309363
Test - acc:         0.862800 loss:        0.425214
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.892600 loss:        0.315319
Test - acc:         0.828300 loss:        0.540188
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.889540 loss:        0.322398
Test - acc:         0.828300 loss:        0.567089
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.893240 loss:        0.310370
Test - acc:         0.856300 loss:        0.439244
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.893560 loss:        0.309886
Test - acc:         0.863500 loss:        0.424603
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.889240 loss:        0.318533
Test - acc:         0.851900 loss:        0.451242
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.889640 loss:        0.317678
Test - acc:         0.850500 loss:        0.448196
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.891200 loss:        0.314175
Test - acc:         0.853900 loss:        0.447376
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.892040 loss:        0.313248
Test - acc:         0.858500 loss:        0.425783
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.891500 loss:        0.313799
Test - acc:         0.802300 loss:        0.607190
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.890740 loss:        0.319067
Test - acc:         0.825500 loss:        0.529057
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.888700 loss:        0.322704
Test - acc:         0.804200 loss:        0.595015
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.892820 loss:        0.313964
Test - acc:         0.830400 loss:        0.523178
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.892260 loss:        0.313159
Test - acc:         0.841800 loss:        0.480917
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.893420 loss:        0.313573
Test - acc:         0.847500 loss:        0.468051
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.892620 loss:        0.315196
Test - acc:         0.828300 loss:        0.525869
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.891200 loss:        0.317644
Test - acc:         0.845600 loss:        0.477477
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.893580 loss:        0.312379
Test - acc:         0.866200 loss:        0.395409
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.890420 loss:        0.313357
Test - acc:         0.857300 loss:        0.433848
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.890120 loss:        0.317889
Test - acc:         0.831300 loss:        0.516739
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.892060 loss:        0.317807
Test - acc:         0.825800 loss:        0.524753
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.890140 loss:        0.318345
Test - acc:         0.872500 loss:        0.382756
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.890060 loss:        0.321031
Test - acc:         0.799800 loss:        0.627448
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.893100 loss:        0.313918
Test - acc:         0.831800 loss:        0.517092
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.890500 loss:        0.318722
Test - acc:         0.845800 loss:        0.459864
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.892220 loss:        0.312713
Test - acc:         0.845400 loss:        0.471929
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.891440 loss:        0.314547
Test - acc:         0.865700 loss:        0.417267
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.905180 loss:        0.277574
Test - acc:         0.858100 loss:        0.440451
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.902060 loss:        0.287243
Test - acc:         0.834700 loss:        0.510448
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.898880 loss:        0.292419
Test - acc:         0.860000 loss:        0.428838
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.901080 loss:        0.286068
Test - acc:         0.865700 loss:        0.408122
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.899720 loss:        0.292935
Test - acc:         0.873600 loss:        0.368664
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.897560 loss:        0.296892
Test - acc:         0.861700 loss:        0.416402
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.897740 loss:        0.294945
Test - acc:         0.843400 loss:        0.503594
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.898080 loss:        0.292620
Test - acc:         0.859800 loss:        0.433765
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.899560 loss:        0.294206
Test - acc:         0.842200 loss:        0.499542
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.898680 loss:        0.292202
Test - acc:         0.853100 loss:        0.450578
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.900480 loss:        0.291131
Test - acc:         0.859900 loss:        0.434323
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.898520 loss:        0.293895
Test - acc:         0.828200 loss:        0.546170
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.898580 loss:        0.296880
Test - acc:         0.822500 loss:        0.561295
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.900600 loss:        0.289780
Test - acc:         0.853500 loss:        0.468118
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.897780 loss:        0.295509
Test - acc:         0.866200 loss:        0.386487
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.898400 loss:        0.295200
Test - acc:         0.873700 loss:        0.378248
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.898980 loss:        0.294073
Test - acc:         0.846900 loss:        0.465640
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.897720 loss:        0.299451
Test - acc:         0.820900 loss:        0.561184
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.897500 loss:        0.294355
Test - acc:         0.861100 loss:        0.426674
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.898180 loss:        0.292517
Test - acc:         0.858500 loss:        0.427543
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.897420 loss:        0.297269
Test - acc:         0.838200 loss:        0.507912
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.899060 loss:        0.293890
Test - acc:         0.834000 loss:        0.527832
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.938720 loss:        0.181681
Test - acc:         0.925100 loss:        0.224844
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.952560 loss:        0.142093
Test - acc:         0.926900 loss:        0.214766
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.959380 loss:        0.123265
Test - acc:         0.928800 loss:        0.211065
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.961560 loss:        0.114060
Test - acc:         0.929300 loss:        0.212255
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.964480 loss:        0.107785
Test - acc:         0.929300 loss:        0.214752
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.966540 loss:        0.098856
Test - acc:         0.931700 loss:        0.205937
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.969680 loss:        0.091073
Test - acc:         0.929700 loss:        0.210731
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.970660 loss:        0.086703
Test - acc:         0.933200 loss:        0.211120
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.972000 loss:        0.082683
Test - acc:         0.932700 loss:        0.214527
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.972800 loss:        0.079608
Test - acc:         0.933800 loss:        0.209320
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.973840 loss:        0.079387
Test - acc:         0.932700 loss:        0.211671
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.975720 loss:        0.074172
Test - acc:         0.933100 loss:        0.208679
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.976100 loss:        0.071774
Test - acc:         0.933800 loss:        0.207363
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.976820 loss:        0.069324
Test - acc:         0.930400 loss:        0.217318
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.977780 loss:        0.067872
Test - acc:         0.931400 loss:        0.217705
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.976600 loss:        0.067651
Test - acc:         0.933900 loss:        0.217654
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.977820 loss:        0.064255
Test - acc:         0.933400 loss:        0.217331
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.979640 loss:        0.060785
Test - acc:         0.931200 loss:        0.230687
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.980520 loss:        0.058017
Test - acc:         0.930000 loss:        0.245895
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.059986
Test - acc:         0.932300 loss:        0.227245
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.978920 loss:        0.061755
Test - acc:         0.932600 loss:        0.230063
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.980560 loss:        0.058071
Test - acc:         0.932000 loss:        0.229091
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.981100 loss:        0.056362
Test - acc:         0.930800 loss:        0.243358
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.980400 loss:        0.058595
Test - acc:         0.928900 loss:        0.238132
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.981300 loss:        0.057054
Test - acc:         0.931400 loss:        0.228464
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.978940 loss:        0.062210
Test - acc:         0.927300 loss:        0.251447
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.980200 loss:        0.059384
Test - acc:         0.932100 loss:        0.242959
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.980140 loss:        0.058979
Test - acc:         0.926200 loss:        0.248866
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.980780 loss:        0.057979
Test - acc:         0.929600 loss:        0.242015
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.979480 loss:        0.061851
Test - acc:         0.925600 loss:        0.256281
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.980480 loss:        0.059758
Test - acc:         0.926500 loss:        0.244587
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.979940 loss:        0.060205
Test - acc:         0.926600 loss:        0.259023
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.979040 loss:        0.060606
Test - acc:         0.924700 loss:        0.263156
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.980460 loss:        0.059140
Test - acc:         0.929300 loss:        0.250580
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.978620 loss:        0.063289
Test - acc:         0.924400 loss:        0.266332
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.978940 loss:        0.062928
Test - acc:         0.926000 loss:        0.259878
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.979380 loss:        0.061403
Test - acc:         0.925200 loss:        0.264273
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.978620 loss:        0.062512
Test - acc:         0.917900 loss:        0.279351
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.979140 loss:        0.062132
Test - acc:         0.926900 loss:        0.263169
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.977180 loss:        0.066826
Test - acc:         0.923400 loss:        0.265880
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.976980 loss:        0.066512
Test - acc:         0.927200 loss:        0.248398
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.977340 loss:        0.065897
Test - acc:         0.921800 loss:        0.260650
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.963700 loss:        0.103586
Test - acc:         0.914600 loss:        0.280885
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.969780 loss:        0.088171
Test - acc:         0.922100 loss:        0.252122
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.971200 loss:        0.084762
Test - acc:         0.920600 loss:        0.265044
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.972040 loss:        0.081255
Test - acc:         0.923900 loss:        0.252703
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.971720 loss:        0.081860
Test - acc:         0.920800 loss:        0.268923
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.973120 loss:        0.079058
Test - acc:         0.920500 loss:        0.258797
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.970980 loss:        0.084723
Test - acc:         0.923500 loss:        0.257528
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.973060 loss:        0.079077
Test - acc:         0.917100 loss:        0.286834
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.972300 loss:        0.078832
Test - acc:         0.923600 loss:        0.263236
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.972840 loss:        0.078621
Test - acc:         0.920100 loss:        0.279417
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.972560 loss:        0.080017
Test - acc:         0.924500 loss:        0.266284
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.972860 loss:        0.078183
Test - acc:         0.921200 loss:        0.267408
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.973860 loss:        0.077441
Test - acc:         0.919900 loss:        0.272756
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.974220 loss:        0.076285
Test - acc:         0.917500 loss:        0.276190
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.971820 loss:        0.081325
Test - acc:         0.916800 loss:        0.288439
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.972800 loss:        0.079110
Test - acc:         0.921300 loss:        0.262067
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.972260 loss:        0.079415
Test - acc:         0.921700 loss:        0.264139
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.972640 loss:        0.080176
Test - acc:         0.923400 loss:        0.261445
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.973560 loss:        0.076546
Test - acc:         0.920100 loss:        0.274304
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.972000 loss:        0.079670
Test - acc:         0.920600 loss:        0.267497
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.972660 loss:        0.077812
Test - acc:         0.921000 loss:        0.275222
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.971440 loss:        0.080001
Test - acc:         0.918000 loss:        0.273746
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.972620 loss:        0.078954
Test - acc:         0.917900 loss:        0.284349
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.972600 loss:        0.079977
Test - acc:         0.920300 loss:        0.270390
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.975320 loss:        0.075108
Test - acc:         0.922000 loss:        0.273690
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.973660 loss:        0.077112
Test - acc:         0.922000 loss:        0.270504
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.973780 loss:        0.077063
Test - acc:         0.920800 loss:        0.267483
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.975300 loss:        0.071669
Test - acc:         0.915100 loss:        0.294567
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.974720 loss:        0.075400
Test - acc:         0.922200 loss:        0.261565
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.972280 loss:        0.079530
Test - acc:         0.922700 loss:        0.261230
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.973020 loss:        0.079352
Test - acc:         0.923900 loss:        0.261462
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.972700 loss:        0.081230
Test - acc:         0.921900 loss:        0.270260
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.936600 loss:        0.181872
Test - acc:         0.902600 loss:        0.296529
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.944760 loss:        0.158179
Test - acc:         0.911300 loss:        0.288003
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.948440 loss:        0.148271
Test - acc:         0.907600 loss:        0.296419
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.948920 loss:        0.144552
Test - acc:         0.906100 loss:        0.304291
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.954020 loss:        0.135792
Test - acc:         0.914100 loss:        0.275194
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.952560 loss:        0.137030
Test - acc:         0.906900 loss:        0.302565
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.952880 loss:        0.133264
Test - acc:         0.907900 loss:        0.286247
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
