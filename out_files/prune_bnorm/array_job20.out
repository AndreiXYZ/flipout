Running --prune_bnorm --model resnet18 --prune_criterion global_magnitude --seed 43 --prune_freq 32 --prune_rate 0.5 --comment=resnet18_crit=global_magnitude_pf=32_seed=43 --save_model=pre-finetune/resnet18_global_magnitude_pf32_s43 --logdir=criterion_experiment_prune_bnorm/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "global_magnitude",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": true,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_prune_bnorm/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_global_magnitude_pf32_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11169152
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11169152
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.348320 loss:        1.810961
Test - acc:         0.439400 loss:        1.484592
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.526380 loss:        1.296187
Test - acc:         0.587300 loss:        1.162469
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.635500 loss:        1.021202
Test - acc:         0.626700 loss:        1.042242
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.702640 loss:        0.843171
Test - acc:         0.711600 loss:        0.860480
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.752060 loss:        0.713263
Test - acc:         0.752700 loss:        0.721025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.783660 loss:        0.623389
Test - acc:         0.733700 loss:        0.786503
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.801860 loss:        0.570900
Test - acc:         0.757000 loss:        0.762860
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.814640 loss:        0.539573
Test - acc:         0.704900 loss:        0.941443
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.825480 loss:        0.505140
Test - acc:         0.784800 loss:        0.633475
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.830340 loss:        0.493756
Test - acc:         0.779500 loss:        0.657745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.838340 loss:        0.474250
Test - acc:         0.804200 loss:        0.593576
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.844280 loss:        0.458721
Test - acc:         0.834400 loss:        0.502315
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.846500 loss:        0.449426
Test - acc:         0.817100 loss:        0.554424
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.847180 loss:        0.443512
Test - acc:         0.812100 loss:        0.573417
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.853420 loss:        0.425940
Test - acc:         0.804500 loss:        0.581330
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.857080 loss:        0.415865
Test - acc:         0.827800 loss:        0.517806
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.861940 loss:        0.404304
Test - acc:         0.815300 loss:        0.557021
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.863300 loss:        0.400604
Test - acc:         0.797600 loss:        0.647381
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.864700 loss:        0.398190
Test - acc:         0.820300 loss:        0.561645
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.869280 loss:        0.387392
Test - acc:         0.815000 loss:        0.584442
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.870620 loss:        0.382326
Test - acc:         0.818900 loss:        0.536924
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870120 loss:        0.380016
Test - acc:         0.783000 loss:        0.719661
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.869920 loss:        0.379140
Test - acc:         0.822700 loss:        0.551593
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.874520 loss:        0.374324
Test - acc:         0.836900 loss:        0.490705
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.874720 loss:        0.370254
Test - acc:         0.827900 loss:        0.513918
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.876980 loss:        0.362354
Test - acc:         0.850800 loss:        0.456385
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.873500 loss:        0.368452
Test - acc:         0.823200 loss:        0.540426
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.876640 loss:        0.360789
Test - acc:         0.848400 loss:        0.446691
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.876560 loss:        0.360781
Test - acc:         0.803700 loss:        0.617252
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.879380 loss:        0.354437
Test - acc:         0.703300 loss:        1.070219
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.879920 loss:        0.350535
Test - acc:         0.835700 loss:        0.514332
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.877980 loss:        0.357668
Test - acc:         0.852200 loss:        0.417868
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.888860 loss:        0.325086
Test - acc:         0.859200 loss:        0.426019
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.889740 loss:        0.325498
Test - acc:         0.852100 loss:        0.444269
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.890420 loss:        0.324798
Test - acc:         0.854300 loss:        0.456042
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.886740 loss:        0.329179
Test - acc:         0.815700 loss:        0.584993
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.888600 loss:        0.328274
Test - acc:         0.826200 loss:        0.531779
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.890080 loss:        0.319825
Test - acc:         0.855800 loss:        0.458956
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.890420 loss:        0.322359
Test - acc:         0.849500 loss:        0.440218
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.887800 loss:        0.326614
Test - acc:         0.834300 loss:        0.493967
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.892000 loss:        0.314774
Test - acc:         0.849300 loss:        0.486136
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.891200 loss:        0.318124
Test - acc:         0.761600 loss:        0.833418
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.890720 loss:        0.319437
Test - acc:         0.849800 loss:        0.459826
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.892360 loss:        0.315437
Test - acc:         0.845000 loss:        0.470628
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.890920 loss:        0.317545
Test - acc:         0.809000 loss:        0.595094
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.892220 loss:        0.314706
Test - acc:         0.795900 loss:        0.637849
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.892040 loss:        0.314702
Test - acc:         0.861700 loss:        0.420290
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.894720 loss:        0.309719
Test - acc:         0.853300 loss:        0.445235
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.892460 loss:        0.314235
Test - acc:         0.833200 loss:        0.513134
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.895060 loss:        0.310046
Test - acc:         0.837400 loss:        0.492220
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.893080 loss:        0.314756
Test - acc:         0.859400 loss:        0.429506
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.895040 loss:        0.306135
Test - acc:         0.813100 loss:        0.582560
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.893320 loss:        0.311774
Test - acc:         0.821900 loss:        0.559383
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.894980 loss:        0.308590
Test - acc:         0.849600 loss:        0.454528
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.892840 loss:        0.310097
Test - acc:         0.839300 loss:        0.497342
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.896780 loss:        0.303095
Test - acc:         0.817300 loss:        0.567395
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.894540 loss:        0.309387
Test - acc:         0.844400 loss:        0.468546
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.892980 loss:        0.312387
Test - acc:         0.852200 loss:        0.449182
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.895920 loss:        0.305044
Test - acc:         0.825100 loss:        0.557420
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.895280 loss:        0.303404
Test - acc:         0.837400 loss:        0.488198
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.893620 loss:        0.310150
Test - acc:         0.814400 loss:        0.557243
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.895800 loss:        0.305389
Test - acc:         0.848900 loss:        0.467101
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.895800 loss:        0.306325
Test - acc:         0.849800 loss:        0.463997
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.895340 loss:        0.307257
Test - acc:         0.792900 loss:        0.684491
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.906560 loss:        0.270077
Test - acc:         0.835600 loss:        0.528510
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.906640 loss:        0.271401
Test - acc:         0.861300 loss:        0.441182
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.906120 loss:        0.273638
Test - acc:         0.856500 loss:        0.430155
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.904680 loss:        0.277501
Test - acc:         0.845500 loss:        0.477247
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.904620 loss:        0.277618
Test - acc:         0.820200 loss:        0.542801
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.903680 loss:        0.282133
Test - acc:         0.856600 loss:        0.458070
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.903920 loss:        0.278451
Test - acc:         0.810300 loss:        0.581604
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.904440 loss:        0.278076
Test - acc:         0.877900 loss:        0.380675
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.904580 loss:        0.279395
Test - acc:         0.875400 loss:        0.374613
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.905180 loss:        0.275293
Test - acc:         0.865500 loss:        0.417559
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.906320 loss:        0.275277
Test - acc:         0.841300 loss:        0.469985
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.905360 loss:        0.277491
Test - acc:         0.874100 loss:        0.394553
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.907300 loss:        0.274738
Test - acc:         0.850200 loss:        0.470843
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.904680 loss:        0.276369
Test - acc:         0.867800 loss:        0.392984
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.903900 loss:        0.277893
Test - acc:         0.878300 loss:        0.357294
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.904740 loss:        0.280081
Test - acc:         0.807700 loss:        0.610165
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.906640 loss:        0.273104
Test - acc:         0.837100 loss:        0.486114
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.905620 loss:        0.274094
Test - acc:         0.858900 loss:        0.423486
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.905400 loss:        0.275722
Test - acc:         0.852600 loss:        0.462345
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.907300 loss:        0.276126
Test - acc:         0.864800 loss:        0.396810
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.903860 loss:        0.276667
Test - acc:         0.853600 loss:        0.452001
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.905160 loss:        0.275107
Test - acc:         0.856500 loss:        0.446317
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.903860 loss:        0.277697
Test - acc:         0.867900 loss:        0.403106
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.904920 loss:        0.276257
Test - acc:         0.862800 loss:        0.415966
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.905120 loss:        0.274956
Test - acc:         0.830100 loss:        0.546464
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.903660 loss:        0.279229
Test - acc:         0.865400 loss:        0.416180
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.905920 loss:        0.271490
Test - acc:         0.840400 loss:        0.499292
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.906340 loss:        0.274335
Test - acc:         0.854100 loss:        0.442219
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.906380 loss:        0.271696
Test - acc:         0.816300 loss:        0.583284
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.906300 loss:        0.275692
Test - acc:         0.807400 loss:        0.633870
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.903240 loss:        0.278546
Test - acc:         0.855600 loss:        0.423972
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.909600 loss:        0.267510
Test - acc:         0.849000 loss:        0.474311
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.919600 loss:        0.236024
Test - acc:         0.862300 loss:        0.416551
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.915860 loss:        0.242612
Test - acc:         0.857400 loss:        0.419843
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.915080 loss:        0.245312
Test - acc:         0.855100 loss:        0.445406
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.913000 loss:        0.248662
Test - acc:         0.829000 loss:        0.519327
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.913760 loss:        0.248750
Test - acc:         0.875100 loss:        0.372948
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.913940 loss:        0.250591
Test - acc:         0.839500 loss:        0.475505
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.912940 loss:        0.254044
Test - acc:         0.862500 loss:        0.435323
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.912840 loss:        0.253774
Test - acc:         0.849400 loss:        0.477807
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.913920 loss:        0.249268
Test - acc:         0.820300 loss:        0.598040
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.914340 loss:        0.248633
Test - acc:         0.880100 loss:        0.371475
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.912420 loss:        0.251529
Test - acc:         0.875100 loss:        0.377327
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.914020 loss:        0.250036
Test - acc:         0.853000 loss:        0.468835
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.913300 loss:        0.248961
Test - acc:         0.851900 loss:        0.448019
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.916320 loss:        0.243643
Test - acc:         0.816300 loss:        0.592848
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.914800 loss:        0.249963
Test - acc:         0.844000 loss:        0.510651
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.914740 loss:        0.248517
Test - acc:         0.855200 loss:        0.450885
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.913540 loss:        0.249745
Test - acc:         0.866500 loss:        0.421708
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.913400 loss:        0.251599
Test - acc:         0.868400 loss:        0.409756
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.914160 loss:        0.248892
Test - acc:         0.860600 loss:        0.436309
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.914520 loss:        0.251366
Test - acc:         0.882800 loss:        0.362217
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.912800 loss:        0.250937
Test - acc:         0.865200 loss:        0.423699
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.914560 loss:        0.249732
Test - acc:         0.870300 loss:        0.418916
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.913200 loss:        0.251229
Test - acc:         0.839700 loss:        0.516641
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.914460 loss:        0.249692
Test - acc:         0.845600 loss:        0.492751
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.911740 loss:        0.252859
Test - acc:         0.851600 loss:        0.484648
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.914140 loss:        0.250733
Test - acc:         0.871700 loss:        0.409753
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.916700 loss:        0.243325
Test - acc:         0.852800 loss:        0.460397
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.914460 loss:        0.248302
Test - acc:         0.881600 loss:        0.352556
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.914660 loss:        0.247575
Test - acc:         0.853000 loss:        0.460610
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.914680 loss:        0.248535
Test - acc:         0.858300 loss:        0.471317
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.911180 loss:        0.253052
Test - acc:         0.834100 loss:        0.532262
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.913180 loss:        0.251529
Test - acc:         0.863300 loss:        0.411923
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.923360 loss:        0.219613
Test - acc:         0.850100 loss:        0.461701
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.923760 loss:        0.226402
Test - acc:         0.881200 loss:        0.367393
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.922140 loss:        0.226725
Test - acc:         0.863700 loss:        0.415430
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.920240 loss:        0.230839
Test - acc:         0.874600 loss:        0.385382
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.920120 loss:        0.232175
Test - acc:         0.856800 loss:        0.440876
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.919560 loss:        0.232676
Test - acc:         0.825700 loss:        0.547695
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.920120 loss:        0.231884
Test - acc:         0.823300 loss:        0.528753
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.918680 loss:        0.238324
Test - acc:         0.871200 loss:        0.394259
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.919300 loss:        0.231182
Test - acc:         0.867200 loss:        0.411732
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.918040 loss:        0.236796
Test - acc:         0.864200 loss:        0.420501
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.920360 loss:        0.231944
Test - acc:         0.878000 loss:        0.362806
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.918900 loss:        0.237079
Test - acc:         0.840000 loss:        0.515178
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.918720 loss:        0.234358
Test - acc:         0.883500 loss:        0.363189
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.917620 loss:        0.237275
Test - acc:         0.864700 loss:        0.428615
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.919560 loss:        0.236207
Test - acc:         0.887300 loss:        0.334633
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.919160 loss:        0.234917
Test - acc:         0.858800 loss:        0.451053
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.917660 loss:        0.234493
Test - acc:         0.836700 loss:        0.541386
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.920120 loss:        0.231563
Test - acc:         0.847100 loss:        0.475827
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.920060 loss:        0.231263
Test - acc:         0.851900 loss:        0.493600
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.919560 loss:        0.231396
Test - acc:         0.882000 loss:        0.357954
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.919800 loss:        0.235407
Test - acc:         0.872800 loss:        0.380982
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.920420 loss:        0.234734
Test - acc:         0.875500 loss:        0.391806
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.959280 loss:        0.124182
Test - acc:         0.933900 loss:        0.194144
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.970960 loss:        0.089494
Test - acc:         0.936300 loss:        0.186899
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.975340 loss:        0.075636
Test - acc:         0.939700 loss:        0.180457
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.978800 loss:        0.067775
Test - acc:         0.938600 loss:        0.182735
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.980940 loss:        0.060829
Test - acc:         0.942000 loss:        0.179777
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.982940 loss:        0.054441
Test - acc:         0.941400 loss:        0.182074
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.984680 loss:        0.048828
Test - acc:         0.940600 loss:        0.184977
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.986180 loss:        0.045538
Test - acc:         0.942500 loss:        0.182151
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.987220 loss:        0.041525
Test - acc:         0.941200 loss:        0.180560
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.988680 loss:        0.038870
Test - acc:         0.940100 loss:        0.191060
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.987300 loss:        0.044949
Test - acc:         0.940900 loss:        0.189000
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.988800 loss:        0.040268
Test - acc:         0.941400 loss:        0.188050
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.989360 loss:        0.037662
Test - acc:         0.941800 loss:        0.188930
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.989200 loss:        0.036842
Test - acc:         0.940500 loss:        0.198194
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.989560 loss:        0.035356
Test - acc:         0.941700 loss:        0.198126
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.990820 loss:        0.032114
Test - acc:         0.939100 loss:        0.200021
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.990360 loss:        0.033445
Test - acc:         0.939400 loss:        0.207455
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.990640 loss:        0.031597
Test - acc:         0.940100 loss:        0.202225
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.991800 loss:        0.029811
Test - acc:         0.940200 loss:        0.198498
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.992120 loss:        0.028803
Test - acc:         0.938300 loss:        0.205419
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.991620 loss:        0.028979
Test - acc:         0.938300 loss:        0.216230
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.991900 loss:        0.028280
Test - acc:         0.937000 loss:        0.213443
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.991840 loss:        0.028886
Test - acc:         0.938600 loss:        0.210427
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.993700 loss:        0.024305
Test - acc:         0.937900 loss:        0.215134
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.992740 loss:        0.026126
Test - acc:         0.937700 loss:        0.223460
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.992360 loss:        0.027697
Test - acc:         0.938800 loss:        0.211151
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.992560 loss:        0.027099
Test - acc:         0.940000 loss:        0.215465
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.991700 loss:        0.028838
Test - acc:         0.940300 loss:        0.217081
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.991700 loss:        0.028086
Test - acc:         0.941600 loss:        0.209687
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.991260 loss:        0.029961
Test - acc:         0.939200 loss:        0.222990
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.991920 loss:        0.029261
Test - acc:         0.936300 loss:        0.222204
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.990680 loss:        0.030909
Test - acc:         0.933100 loss:        0.226278
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.991120 loss:        0.031263
Test - acc:         0.936200 loss:        0.225492
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.991200 loss:        0.030659
Test - acc:         0.937400 loss:        0.215564
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.989180 loss:        0.035475
Test - acc:         0.931200 loss:        0.233220
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.990440 loss:        0.031925
Test - acc:         0.937600 loss:        0.221379
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.989420 loss:        0.034969
Test - acc:         0.933000 loss:        0.227397
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.990020 loss:        0.033216
Test - acc:         0.928500 loss:        0.249770
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.988620 loss:        0.036443
Test - acc:         0.932900 loss:        0.232389
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.988820 loss:        0.035767
Test - acc:         0.933100 loss:        0.222630
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.987560 loss:        0.039358
Test - acc:         0.928700 loss:        0.243375
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.988160 loss:        0.039197
Test - acc:         0.933400 loss:        0.235136
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.965640 loss:        0.105785
Test - acc:         0.912300 loss:        0.284912
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.973040 loss:        0.081228
Test - acc:         0.926900 loss:        0.243752
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.975000 loss:        0.074732
Test - acc:         0.923100 loss:        0.282255
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.975780 loss:        0.073362
Test - acc:         0.912900 loss:        0.284478
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.977360 loss:        0.070392
Test - acc:         0.921200 loss:        0.274483
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.977200 loss:        0.069124
Test - acc:         0.913900 loss:        0.301004
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.977540 loss:        0.066807
Test - acc:         0.921800 loss:        0.275216
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.978800 loss:        0.064015
Test - acc:         0.923300 loss:        0.275104
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.979740 loss:        0.062541
Test - acc:         0.921000 loss:        0.283697
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.978360 loss:        0.065146
Test - acc:         0.916300 loss:        0.297202
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.979260 loss:        0.062999
Test - acc:         0.923400 loss:        0.257169
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.980420 loss:        0.060168
Test - acc:         0.923400 loss:        0.266769
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.059742
Test - acc:         0.922000 loss:        0.273266
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.980040 loss:        0.059953
Test - acc:         0.925100 loss:        0.255829
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.981960 loss:        0.056615
Test - acc:         0.912600 loss:        0.306998
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.980440 loss:        0.059996
Test - acc:         0.921100 loss:        0.276473
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.982440 loss:        0.055257
Test - acc:         0.923600 loss:        0.260993
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.980080 loss:        0.062865
Test - acc:         0.925900 loss:        0.268949
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.980760 loss:        0.059519
Test - acc:         0.925800 loss:        0.252816
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.980860 loss:        0.058471
Test - acc:         0.927900 loss:        0.254613
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.981960 loss:        0.056125
Test - acc:         0.926600 loss:        0.255573
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.978800 loss:        0.063853
Test - acc:         0.925800 loss:        0.264651
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.980720 loss:        0.058935
Test - acc:         0.927100 loss:        0.247927
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.981220 loss:        0.056387
Test - acc:         0.921300 loss:        0.274624
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.981300 loss:        0.056310
Test - acc:         0.923200 loss:        0.279406
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.980840 loss:        0.058894
Test - acc:         0.923800 loss:        0.270194
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.980820 loss:        0.058654
Test - acc:         0.929200 loss:        0.253493
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.982840 loss:        0.054176
Test - acc:         0.923100 loss:        0.270387
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.058274
Test - acc:         0.925500 loss:        0.260471
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.981320 loss:        0.057322
Test - acc:         0.930300 loss:        0.246421
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.981100 loss:        0.058255
Test - acc:         0.922100 loss:        0.284289
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.981120 loss:        0.058577
Test - acc:         0.922800 loss:        0.270189
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.946200 loss:        0.159060
Test - acc:         0.902100 loss:        0.319673
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.956260 loss:        0.130399
Test - acc:         0.903700 loss:        0.304152
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.958560 loss:        0.123236
Test - acc:         0.909800 loss:        0.302677
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.960660 loss:        0.115931
Test - acc:         0.913400 loss:        0.279798
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.961940 loss:        0.112786
Test - acc:         0.911400 loss:        0.297224
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.961660 loss:        0.111413
Test - acc:         0.909500 loss:        0.297359
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.962620 loss:        0.109202
Test - acc:         0.904700 loss:        0.315112
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.962400 loss:        0.109809
Test - acc:         0.915600 loss:        0.276248
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.963100 loss:        0.108654
Test - acc:         0.917900 loss:        0.273895
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.962540 loss:        0.109623
Test - acc:         0.913700 loss:        0.282476
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.965880 loss:        0.100512
Test - acc:         0.914800 loss:        0.285849
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.966400 loss:        0.098869
Test - acc:         0.913900 loss:        0.283829
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.964280 loss:        0.102827
Test - acc:         0.907200 loss:        0.327554
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.966000 loss:        0.098954
Test - acc:         0.907600 loss:        0.320735
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.966200 loss:        0.100791
Test - acc:         0.910300 loss:        0.303551
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.965720 loss:        0.100651
Test - acc:         0.916000 loss:        0.288454
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.966740 loss:        0.098774
Test - acc:         0.911500 loss:        0.292184
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.967240 loss:        0.097204
Test - acc:         0.914300 loss:        0.301122
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.966200 loss:        0.100149
Test - acc:         0.915500 loss:        0.296014
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.967840 loss:        0.095613
Test - acc:         0.913000 loss:        0.298165
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.967200 loss:        0.097451
Test - acc:         0.914000 loss:        0.290633
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.967600 loss:        0.094203
Test - acc:         0.918900 loss:        0.270636
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.967760 loss:        0.093904
Test - acc:         0.911400 loss:        0.318427
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.967060 loss:        0.094827
Test - acc:         0.915100 loss:        0.291169
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.967680 loss:        0.096939
Test - acc:         0.914000 loss:        0.290494
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.967760 loss:        0.095445
Test - acc:         0.915300 loss:        0.288585
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.979380 loss:        0.065079
Test - acc:         0.930200 loss:        0.229883
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.984700 loss:        0.053176
Test - acc:         0.931800 loss:        0.225134
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.984980 loss:        0.048850
Test - acc:         0.933200 loss:        0.223552
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.987660 loss:        0.045084
Test - acc:         0.934500 loss:        0.225457
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.987940 loss:        0.042915
Test - acc:         0.933900 loss:        0.226854
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.988220 loss:        0.040906
Test - acc:         0.933100 loss:        0.226845
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.910320 loss:        0.295689
Test - acc:         0.894100 loss:        0.319839
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.932300 loss:        0.221248
Test - acc:         0.901700 loss:        0.304064
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.935440 loss:        0.203506
Test - acc:         0.903600 loss:        0.295297
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.941920 loss:        0.188253
Test - acc:         0.905800 loss:        0.287246
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.943940 loss:        0.179236
Test - acc:         0.908200 loss:        0.281865
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.946500 loss:        0.170722
Test - acc:         0.909200 loss:        0.280664
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.948160 loss:        0.163817
Test - acc:         0.907900 loss:        0.281527
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.949260 loss:        0.160501
Test - acc:         0.909400 loss:        0.276629
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.951700 loss:        0.153375
Test - acc:         0.911300 loss:        0.280717
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.951540 loss:        0.152219
Test - acc:         0.910100 loss:        0.278794
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.952960 loss:        0.148622
Test - acc:         0.911300 loss:        0.276524
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.954620 loss:        0.144404
Test - acc:         0.912600 loss:        0.275181
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.955340 loss:        0.140911
Test - acc:         0.913400 loss:        0.272048
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.955020 loss:        0.141079
Test - acc:         0.912400 loss:        0.269970
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.956200 loss:        0.135826
Test - acc:         0.913400 loss:        0.272133
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.957700 loss:        0.133468
Test - acc:         0.913900 loss:        0.270331
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.957800 loss:        0.133036
Test - acc:         0.914400 loss:        0.269795
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.957540 loss:        0.131432
Test - acc:         0.911300 loss:        0.271134
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.957060 loss:        0.132099
Test - acc:         0.913000 loss:        0.273448
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.959320 loss:        0.129409
Test - acc:         0.914200 loss:        0.272602
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.958180 loss:        0.128238
Test - acc:         0.915300 loss:        0.269933
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.959780 loss:        0.125520
Test - acc:         0.913900 loss:        0.271165
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.961040 loss:        0.123893
Test - acc:         0.911800 loss:        0.273164
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.961640 loss:        0.118978
Test - acc:         0.916300 loss:        0.268788
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.961340 loss:        0.121167
Test - acc:         0.916800 loss:        0.268693
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.961340 loss:        0.120465
Test - acc:         0.915000 loss:        0.273739
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.960980 loss:        0.119792
Test - acc:         0.915700 loss:        0.269721
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.961680 loss:        0.118734
Test - acc:         0.916800 loss:        0.269515
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.961400 loss:        0.118292
Test - acc:         0.915200 loss:        0.273112
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.961040 loss:        0.118701
Test - acc:         0.915800 loss:        0.271401
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.960920 loss:        0.118420
Test - acc:         0.916600 loss:        0.268268
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.963440 loss:        0.114031
Test - acc:         0.916500 loss:        0.271493
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.723960 loss:        0.817563
Test - acc:         0.781400 loss:        0.641702
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.797600 loss:        0.607228
Test - acc:         0.809400 loss:        0.556126
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.815400 loss:        0.548538
Test - acc:         0.819200 loss:        0.524775
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.828180 loss:        0.514655
Test - acc:         0.827200 loss:        0.508428
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.835520 loss:        0.495912
Test - acc:         0.831900 loss:        0.491818
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.838400 loss:        0.477475
Test - acc:         0.833400 loss:        0.486562
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.846120 loss:        0.461301
Test - acc:         0.838600 loss:        0.475093
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.848840 loss:        0.453782
Test - acc:         0.841900 loss:        0.466740
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.850180 loss:        0.443524
Test - acc:         0.851000 loss:        0.448670
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.853520 loss:        0.438604
Test - acc:         0.847800 loss:        0.448278
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.854380 loss:        0.428910
Test - acc:         0.848800 loss:        0.449620
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.857240 loss:        0.424735
Test - acc:         0.851800 loss:        0.442495
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.859320 loss:        0.415593
Test - acc:         0.849400 loss:        0.443658
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.861080 loss:        0.411327
Test - acc:         0.850700 loss:        0.436479
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.860840 loss:        0.409733
Test - acc:         0.852200 loss:        0.435597
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.865080 loss:        0.404270
Test - acc:         0.853400 loss:        0.432958
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.863800 loss:        0.401816
Test - acc:         0.857200 loss:        0.429670
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.866800 loss:        0.395060
Test - acc:         0.855400 loss:        0.427657
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.867480 loss:        0.391459
Test - acc:         0.852000 loss:        0.431427
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.867680 loss:        0.392679
Test - acc:         0.854300 loss:        0.433988
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.867260 loss:        0.389061
Test - acc:         0.854700 loss:        0.427852
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.869000 loss:        0.388698
Test - acc:         0.857000 loss:        0.416691
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.870980 loss:        0.382509
Test - acc:         0.859200 loss:        0.419207
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.871380 loss:        0.380785
Test - acc:         0.858200 loss:        0.417660
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.871060 loss:        0.378500
Test - acc:         0.858600 loss:        0.412908
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.874080 loss:        0.375801
Test - acc:         0.857300 loss:        0.421261
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.871920 loss:        0.375127
Test - acc:         0.860500 loss:        0.410190
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.874260 loss:        0.372811
Test - acc:         0.859200 loss:        0.415224
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.874200 loss:        0.372607
Test - acc:         0.860400 loss:        0.412565
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.875640 loss:        0.369453
Test - acc:         0.861700 loss:        0.411623
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.875980 loss:        0.365762
Test - acc:         0.861200 loss:        0.412911
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.877020 loss:        0.364810
Test - acc:         0.863300 loss:        0.411961
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.486540 loss:        1.441005
Test - acc:         0.577900 loss:        1.176272
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.594940 loss:        1.172861
Test - acc:         0.626500 loss:        1.073552
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.626980 loss:        1.090339
Test - acc:         0.646300 loss:        1.026754
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.645400 loss:        1.044528
Test - acc:         0.663100 loss:        0.979706
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.656760 loss:        1.011172
Test - acc:         0.667700 loss:        0.955724
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.665920 loss:        0.981716
Test - acc:         0.682800 loss:        0.922965
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.676300 loss:        0.955977
Test - acc:         0.686500 loss:        0.903637
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.681860 loss:        0.938039
Test - acc:         0.688400 loss:        0.892660
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.688180 loss:        0.916969
Test - acc:         0.702200 loss:        0.865081
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.692240 loss:        0.905257
Test - acc:         0.695800 loss:        0.864878
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.696920 loss:        0.888950
Test - acc:         0.703100 loss:        0.845838
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.698900 loss:        0.880411
Test - acc:         0.703500 loss:        0.840968
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.701100 loss:        0.871842
Test - acc:         0.706700 loss:        0.830458
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.704780 loss:        0.860520
Test - acc:         0.709300 loss:        0.821653
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.708620 loss:        0.849452
Test - acc:         0.717100 loss:        0.807399
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.712200 loss:        0.844119
Test - acc:         0.712900 loss:        0.813508
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.713440 loss:        0.836865
Test - acc:         0.710800 loss:        0.809472
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.715520 loss:        0.833382
Test - acc:         0.723400 loss:        0.789428
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.716520 loss:        0.825074
Test - acc:         0.718500 loss:        0.796064
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.719500 loss:        0.815494
Test - acc:         0.730900 loss:        0.777093
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.723820 loss:        0.807358
Test - acc:         0.726600 loss:        0.788469
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.722120 loss:        0.807811
Test - acc:         0.720000 loss:        0.787026
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.721500 loss:        0.807895
Test - acc:         0.731000 loss:        0.766703
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.726760 loss:        0.798447
Test - acc:         0.728900 loss:        0.770472
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.724060 loss:        0.797818
Test - acc:         0.731300 loss:        0.763684
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.729660 loss:        0.793252
Test - acc:         0.735100 loss:        0.757714
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.728200 loss:        0.789239
Test - acc:         0.729100 loss:        0.767724
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.727660 loss:        0.788780
Test - acc:         0.738700 loss:        0.753586
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.732660 loss:        0.780788
Test - acc:         0.736600 loss:        0.759085
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.730620 loss:        0.779822
Test - acc:         0.736100 loss:        0.754528
Sparsity :          0.9990
Wdecay :        0.000500
