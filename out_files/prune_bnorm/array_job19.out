Running --prune_bnorm --model resnet18 --noise --prune_criterion weight_div_flips --seed 43 --prune_freq 32 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=32_seed=43 --save_model=pre-finetune/resnet18_weight_div_flips_pf32_s43 --logdir=criterion_experiment_prune_bnorm/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": true,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_prune_bnorm/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf32_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11169152
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11169152
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.275580 loss:        2.206162
Test - acc:         0.379300 loss:        1.773930
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.448080 loss:        1.503151
Test - acc:         0.529700 loss:        1.350102
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.561660 loss:        1.220748
Test - acc:         0.589900 loss:        1.153149
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.644920 loss:        1.002910
Test - acc:         0.641300 loss:        1.060353
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.705420 loss:        0.837933
Test - acc:         0.662400 loss:        0.967185
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.753280 loss:        0.707195
Test - acc:         0.719800 loss:        0.820684
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.778140 loss:        0.637992
Test - acc:         0.746700 loss:        0.735908
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.797340 loss:        0.584231
Test - acc:         0.764400 loss:        0.731834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.810600 loss:        0.549598
Test - acc:         0.739700 loss:        0.764843
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.816060 loss:        0.533603
Test - acc:         0.747800 loss:        0.758131
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.822860 loss:        0.513958
Test - acc:         0.768500 loss:        0.701588
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.827900 loss:        0.501282
Test - acc:         0.816000 loss:        0.537637
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.833580 loss:        0.483668
Test - acc:         0.810200 loss:        0.568396
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.836280 loss:        0.476076
Test - acc:         0.768400 loss:        0.720073
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.839420 loss:        0.464692
Test - acc:         0.759300 loss:        0.735929
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845000 loss:        0.453000
Test - acc:         0.824500 loss:        0.526457
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.849680 loss:        0.444501
Test - acc:         0.826500 loss:        0.520368
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.849380 loss:        0.438538
Test - acc:         0.798100 loss:        0.637503
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.850040 loss:        0.440032
Test - acc:         0.829700 loss:        0.507356
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.854180 loss:        0.426594
Test - acc:         0.802500 loss:        0.589427
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.855560 loss:        0.423533
Test - acc:         0.786400 loss:        0.632288
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.855960 loss:        0.420786
Test - acc:         0.789600 loss:        0.646576
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.858360 loss:        0.416946
Test - acc:         0.752800 loss:        0.756219
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.860040 loss:        0.415164
Test - acc:         0.812200 loss:        0.558783
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.858620 loss:        0.414178
Test - acc:         0.814800 loss:        0.577834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.859180 loss:        0.410590
Test - acc:         0.813800 loss:        0.559850
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.860320 loss:        0.407069
Test - acc:         0.851000 loss:        0.442927
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.857900 loss:        0.414354
Test - acc:         0.838400 loss:        0.479358
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.861760 loss:        0.403122
Test - acc:         0.813600 loss:        0.552473
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.864300 loss:        0.400279
Test - acc:         0.798500 loss:        0.615302
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.864260 loss:        0.395054
Test - acc:         0.802800 loss:        0.596695
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.862920 loss:        0.397425
Test - acc:         0.813300 loss:        0.539908
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.876200 loss:        0.362769
Test - acc:         0.848100 loss:        0.458425
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.877460 loss:        0.361515
Test - acc:         0.842500 loss:        0.470686
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.874680 loss:        0.365791
Test - acc:         0.834900 loss:        0.526018
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.873000 loss:        0.365293
Test - acc:         0.825900 loss:        0.535198
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.872460 loss:        0.374770
Test - acc:         0.781100 loss:        0.660401
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.871580 loss:        0.372957
Test - acc:         0.850700 loss:        0.451625
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.874460 loss:        0.369203
Test - acc:         0.842400 loss:        0.469939
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.871360 loss:        0.376524
Test - acc:         0.832600 loss:        0.493584
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.873440 loss:        0.368982
Test - acc:         0.849700 loss:        0.445290
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.874180 loss:        0.369457
Test - acc:         0.829100 loss:        0.526437
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.872680 loss:        0.371507
Test - acc:         0.855000 loss:        0.449211
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.873240 loss:        0.369764
Test - acc:         0.840900 loss:        0.501636
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.872540 loss:        0.372499
Test - acc:         0.787100 loss:        0.658943
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.876160 loss:        0.362971
Test - acc:         0.800100 loss:        0.604537
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.875120 loss:        0.366492
Test - acc:         0.824600 loss:        0.531153
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.873960 loss:        0.367293
Test - acc:         0.848400 loss:        0.454094
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.874420 loss:        0.366611
Test - acc:         0.808000 loss:        0.603485
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.877060 loss:        0.361551
Test - acc:         0.834800 loss:        0.497589
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.874560 loss:        0.366578
Test - acc:         0.816400 loss:        0.568597
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.877740 loss:        0.355197
Test - acc:         0.797400 loss:        0.623583
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.875540 loss:        0.365412
Test - acc:         0.820900 loss:        0.559600
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.877900 loss:        0.359256
Test - acc:         0.837100 loss:        0.495410
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.877700 loss:        0.359886
Test - acc:         0.829000 loss:        0.510393
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.876500 loss:        0.365768
Test - acc:         0.826300 loss:        0.531476
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.873440 loss:        0.366760
Test - acc:         0.837400 loss:        0.484383
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.875200 loss:        0.365141
Test - acc:         0.843300 loss:        0.488681
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.878740 loss:        0.356892
Test - acc:         0.820600 loss:        0.559323
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.876540 loss:        0.361720
Test - acc:         0.806400 loss:        0.587757
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.874540 loss:        0.366167
Test - acc:         0.832400 loss:        0.507407
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.875680 loss:        0.362830
Test - acc:         0.801700 loss:        0.600232
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.878560 loss:        0.356529
Test - acc:         0.796400 loss:        0.639476
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.875040 loss:        0.363506
Test - acc:         0.840900 loss:        0.499616
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.890160 loss:        0.322498
Test - acc:         0.846600 loss:        0.483714
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.885340 loss:        0.332573
Test - acc:         0.848300 loss:        0.464969
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.884400 loss:        0.341327
Test - acc:         0.842100 loss:        0.474339
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.884640 loss:        0.335929
Test - acc:         0.846600 loss:        0.468611
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.884100 loss:        0.334882
Test - acc:         0.821100 loss:        0.519620
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.884160 loss:        0.338151
Test - acc:         0.835000 loss:        0.518121
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.882260 loss:        0.339300
Test - acc:         0.844200 loss:        0.477218
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.886600 loss:        0.335137
Test - acc:         0.848900 loss:        0.458858
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.882940 loss:        0.341179
Test - acc:         0.851600 loss:        0.444014
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.884960 loss:        0.337803
Test - acc:         0.859800 loss:        0.435548
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.884440 loss:        0.334612
Test - acc:         0.851200 loss:        0.445400
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.883880 loss:        0.341155
Test - acc:         0.818100 loss:        0.593061
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.885320 loss:        0.339952
Test - acc:         0.868600 loss:        0.395070
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.886860 loss:        0.332481
Test - acc:         0.841000 loss:        0.481506
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.883760 loss:        0.339912
Test - acc:         0.831600 loss:        0.538915
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.881840 loss:        0.345809
Test - acc:         0.828500 loss:        0.515944
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.882780 loss:        0.339835
Test - acc:         0.803600 loss:        0.624769
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.881320 loss:        0.345873
Test - acc:         0.832600 loss:        0.529277
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.882220 loss:        0.339639
Test - acc:         0.850300 loss:        0.442134
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.884140 loss:        0.340976
Test - acc:         0.853500 loss:        0.437267
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.884560 loss:        0.338355
Test - acc:         0.788700 loss:        0.691329
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.884920 loss:        0.339589
Test - acc:         0.828000 loss:        0.519224
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.886560 loss:        0.333063
Test - acc:         0.828200 loss:        0.527704
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.882180 loss:        0.342966
Test - acc:         0.820600 loss:        0.536989
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.882580 loss:        0.339507
Test - acc:         0.830300 loss:        0.524466
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.884080 loss:        0.340367
Test - acc:         0.824600 loss:        0.525148
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.881640 loss:        0.340246
Test - acc:         0.790400 loss:        0.647591
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.884680 loss:        0.339715
Test - acc:         0.858100 loss:        0.418243
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.883000 loss:        0.338741
Test - acc:         0.801900 loss:        0.620300
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.883820 loss:        0.339705
Test - acc:         0.773100 loss:        0.754178
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.883140 loss:        0.341783
Test - acc:         0.843400 loss:        0.452027
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.885180 loss:        0.333751
Test - acc:         0.800900 loss:        0.627919
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.897420 loss:        0.299347
Test - acc:         0.852700 loss:        0.450460
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.894660 loss:        0.311103
Test - acc:         0.834700 loss:        0.524340
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.892640 loss:        0.316381
Test - acc:         0.825800 loss:        0.543831
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.892640 loss:        0.312795
Test - acc:         0.834000 loss:        0.491125
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.891040 loss:        0.314710
Test - acc:         0.866000 loss:        0.394357
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.891840 loss:        0.312644
Test - acc:         0.837100 loss:        0.484120
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.891940 loss:        0.314471
Test - acc:         0.791600 loss:        0.678426
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.888780 loss:        0.324500
Test - acc:         0.839700 loss:        0.500201
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.891740 loss:        0.315732
Test - acc:         0.825500 loss:        0.534609
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.890060 loss:        0.317461
Test - acc:         0.863200 loss:        0.419816
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.888440 loss:        0.324794
Test - acc:         0.845000 loss:        0.468914
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.891480 loss:        0.314202
Test - acc:         0.871600 loss:        0.396371
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.893200 loss:        0.311722
Test - acc:         0.811600 loss:        0.599654
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.891480 loss:        0.313428
Test - acc:         0.866100 loss:        0.394595
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.889340 loss:        0.317515
Test - acc:         0.840900 loss:        0.491918
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.891460 loss:        0.319379
Test - acc:         0.871000 loss:        0.398802
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.888680 loss:        0.318086
Test - acc:         0.846700 loss:        0.494106
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.892320 loss:        0.316651
Test - acc:         0.851500 loss:        0.445029
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.889060 loss:        0.319285
Test - acc:         0.843400 loss:        0.477198
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.890120 loss:        0.319848
Test - acc:         0.835300 loss:        0.493361
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.892540 loss:        0.315440
Test - acc:         0.810400 loss:        0.576883
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.890520 loss:        0.320071
Test - acc:         0.852500 loss:        0.450057
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.890540 loss:        0.316688
Test - acc:         0.844700 loss:        0.482877
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.891020 loss:        0.318393
Test - acc:         0.831000 loss:        0.546237
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.891520 loss:        0.315762
Test - acc:         0.843800 loss:        0.469856
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.892760 loss:        0.317053
Test - acc:         0.865400 loss:        0.395998
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.892920 loss:        0.313428
Test - acc:         0.840000 loss:        0.482984
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.890800 loss:        0.316217
Test - acc:         0.863100 loss:        0.406443
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.891140 loss:        0.312679
Test - acc:         0.870600 loss:        0.391809
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.889660 loss:        0.319694
Test - acc:         0.851200 loss:        0.474689
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.893480 loss:        0.311846
Test - acc:         0.832900 loss:        0.525851
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.889680 loss:        0.319827
Test - acc:         0.850100 loss:        0.450050
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.906320 loss:        0.273148
Test - acc:         0.872400 loss:        0.377136
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.900460 loss:        0.287911
Test - acc:         0.839300 loss:        0.504999
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.897660 loss:        0.292543
Test - acc:         0.841200 loss:        0.461235
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.897120 loss:        0.295549
Test - acc:         0.854700 loss:        0.459989
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.898660 loss:        0.294693
Test - acc:         0.829500 loss:        0.527489
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.898220 loss:        0.292390
Test - acc:         0.862800 loss:        0.400448
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.899640 loss:        0.293097
Test - acc:         0.854500 loss:        0.443067
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.899300 loss:        0.296626
Test - acc:         0.866400 loss:        0.391540
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.898040 loss:        0.295302
Test - acc:         0.854400 loss:        0.453760
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.897400 loss:        0.297291
Test - acc:         0.840000 loss:        0.482100
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.897060 loss:        0.296375
Test - acc:         0.866200 loss:        0.398839
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.898340 loss:        0.297240
Test - acc:         0.865800 loss:        0.397193
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.897960 loss:        0.296027
Test - acc:         0.841700 loss:        0.485628
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.897740 loss:        0.301731
Test - acc:         0.866300 loss:        0.419462
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.897940 loss:        0.297063
Test - acc:         0.864000 loss:        0.406065
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.897640 loss:        0.294822
Test - acc:         0.855500 loss:        0.451735
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.897500 loss:        0.295612
Test - acc:         0.861800 loss:        0.418784
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.896940 loss:        0.298957
Test - acc:         0.801100 loss:        0.611388
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.897240 loss:        0.294119
Test - acc:         0.839200 loss:        0.513477
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.898540 loss:        0.295113
Test - acc:         0.851400 loss:        0.463074
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.897840 loss:        0.299210
Test - acc:         0.804300 loss:        0.611763
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.895680 loss:        0.301316
Test - acc:         0.868500 loss:        0.395283
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.940480 loss:        0.177868
Test - acc:         0.923600 loss:        0.224233
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.953500 loss:        0.140177
Test - acc:         0.926500 loss:        0.219837
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.958280 loss:        0.122386
Test - acc:         0.927700 loss:        0.215007
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.960740 loss:        0.115316
Test - acc:         0.929100 loss:        0.214891
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.964280 loss:        0.107027
Test - acc:         0.929600 loss:        0.209044
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.968100 loss:        0.097710
Test - acc:         0.932000 loss:        0.209778
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.968380 loss:        0.091007
Test - acc:         0.931600 loss:        0.216516
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.970420 loss:        0.087015
Test - acc:         0.930800 loss:        0.212821
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.972260 loss:        0.083529
Test - acc:         0.933600 loss:        0.210745
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.974280 loss:        0.076989
Test - acc:         0.934100 loss:        0.210076
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.973160 loss:        0.079908
Test - acc:         0.931800 loss:        0.210269
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.975340 loss:        0.074043
Test - acc:         0.930600 loss:        0.214009
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.976300 loss:        0.070140
Test - acc:         0.933700 loss:        0.214673
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.978260 loss:        0.066869
Test - acc:         0.933800 loss:        0.217518
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.977340 loss:        0.066301
Test - acc:         0.930300 loss:        0.226884
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.978440 loss:        0.065037
Test - acc:         0.928700 loss:        0.230421
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.978880 loss:        0.063398
Test - acc:         0.934500 loss:        0.225369
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.979080 loss:        0.061571
Test - acc:         0.930600 loss:        0.231103
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.979820 loss:        0.061380
Test - acc:         0.927200 loss:        0.245735
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.059718
Test - acc:         0.929900 loss:        0.235709
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.059424
Test - acc:         0.928400 loss:        0.242193
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.059975
Test - acc:         0.930400 loss:        0.231653
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.980780 loss:        0.058593
Test - acc:         0.930400 loss:        0.233964
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.981360 loss:        0.056021
Test - acc:         0.923100 loss:        0.262086
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.056150
Test - acc:         0.929400 loss:        0.246519
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.055338
Test - acc:         0.926100 loss:        0.252248
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.979820 loss:        0.059206
Test - acc:         0.921600 loss:        0.272544
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.980820 loss:        0.057706
Test - acc:         0.922900 loss:        0.264604
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.981380 loss:        0.055920
Test - acc:         0.930400 loss:        0.243490
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.979140 loss:        0.061000
Test - acc:         0.923000 loss:        0.254161
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.980160 loss:        0.059830
Test - acc:         0.927300 loss:        0.248538
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.978680 loss:        0.061622
Test - acc:         0.925900 loss:        0.252286
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.980020 loss:        0.060047
Test - acc:         0.925700 loss:        0.258786
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.979900 loss:        0.059259
Test - acc:         0.927200 loss:        0.246258
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.978160 loss:        0.064633
Test - acc:         0.925700 loss:        0.254417
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.980520 loss:        0.060351
Test - acc:         0.927300 loss:        0.252884
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.978440 loss:        0.063054
Test - acc:         0.922000 loss:        0.272535
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.978540 loss:        0.064742
Test - acc:         0.923700 loss:        0.267922
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.977320 loss:        0.066862
Test - acc:         0.922700 loss:        0.273460
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.978680 loss:        0.063009
Test - acc:         0.917600 loss:        0.286153
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.975840 loss:        0.069617
Test - acc:         0.924600 loss:        0.254267
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.977180 loss:        0.068870
Test - acc:         0.925300 loss:        0.267509
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.965800 loss:        0.099424
Test - acc:         0.923800 loss:        0.252145
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.970640 loss:        0.086474
Test - acc:         0.918600 loss:        0.262511
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.972060 loss:        0.082007
Test - acc:         0.925000 loss:        0.244567
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.972620 loss:        0.081877
Test - acc:         0.922900 loss:        0.256305
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.972960 loss:        0.079318
Test - acc:         0.921900 loss:        0.259757
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.972880 loss:        0.077294
Test - acc:         0.923400 loss:        0.264325
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.971780 loss:        0.080581
Test - acc:         0.923800 loss:        0.251781
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.973880 loss:        0.077124
Test - acc:         0.924000 loss:        0.251474
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.970460 loss:        0.083558
Test - acc:         0.924700 loss:        0.251941
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.973820 loss:        0.076332
Test - acc:         0.925800 loss:        0.248978
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.974160 loss:        0.075921
Test - acc:         0.925000 loss:        0.249346
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.972320 loss:        0.081059
Test - acc:         0.924000 loss:        0.256071
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.974540 loss:        0.076979
Test - acc:         0.922400 loss:        0.269361
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.973860 loss:        0.076566
Test - acc:         0.918400 loss:        0.266333
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.974420 loss:        0.074983
Test - acc:         0.916500 loss:        0.285288
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.974500 loss:        0.074028
Test - acc:         0.916000 loss:        0.279762
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.974620 loss:        0.075038
Test - acc:         0.922200 loss:        0.267340
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.971740 loss:        0.082176
Test - acc:         0.922300 loss:        0.255840
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.973300 loss:        0.077860
Test - acc:         0.917500 loss:        0.284830
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.972360 loss:        0.079048
Test - acc:         0.921400 loss:        0.260290
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.973700 loss:        0.077578
Test - acc:         0.920700 loss:        0.271653
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.971420 loss:        0.082107
Test - acc:         0.919300 loss:        0.288316
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.974180 loss:        0.076299
Test - acc:         0.924200 loss:        0.264151
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.972580 loss:        0.079219
Test - acc:         0.919600 loss:        0.265028
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.974480 loss:        0.074717
Test - acc:         0.919200 loss:        0.286924
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.974620 loss:        0.073891
Test - acc:         0.920300 loss:        0.278899
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.972580 loss:        0.079501
Test - acc:         0.917700 loss:        0.280658
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.973240 loss:        0.078399
Test - acc:         0.920900 loss:        0.261203
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.972980 loss:        0.080364
Test - acc:         0.918300 loss:        0.276465
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.972880 loss:        0.077869
Test - acc:         0.922700 loss:        0.266820
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.974500 loss:        0.074745
Test - acc:         0.918700 loss:        0.281030
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.974420 loss:        0.077133
Test - acc:         0.917200 loss:        0.280624
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.935240 loss:        0.183036
Test - acc:         0.903100 loss:        0.300559
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.948160 loss:        0.151073
Test - acc:         0.911900 loss:        0.293152
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.950500 loss:        0.141715
Test - acc:         0.910700 loss:        0.289828
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.952120 loss:        0.138097
Test - acc:         0.909800 loss:        0.284081
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.954720 loss:        0.133738
Test - acc:         0.910900 loss:        0.281066
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.954420 loss:        0.129516
Test - acc:         0.905000 loss:        0.303027
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.954720 loss:        0.129130
Test - acc:         0.905700 loss:        0.299341
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.955220 loss:        0.128430
Test - acc:         0.908100 loss:        0.301805
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.956520 loss:        0.124653
Test - acc:         0.907500 loss:        0.296516
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.958920 loss:        0.120161
Test - acc:         0.910400 loss:        0.279425
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.956720 loss:        0.123993
Test - acc:         0.913500 loss:        0.279603
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.957840 loss:        0.119780
Test - acc:         0.910400 loss:        0.287473
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.957700 loss:        0.123122
Test - acc:         0.911700 loss:        0.289929
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.957960 loss:        0.121510
Test - acc:         0.909700 loss:        0.296229
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.958920 loss:        0.119280
Test - acc:         0.914300 loss:        0.279494
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.956920 loss:        0.121559
Test - acc:         0.909700 loss:        0.293306
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.957860 loss:        0.119756
Test - acc:         0.908600 loss:        0.291964
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.960620 loss:        0.114631
Test - acc:         0.914800 loss:        0.273436
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.956160 loss:        0.121691
Test - acc:         0.911700 loss:        0.291566
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.959140 loss:        0.117193
Test - acc:         0.914800 loss:        0.279652
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.958260 loss:        0.117337
Test - acc:         0.908500 loss:        0.290633
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.958580 loss:        0.119746
Test - acc:         0.904600 loss:        0.314408
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.960700 loss:        0.112106
Test - acc:         0.912400 loss:        0.283962
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.961680 loss:        0.111738
Test - acc:         0.912900 loss:        0.287729
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.959100 loss:        0.118782
Test - acc:         0.908700 loss:        0.293123
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.959360 loss:        0.116889
Test - acc:         0.913300 loss:        0.286702
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.971180 loss:        0.087590
Test - acc:         0.927100 loss:        0.228734
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.976200 loss:        0.073800
Test - acc:         0.926600 loss:        0.226492
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.977880 loss:        0.068615
Test - acc:         0.928300 loss:        0.226345
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.978580 loss:        0.065669
Test - acc:         0.929100 loss:        0.224893
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.980480 loss:        0.062472
Test - acc:         0.930100 loss:        0.224893
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.980460 loss:        0.061399
Test - acc:         0.930900 loss:        0.227282
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.888800 loss:        0.322232
Test - acc:         0.891100 loss:        0.330088
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.911480 loss:        0.254004
Test - acc:         0.894200 loss:        0.314982
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.920780 loss:        0.233074
Test - acc:         0.901400 loss:        0.298552
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.925980 loss:        0.216092
Test - acc:         0.903600 loss:        0.289926
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.927380 loss:        0.208242
Test - acc:         0.903300 loss:        0.284058
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.929600 loss:        0.203836
Test - acc:         0.906100 loss:        0.283462
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.932580 loss:        0.194971
Test - acc:         0.906000 loss:        0.280935
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.933340 loss:        0.192971
Test - acc:         0.908700 loss:        0.275745
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.936780 loss:        0.185508
Test - acc:         0.908300 loss:        0.273607
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.936860 loss:        0.182319
Test - acc:         0.910100 loss:        0.272821
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.937900 loss:        0.178632
Test - acc:         0.911200 loss:        0.270118
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.939580 loss:        0.175119
Test - acc:         0.909000 loss:        0.273317
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.939880 loss:        0.174251
Test - acc:         0.909000 loss:        0.273893
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.940180 loss:        0.171003
Test - acc:         0.910900 loss:        0.270137
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.941900 loss:        0.167177
Test - acc:         0.910400 loss:        0.270217
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.944700 loss:        0.165609
Test - acc:         0.911900 loss:        0.269462
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.943980 loss:        0.162273
Test - acc:         0.912500 loss:        0.266626
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.943780 loss:        0.161972
Test - acc:         0.911800 loss:        0.266115
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.942880 loss:        0.164113
Test - acc:         0.911200 loss:        0.269597
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.944840 loss:        0.161616
Test - acc:         0.911200 loss:        0.270090
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.944480 loss:        0.159201
Test - acc:         0.911700 loss:        0.266860
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.946280 loss:        0.156598
Test - acc:         0.912900 loss:        0.266003
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.946540 loss:        0.154942
Test - acc:         0.911700 loss:        0.263283
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.945060 loss:        0.157118
Test - acc:         0.913500 loss:        0.262790
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.946640 loss:        0.153301
Test - acc:         0.914900 loss:        0.262424
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.947720 loss:        0.154842
Test - acc:         0.912700 loss:        0.267039
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.948600 loss:        0.151882
Test - acc:         0.914600 loss:        0.262368
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.947620 loss:        0.150528
Test - acc:         0.914700 loss:        0.261341
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.948060 loss:        0.150004
Test - acc:         0.914300 loss:        0.263180
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.948920 loss:        0.148721
Test - acc:         0.914000 loss:        0.261126
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.949180 loss:        0.149129
Test - acc:         0.915100 loss:        0.260875
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.948880 loss:        0.148546
Test - acc:         0.914300 loss:        0.260887
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.737900 loss:        0.758956
Test - acc:         0.797400 loss:        0.591531
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.804760 loss:        0.573067
Test - acc:         0.821700 loss:        0.527746
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.824160 loss:        0.517811
Test - acc:         0.832200 loss:        0.496521
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.832140 loss:        0.492330
Test - acc:         0.836100 loss:        0.482274
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.838440 loss:        0.474581
Test - acc:         0.835200 loss:        0.474814
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.842300 loss:        0.458513
Test - acc:         0.844700 loss:        0.458761
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.845080 loss:        0.449512
Test - acc:         0.844800 loss:        0.451340
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.849480 loss:        0.439100
Test - acc:         0.848600 loss:        0.448110
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.851640 loss:        0.430616
Test - acc:         0.849200 loss:        0.440742
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.854900 loss:        0.424163
Test - acc:         0.849300 loss:        0.439757
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.856820 loss:        0.419395
Test - acc:         0.853800 loss:        0.431926
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.856860 loss:        0.413301
Test - acc:         0.851700 loss:        0.429388
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.858520 loss:        0.409670
Test - acc:         0.853800 loss:        0.433150
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.861380 loss:        0.406333
Test - acc:         0.856100 loss:        0.424622
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.862660 loss:        0.397890
Test - acc:         0.857500 loss:        0.421038
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.863980 loss:        0.395708
Test - acc:         0.857100 loss:        0.417894
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.864140 loss:        0.394481
Test - acc:         0.860100 loss:        0.418100
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.866520 loss:        0.389018
Test - acc:         0.861400 loss:        0.415441
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.865800 loss:        0.388669
Test - acc:         0.859000 loss:        0.416789
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.866620 loss:        0.385364
Test - acc:         0.859000 loss:        0.413291
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.869680 loss:        0.379072
Test - acc:         0.859000 loss:        0.411221
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.869200 loss:        0.380467
Test - acc:         0.860100 loss:        0.408684
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.869360 loss:        0.381515
Test - acc:         0.863700 loss:        0.406863
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.871900 loss:        0.374611
Test - acc:         0.859500 loss:        0.411828
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.872700 loss:        0.371801
Test - acc:         0.862200 loss:        0.402842
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.871560 loss:        0.371023
Test - acc:         0.862200 loss:        0.408960
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.872360 loss:        0.370585
Test - acc:         0.863300 loss:        0.404469
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.871820 loss:        0.370665
Test - acc:         0.864500 loss:        0.404213
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.872440 loss:        0.368831
Test - acc:         0.862300 loss:        0.402335
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.873580 loss:        0.365519
Test - acc:         0.861300 loss:        0.406872
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.873780 loss:        0.366709
Test - acc:         0.866500 loss:        0.397415
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.874020 loss:        0.364066
Test - acc:         0.863900 loss:        0.403227
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.565720 loss:        1.218666
Test - acc:         0.648000 loss:        0.988687
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.655820 loss:        0.978099
Test - acc:         0.676500 loss:        0.897779
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.682120 loss:        0.911759
Test - acc:         0.696800 loss:        0.849193
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.695420 loss:        0.874822
Test - acc:         0.708800 loss:        0.825973
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.704600 loss:        0.845852
Test - acc:         0.715300 loss:        0.798852
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.711400 loss:        0.828104
Test - acc:         0.721400 loss:        0.792089
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.717560 loss:        0.812571
Test - acc:         0.722600 loss:        0.796016
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.720980 loss:        0.801846
Test - acc:         0.731300 loss:        0.765479
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.725480 loss:        0.792847
Test - acc:         0.734900 loss:        0.752958
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.729640 loss:        0.781820
Test - acc:         0.734600 loss:        0.746373
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.731960 loss:        0.771175
Test - acc:         0.740700 loss:        0.739117
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.735080 loss:        0.764965
Test - acc:         0.740300 loss:        0.740670
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.736680 loss:        0.761548
Test - acc:         0.740700 loss:        0.735211
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.737880 loss:        0.755971
Test - acc:         0.747400 loss:        0.721618
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.741360 loss:        0.747754
Test - acc:         0.750500 loss:        0.713974
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.743580 loss:        0.742124
Test - acc:         0.750400 loss:        0.714859
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.743760 loss:        0.739435
Test - acc:         0.754800 loss:        0.711053
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.742800 loss:        0.740389
Test - acc:         0.734300 loss:        0.765114
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.741660 loss:        0.746292
Test - acc:         0.742200 loss:        0.748075
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.743600 loss:        0.737264
Test - acc:         0.721900 loss:        0.802630
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.747660 loss:        0.732748
Test - acc:         0.749500 loss:        0.736851
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.747820 loss:        0.728020
Test - acc:         0.751700 loss:        0.729864
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.751300 loss:        0.721516
Test - acc:         0.754600 loss:        0.712007
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.751500 loss:        0.719376
Test - acc:         0.764000 loss:        0.688850
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.751400 loss:        0.716601
Test - acc:         0.754300 loss:        0.720755
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.750320 loss:        0.718833
Test - acc:         0.747400 loss:        0.748647
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.755060 loss:        0.707536
Test - acc:         0.762200 loss:        0.685029
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.753220 loss:        0.708634
Test - acc:         0.759900 loss:        0.686447
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.753520 loss:        0.709964
Test - acc:         0.760800 loss:        0.699714
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.756040 loss:        0.702118
Test - acc:         0.761200 loss:        0.687204
Sparsity :          0.9990
Wdecay :        0.000500
