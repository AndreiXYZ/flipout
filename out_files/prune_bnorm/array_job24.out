Running --prune_bnorm --model resnet18 --prune_criterion global_magnitude --seed 44 --prune_freq 70 --prune_rate 0.5 --comment=resnet18_crit=global_magnitude_pf=70_seed=44 --save_model=pre-finetune/resnet18_global_magnitude_pf70_s44 --logdir=criterion_experiment_prune_bnorm/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "global_magnitude",
    "prune_freq": 70,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": true,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_prune_bnorm/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_global_magnitude_pf70_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11169152
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11169152
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.326500 loss:        1.844461
Test - acc:         0.423100 loss:        1.566802
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.523060 loss:        1.311480
Test - acc:         0.573700 loss:        1.162164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.631560 loss:        1.037143
Test - acc:         0.605500 loss:        1.154360
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.690180 loss:        0.879254
Test - acc:         0.698600 loss:        0.870134
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.743000 loss:        0.739316
Test - acc:         0.741600 loss:        0.760918
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.775920 loss:        0.646051
Test - acc:         0.761800 loss:        0.712189
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.798040 loss:        0.583349
Test - acc:         0.699400 loss:        0.960318
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.808880 loss:        0.550098
Test - acc:         0.719800 loss:        0.895329
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.822400 loss:        0.517782
Test - acc:         0.798300 loss:        0.592408
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.828280 loss:        0.498775
Test - acc:         0.803500 loss:        0.578936
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.838380 loss:        0.470293
Test - acc:         0.802200 loss:        0.613507
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.840980 loss:        0.464587
Test - acc:         0.794200 loss:        0.611622
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.847140 loss:        0.446681
Test - acc:         0.812200 loss:        0.584225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.851420 loss:        0.433831
Test - acc:         0.745900 loss:        0.817999
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.852860 loss:        0.428770
Test - acc:         0.797000 loss:        0.603769
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.856840 loss:        0.417291
Test - acc:         0.793900 loss:        0.644432
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.862140 loss:        0.409136
Test - acc:         0.824900 loss:        0.529044
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.861320 loss:        0.401354
Test - acc:         0.815400 loss:        0.555978
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.863660 loss:        0.396668
Test - acc:         0.817300 loss:        0.531897
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.866860 loss:        0.390623
Test - acc:         0.770700 loss:        0.716770
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.868780 loss:        0.385096
Test - acc:         0.832300 loss:        0.500326
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870100 loss:        0.382342
Test - acc:         0.813700 loss:        0.540815
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.869500 loss:        0.384136
Test - acc:         0.837000 loss:        0.476577
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.872000 loss:        0.375453
Test - acc:         0.842100 loss:        0.462574
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.874400 loss:        0.369435
Test - acc:         0.822800 loss:        0.543520
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.873880 loss:        0.369378
Test - acc:         0.853400 loss:        0.432042
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.876600 loss:        0.359764
Test - acc:         0.824500 loss:        0.514843
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.877660 loss:        0.357625
Test - acc:         0.769200 loss:        0.763895
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.878520 loss:        0.356873
Test - acc:         0.815700 loss:        0.576099
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.878700 loss:        0.357575
Test - acc:         0.829500 loss:        0.503405
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.877880 loss:        0.360216
Test - acc:         0.826900 loss:        0.531613
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.881500 loss:        0.347606
Test - acc:         0.833600 loss:        0.533857
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.882500 loss:        0.345342
Test - acc:         0.836500 loss:        0.490184
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.882180 loss:        0.346714
Test - acc:         0.849900 loss:        0.453817
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.879700 loss:        0.350174
Test - acc:         0.835300 loss:        0.499439
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.882800 loss:        0.345306
Test - acc:         0.863100 loss:        0.403419
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.883300 loss:        0.343236
Test - acc:         0.805800 loss:        0.641906
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.884100 loss:        0.341043
Test - acc:         0.840200 loss:        0.479719
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.882040 loss:        0.345139
Test - acc:         0.836000 loss:        0.488765
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.884540 loss:        0.340708
Test - acc:         0.828600 loss:        0.535916
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.883400 loss:        0.339313
Test - acc:         0.836300 loss:        0.489561
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.887140 loss:        0.334405
Test - acc:         0.826400 loss:        0.517429
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.886760 loss:        0.331727
Test - acc:         0.789900 loss:        0.652802
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.886920 loss:        0.331868
Test - acc:         0.827400 loss:        0.525053
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.885080 loss:        0.334988
Test - acc:         0.842400 loss:        0.488818
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.888360 loss:        0.327419
Test - acc:         0.839900 loss:        0.499086
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.887520 loss:        0.329209
Test - acc:         0.863000 loss:        0.408118
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.888180 loss:        0.332175
Test - acc:         0.850200 loss:        0.455936
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.889440 loss:        0.328725
Test - acc:         0.866300 loss:        0.402327
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.887360 loss:        0.328156
Test - acc:         0.830100 loss:        0.535062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.888840 loss:        0.328968
Test - acc:         0.819300 loss:        0.546173
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.888720 loss:        0.328312
Test - acc:         0.848700 loss:        0.465342
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.887300 loss:        0.330598
Test - acc:         0.820000 loss:        0.558986
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.887840 loss:        0.328234
Test - acc:         0.852100 loss:        0.442695
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.887540 loss:        0.331423
Test - acc:         0.797900 loss:        0.652921
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.891100 loss:        0.322510
Test - acc:         0.833300 loss:        0.502770
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.888900 loss:        0.326872
Test - acc:         0.834100 loss:        0.514444
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.888100 loss:        0.329538
Test - acc:         0.835500 loss:        0.504231
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.890240 loss:        0.321956
Test - acc:         0.831700 loss:        0.507176
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.892060 loss:        0.319041
Test - acc:         0.866200 loss:        0.402339
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.893460 loss:        0.317760
Test - acc:         0.847000 loss:        0.479741
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.888860 loss:        0.326088
Test - acc:         0.793100 loss:        0.654160
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.889460 loss:        0.321849
Test - acc:         0.839900 loss:        0.487656
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.889700 loss:        0.320728
Test - acc:         0.861800 loss:        0.423869
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.892620 loss:        0.318095
Test - acc:         0.858800 loss:        0.427822
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.891260 loss:        0.317330
Test - acc:         0.813000 loss:        0.571739
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.893540 loss:        0.316204
Test - acc:         0.807500 loss:        0.600333
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.889000 loss:        0.324714
Test - acc:         0.858900 loss:        0.432573
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.887780 loss:        0.326101
Test - acc:         0.836300 loss:        0.498860
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.894400 loss:        0.310078
Test - acc:         0.812000 loss:        0.571622
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.900440 loss:        0.294180
Test - acc:         0.852000 loss:        0.461340
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.897940 loss:        0.297307
Test - acc:         0.829200 loss:        0.524315
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.897680 loss:        0.298681
Test - acc:         0.880200 loss:        0.354391
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.898620 loss:        0.297166
Test - acc:         0.851300 loss:        0.433680
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.896400 loss:        0.302878
Test - acc:         0.803400 loss:        0.628127
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.897360 loss:        0.297822
Test - acc:         0.857900 loss:        0.433391
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.899920 loss:        0.293208
Test - acc:         0.765500 loss:        0.764428
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.897160 loss:        0.301394
Test - acc:         0.855900 loss:        0.423342
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.899080 loss:        0.299403
Test - acc:         0.826900 loss:        0.544979
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.896060 loss:        0.302367
Test - acc:         0.848100 loss:        0.496202
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.897360 loss:        0.298881
Test - acc:         0.850200 loss:        0.443676
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.897740 loss:        0.297489
Test - acc:         0.853900 loss:        0.453120
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.897900 loss:        0.295516
Test - acc:         0.833000 loss:        0.558488
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.898820 loss:        0.298989
Test - acc:         0.883200 loss:        0.350469
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.898660 loss:        0.294835
Test - acc:         0.811600 loss:        0.575932
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.898160 loss:        0.299550
Test - acc:         0.849400 loss:        0.464038
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.900040 loss:        0.293828
Test - acc:         0.826000 loss:        0.530064
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.898140 loss:        0.297065
Test - acc:         0.864000 loss:        0.403343
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.900040 loss:        0.292816
Test - acc:         0.876500 loss:        0.369031
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.900020 loss:        0.293523
Test - acc:         0.804200 loss:        0.601014
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.900000 loss:        0.294777
Test - acc:         0.850700 loss:        0.453209
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.899240 loss:        0.298429
Test - acc:         0.826000 loss:        0.577060
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.899500 loss:        0.289913
Test - acc:         0.869200 loss:        0.398220
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.898700 loss:        0.296163
Test - acc:         0.879300 loss:        0.359548
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.899280 loss:        0.294375
Test - acc:         0.827600 loss:        0.512104
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.900400 loss:        0.294516
Test - acc:         0.831800 loss:        0.528552
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.898280 loss:        0.296588
Test - acc:         0.870200 loss:        0.386809
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.899300 loss:        0.292388
Test - acc:         0.857500 loss:        0.423090
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.901020 loss:        0.289559
Test - acc:         0.863200 loss:        0.410369
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.899680 loss:        0.295412
Test - acc:         0.771400 loss:        0.776554
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.898360 loss:        0.296027
Test - acc:         0.836700 loss:        0.519914
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.901240 loss:        0.288355
Test - acc:         0.853800 loss:        0.449697
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.901560 loss:        0.289601
Test - acc:         0.847500 loss:        0.462427
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.900080 loss:        0.293044
Test - acc:         0.840400 loss:        0.499031
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.898360 loss:        0.293453
Test - acc:         0.849900 loss:        0.474947
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.899400 loss:        0.294118
Test - acc:         0.854900 loss:        0.465402
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.901420 loss:        0.290515
Test - acc:         0.860400 loss:        0.421639
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.899960 loss:        0.292943
Test - acc:         0.843400 loss:        0.488884
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.901820 loss:        0.292847
Test - acc:         0.874700 loss:        0.372593
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.900800 loss:        0.292351
Test - acc:         0.872400 loss:        0.404179
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.900080 loss:        0.290924
Test - acc:         0.843100 loss:        0.467823
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.899080 loss:        0.292319
Test - acc:         0.860600 loss:        0.430089
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.901500 loss:        0.291471
Test - acc:         0.825800 loss:        0.571840
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.900440 loss:        0.294667
Test - acc:         0.818500 loss:        0.579786
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.900060 loss:        0.291245
Test - acc:         0.849500 loss:        0.455389
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.904200 loss:        0.284371
Test - acc:         0.857600 loss:        0.423869
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.901680 loss:        0.286248
Test - acc:         0.852000 loss:        0.455751
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.899100 loss:        0.295144
Test - acc:         0.865800 loss:        0.411958
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.901160 loss:        0.288139
Test - acc:         0.844700 loss:        0.464989
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.900440 loss:        0.292373
Test - acc:         0.871100 loss:        0.386719
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.902540 loss:        0.287154
Test - acc:         0.868600 loss:        0.404807
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.900720 loss:        0.291926
Test - acc:         0.866000 loss:        0.411753
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.897940 loss:        0.296448
Test - acc:         0.865100 loss:        0.414794
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.900460 loss:        0.291082
Test - acc:         0.862200 loss:        0.428898
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.899300 loss:        0.294102
Test - acc:         0.847000 loss:        0.479403
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.901280 loss:        0.287912
Test - acc:         0.855100 loss:        0.438201
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.900000 loss:        0.289209
Test - acc:         0.847000 loss:        0.484778
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.900940 loss:        0.290540
Test - acc:         0.852300 loss:        0.451445
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.903320 loss:        0.286450
Test - acc:         0.826700 loss:        0.532968
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.899540 loss:        0.295226
Test - acc:         0.826200 loss:        0.560269
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.900700 loss:        0.292779
Test - acc:         0.865100 loss:        0.401405
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.901200 loss:        0.288770
Test - acc:         0.821800 loss:        0.579599
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.903300 loss:        0.285882
Test - acc:         0.819600 loss:        0.564525
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.898240 loss:        0.293349
Test - acc:         0.847400 loss:        0.471708
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.901320 loss:        0.286956
Test - acc:         0.844200 loss:        0.481148
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.902180 loss:        0.286946
Test - acc:         0.832500 loss:        0.507793
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.900660 loss:        0.289824
Test - acc:         0.850800 loss:        0.447322
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.901800 loss:        0.290406
Test - acc:         0.861000 loss:        0.433116
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.900080 loss:        0.291443
Test - acc:         0.858800 loss:        0.437004
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.900260 loss:        0.290695
Test - acc:         0.856600 loss:        0.435926
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.912600 loss:        0.258833
Test - acc:         0.834800 loss:        0.516865
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.908480 loss:        0.262860
Test - acc:         0.858700 loss:        0.468071
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.909180 loss:        0.263856
Test - acc:         0.869600 loss:        0.380365
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.906660 loss:        0.269611
Test - acc:         0.835200 loss:        0.513011
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.908680 loss:        0.268379
Test - acc:         0.835400 loss:        0.497733
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.906760 loss:        0.271167
Test - acc:         0.852200 loss:        0.474808
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.907240 loss:        0.270016
Test - acc:         0.865600 loss:        0.413095
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.909660 loss:        0.264430
Test - acc:         0.842800 loss:        0.491661
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.907880 loss:        0.272872
Test - acc:         0.857200 loss:        0.431483
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.910880 loss:        0.264932
Test - acc:         0.854300 loss:        0.436679
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.954600 loss:        0.137879
Test - acc:         0.935100 loss:        0.189788
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.968760 loss:        0.095541
Test - acc:         0.937300 loss:        0.185205
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.973620 loss:        0.079914
Test - acc:         0.941300 loss:        0.174744
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.977900 loss:        0.068853
Test - acc:         0.942700 loss:        0.173293
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.979700 loss:        0.061050
Test - acc:         0.943800 loss:        0.172459
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.982880 loss:        0.054447
Test - acc:         0.943200 loss:        0.174825
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.984640 loss:        0.048074
Test - acc:         0.945100 loss:        0.173784
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.986980 loss:        0.042233
Test - acc:         0.945500 loss:        0.176173
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.988000 loss:        0.038483
Test - acc:         0.943100 loss:        0.182940
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.989940 loss:        0.033451
Test - acc:         0.942000 loss:        0.191391
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.990220 loss:        0.032689
Test - acc:         0.940800 loss:        0.197705
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.991120 loss:        0.030242
Test - acc:         0.944700 loss:        0.182114
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.991300 loss:        0.027969
Test - acc:         0.940900 loss:        0.198735
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.991420 loss:        0.027201
Test - acc:         0.942800 loss:        0.197659
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.991900 loss:        0.025086
Test - acc:         0.944600 loss:        0.197404
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.991780 loss:        0.025957
Test - acc:         0.944000 loss:        0.190922
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.992500 loss:        0.024845
Test - acc:         0.945600 loss:        0.192343
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.992780 loss:        0.023953
Test - acc:         0.944800 loss:        0.201419
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.993300 loss:        0.021845
Test - acc:         0.943600 loss:        0.199858
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.993680 loss:        0.020771
Test - acc:         0.944000 loss:        0.198948
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.992800 loss:        0.023754
Test - acc:         0.945900 loss:        0.194091
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.993720 loss:        0.022265
Test - acc:         0.938300 loss:        0.220138
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.994360 loss:        0.020439
Test - acc:         0.944700 loss:        0.208210
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.994340 loss:        0.019851
Test - acc:         0.942600 loss:        0.204660
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.992400 loss:        0.024345
Test - acc:         0.942600 loss:        0.207500
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.992060 loss:        0.026489
Test - acc:         0.939300 loss:        0.226630
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.992360 loss:        0.024552
Test - acc:         0.938800 loss:        0.221053
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.992140 loss:        0.025058
Test - acc:         0.938300 loss:        0.223191
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.992280 loss:        0.025394
Test - acc:         0.942000 loss:        0.217259
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.990320 loss:        0.031228
Test - acc:         0.937700 loss:        0.220358
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.990440 loss:        0.029731
Test - acc:         0.930800 loss:        0.244173
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.991040 loss:        0.029456
Test - acc:         0.937300 loss:        0.218212
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.991820 loss:        0.027253
Test - acc:         0.936600 loss:        0.218217
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.989080 loss:        0.032667
Test - acc:         0.934700 loss:        0.233130
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.989180 loss:        0.033720
Test - acc:         0.929800 loss:        0.240028
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.989340 loss:        0.033445
Test - acc:         0.934200 loss:        0.229784
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.989780 loss:        0.031974
Test - acc:         0.933900 loss:        0.235300
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.989740 loss:        0.032523
Test - acc:         0.937500 loss:        0.230905
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.989100 loss:        0.035235
Test - acc:         0.929800 loss:        0.254430
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.988180 loss:        0.036148
Test - acc:         0.931500 loss:        0.242414
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.990180 loss:        0.032116
Test - acc:         0.933800 loss:        0.239706
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.988420 loss:        0.035902
Test - acc:         0.936300 loss:        0.220491
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.987920 loss:        0.038299
Test - acc:         0.931500 loss:        0.231020
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.988600 loss:        0.036768
Test - acc:         0.930700 loss:        0.251790
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.986840 loss:        0.039517
Test - acc:         0.930000 loss:        0.245927
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.985980 loss:        0.042287
Test - acc:         0.935900 loss:        0.227877
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.987860 loss:        0.037210
Test - acc:         0.934100 loss:        0.230171
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.987340 loss:        0.037972
Test - acc:         0.926600 loss:        0.270218
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.986520 loss:        0.040309
Test - acc:         0.933300 loss:        0.236600
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.986140 loss:        0.042735
Test - acc:         0.924900 loss:        0.260878
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.986020 loss:        0.042928
Test - acc:         0.927600 loss:        0.269408
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.985780 loss:        0.043453
Test - acc:         0.937000 loss:        0.230501
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.986900 loss:        0.040357
Test - acc:         0.931100 loss:        0.242854
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.986120 loss:        0.042591
Test - acc:         0.926400 loss:        0.254381
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.988300 loss:        0.038192
Test - acc:         0.919000 loss:        0.297888
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.986960 loss:        0.040337
Test - acc:         0.931000 loss:        0.240278
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.987000 loss:        0.039486
Test - acc:         0.926700 loss:        0.258657
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.987660 loss:        0.038421
Test - acc:         0.939100 loss:        0.220146
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.987480 loss:        0.038102
Test - acc:         0.931600 loss:        0.236348
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.987640 loss:        0.039086
Test - acc:         0.927700 loss:        0.253313
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.991380 loss:        0.028817
Test - acc:         0.938200 loss:        0.215251
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.993400 loss:        0.023342
Test - acc:         0.944400 loss:        0.200379
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.994120 loss:        0.021244
Test - acc:         0.940500 loss:        0.205804
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.993380 loss:        0.022531
Test - acc:         0.936500 loss:        0.231051
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.993800 loss:        0.020541
Test - acc:         0.937200 loss:        0.219976
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.992860 loss:        0.022962
Test - acc:         0.938500 loss:        0.229685
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.992480 loss:        0.025436
Test - acc:         0.933300 loss:        0.245378
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.992660 loss:        0.024720
Test - acc:         0.934900 loss:        0.235905
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.992060 loss:        0.026533
Test - acc:         0.937500 loss:        0.222727
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.991580 loss:        0.027490
Test - acc:         0.940800 loss:        0.215547
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.991920 loss:        0.026010
Test - acc:         0.937800 loss:        0.229233
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.990300 loss:        0.030678
Test - acc:         0.932500 loss:        0.252036
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.990800 loss:        0.028862
Test - acc:         0.936200 loss:        0.236900
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.991240 loss:        0.028781
Test - acc:         0.926000 loss:        0.274161
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.990600 loss:        0.029928
Test - acc:         0.937500 loss:        0.232284
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.990760 loss:        0.029758
Test - acc:         0.934200 loss:        0.239398
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.989420 loss:        0.033967
Test - acc:         0.934200 loss:        0.235897
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.989920 loss:        0.031906
Test - acc:         0.932600 loss:        0.242860
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.989420 loss:        0.032826
Test - acc:         0.930400 loss:        0.245604
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.988340 loss:        0.037059
Test - acc:         0.933200 loss:        0.237565
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.990440 loss:        0.031804
Test - acc:         0.936300 loss:        0.233974
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.991280 loss:        0.028013
Test - acc:         0.932800 loss:        0.239418
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.989740 loss:        0.032715
Test - acc:         0.934800 loss:        0.232583
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.988100 loss:        0.034965
Test - acc:         0.932100 loss:        0.238040
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.989880 loss:        0.032808
Test - acc:         0.933700 loss:        0.247587
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.989860 loss:        0.031922
Test - acc:         0.936000 loss:        0.241149
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.989880 loss:        0.030824
Test - acc:         0.924600 loss:        0.273006
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.987380 loss:        0.038270
Test - acc:         0.933400 loss:        0.236972
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.991000 loss:        0.029156
Test - acc:         0.926800 loss:        0.265342
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.988980 loss:        0.035694
Test - acc:         0.934500 loss:        0.234260
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.990840 loss:        0.030659
Test - acc:         0.930000 loss:        0.256651
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.989160 loss:        0.033778
Test - acc:         0.929000 loss:        0.264939
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.990420 loss:        0.030664
Test - acc:         0.926800 loss:        0.263214
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.989120 loss:        0.033526
Test - acc:         0.930100 loss:        0.265473
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.990740 loss:        0.029046
Test - acc:         0.929300 loss:        0.271832
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.989660 loss:        0.033071
Test - acc:         0.931600 loss:        0.258615
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.990480 loss:        0.030235
Test - acc:         0.930800 loss:        0.245445
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.988480 loss:        0.036986
Test - acc:         0.928800 loss:        0.247381
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.988960 loss:        0.034406
Test - acc:         0.931700 loss:        0.248054
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.989600 loss:        0.032030
Test - acc:         0.929700 loss:        0.262027
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.995780 loss:        0.016655
Test - acc:         0.946100 loss:        0.187267
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.997660 loss:        0.009998
Test - acc:         0.948000 loss:        0.183744
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.998400 loss:        0.008197
Test - acc:         0.948400 loss:        0.179094
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.006859
Test - acc:         0.949300 loss:        0.177845
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.005438
Test - acc:         0.949500 loss:        0.177214
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.999160 loss:        0.005731
Test - acc:         0.949800 loss:        0.176395
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.999280 loss:        0.004890
Test - acc:         0.951100 loss:        0.177098
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.004606
Test - acc:         0.951100 loss:        0.175802
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.999420 loss:        0.004432
Test - acc:         0.952200 loss:        0.176595
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.004220
Test - acc:         0.953100 loss:        0.176429
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.003872
Test - acc:         0.952200 loss:        0.176005
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.004166
Test - acc:         0.952600 loss:        0.174508
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.003705
Test - acc:         0.951300 loss:        0.174610
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.003746
Test - acc:         0.952000 loss:        0.172557
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.003300
Test - acc:         0.952700 loss:        0.171307
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.003137
Test - acc:         0.953100 loss:        0.169765
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.003239
Test - acc:         0.953200 loss:        0.171489
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003368
Test - acc:         0.954300 loss:        0.171916
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.003254
Test - acc:         0.953300 loss:        0.170789
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999860 loss:        0.002897
Test - acc:         0.952900 loss:        0.171887
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.002908
Test - acc:         0.953800 loss:        0.169757
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999920 loss:        0.002794
Test - acc:         0.953300 loss:        0.170041
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.002906
Test - acc:         0.953300 loss:        0.171109
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.002990
Test - acc:         0.954200 loss:        0.168696
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002877
Test - acc:         0.955100 loss:        0.168846
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.002929
Test - acc:         0.955000 loss:        0.169035
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999800 loss:        0.002911
Test - acc:         0.953500 loss:        0.169640
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002671
Test - acc:         0.954300 loss:        0.168970
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.002865
Test - acc:         0.954400 loss:        0.169667
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999880 loss:        0.002669
Test - acc:         0.955000 loss:        0.168401
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.008158
Test - acc:         0.951900 loss:        0.169891
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.999380 loss:        0.006939
Test - acc:         0.952400 loss:        0.170156
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.006707
Test - acc:         0.951500 loss:        0.170212
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.999380 loss:        0.006246
Test - acc:         0.952000 loss:        0.169770
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.005787
Test - acc:         0.953400 loss:        0.169591
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.005352
Test - acc:         0.952400 loss:        0.169834
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.005281
Test - acc:         0.951600 loss:        0.170595
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 288 ==========
