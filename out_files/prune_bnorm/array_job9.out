Running --prune_bnorm --model resnet18 --noise --prune_criterion weight_div_flips --seed 42 --prune_freq 32 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=32_seed=42 --save_model=pre-finetune/resnet18_weight_div_flips_pf32_s42 --logdir=criterion_experiment_prune_bnorm/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": true,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_prune_bnorm/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf32_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11169152
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11169152
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.319760 loss:        2.027701
Test - acc:         0.373600 loss:        1.709656
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.483520 loss:        1.421054
Test - acc:         0.526700 loss:        1.263736
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.596320 loss:        1.129661
Test - acc:         0.595600 loss:        1.171379
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.673280 loss:        0.927933
Test - acc:         0.610100 loss:        1.134739
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.723460 loss:        0.790148
Test - acc:         0.705200 loss:        0.872062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.766400 loss:        0.675023
Test - acc:         0.746900 loss:        0.720768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.790000 loss:        0.608257
Test - acc:         0.766300 loss:        0.692052
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.805140 loss:        0.568890
Test - acc:         0.777000 loss:        0.661023
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.812440 loss:        0.540974
Test - acc:         0.748500 loss:        0.765813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.823580 loss:        0.513555
Test - acc:         0.742700 loss:        0.756514
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.831620 loss:        0.490783
Test - acc:         0.778800 loss:        0.671311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.834560 loss:        0.478143
Test - acc:         0.793000 loss:        0.626211
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.841480 loss:        0.462775
Test - acc:         0.792800 loss:        0.604948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838760 loss:        0.468699
Test - acc:         0.789100 loss:        0.624176
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844780 loss:        0.450396
Test - acc:         0.816100 loss:        0.559841
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.846280 loss:        0.448807
Test - acc:         0.764600 loss:        0.719807
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.850540 loss:        0.438978
Test - acc:         0.818300 loss:        0.530640
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.850840 loss:        0.433470
Test - acc:         0.796600 loss:        0.604748
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.853620 loss:        0.425959
Test - acc:         0.841700 loss:        0.488544
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.854620 loss:        0.421329
Test - acc:         0.795500 loss:        0.623324
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.859220 loss:        0.417579
Test - acc:         0.823000 loss:        0.516899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.856440 loss:        0.420470
Test - acc:         0.758600 loss:        0.712926
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.858640 loss:        0.411732
Test - acc:         0.793800 loss:        0.625225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.858100 loss:        0.413444
Test - acc:         0.793100 loss:        0.628817
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.862000 loss:        0.407803
Test - acc:         0.819100 loss:        0.573117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.861660 loss:        0.407108
Test - acc:         0.805500 loss:        0.599522
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.864180 loss:        0.397832
Test - acc:         0.836000 loss:        0.468893
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.863120 loss:        0.402166
Test - acc:         0.833700 loss:        0.486745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.863140 loss:        0.396906
Test - acc:         0.855500 loss:        0.423829
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.867600 loss:        0.394886
Test - acc:         0.810100 loss:        0.579276
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.865620 loss:        0.394207
Test - acc:         0.853400 loss:        0.434255
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.866820 loss:        0.393427
Test - acc:         0.698200 loss:        1.116456
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.878220 loss:        0.357416
Test - acc:         0.831900 loss:        0.502272
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.875560 loss:        0.368774
Test - acc:         0.845100 loss:        0.455459
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.874760 loss:        0.362451
Test - acc:         0.798800 loss:        0.608268
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.874420 loss:        0.366105
Test - acc:         0.848100 loss:        0.475141
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.873160 loss:        0.371459
Test - acc:         0.847300 loss:        0.449282
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.871540 loss:        0.372994
Test - acc:         0.818000 loss:        0.582462
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.874240 loss:        0.372430
Test - acc:         0.801800 loss:        0.619067
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.874620 loss:        0.364006
Test - acc:         0.857900 loss:        0.422959
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.873580 loss:        0.370467
Test - acc:         0.765100 loss:        0.776512
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.876280 loss:        0.361660
Test - acc:         0.779100 loss:        0.666912
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.876680 loss:        0.358936
Test - acc:         0.828500 loss:        0.521778
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.876120 loss:        0.361558
Test - acc:         0.852500 loss:        0.447677
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.873540 loss:        0.368010
Test - acc:         0.835100 loss:        0.481274
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.876480 loss:        0.365464
Test - acc:         0.786700 loss:        0.655263
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.875620 loss:        0.365184
Test - acc:         0.838600 loss:        0.472536
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.877120 loss:        0.361658
Test - acc:         0.775000 loss:        0.673582
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.877240 loss:        0.359813
Test - acc:         0.769100 loss:        0.809166
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.872680 loss:        0.371894
Test - acc:         0.826800 loss:        0.510527
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.877020 loss:        0.363342
Test - acc:         0.816700 loss:        0.609423
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.876220 loss:        0.365504
Test - acc:         0.836200 loss:        0.486450
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.874840 loss:        0.366928
Test - acc:         0.847500 loss:        0.457966
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.878620 loss:        0.354859
Test - acc:         0.823900 loss:        0.523899
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.875500 loss:        0.361523
Test - acc:         0.844300 loss:        0.461119
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.872880 loss:        0.370940
Test - acc:         0.816700 loss:        0.562532
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.876060 loss:        0.361048
Test - acc:         0.843900 loss:        0.449658
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.878220 loss:        0.358494
Test - acc:         0.836800 loss:        0.497310
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.876440 loss:        0.358869
Test - acc:         0.803800 loss:        0.597709
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.875160 loss:        0.366139
Test - acc:         0.838500 loss:        0.496413
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.877760 loss:        0.358679
Test - acc:         0.832000 loss:        0.515312
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.874200 loss:        0.368250
Test - acc:         0.768500 loss:        0.737470
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.877620 loss:        0.358987
Test - acc:         0.852500 loss:        0.431070
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.874660 loss:        0.365897
Test - acc:         0.817300 loss:        0.557571
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.887700 loss:        0.330674
Test - acc:         0.837600 loss:        0.492993
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.888480 loss:        0.325892
Test - acc:         0.841200 loss:        0.463625
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.884620 loss:        0.335947
Test - acc:         0.812200 loss:        0.593527
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.882660 loss:        0.343365
Test - acc:         0.809400 loss:        0.599454
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.884080 loss:        0.338777
Test - acc:         0.840800 loss:        0.510603
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.882080 loss:        0.342281
Test - acc:         0.836000 loss:        0.509050
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.883840 loss:        0.339723
Test - acc:         0.831200 loss:        0.508035
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.881320 loss:        0.343464
Test - acc:         0.796900 loss:        0.603874
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.881300 loss:        0.343963
Test - acc:         0.854500 loss:        0.433659
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.883540 loss:        0.343780
Test - acc:         0.776400 loss:        0.701778
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.885700 loss:        0.334114
Test - acc:         0.835100 loss:        0.512245
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.882760 loss:        0.340309
Test - acc:         0.834900 loss:        0.499102
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.882960 loss:        0.341883
Test - acc:         0.791700 loss:        0.681077
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.883760 loss:        0.339387
Test - acc:         0.843500 loss:        0.453043
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.882500 loss:        0.343512
Test - acc:         0.840500 loss:        0.466194
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.884980 loss:        0.336009
Test - acc:         0.855900 loss:        0.437608
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.882720 loss:        0.340678
Test - acc:         0.774700 loss:        0.798082
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.883740 loss:        0.338536
Test - acc:         0.827800 loss:        0.510286
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.883960 loss:        0.337641
Test - acc:         0.832200 loss:        0.516848
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.884960 loss:        0.334170
Test - acc:         0.809800 loss:        0.594744
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.884240 loss:        0.339825
Test - acc:         0.849700 loss:        0.450782
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.881640 loss:        0.345404
Test - acc:         0.834900 loss:        0.495574
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.884080 loss:        0.338697
Test - acc:         0.824600 loss:        0.520666
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.884720 loss:        0.336872
Test - acc:         0.754800 loss:        0.740031
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.881380 loss:        0.345171
Test - acc:         0.818400 loss:        0.561359
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.884160 loss:        0.334180
Test - acc:         0.818300 loss:        0.557101
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.882000 loss:        0.342732
Test - acc:         0.850600 loss:        0.448805
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.885340 loss:        0.337317
Test - acc:         0.856900 loss:        0.429645
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.885420 loss:        0.336242
Test - acc:         0.845700 loss:        0.468912
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.884780 loss:        0.339767
Test - acc:         0.834400 loss:        0.503024
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.882820 loss:        0.340831
Test - acc:         0.784000 loss:        0.690515
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.882240 loss:        0.341501
Test - acc:         0.827400 loss:        0.543222
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.898700 loss:        0.300769
Test - acc:         0.815100 loss:        0.610018
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.892220 loss:        0.312097
Test - acc:         0.858000 loss:        0.443179
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.893100 loss:        0.310796
Test - acc:         0.864000 loss:        0.415648
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.890340 loss:        0.317452
Test - acc:         0.849700 loss:        0.456181
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.893520 loss:        0.312553
Test - acc:         0.861600 loss:        0.419586
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.891260 loss:        0.315430
Test - acc:         0.851900 loss:        0.445214
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.893700 loss:        0.312611
Test - acc:         0.841200 loss:        0.476964
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.889640 loss:        0.320399
Test - acc:         0.865200 loss:        0.399911
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.890840 loss:        0.316780
Test - acc:         0.866600 loss:        0.405825
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.890480 loss:        0.319591
Test - acc:         0.832000 loss:        0.557931
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.892800 loss:        0.314330
Test - acc:         0.852500 loss:        0.441874
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.891220 loss:        0.317253
Test - acc:         0.823500 loss:        0.604813
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.891560 loss:        0.315371
Test - acc:         0.820600 loss:        0.602840
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.891960 loss:        0.315138
Test - acc:         0.858200 loss:        0.441970
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.890080 loss:        0.315792
Test - acc:         0.781000 loss:        0.727118
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.891080 loss:        0.314942
Test - acc:         0.805100 loss:        0.588297
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.890340 loss:        0.321090
Test - acc:         0.850500 loss:        0.441794
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.890280 loss:        0.315809
Test - acc:         0.856800 loss:        0.419217
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.892480 loss:        0.317032
Test - acc:         0.858600 loss:        0.416819
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.890920 loss:        0.318519
Test - acc:         0.841600 loss:        0.504536
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.893540 loss:        0.313240
Test - acc:         0.853000 loss:        0.443146
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.890960 loss:        0.314338
Test - acc:         0.814700 loss:        0.577059
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.891220 loss:        0.316819
Test - acc:         0.804000 loss:        0.629565
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.890300 loss:        0.319625
Test - acc:         0.866600 loss:        0.397994
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.892860 loss:        0.314521
Test - acc:         0.862700 loss:        0.407892
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.892020 loss:        0.316952
Test - acc:         0.738800 loss:        0.875378
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.891840 loss:        0.314281
Test - acc:         0.864700 loss:        0.410635
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.888780 loss:        0.319923
Test - acc:         0.859200 loss:        0.426010
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.890920 loss:        0.318332
Test - acc:         0.857800 loss:        0.436984
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.888980 loss:        0.321691
Test - acc:         0.846700 loss:        0.453995
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.891040 loss:        0.314900
Test - acc:         0.819500 loss:        0.578383
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.892140 loss:        0.315858
Test - acc:         0.808700 loss:        0.587909
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.903120 loss:        0.279834
Test - acc:         0.877000 loss:        0.377376
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.902680 loss:        0.287187
Test - acc:         0.873200 loss:        0.379068
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.901440 loss:        0.289187
Test - acc:         0.832000 loss:        0.496084
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.896060 loss:        0.301149
Test - acc:         0.843300 loss:        0.495358
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.896040 loss:        0.298318
Test - acc:         0.871900 loss:        0.378405
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.898000 loss:        0.294401
Test - acc:         0.851700 loss:        0.452477
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.898080 loss:        0.297957
Test - acc:         0.876800 loss:        0.368163
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.898480 loss:        0.293643
Test - acc:         0.857200 loss:        0.421485
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.899280 loss:        0.295124
Test - acc:         0.839100 loss:        0.484570
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.899000 loss:        0.297418
Test - acc:         0.845800 loss:        0.432027
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.896900 loss:        0.296599
Test - acc:         0.819000 loss:        0.579357
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.895780 loss:        0.299243
Test - acc:         0.864200 loss:        0.405576
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.897620 loss:        0.300480
Test - acc:         0.841600 loss:        0.510734
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.899380 loss:        0.295397
Test - acc:         0.882600 loss:        0.371060
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.899220 loss:        0.294809
Test - acc:         0.838700 loss:        0.495403
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.899040 loss:        0.291425
Test - acc:         0.872900 loss:        0.384087
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.898740 loss:        0.295651
Test - acc:         0.860200 loss:        0.441617
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.897920 loss:        0.299782
Test - acc:         0.865000 loss:        0.408347
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.897180 loss:        0.299136
Test - acc:         0.843100 loss:        0.464273
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.899380 loss:        0.295057
Test - acc:         0.858500 loss:        0.423325
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.897920 loss:        0.296628
Test - acc:         0.812000 loss:        0.603593
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.898840 loss:        0.293855
Test - acc:         0.846100 loss:        0.480563
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.940380 loss:        0.173044
Test - acc:         0.921500 loss:        0.228774
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.954340 loss:        0.137099
Test - acc:         0.926200 loss:        0.220780
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.958480 loss:        0.122594
Test - acc:         0.925900 loss:        0.218679
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.963240 loss:        0.108335
Test - acc:         0.927900 loss:        0.216347
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.965400 loss:        0.104893
Test - acc:         0.927400 loss:        0.216203
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.968380 loss:        0.094988
Test - acc:         0.930400 loss:        0.213234
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.970440 loss:        0.089244
Test - acc:         0.929000 loss:        0.212593
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.971560 loss:        0.084784
Test - acc:         0.931000 loss:        0.211098
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.973180 loss:        0.081465
Test - acc:         0.930700 loss:        0.222418
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.974620 loss:        0.075752
Test - acc:         0.931600 loss:        0.215410
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.974600 loss:        0.076348
Test - acc:         0.930900 loss:        0.219136
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.976020 loss:        0.072739
Test - acc:         0.929200 loss:        0.220928
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.977140 loss:        0.068851
Test - acc:         0.930400 loss:        0.217379
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.977420 loss:        0.067100
Test - acc:         0.930600 loss:        0.221193
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.979640 loss:        0.062660
Test - acc:         0.929800 loss:        0.218930
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.978880 loss:        0.065057
Test - acc:         0.928200 loss:        0.229248
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.979980 loss:        0.061275
Test - acc:         0.927100 loss:        0.236662
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.979280 loss:        0.061101
Test - acc:         0.930800 loss:        0.229160
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.980240 loss:        0.059485
Test - acc:         0.929700 loss:        0.231803
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.979540 loss:        0.060649
Test - acc:         0.928400 loss:        0.230038
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.980240 loss:        0.058237
Test - acc:         0.929100 loss:        0.233198
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.980040 loss:        0.057158
Test - acc:         0.925800 loss:        0.239517
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.980640 loss:        0.057985
Test - acc:         0.925800 loss:        0.243119
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.980760 loss:        0.055926
Test - acc:         0.931300 loss:        0.231081
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.981580 loss:        0.056694
Test - acc:         0.927300 loss:        0.243275
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.981320 loss:        0.054520
Test - acc:         0.928000 loss:        0.243229
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.980660 loss:        0.056824
Test - acc:         0.923900 loss:        0.248313
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.979340 loss:        0.059322
Test - acc:         0.927600 loss:        0.239286
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.981900 loss:        0.055165
Test - acc:         0.923000 loss:        0.267800
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.979960 loss:        0.057675
Test - acc:         0.924600 loss:        0.254986
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.979960 loss:        0.059043
Test - acc:         0.922400 loss:        0.265212
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.979240 loss:        0.059793
Test - acc:         0.925400 loss:        0.255129
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.979640 loss:        0.062489
Test - acc:         0.924100 loss:        0.246934
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.978820 loss:        0.061155
Test - acc:         0.927000 loss:        0.249534
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.979920 loss:        0.059179
Test - acc:         0.925100 loss:        0.242081
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.980180 loss:        0.060121
Test - acc:         0.923300 loss:        0.257660
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.978120 loss:        0.063987
Test - acc:         0.923700 loss:        0.251482
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.979120 loss:        0.062125
Test - acc:         0.928500 loss:        0.236376
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.978060 loss:        0.063659
Test - acc:         0.921100 loss:        0.254375
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.977040 loss:        0.064724
Test - acc:         0.920200 loss:        0.283837
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.976920 loss:        0.067539
Test - acc:         0.922700 loss:        0.265780
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.977320 loss:        0.065847
Test - acc:         0.923000 loss:        0.269913
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.963880 loss:        0.104475
Test - acc:         0.918100 loss:        0.266378
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.970540 loss:        0.085450
Test - acc:         0.919500 loss:        0.266403
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.971020 loss:        0.085440
Test - acc:         0.921300 loss:        0.264706
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.973220 loss:        0.078618
Test - acc:         0.919100 loss:        0.271034
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.972880 loss:        0.079344
Test - acc:         0.918200 loss:        0.278836
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.973800 loss:        0.077002
Test - acc:         0.915300 loss:        0.278587
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.974580 loss:        0.074564
Test - acc:         0.920500 loss:        0.261416
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.973140 loss:        0.078693
Test - acc:         0.921500 loss:        0.251329
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.975280 loss:        0.073003
Test - acc:         0.924000 loss:        0.252110
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.973860 loss:        0.075471
Test - acc:         0.917700 loss:        0.282706
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.975080 loss:        0.073099
Test - acc:         0.920500 loss:        0.273728
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.975320 loss:        0.072712
Test - acc:         0.921800 loss:        0.270405
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.971580 loss:        0.081071
Test - acc:         0.921100 loss:        0.262436
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.973160 loss:        0.078131
Test - acc:         0.917800 loss:        0.268740
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.974540 loss:        0.075568
Test - acc:         0.918200 loss:        0.274280
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.973540 loss:        0.076624
Test - acc:         0.914000 loss:        0.295202
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.973660 loss:        0.076652
Test - acc:         0.914700 loss:        0.295215
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.972980 loss:        0.078076
Test - acc:         0.924000 loss:        0.265432
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.973620 loss:        0.076666
Test - acc:         0.919200 loss:        0.271544
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.973880 loss:        0.076223
Test - acc:         0.918500 loss:        0.271948
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.975160 loss:        0.074982
Test - acc:         0.915200 loss:        0.283483
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.973220 loss:        0.078198
Test - acc:         0.921500 loss:        0.266153
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.973980 loss:        0.076519
Test - acc:         0.917500 loss:        0.284778
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.974240 loss:        0.075574
Test - acc:         0.913900 loss:        0.296122
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.973960 loss:        0.076704
Test - acc:         0.922500 loss:        0.260851
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.974000 loss:        0.075326
Test - acc:         0.919900 loss:        0.269796
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.975720 loss:        0.074619
Test - acc:         0.919500 loss:        0.272899
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.972880 loss:        0.076868
Test - acc:         0.921200 loss:        0.279321
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.974340 loss:        0.074239
Test - acc:         0.916500 loss:        0.283633
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.974460 loss:        0.074434
Test - acc:         0.924700 loss:        0.259911
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.972720 loss:        0.079682
Test - acc:         0.918600 loss:        0.281259
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.974240 loss:        0.074964
Test - acc:         0.917500 loss:        0.270480
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.939020 loss:        0.176444
Test - acc:         0.904800 loss:        0.298233
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.948200 loss:        0.150018
Test - acc:         0.908000 loss:        0.287937
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.949640 loss:        0.143694
Test - acc:         0.906500 loss:        0.294761
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.950420 loss:        0.140311
Test - acc:         0.911500 loss:        0.282097
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.955000 loss:        0.130945
Test - acc:         0.905600 loss:        0.294586
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.953980 loss:        0.132143
Test - acc:         0.903600 loss:        0.297831
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.955660 loss:        0.127789
Test - acc:         0.909800 loss:        0.284000
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.956800 loss:        0.122420
Test - acc:         0.911900 loss:        0.277518
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.956740 loss:        0.124397
Test - acc:         0.909300 loss:        0.285474
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.957420 loss:        0.122682
Test - acc:         0.913600 loss:        0.272272
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.956500 loss:        0.124247
Test - acc:         0.915100 loss:        0.272072
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.956820 loss:        0.124624
Test - acc:         0.915700 loss:        0.271284
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.957120 loss:        0.122271
Test - acc:         0.908100 loss:        0.289796
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.958580 loss:        0.118398
Test - acc:         0.909200 loss:        0.292363
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.958520 loss:        0.117873
Test - acc:         0.910100 loss:        0.281751
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.959560 loss:        0.115680
Test - acc:         0.910900 loss:        0.292469
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.958500 loss:        0.119431
Test - acc:         0.912900 loss:        0.286641
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.959900 loss:        0.116908
Test - acc:         0.914700 loss:        0.280224
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.959840 loss:        0.116244
Test - acc:         0.914800 loss:        0.273641
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.960220 loss:        0.114610
Test - acc:         0.908500 loss:        0.296484
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.959360 loss:        0.118810
Test - acc:         0.909800 loss:        0.290523
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.959640 loss:        0.116786
Test - acc:         0.912800 loss:        0.289395
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.958720 loss:        0.117558
Test - acc:         0.911100 loss:        0.274286
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.960880 loss:        0.114996
Test - acc:         0.914700 loss:        0.285797
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.960380 loss:        0.116405
Test - acc:         0.909000 loss:        0.295959
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.958680 loss:        0.117590
Test - acc:         0.912700 loss:        0.288190
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.971040 loss:        0.086740
Test - acc:         0.925300 loss:        0.233828
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.976380 loss:        0.073243
Test - acc:         0.926800 loss:        0.231951
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.977640 loss:        0.069754
Test - acc:         0.927600 loss:        0.232763
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.979680 loss:        0.064539
Test - acc:         0.928000 loss:        0.232054
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.980500 loss:        0.062988
Test - acc:         0.927800 loss:        0.231418
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.981500 loss:        0.059846
Test - acc:         0.926700 loss:        0.232443
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.887480 loss:        0.326873
Test - acc:         0.880800 loss:        0.350565
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.912520 loss:        0.253656
Test - acc:         0.889800 loss:        0.327918
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.921300 loss:        0.230406
Test - acc:         0.894700 loss:        0.315930
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.926740 loss:        0.214091
Test - acc:         0.897500 loss:        0.307511
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.927900 loss:        0.209268
Test - acc:         0.900000 loss:        0.298800
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.931080 loss:        0.199910
Test - acc:         0.900900 loss:        0.295985
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.933660 loss:        0.194196
Test - acc:         0.904500 loss:        0.289535
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.934180 loss:        0.189810
Test - acc:         0.902200 loss:        0.288737
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.936560 loss:        0.184723
Test - acc:         0.903200 loss:        0.286760
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.937700 loss:        0.179803
Test - acc:         0.904900 loss:        0.287112
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.938960 loss:        0.179755
Test - acc:         0.907600 loss:        0.283333
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.941300 loss:        0.176935
Test - acc:         0.906100 loss:        0.281664
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.940580 loss:        0.173373
Test - acc:         0.906900 loss:        0.282474
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.941120 loss:        0.170086
Test - acc:         0.908600 loss:        0.280454
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.943060 loss:        0.166964
Test - acc:         0.907100 loss:        0.280244
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.941480 loss:        0.168564
Test - acc:         0.911500 loss:        0.277125
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.942480 loss:        0.165476
Test - acc:         0.909000 loss:        0.276866
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.944000 loss:        0.163436
Test - acc:         0.911600 loss:        0.274861
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.944920 loss:        0.159235
Test - acc:         0.910100 loss:        0.272880
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.943660 loss:        0.163691
Test - acc:         0.909800 loss:        0.275080
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.945240 loss:        0.158064
Test - acc:         0.912100 loss:        0.273058
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.947420 loss:        0.156216
Test - acc:         0.911600 loss:        0.273797
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.945440 loss:        0.158103
Test - acc:         0.912100 loss:        0.273252
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.946840 loss:        0.154605
Test - acc:         0.910400 loss:        0.271246
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.949260 loss:        0.151480
Test - acc:         0.911200 loss:        0.270016
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.946780 loss:        0.151676
Test - acc:         0.912300 loss:        0.270948
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.947320 loss:        0.150997
Test - acc:         0.914300 loss:        0.270811
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.949220 loss:        0.148009
Test - acc:         0.913600 loss:        0.269860
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.948660 loss:        0.150388
Test - acc:         0.913900 loss:        0.271398
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.948780 loss:        0.147926
Test - acc:         0.912600 loss:        0.273104
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.950180 loss:        0.145251
Test - acc:         0.910400 loss:        0.270550
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.950200 loss:        0.145668
Test - acc:         0.913100 loss:        0.271047
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.743760 loss:        0.745902
Test - acc:         0.798900 loss:        0.596953
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.808200 loss:        0.565082
Test - acc:         0.820200 loss:        0.533151
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.823080 loss:        0.516869
Test - acc:         0.829200 loss:        0.507954
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.831160 loss:        0.492258
Test - acc:         0.833800 loss:        0.488063
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.837940 loss:        0.474057
Test - acc:         0.835800 loss:        0.474188
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.842920 loss:        0.458737
Test - acc:         0.840600 loss:        0.462605
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.847820 loss:        0.447095
Test - acc:         0.844000 loss:        0.456801
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.850360 loss:        0.436470
Test - acc:         0.846500 loss:        0.447919
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.852860 loss:        0.429632
Test - acc:         0.850700 loss:        0.444062
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.855520 loss:        0.423716
Test - acc:         0.849800 loss:        0.440101
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.856200 loss:        0.417952
Test - acc:         0.851600 loss:        0.437270
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.859100 loss:        0.413227
Test - acc:         0.852900 loss:        0.435716
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.858860 loss:        0.408463
Test - acc:         0.854600 loss:        0.432078
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.859540 loss:        0.404978
Test - acc:         0.856500 loss:        0.431278
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.863760 loss:        0.396899
Test - acc:         0.856400 loss:        0.425804
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.865420 loss:        0.393995
Test - acc:         0.857400 loss:        0.424554
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.863740 loss:        0.395533
Test - acc:         0.858500 loss:        0.424689
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.865660 loss:        0.391028
Test - acc:         0.856000 loss:        0.428564
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.864920 loss:        0.389453
Test - acc:         0.860400 loss:        0.421445
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.867260 loss:        0.387173
Test - acc:         0.861400 loss:        0.417895
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.867260 loss:        0.384975
Test - acc:         0.861200 loss:        0.413325
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.868320 loss:        0.381328
Test - acc:         0.861200 loss:        0.416260
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.869760 loss:        0.378299
Test - acc:         0.864500 loss:        0.414009
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.869860 loss:        0.378060
Test - acc:         0.861300 loss:        0.411266
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.871060 loss:        0.375726
Test - acc:         0.862200 loss:        0.411684
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.871300 loss:        0.372307
Test - acc:         0.866200 loss:        0.407355
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.870960 loss:        0.374259
Test - acc:         0.862000 loss:        0.407189
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.872520 loss:        0.370241
Test - acc:         0.861300 loss:        0.407641
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.871980 loss:        0.373093
Test - acc:         0.865600 loss:        0.406444
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.874540 loss:        0.365465
Test - acc:         0.864000 loss:        0.407735
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.874340 loss:        0.366644
Test - acc:         0.865200 loss:        0.406535
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.875180 loss:        0.366284
Test - acc:         0.866800 loss:        0.405197
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.573240 loss:        1.210944
Test - acc:         0.651800 loss:        0.979855
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.659620 loss:        0.969676
Test - acc:         0.686700 loss:        0.885073
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.687940 loss:        0.904229
Test - acc:         0.701500 loss:        0.844941
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.699260 loss:        0.869416
Test - acc:         0.711800 loss:        0.822827
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.706360 loss:        0.841894
Test - acc:         0.718900 loss:        0.800334
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.714460 loss:        0.825809
Test - acc:         0.727900 loss:        0.776461
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.718400 loss:        0.809632
Test - acc:         0.728200 loss:        0.773027
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.722880 loss:        0.795489
Test - acc:         0.732700 loss:        0.760262
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.729700 loss:        0.783596
Test - acc:         0.736800 loss:        0.747772
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.732280 loss:        0.773595
Test - acc:         0.738900 loss:        0.741844
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.733860 loss:        0.763942
Test - acc:         0.746900 loss:        0.728378
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.739680 loss:        0.754589
Test - acc:         0.742900 loss:        0.726891
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.738380 loss:        0.753401
Test - acc:         0.746000 loss:        0.729296
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.745420 loss:        0.743429
Test - acc:         0.752700 loss:        0.715151
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.743920 loss:        0.739606
Test - acc:         0.749700 loss:        0.713144
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.745720 loss:        0.732235
Test - acc:         0.752300 loss:        0.705243
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.744600 loss:        0.730453
Test - acc:         0.751200 loss:        0.709659
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.750020 loss:        0.721581
Test - acc:         0.757200 loss:        0.695753
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.751120 loss:        0.722534
Test - acc:         0.758600 loss:        0.697367
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.752520 loss:        0.716015
Test - acc:         0.758800 loss:        0.694751
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.754700 loss:        0.711524
Test - acc:         0.764000 loss:        0.681111
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.755460 loss:        0.706568
Test - acc:         0.760800 loss:        0.681848
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.756980 loss:        0.706108
Test - acc:         0.762900 loss:        0.677410
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.757080 loss:        0.701445
Test - acc:         0.767400 loss:        0.677827
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.759020 loss:        0.696370
Test - acc:         0.766900 loss:        0.675114
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.760500 loss:        0.695176
Test - acc:         0.766900 loss:        0.674430
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.760000 loss:        0.692681
Test - acc:         0.771200 loss:        0.668747
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.761740 loss:        0.688988
Test - acc:         0.770000 loss:        0.672168
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.765400 loss:        0.685792
Test - acc:         0.770700 loss:        0.662241
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.762040 loss:        0.687244
Test - acc:         0.772400 loss:        0.663392
Sparsity :          0.9990
Wdecay :        0.000500
