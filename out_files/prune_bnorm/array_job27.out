Running --prune_bnorm --model resnet18 --noise --prune_criterion weight_div_flips --seed 44 --prune_freq 39 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=39_seed=44 --save_model=pre-finetune/resnet18_weight_div_flips_pf39_s44 --logdir=criterion_experiment_prune_bnorm/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": true,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_prune_bnorm/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf39_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11169152
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11169152
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.303340 loss:        2.045872
Test - acc:         0.449200 loss:        1.518474
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.486940 loss:        1.408749
Test - acc:         0.553600 loss:        1.216369
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.595040 loss:        1.130263
Test - acc:         0.627400 loss:        1.061316
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.662820 loss:        0.954220
Test - acc:         0.666000 loss:        0.963036
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.710980 loss:        0.823103
Test - acc:         0.700400 loss:        0.900422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.754080 loss:        0.701555
Test - acc:         0.719300 loss:        0.843699
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.784640 loss:        0.626890
Test - acc:         0.762900 loss:        0.702303
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.800120 loss:        0.580976
Test - acc:         0.678300 loss:        1.056559
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.809040 loss:        0.553956
Test - acc:         0.750400 loss:        0.725537
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.817400 loss:        0.527065
Test - acc:         0.750100 loss:        0.776029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.828080 loss:        0.504549
Test - acc:         0.806100 loss:        0.560775
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.827040 loss:        0.498283
Test - acc:         0.787800 loss:        0.637411
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.835700 loss:        0.482435
Test - acc:         0.794000 loss:        0.615528
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838900 loss:        0.468737
Test - acc:         0.820400 loss:        0.537681
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844200 loss:        0.459359
Test - acc:         0.804800 loss:        0.583139
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845740 loss:        0.448696
Test - acc:         0.757400 loss:        0.755301
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.845840 loss:        0.444416
Test - acc:         0.784900 loss:        0.670693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.849560 loss:        0.437153
Test - acc:         0.836300 loss:        0.472858
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.852000 loss:        0.431503
Test - acc:         0.805300 loss:        0.564962
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.853780 loss:        0.428139
Test - acc:         0.822000 loss:        0.534250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.417603
Test - acc:         0.776100 loss:        0.709650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.857220 loss:        0.416609
Test - acc:         0.822800 loss:        0.534768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.856600 loss:        0.418318
Test - acc:         0.835400 loss:        0.503948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.857700 loss:        0.416371
Test - acc:         0.797500 loss:        0.599861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.861780 loss:        0.406843
Test - acc:         0.789200 loss:        0.638827
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.858420 loss:        0.412091
Test - acc:         0.812200 loss:        0.574223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.861500 loss:        0.402603
Test - acc:         0.834200 loss:        0.487177
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.861100 loss:        0.403876
Test - acc:         0.788300 loss:        0.694953
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.862260 loss:        0.397948
Test - acc:         0.834000 loss:        0.496371
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.861600 loss:        0.404324
Test - acc:         0.767800 loss:        0.741355
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.863460 loss:        0.399084
Test - acc:         0.813800 loss:        0.548094
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.862380 loss:        0.399397
Test - acc:         0.821100 loss:        0.548047
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.866720 loss:        0.392388
Test - acc:         0.831100 loss:        0.522133
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.863960 loss:        0.395103
Test - acc:         0.791300 loss:        0.647549
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.865880 loss:        0.393367
Test - acc:         0.748300 loss:        0.861032
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.867420 loss:        0.392783
Test - acc:         0.793100 loss:        0.629516
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.867160 loss:        0.387712
Test - acc:         0.855000 loss:        0.438008
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.387555
Test - acc:         0.817900 loss:        0.557149
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.867240 loss:        0.389858
Test - acc:         0.841200 loss:        0.478896
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.879140 loss:        0.353561
Test - acc:         0.851500 loss:        0.441347
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.875180 loss:        0.365735
Test - acc:         0.818500 loss:        0.539037
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.877540 loss:        0.360150
Test - acc:         0.799600 loss:        0.644030
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.875260 loss:        0.364557
Test - acc:         0.837700 loss:        0.478499
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.875040 loss:        0.365903
Test - acc:         0.815800 loss:        0.589264
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.875140 loss:        0.361583
Test - acc:         0.854800 loss:        0.428962
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.876700 loss:        0.359765
Test - acc:         0.838100 loss:        0.513115
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.875220 loss:        0.366946
Test - acc:         0.824300 loss:        0.526638
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.876380 loss:        0.363579
Test - acc:         0.814900 loss:        0.562262
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.878820 loss:        0.357529
Test - acc:         0.799000 loss:        0.661048
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.873800 loss:        0.365791
Test - acc:         0.830700 loss:        0.518581
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.875520 loss:        0.362014
Test - acc:         0.838500 loss:        0.475234
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.877920 loss:        0.361430
Test - acc:         0.852300 loss:        0.444673
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.875520 loss:        0.364077
Test - acc:         0.817800 loss:        0.558588
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.876640 loss:        0.362975
Test - acc:         0.835100 loss:        0.500934
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.875600 loss:        0.362780
Test - acc:         0.814500 loss:        0.550857
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.877860 loss:        0.358199
Test - acc:         0.815500 loss:        0.570830
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.878600 loss:        0.355946
Test - acc:         0.785400 loss:        0.673566
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.877600 loss:        0.359389
Test - acc:         0.846000 loss:        0.472494
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.875640 loss:        0.363196
Test - acc:         0.835900 loss:        0.498102
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.879760 loss:        0.352937
Test - acc:         0.846500 loss:        0.483814
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.876760 loss:        0.360379
Test - acc:         0.847300 loss:        0.482047
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.876000 loss:        0.366101
Test - acc:         0.824000 loss:        0.534735
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.875600 loss:        0.362183
Test - acc:         0.798800 loss:        0.610648
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.875880 loss:        0.358896
Test - acc:         0.834500 loss:        0.493058
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.877560 loss:        0.357029
Test - acc:         0.860600 loss:        0.429732
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.876040 loss:        0.361073
Test - acc:         0.794400 loss:        0.648821
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.875520 loss:        0.359843
Test - acc:         0.832900 loss:        0.523453
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.877080 loss:        0.361193
Test - acc:         0.835500 loss:        0.521591
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.875900 loss:        0.359981
Test - acc:         0.810900 loss:        0.587930
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.879880 loss:        0.347282
Test - acc:         0.829600 loss:        0.534980
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.878620 loss:        0.355958
Test - acc:         0.810200 loss:        0.616764
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.875580 loss:        0.359883
Test - acc:         0.856500 loss:        0.426234
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.876960 loss:        0.359089
Test - acc:         0.849400 loss:        0.459429
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.877680 loss:        0.356296
Test - acc:         0.848200 loss:        0.442359
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.877720 loss:        0.359289
Test - acc:         0.797900 loss:        0.612257
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.877160 loss:        0.355060
Test - acc:         0.825300 loss:        0.546224
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.879480 loss:        0.352237
Test - acc:         0.759500 loss:        0.801405
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.877180 loss:        0.360529
Test - acc:         0.822900 loss:        0.529712
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.889060 loss:        0.320956
Test - acc:         0.824400 loss:        0.531305
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.887100 loss:        0.326114
Test - acc:         0.833400 loss:        0.523217
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.884540 loss:        0.334649
Test - acc:         0.856000 loss:        0.429885
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.885760 loss:        0.332670
Test - acc:         0.841000 loss:        0.482580
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.886600 loss:        0.332613
Test - acc:         0.829100 loss:        0.535884
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.885860 loss:        0.336039
Test - acc:         0.829800 loss:        0.522442
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.884460 loss:        0.336421
Test - acc:         0.839500 loss:        0.483109
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.885600 loss:        0.333905
Test - acc:         0.857700 loss:        0.440951
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.886540 loss:        0.331298
Test - acc:         0.800300 loss:        0.678227
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.885800 loss:        0.333128
Test - acc:         0.841600 loss:        0.480565
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.885260 loss:        0.336806
Test - acc:         0.829800 loss:        0.522471
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.886080 loss:        0.334641
Test - acc:         0.835500 loss:        0.522160
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.888020 loss:        0.330391
Test - acc:         0.864200 loss:        0.401396
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.884140 loss:        0.337723
Test - acc:         0.849000 loss:        0.457081
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.883960 loss:        0.337182
Test - acc:         0.849400 loss:        0.463459
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.884340 loss:        0.336131
Test - acc:         0.837200 loss:        0.484179
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.886760 loss:        0.329156
Test - acc:         0.860800 loss:        0.422431
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.887340 loss:        0.335627
Test - acc:         0.823400 loss:        0.542004
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.884680 loss:        0.333842
Test - acc:         0.843800 loss:        0.461918
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.884660 loss:        0.335344
Test - acc:         0.832600 loss:        0.526149
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.884640 loss:        0.335072
Test - acc:         0.843100 loss:        0.482108
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.884720 loss:        0.335989
Test - acc:         0.822800 loss:        0.539443
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.884400 loss:        0.336380
Test - acc:         0.857300 loss:        0.439142
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.888100 loss:        0.329689
Test - acc:         0.826600 loss:        0.528315
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.886520 loss:        0.333550
Test - acc:         0.839900 loss:        0.475380
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.884800 loss:        0.338380
Test - acc:         0.794600 loss:        0.702761
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.884020 loss:        0.335652
Test - acc:         0.845700 loss:        0.473274
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.884660 loss:        0.334540
Test - acc:         0.861400 loss:        0.417549
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.882740 loss:        0.341270
Test - acc:         0.867000 loss:        0.396867
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.885660 loss:        0.334268
Test - acc:         0.831800 loss:        0.519044
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.886160 loss:        0.331975
Test - acc:         0.856100 loss:        0.424179
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.886300 loss:        0.333034
Test - acc:         0.849100 loss:        0.455024
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.885300 loss:        0.333248
Test - acc:         0.830000 loss:        0.520109
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.885060 loss:        0.338335
Test - acc:         0.816100 loss:        0.548852
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.885220 loss:        0.338208
Test - acc:         0.837800 loss:        0.498150
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.886520 loss:        0.333960
Test - acc:         0.834000 loss:        0.497248
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.886360 loss:        0.332478
Test - acc:         0.829100 loss:        0.504632
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.885280 loss:        0.336242
Test - acc:         0.845700 loss:        0.466694
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.884680 loss:        0.338796
Test - acc:         0.837700 loss:        0.511663
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.899480 loss:        0.298055
Test - acc:         0.879700 loss:        0.364981
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.896360 loss:        0.300031
Test - acc:         0.853000 loss:        0.449095
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.893140 loss:        0.308327
Test - acc:         0.848800 loss:        0.458839
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.891300 loss:        0.315647
Test - acc:         0.846400 loss:        0.467820
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.892300 loss:        0.312579
Test - acc:         0.820400 loss:        0.541946
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.890920 loss:        0.314851
Test - acc:         0.844600 loss:        0.476547
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.891980 loss:        0.313462
Test - acc:         0.833800 loss:        0.541448
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.892380 loss:        0.314130
Test - acc:         0.861900 loss:        0.433724
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.891600 loss:        0.314481
Test - acc:         0.833400 loss:        0.504354
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.893480 loss:        0.311597
Test - acc:         0.838900 loss:        0.489131
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.892800 loss:        0.312514
Test - acc:         0.828700 loss:        0.508650
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.891780 loss:        0.313843
Test - acc:         0.854500 loss:        0.448528
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.891720 loss:        0.313732
Test - acc:         0.841900 loss:        0.482302
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.891940 loss:        0.312276
Test - acc:         0.857200 loss:        0.429603
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.892440 loss:        0.309212
Test - acc:         0.866900 loss:        0.417256
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.892480 loss:        0.315697
Test - acc:         0.825000 loss:        0.539200
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.892500 loss:        0.311153
Test - acc:         0.838500 loss:        0.483313
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.892420 loss:        0.311374
Test - acc:         0.827500 loss:        0.542662
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.892820 loss:        0.310346
Test - acc:         0.839000 loss:        0.501963
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.892140 loss:        0.310339
Test - acc:         0.798300 loss:        0.670618
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.892140 loss:        0.315645
Test - acc:         0.824400 loss:        0.548179
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.892500 loss:        0.313249
Test - acc:         0.849700 loss:        0.448419
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.894420 loss:        0.306917
Test - acc:         0.810600 loss:        0.602443
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.892040 loss:        0.313250
Test - acc:         0.838800 loss:        0.506120
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.893600 loss:        0.310946
Test - acc:         0.847800 loss:        0.473268
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.893320 loss:        0.309629
Test - acc:         0.843300 loss:        0.472365
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.893260 loss:        0.312033
Test - acc:         0.854600 loss:        0.450669
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.891940 loss:        0.313782
Test - acc:         0.809300 loss:        0.594737
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.893200 loss:        0.311648
Test - acc:         0.787600 loss:        0.657029
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.892440 loss:        0.315243
Test - acc:         0.846000 loss:        0.463775
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.894100 loss:        0.310760
Test - acc:         0.819500 loss:        0.553435
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.894720 loss:        0.309749
Test - acc:         0.802800 loss:        0.637006
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.893420 loss:        0.313827
Test - acc:         0.852100 loss:        0.448971
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.938980 loss:        0.184802
Test - acc:         0.919300 loss:        0.228249
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.951760 loss:        0.142659
Test - acc:         0.925600 loss:        0.220184
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.958280 loss:        0.124933
Test - acc:         0.927400 loss:        0.213068
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.962220 loss:        0.113657
Test - acc:         0.931700 loss:        0.210089
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.964120 loss:        0.106614
Test - acc:         0.929600 loss:        0.211653
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.965920 loss:        0.098926
Test - acc:         0.930300 loss:        0.209402
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.969080 loss:        0.091030
Test - acc:         0.931800 loss:        0.206455
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.971180 loss:        0.086733
Test - acc:         0.934300 loss:        0.212678
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.972480 loss:        0.081494
Test - acc:         0.930500 loss:        0.219311
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.974740 loss:        0.077421
Test - acc:         0.933000 loss:        0.214455
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.975380 loss:        0.073534
Test - acc:         0.930600 loss:        0.220261
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.976660 loss:        0.071188
Test - acc:         0.930900 loss:        0.224076
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.977840 loss:        0.067195
Test - acc:         0.934300 loss:        0.218197
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.978560 loss:        0.064664
Test - acc:         0.931500 loss:        0.232248
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.977780 loss:        0.065901
Test - acc:         0.933000 loss:        0.222809
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.978220 loss:        0.064352
Test - acc:         0.932900 loss:        0.223564
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.980120 loss:        0.058305
Test - acc:         0.933800 loss:        0.219142
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.980400 loss:        0.059323
Test - acc:         0.931200 loss:        0.227019
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.982340 loss:        0.054625
Test - acc:         0.928700 loss:        0.244152
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.057764
Test - acc:         0.927000 loss:        0.253396
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.982040 loss:        0.054819
Test - acc:         0.930200 loss:        0.236327
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.982420 loss:        0.054058
Test - acc:         0.929900 loss:        0.239255
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.982120 loss:        0.053481
Test - acc:         0.928800 loss:        0.248986
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.981900 loss:        0.053949
Test - acc:         0.926900 loss:        0.257110
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.982100 loss:        0.052913
Test - acc:         0.928200 loss:        0.246558
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.981420 loss:        0.056544
Test - acc:         0.929200 loss:        0.246739
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.981920 loss:        0.053668
Test - acc:         0.929300 loss:        0.249003
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.981380 loss:        0.055658
Test - acc:         0.922800 loss:        0.270102
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.981100 loss:        0.056089
Test - acc:         0.925600 loss:        0.261382
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.981380 loss:        0.056018
Test - acc:         0.928300 loss:        0.254625
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.981280 loss:        0.056092
Test - acc:         0.924100 loss:        0.265526
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.058997
Test - acc:         0.917400 loss:        0.279166
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.980740 loss:        0.056967
Test - acc:         0.923100 loss:        0.266017
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.981860 loss:        0.053676
Test - acc:         0.921100 loss:        0.264153
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.979360 loss:        0.061057
Test - acc:         0.918700 loss:        0.287064
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.980320 loss:        0.058202
Test - acc:         0.922900 loss:        0.265470
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.058176
Test - acc:         0.924500 loss:        0.266297
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.980000 loss:        0.058911
Test - acc:         0.926800 loss:        0.259374
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.057781
Test - acc:         0.924400 loss:        0.258512
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.979220 loss:        0.062215
Test - acc:         0.924000 loss:        0.271445
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.978640 loss:        0.063955
Test - acc:         0.926300 loss:        0.255890
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.977580 loss:        0.063744
Test - acc:         0.922100 loss:        0.258916
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.980100 loss:        0.059635
Test - acc:         0.922300 loss:        0.278613
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.979700 loss:        0.060923
Test - acc:         0.918900 loss:        0.290271
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.060645
Test - acc:         0.923200 loss:        0.271350
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.976600 loss:        0.069687
Test - acc:         0.922300 loss:        0.266098
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.979600 loss:        0.062259
Test - acc:         0.927300 loss:        0.248811
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.980960 loss:        0.058390
Test - acc:         0.922700 loss:        0.269346
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.981140 loss:        0.056527
Test - acc:         0.926100 loss:        0.262706
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.981640 loss:        0.054444
Test - acc:         0.919000 loss:        0.285389
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.979880 loss:        0.058953
Test - acc:         0.924500 loss:        0.274035
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.980500 loss:        0.058719
Test - acc:         0.922000 loss:        0.266185
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.980240 loss:        0.059323
Test - acc:         0.922700 loss:        0.271646
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.981180 loss:        0.056738
Test - acc:         0.927200 loss:        0.254658
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.981500 loss:        0.055871
Test - acc:         0.922900 loss:        0.278966
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.980480 loss:        0.058386
Test - acc:         0.918000 loss:        0.290950
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.058808
Test - acc:         0.915800 loss:        0.299569
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.979680 loss:        0.060987
Test - acc:         0.924200 loss:        0.265777
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.979740 loss:        0.060138
Test - acc:         0.923900 loss:        0.266223
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.980680 loss:        0.059873
Test - acc:         0.920500 loss:        0.284855
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.978680 loss:        0.061538
Test - acc:         0.922400 loss:        0.271126
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.978360 loss:        0.062343
Test - acc:         0.926400 loss:        0.265121
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.980480 loss:        0.058602
Test - acc:         0.927200 loss:        0.270743
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.978420 loss:        0.063007
Test - acc:         0.918700 loss:        0.282901
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.980640 loss:        0.057301
Test - acc:         0.917800 loss:        0.299211
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.978960 loss:        0.062948
Test - acc:         0.915100 loss:        0.305710
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.980200 loss:        0.059435
Test - acc:         0.923000 loss:        0.280281
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.978880 loss:        0.062094
Test - acc:         0.915300 loss:        0.301428
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.980480 loss:        0.059459
Test - acc:         0.922200 loss:        0.270067
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.981600 loss:        0.055933
Test - acc:         0.923400 loss:        0.276868
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.980200 loss:        0.059552
Test - acc:         0.921600 loss:        0.282953
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.977300 loss:        0.066144
Test - acc:         0.922000 loss:        0.267953
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.978960 loss:        0.062143
Test - acc:         0.922300 loss:        0.273180
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.977780 loss:        0.064696
Test - acc:         0.912600 loss:        0.311567
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.979240 loss:        0.062919
Test - acc:         0.918100 loss:        0.282288
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.979180 loss:        0.062419
Test - acc:         0.925400 loss:        0.265851
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.980620 loss:        0.059462
Test - acc:         0.921800 loss:        0.273871
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.978260 loss:        0.063289
Test - acc:         0.918900 loss:        0.285652
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.979360 loss:        0.061917
Test - acc:         0.924500 loss:        0.273426
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.977920 loss:        0.065603
Test - acc:         0.923600 loss:        0.268454
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.979980 loss:        0.060324
Test - acc:         0.924400 loss:        0.270049
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.980560 loss:        0.058348
Test - acc:         0.911400 loss:        0.326394
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.978840 loss:        0.063335
Test - acc:         0.924100 loss:        0.272231
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.978960 loss:        0.060781
Test - acc:         0.929300 loss:        0.253705
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.965460 loss:        0.098251
Test - acc:         0.912000 loss:        0.299348
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.970800 loss:        0.084890
Test - acc:         0.919300 loss:        0.279062
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.971100 loss:        0.085343
Test - acc:         0.915800 loss:        0.271195
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.972700 loss:        0.078390
Test - acc:         0.922600 loss:        0.264073
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.972740 loss:        0.079205
Test - acc:         0.914100 loss:        0.283969
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.973840 loss:        0.076266
Test - acc:         0.920200 loss:        0.265225
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.974180 loss:        0.075300
Test - acc:         0.919300 loss:        0.277088
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.972720 loss:        0.079104
Test - acc:         0.917700 loss:        0.279992
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.973860 loss:        0.077203
Test - acc:         0.915300 loss:        0.299716
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.975940 loss:        0.069525
Test - acc:         0.922500 loss:        0.270527
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.974620 loss:        0.073491
Test - acc:         0.922100 loss:        0.269168
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.975140 loss:        0.072313
Test - acc:         0.925000 loss:        0.257908
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.975220 loss:        0.072829
Test - acc:         0.917700 loss:        0.290267
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.975320 loss:        0.071758
Test - acc:         0.919500 loss:        0.278165
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.976100 loss:        0.069607
Test - acc:         0.923000 loss:        0.278898
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.976020 loss:        0.070678
Test - acc:         0.920600 loss:        0.278071
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.984580 loss:        0.047595
Test - acc:         0.934100 loss:        0.225927
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.989240 loss:        0.036909
Test - acc:         0.933000 loss:        0.226797
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.990560 loss:        0.033527
Test - acc:         0.934700 loss:        0.225005
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.991840 loss:        0.030368
Test - acc:         0.935200 loss:        0.224991
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.992280 loss:        0.028726
Test - acc:         0.935600 loss:        0.226686
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.992780 loss:        0.026679
Test - acc:         0.935900 loss:        0.227889
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.993220 loss:        0.025812
Test - acc:         0.938000 loss:        0.226195
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.993820 loss:        0.024664
Test - acc:         0.937200 loss:        0.226190
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.994420 loss:        0.022585
Test - acc:         0.936500 loss:        0.228499
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.993900 loss:        0.023298
Test - acc:         0.936600 loss:        0.228025
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.994540 loss:        0.022386
Test - acc:         0.935600 loss:        0.227847
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.995020 loss:        0.021808
Test - acc:         0.937900 loss:        0.228964
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.994520 loss:        0.021432
Test - acc:         0.935600 loss:        0.227729
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.994920 loss:        0.020662
Test - acc:         0.937200 loss:        0.227883
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.995220 loss:        0.020411
Test - acc:         0.936000 loss:        0.227402
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.995820 loss:        0.018954
Test - acc:         0.939000 loss:        0.224881
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.995480 loss:        0.019432
Test - acc:         0.937700 loss:        0.230307
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.996000 loss:        0.018260
Test - acc:         0.937300 loss:        0.228538
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.995880 loss:        0.018213
Test - acc:         0.938100 loss:        0.228277
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.996160 loss:        0.017845
Test - acc:         0.937900 loss:        0.230965
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.995920 loss:        0.018129
Test - acc:         0.938100 loss:        0.231032
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.996100 loss:        0.017003
Test - acc:         0.937300 loss:        0.232537
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.996260 loss:        0.016894
Test - acc:         0.937600 loss:        0.232834
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.945280 loss:        0.161426
Test - acc:         0.915600 loss:        0.284967
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.957980 loss:        0.122246
Test - acc:         0.919000 loss:        0.270712
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.962760 loss:        0.109868
Test - acc:         0.920100 loss:        0.262720
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.965040 loss:        0.102753
Test - acc:         0.920400 loss:        0.261737
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.966620 loss:        0.098303
Test - acc:         0.923800 loss:        0.261621
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.969920 loss:        0.091797
Test - acc:         0.923000 loss:        0.260116
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.971360 loss:        0.087518
Test - acc:         0.922500 loss:        0.259865
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.971580 loss:        0.085407
Test - acc:         0.924500 loss:        0.253935
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.972700 loss:        0.083407
Test - acc:         0.925200 loss:        0.256734
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.973740 loss:        0.081679
Test - acc:         0.925500 loss:        0.258495
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.973340 loss:        0.079780
Test - acc:         0.928100 loss:        0.256490
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.974560 loss:        0.077318
Test - acc:         0.927100 loss:        0.254163
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.974980 loss:        0.077278
Test - acc:         0.926800 loss:        0.253610
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.975480 loss:        0.075496
Test - acc:         0.928400 loss:        0.252392
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.976620 loss:        0.073281
Test - acc:         0.928300 loss:        0.250698
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.977120 loss:        0.069670
Test - acc:         0.925900 loss:        0.251248
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.977260 loss:        0.069847
Test - acc:         0.926300 loss:        0.254632
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.977580 loss:        0.068101
Test - acc:         0.927100 loss:        0.254630
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.978520 loss:        0.066215
Test - acc:         0.928400 loss:        0.256698
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.978640 loss:        0.066298
Test - acc:         0.928700 loss:        0.253526
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.979140 loss:        0.065092
Test - acc:         0.927600 loss:        0.256921
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.978660 loss:        0.065048
Test - acc:         0.927200 loss:        0.254353
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.980100 loss:        0.061877
Test - acc:         0.925700 loss:        0.256688
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.979420 loss:        0.062424
Test - acc:         0.927600 loss:        0.255357
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.980060 loss:        0.060823
Test - acc:         0.926800 loss:        0.256758
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.981020 loss:        0.060095
Test - acc:         0.929700 loss:        0.250172
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.982420 loss:        0.058267
Test - acc:         0.926600 loss:        0.251307
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.980040 loss:        0.059713
Test - acc:         0.927500 loss:        0.251703
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.980660 loss:        0.059646
Test - acc:         0.927900 loss:        0.252902
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.981240 loss:        0.057969
Test - acc:         0.927700 loss:        0.250750
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.982040 loss:        0.057125
Test - acc:         0.926700 loss:        0.253256
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.982000 loss:        0.055812
Test - acc:         0.930200 loss:        0.254419
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.982600 loss:        0.055724
Test - acc:         0.928200 loss:        0.257306
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.982100 loss:        0.056914
Test - acc:         0.928400 loss:        0.255296
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.982480 loss:        0.055390
Test - acc:         0.931500 loss:        0.253227
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.982420 loss:        0.054293
Test - acc:         0.928700 loss:        0.251364
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.983500 loss:        0.053702
Test - acc:         0.928600 loss:        0.251374
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.983680 loss:        0.052372
Test - acc:         0.927700 loss:        0.250600
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.984680 loss:        0.050246
Test - acc:         0.928600 loss:        0.255762
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.855480 loss:        0.420209
Test - acc:         0.867000 loss:        0.382272
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.894660 loss:        0.308538
Test - acc:         0.878400 loss:        0.353340
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.903360 loss:        0.281107
Test - acc:         0.885300 loss:        0.336498
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.909100 loss:        0.263311
Test - acc:         0.890900 loss:        0.324768
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.915140 loss:        0.249076
Test - acc:         0.893200 loss:        0.316560
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.916700 loss:        0.241958
Test - acc:         0.897300 loss:        0.310941
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.919160 loss:        0.235399
Test - acc:         0.896800 loss:        0.306332
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.921700 loss:        0.228552
Test - acc:         0.897100 loss:        0.306183
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.923080 loss:        0.221980
Test - acc:         0.899500 loss:        0.300899
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.925360 loss:        0.216897
Test - acc:         0.900400 loss:        0.299564
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.926840 loss:        0.212532
Test - acc:         0.899700 loss:        0.299939
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.927020 loss:        0.211182
Test - acc:         0.902000 loss:        0.293097
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.928060 loss:        0.207561
Test - acc:         0.906100 loss:        0.290346
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.929300 loss:        0.203876
Test - acc:         0.903000 loss:        0.291686
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.930260 loss:        0.204597
Test - acc:         0.905000 loss:        0.285827
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.932060 loss:        0.201554
Test - acc:         0.903900 loss:        0.291932
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.930980 loss:        0.198749
Test - acc:         0.904300 loss:        0.287516
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.931680 loss:        0.197582
Test - acc:         0.904900 loss:        0.288500
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.933500 loss:        0.194313
Test - acc:         0.906900 loss:        0.288121
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.933460 loss:        0.191832
Test - acc:         0.904200 loss:        0.285257
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.935900 loss:        0.187031
Test - acc:         0.904000 loss:        0.290981
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.935460 loss:        0.189272
Test - acc:         0.903700 loss:        0.286685
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.934880 loss:        0.188018
Test - acc:         0.903500 loss:        0.293713
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.935180 loss:        0.188728
Test - acc:         0.903600 loss:        0.286262
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.935540 loss:        0.185657
Test - acc:         0.905200 loss:        0.288319
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.935400 loss:        0.186752
Test - acc:         0.904700 loss:        0.293209
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.937500 loss:        0.181511
Test - acc:         0.904900 loss:        0.286624
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.936820 loss:        0.183250
Test - acc:         0.906000 loss:        0.289030
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.938800 loss:        0.181327
Test - acc:         0.906700 loss:        0.287254
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.937220 loss:        0.180523
Test - acc:         0.904600 loss:        0.289454
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.939220 loss:        0.178644
Test - acc:         0.906300 loss:        0.287161
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.938260 loss:        0.178953
Test - acc:         0.907200 loss:        0.285257
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.937940 loss:        0.179338
Test - acc:         0.904900 loss:        0.286274
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.939660 loss:        0.178344
Test - acc:         0.908500 loss:        0.280680
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.937440 loss:        0.178352
Test - acc:         0.907800 loss:        0.286822
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.938420 loss:        0.176760
Test - acc:         0.907200 loss:        0.281821
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.939720 loss:        0.173662
Test - acc:         0.907900 loss:        0.289850
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.939600 loss:        0.172774
Test - acc:         0.906900 loss:        0.284903
Sparsity :          0.9961
Wdecay :        0.000500
