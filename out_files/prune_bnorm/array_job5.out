Running --prune_bnorm --model resnet18 --noise --prune_criterion weight_div_flips --seed 42 --prune_freq 50 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=50_seed=42 --save_model=pre-finetune/resnet18_weight_div_flips_pf50_s42 --logdir=criterion_experiment_prune_bnorm/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": true,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_prune_bnorm/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf50_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11169152
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11169152
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.319760 loss:        2.027701
Test - acc:         0.373600 loss:        1.709656
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.483520 loss:        1.421054
Test - acc:         0.526700 loss:        1.263736
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.596320 loss:        1.129661
Test - acc:         0.595600 loss:        1.171379
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.673280 loss:        0.927933
Test - acc:         0.610100 loss:        1.134739
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.723460 loss:        0.790148
Test - acc:         0.705200 loss:        0.872062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.766400 loss:        0.675023
Test - acc:         0.746900 loss:        0.720768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.790000 loss:        0.608257
Test - acc:         0.766300 loss:        0.692052
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.805140 loss:        0.568890
Test - acc:         0.777000 loss:        0.661023
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.812440 loss:        0.540974
Test - acc:         0.748500 loss:        0.765813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.823580 loss:        0.513555
Test - acc:         0.742700 loss:        0.756514
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.831620 loss:        0.490783
Test - acc:         0.778800 loss:        0.671311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.834560 loss:        0.478143
Test - acc:         0.793000 loss:        0.626211
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.841480 loss:        0.462775
Test - acc:         0.792800 loss:        0.604948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838760 loss:        0.468699
Test - acc:         0.789100 loss:        0.624176
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844780 loss:        0.450396
Test - acc:         0.816100 loss:        0.559841
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.846280 loss:        0.448807
Test - acc:         0.764600 loss:        0.719807
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.850540 loss:        0.438978
Test - acc:         0.818300 loss:        0.530640
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.850840 loss:        0.433470
Test - acc:         0.796600 loss:        0.604748
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.853620 loss:        0.425959
Test - acc:         0.841700 loss:        0.488544
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.854620 loss:        0.421329
Test - acc:         0.795500 loss:        0.623324
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.859220 loss:        0.417579
Test - acc:         0.823000 loss:        0.516899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.856440 loss:        0.420470
Test - acc:         0.758600 loss:        0.712926
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.858640 loss:        0.411732
Test - acc:         0.793800 loss:        0.625225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.858100 loss:        0.413444
Test - acc:         0.793100 loss:        0.628817
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.862000 loss:        0.407803
Test - acc:         0.819100 loss:        0.573117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.861660 loss:        0.407108
Test - acc:         0.805500 loss:        0.599522
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.864180 loss:        0.397832
Test - acc:         0.836000 loss:        0.468893
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.863120 loss:        0.402166
Test - acc:         0.833700 loss:        0.486745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.863140 loss:        0.396906
Test - acc:         0.855500 loss:        0.423829
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.867600 loss:        0.394886
Test - acc:         0.810100 loss:        0.579276
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.865620 loss:        0.394207
Test - acc:         0.853400 loss:        0.434255
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.866820 loss:        0.393427
Test - acc:         0.698200 loss:        1.116456
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.866560 loss:        0.390132
Test - acc:         0.833300 loss:        0.506263
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.866820 loss:        0.389891
Test - acc:         0.838100 loss:        0.489114
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.868660 loss:        0.383678
Test - acc:         0.817400 loss:        0.559010
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.384254
Test - acc:         0.822300 loss:        0.562526
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.866740 loss:        0.387204
Test - acc:         0.820700 loss:        0.539781
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.868140 loss:        0.390223
Test - acc:         0.839300 loss:        0.510206
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.866480 loss:        0.388159
Test - acc:         0.846800 loss:        0.458624
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.870960 loss:        0.379702
Test - acc:         0.817800 loss:        0.564097
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.387867
Test - acc:         0.835000 loss:        0.494873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.872200 loss:        0.374784
Test - acc:         0.814500 loss:        0.557098
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.873960 loss:        0.370231
Test - acc:         0.826900 loss:        0.516385
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.869500 loss:        0.379596
Test - acc:         0.837700 loss:        0.498690
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.868400 loss:        0.381868
Test - acc:         0.857200 loss:        0.420949
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.871940 loss:        0.377637
Test - acc:         0.825900 loss:        0.547199
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.870680 loss:        0.378822
Test - acc:         0.831400 loss:        0.497769
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.872980 loss:        0.375904
Test - acc:         0.809000 loss:        0.558676
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.872880 loss:        0.375953
Test - acc:         0.809400 loss:        0.604101
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.870100 loss:        0.383866
Test - acc:         0.825900 loss:        0.523767
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.880400 loss:        0.348074
Test - acc:         0.839700 loss:        0.490296
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.879160 loss:        0.355169
Test - acc:         0.862700 loss:        0.399009
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.881900 loss:        0.352659
Test - acc:         0.835000 loss:        0.512424
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.878260 loss:        0.351804
Test - acc:         0.793700 loss:        0.668560
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.878920 loss:        0.356133
Test - acc:         0.838800 loss:        0.479651
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.877000 loss:        0.361539
Test - acc:         0.840500 loss:        0.501545
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.879700 loss:        0.353705
Test - acc:         0.823600 loss:        0.539215
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.879260 loss:        0.355256
Test - acc:         0.824000 loss:        0.530033
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.876080 loss:        0.361558
Test - acc:         0.820500 loss:        0.560161
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.875620 loss:        0.361349
Test - acc:         0.827500 loss:        0.529334
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.878240 loss:        0.355850
Test - acc:         0.823300 loss:        0.532498
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.875400 loss:        0.365781
Test - acc:         0.756300 loss:        0.877736
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.878120 loss:        0.357277
Test - acc:         0.854000 loss:        0.442968
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.876880 loss:        0.361973
Test - acc:         0.827400 loss:        0.529681
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.878280 loss:        0.360297
Test - acc:         0.832400 loss:        0.528589
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.877600 loss:        0.358238
Test - acc:         0.809500 loss:        0.631681
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.879240 loss:        0.353685
Test - acc:         0.815800 loss:        0.572965
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.878560 loss:        0.359856
Test - acc:         0.797800 loss:        0.641027
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.877720 loss:        0.358855
Test - acc:         0.859500 loss:        0.435410
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.878040 loss:        0.362117
Test - acc:         0.837300 loss:        0.480907
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.878320 loss:        0.357156
Test - acc:         0.837400 loss:        0.499573
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.877100 loss:        0.359377
Test - acc:         0.826500 loss:        0.511771
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.877740 loss:        0.359085
Test - acc:         0.822400 loss:        0.540189
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.877400 loss:        0.358870
Test - acc:         0.834200 loss:        0.496359
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.879020 loss:        0.353518
Test - acc:         0.794700 loss:        0.648123
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.878460 loss:        0.357641
Test - acc:         0.820200 loss:        0.573033
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.878600 loss:        0.354174
Test - acc:         0.824700 loss:        0.535081
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.877980 loss:        0.356555
Test - acc:         0.848300 loss:        0.443855
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.878220 loss:        0.355581
Test - acc:         0.847600 loss:        0.463440
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.876780 loss:        0.356456
Test - acc:         0.821000 loss:        0.545850
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.879120 loss:        0.356156
Test - acc:         0.793500 loss:        0.694966
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.877900 loss:        0.353088
Test - acc:         0.832000 loss:        0.510499
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.880060 loss:        0.351045
Test - acc:         0.847700 loss:        0.452065
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.880300 loss:        0.352830
Test - acc:         0.796700 loss:        0.655725
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.878140 loss:        0.355466
Test - acc:         0.803900 loss:        0.604659
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.875680 loss:        0.360738
Test - acc:         0.824200 loss:        0.545102
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.879140 loss:        0.355509
Test - acc:         0.857200 loss:        0.440166
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.882020 loss:        0.350693
Test - acc:         0.833300 loss:        0.484941
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.876500 loss:        0.361175
Test - acc:         0.823200 loss:        0.536460
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.878980 loss:        0.356039
Test - acc:         0.833700 loss:        0.515516
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.877680 loss:        0.354523
Test - acc:         0.839600 loss:        0.477765
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.881360 loss:        0.351112
Test - acc:         0.840200 loss:        0.489254
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.879020 loss:        0.351435
Test - acc:         0.831500 loss:        0.521994
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.877520 loss:        0.356618
Test - acc:         0.814000 loss:        0.551449
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.875960 loss:        0.357014
Test - acc:         0.834200 loss:        0.500630
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.878060 loss:        0.355387
Test - acc:         0.834900 loss:        0.529317
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.879840 loss:        0.353411
Test - acc:         0.805200 loss:        0.613228
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.876720 loss:        0.358700
Test - acc:         0.806000 loss:        0.592081
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.880360 loss:        0.351146
Test - acc:         0.848000 loss:        0.473475
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.874420 loss:        0.361959
Test - acc:         0.842500 loss:        0.475837
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.890340 loss:        0.321922
Test - acc:         0.839300 loss:        0.500196
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.888040 loss:        0.324743
Test - acc:         0.835800 loss:        0.497839
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.886800 loss:        0.332551
Test - acc:         0.792600 loss:        0.661440
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.887020 loss:        0.331042
Test - acc:         0.810600 loss:        0.624367
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.886780 loss:        0.330422
Test - acc:         0.828000 loss:        0.533086
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.885020 loss:        0.335148
Test - acc:         0.817000 loss:        0.616801
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.885780 loss:        0.336216
Test - acc:         0.827800 loss:        0.530746
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.883420 loss:        0.335564
Test - acc:         0.847900 loss:        0.479054
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.886820 loss:        0.333835
Test - acc:         0.731600 loss:        0.921364
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.883440 loss:        0.338883
Test - acc:         0.850600 loss:        0.451153
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.883840 loss:        0.338707
Test - acc:         0.839800 loss:        0.478431
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.887600 loss:        0.331098
Test - acc:         0.843300 loss:        0.472536
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.882540 loss:        0.343220
Test - acc:         0.844000 loss:        0.490249
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.883340 loss:        0.337789
Test - acc:         0.856900 loss:        0.432893
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.886920 loss:        0.329785
Test - acc:         0.830600 loss:        0.486642
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.885260 loss:        0.335794
Test - acc:         0.855800 loss:        0.446039
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.886880 loss:        0.330329
Test - acc:         0.822400 loss:        0.556717
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.885500 loss:        0.334038
Test - acc:         0.798900 loss:        0.659800
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.884920 loss:        0.335146
Test - acc:         0.797600 loss:        0.654630
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.885220 loss:        0.335744
Test - acc:         0.848600 loss:        0.460272
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.888040 loss:        0.329416
Test - acc:         0.844800 loss:        0.455986
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.885580 loss:        0.338419
Test - acc:         0.819100 loss:        0.566411
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.884920 loss:        0.333718
Test - acc:         0.853200 loss:        0.433088
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.883620 loss:        0.332420
Test - acc:         0.859400 loss:        0.415461
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.885880 loss:        0.335743
Test - acc:         0.851900 loss:        0.438893
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.887780 loss:        0.330764
Test - acc:         0.849400 loss:        0.456089
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.884800 loss:        0.336154
Test - acc:         0.839400 loss:        0.484105
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.883060 loss:        0.339012
Test - acc:         0.837200 loss:        0.495683
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.885060 loss:        0.338049
Test - acc:         0.835800 loss:        0.500198
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.882600 loss:        0.339271
Test - acc:         0.823000 loss:        0.547246
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.886400 loss:        0.334216
Test - acc:         0.789900 loss:        0.658814
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.884080 loss:        0.339513
Test - acc:         0.857900 loss:        0.420464
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.885300 loss:        0.337022
Test - acc:         0.823000 loss:        0.527734
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.883840 loss:        0.336617
Test - acc:         0.824400 loss:        0.538902
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.883700 loss:        0.341646
Test - acc:         0.845600 loss:        0.456736
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.888960 loss:        0.328163
Test - acc:         0.844300 loss:        0.469934
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.883580 loss:        0.338335
Test - acc:         0.827100 loss:        0.537352
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.886020 loss:        0.334089
Test - acc:         0.823400 loss:        0.517206
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.882520 loss:        0.340862
Test - acc:         0.842000 loss:        0.489527
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.882660 loss:        0.341957
Test - acc:         0.814600 loss:        0.569632
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.886880 loss:        0.334082
Test - acc:         0.853300 loss:        0.449302
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.885920 loss:        0.333541
Test - acc:         0.857700 loss:        0.427965
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.886060 loss:        0.331744
Test - acc:         0.805000 loss:        0.604482
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.886020 loss:        0.334510
Test - acc:         0.853400 loss:        0.448046
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.884060 loss:        0.338383
Test - acc:         0.852100 loss:        0.446875
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.884720 loss:        0.337851
Test - acc:         0.841200 loss:        0.486687
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.884360 loss:        0.340272
Test - acc:         0.859600 loss:        0.435377
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.884620 loss:        0.336653
Test - acc:         0.844400 loss:        0.484708
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.884480 loss:        0.339058
Test - acc:         0.854400 loss:        0.433822
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.887060 loss:        0.328887
Test - acc:         0.818000 loss:        0.545280
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.931500 loss:        0.203824
Test - acc:         0.924500 loss:        0.222664
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.948480 loss:        0.153419
Test - acc:         0.928700 loss:        0.214324
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.952800 loss:        0.140274
Test - acc:         0.929000 loss:        0.208669
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.958560 loss:        0.122792
Test - acc:         0.930700 loss:        0.208162
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.961780 loss:        0.115208
Test - acc:         0.930600 loss:        0.206508
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.963840 loss:        0.105985
Test - acc:         0.930200 loss:        0.202177
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.965860 loss:        0.100972
Test - acc:         0.933100 loss:        0.199938
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.967860 loss:        0.094685
Test - acc:         0.930900 loss:        0.208680
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.969740 loss:        0.089181
Test - acc:         0.931000 loss:        0.210897
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.973220 loss:        0.083228
Test - acc:         0.932800 loss:        0.208572
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.972900 loss:        0.081619
Test - acc:         0.929300 loss:        0.212632
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.974640 loss:        0.075527
Test - acc:         0.930000 loss:        0.220966
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.975440 loss:        0.073289
Test - acc:         0.933600 loss:        0.207943
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.976720 loss:        0.069975
Test - acc:         0.932100 loss:        0.218691
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.979380 loss:        0.063669
Test - acc:         0.932600 loss:        0.221644
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.978800 loss:        0.062398
Test - acc:         0.935400 loss:        0.217652
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.978700 loss:        0.062950
Test - acc:         0.929100 loss:        0.239047
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.979620 loss:        0.061309
Test - acc:         0.930900 loss:        0.227777
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.979900 loss:        0.059292
Test - acc:         0.926800 loss:        0.253082
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.980580 loss:        0.057117
Test - acc:         0.931000 loss:        0.228765
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.980100 loss:        0.058205
Test - acc:         0.927200 loss:        0.244983
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.980700 loss:        0.057539
Test - acc:         0.931300 loss:        0.230622
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.979300 loss:        0.060501
Test - acc:         0.924600 loss:        0.249761
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.980140 loss:        0.057503
Test - acc:         0.931500 loss:        0.235074
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.980540 loss:        0.057031
Test - acc:         0.929100 loss:        0.236919
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.981460 loss:        0.055303
Test - acc:         0.925900 loss:        0.252708
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.981120 loss:        0.055222
Test - acc:         0.925200 loss:        0.267959
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.979580 loss:        0.059576
Test - acc:         0.925100 loss:        0.256232
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.981700 loss:        0.053928
Test - acc:         0.918500 loss:        0.290408
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.979800 loss:        0.058828
Test - acc:         0.924300 loss:        0.259354
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.980080 loss:        0.058630
Test - acc:         0.929400 loss:        0.241235
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.980400 loss:        0.056492
Test - acc:         0.925500 loss:        0.257826
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.978920 loss:        0.061582
Test - acc:         0.924800 loss:        0.255968
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.980960 loss:        0.057139
Test - acc:         0.924300 loss:        0.256107
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.980100 loss:        0.058849
Test - acc:         0.911600 loss:        0.307185
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.979120 loss:        0.060382
Test - acc:         0.923300 loss:        0.265164
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.979960 loss:        0.060199
Test - acc:         0.924600 loss:        0.258736
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.979480 loss:        0.060716
Test - acc:         0.924800 loss:        0.261889
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.977380 loss:        0.064892
Test - acc:         0.925000 loss:        0.246177
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.059412
Test - acc:         0.914500 loss:        0.304096
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.978580 loss:        0.062918
Test - acc:         0.927300 loss:        0.254013
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.978560 loss:        0.063317
Test - acc:         0.921200 loss:        0.271447
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.978860 loss:        0.062206
Test - acc:         0.914700 loss:        0.293904
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.979300 loss:        0.061423
Test - acc:         0.924500 loss:        0.261630
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.978060 loss:        0.063219
Test - acc:         0.925100 loss:        0.264487
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.978320 loss:        0.064067
Test - acc:         0.917900 loss:        0.289180
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.978520 loss:        0.063304
Test - acc:         0.922100 loss:        0.267198
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.979140 loss:        0.061925
Test - acc:         0.919100 loss:        0.284942
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.978020 loss:        0.065938
Test - acc:         0.918500 loss:        0.279936
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.978180 loss:        0.064261
Test - acc:         0.921900 loss:        0.273995
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.981300 loss:        0.055344
Test - acc:         0.928600 loss:        0.244122
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.984020 loss:        0.047882
Test - acc:         0.929000 loss:        0.255899
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.984700 loss:        0.046429
Test - acc:         0.926000 loss:        0.260571
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.984020 loss:        0.048985
Test - acc:         0.925000 loss:        0.267624
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.983960 loss:        0.048329
Test - acc:         0.927500 loss:        0.250077
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.984260 loss:        0.047810
Test - acc:         0.922700 loss:        0.274086
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.981480 loss:        0.054634
Test - acc:         0.925500 loss:        0.270917
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.983260 loss:        0.050769
Test - acc:         0.917800 loss:        0.296053
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.981800 loss:        0.054805
Test - acc:         0.919600 loss:        0.292678
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.055697
Test - acc:         0.925400 loss:        0.266897
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.981580 loss:        0.054903
Test - acc:         0.926000 loss:        0.272237
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.981420 loss:        0.053896
Test - acc:         0.921000 loss:        0.271957
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.982100 loss:        0.053688
Test - acc:         0.923000 loss:        0.270081
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.981700 loss:        0.052704
Test - acc:         0.922100 loss:        0.280016
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.981200 loss:        0.056649
Test - acc:         0.930600 loss:        0.253452
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.982640 loss:        0.052931
Test - acc:         0.925800 loss:        0.269246
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.979940 loss:        0.057992
Test - acc:         0.928100 loss:        0.258018
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.981520 loss:        0.055179
Test - acc:         0.910900 loss:        0.318803
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.981060 loss:        0.056327
Test - acc:         0.923600 loss:        0.272942
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.980940 loss:        0.055833
Test - acc:         0.919100 loss:        0.282300
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.980200 loss:        0.057537
Test - acc:         0.923700 loss:        0.274257
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.981620 loss:        0.055326
Test - acc:         0.924700 loss:        0.261889
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.980200 loss:        0.058554
Test - acc:         0.916400 loss:        0.311211
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.979100 loss:        0.061843
Test - acc:         0.922500 loss:        0.272067
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.980700 loss:        0.058384
Test - acc:         0.921300 loss:        0.277880
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.983140 loss:        0.051433
Test - acc:         0.925300 loss:        0.272794
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.980160 loss:        0.059886
Test - acc:         0.914900 loss:        0.313414
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.980040 loss:        0.057451
Test - acc:         0.922300 loss:        0.289006
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.981200 loss:        0.055995
Test - acc:         0.924300 loss:        0.272727
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.980460 loss:        0.059782
Test - acc:         0.919300 loss:        0.286552
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.981040 loss:        0.055673
Test - acc:         0.917300 loss:        0.296957
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.981960 loss:        0.053591
Test - acc:         0.909700 loss:        0.323421
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.979920 loss:        0.058355
Test - acc:         0.913100 loss:        0.312479
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.980680 loss:        0.058255
Test - acc:         0.918900 loss:        0.292772
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.979580 loss:        0.059129
Test - acc:         0.924000 loss:        0.269631
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.981460 loss:        0.055536
Test - acc:         0.929300 loss:        0.257071
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.980180 loss:        0.059420
Test - acc:         0.920600 loss:        0.277693
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.980240 loss:        0.058272
Test - acc:         0.925400 loss:        0.247096
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.979420 loss:        0.059635
Test - acc:         0.914100 loss:        0.309442
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.980140 loss:        0.058567
Test - acc:         0.925900 loss:        0.264609
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.980220 loss:        0.059499
Test - acc:         0.924500 loss:        0.259253
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.980940 loss:        0.056303
Test - acc:         0.923100 loss:        0.266074
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.979780 loss:        0.060305
Test - acc:         0.918100 loss:        0.301904
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.981580 loss:        0.055978
Test - acc:         0.917900 loss:        0.284967
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.981240 loss:        0.055349
Test - acc:         0.922400 loss:        0.280421
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.981000 loss:        0.056047
Test - acc:         0.920800 loss:        0.276023
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.980860 loss:        0.056171
Test - acc:         0.921200 loss:        0.287582
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.979440 loss:        0.061618
Test - acc:         0.914500 loss:        0.304232
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.980640 loss:        0.058512
Test - acc:         0.921700 loss:        0.264019
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.981680 loss:        0.055102
Test - acc:         0.921100 loss:        0.274407
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.985520 loss:        0.046823
Test - acc:         0.935200 loss:        0.225773
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.990920 loss:        0.032313
Test - acc:         0.937200 loss:        0.220350
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.992300 loss:        0.028238
Test - acc:         0.938100 loss:        0.216916
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.993020 loss:        0.025118
Test - acc:         0.940200 loss:        0.216134
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.993920 loss:        0.022727
Test - acc:         0.939500 loss:        0.215463
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.994980 loss:        0.020538
Test - acc:         0.940600 loss:        0.215676
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.995360 loss:        0.019186
Test - acc:         0.941800 loss:        0.212149
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.995260 loss:        0.019244
Test - acc:         0.942100 loss:        0.213536
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.995420 loss:        0.018127
Test - acc:         0.940800 loss:        0.216480
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.996080 loss:        0.017169
Test - acc:         0.942700 loss:        0.214990
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.996180 loss:        0.016127
Test - acc:         0.941500 loss:        0.215226
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.996220 loss:        0.016427
Test - acc:         0.942600 loss:        0.215109
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.997080 loss:        0.014639
Test - acc:         0.943000 loss:        0.215408
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.996800 loss:        0.014478
Test - acc:         0.941700 loss:        0.216290
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.997000 loss:        0.013952
Test - acc:         0.943400 loss:        0.215346
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.997280 loss:        0.013330
Test - acc:         0.943100 loss:        0.216628
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.996800 loss:        0.013822
Test - acc:         0.942200 loss:        0.216635
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.997320 loss:        0.012504
Test - acc:         0.942100 loss:        0.217092
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.997520 loss:        0.012502
Test - acc:         0.942800 loss:        0.214879
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.997820 loss:        0.011919
Test - acc:         0.943600 loss:        0.216454
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.997900 loss:        0.011399
Test - acc:         0.942500 loss:        0.217465
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.997720 loss:        0.011960
Test - acc:         0.943400 loss:        0.214869
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.997700 loss:        0.011245
Test - acc:         0.942900 loss:        0.217869
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.997380 loss:        0.011893
Test - acc:         0.943400 loss:        0.219035
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.997720 loss:        0.011122
Test - acc:         0.942700 loss:        0.218584
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.997880 loss:        0.010864
Test - acc:         0.944300 loss:        0.216095
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.998140 loss:        0.009868
Test - acc:         0.943100 loss:        0.220148
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.998240 loss:        0.010245
Test - acc:         0.943700 loss:        0.219334
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.998040 loss:        0.009988
Test - acc:         0.943400 loss:        0.220350
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.998380 loss:        0.009935
Test - acc:         0.943000 loss:        0.220448
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.998500 loss:        0.008924
Test - acc:         0.942900 loss:        0.220069
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.998400 loss:        0.009321
Test - acc:         0.943800 loss:        0.218839
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.998200 loss:        0.009414
Test - acc:         0.943700 loss:        0.217952
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.998440 loss:        0.009239
Test - acc:         0.944300 loss:        0.219693
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.998340 loss:        0.009030
Test - acc:         0.944400 loss:        0.219885
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.998160 loss:        0.009196
Test - acc:         0.943700 loss:        0.221676
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.998460 loss:        0.008664
Test - acc:         0.943800 loss:        0.219803
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.998580 loss:        0.008421
Test - acc:         0.944500 loss:        0.219297
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.998620 loss:        0.008428
Test - acc:         0.944900 loss:        0.218586
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.998320 loss:        0.009028
Test - acc:         0.944500 loss:        0.217553
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.998440 loss:        0.008936
Test - acc:         0.944900 loss:        0.216032
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.998500 loss:        0.008384
Test - acc:         0.944000 loss:        0.217438
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.998580 loss:        0.008057
Test - acc:         0.943600 loss:        0.219032
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.998600 loss:        0.007846
Test - acc:         0.945000 loss:        0.219140
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.998620 loss:        0.008036
Test - acc:         0.945200 loss:        0.217300
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.998760 loss:        0.007495
Test - acc:         0.944600 loss:        0.217399
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.998640 loss:        0.007607
Test - acc:         0.944600 loss:        0.217726
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.999040 loss:        0.007170
Test - acc:         0.944000 loss:        0.219171
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.007475
Test - acc:         0.944500 loss:        0.220985
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999040 loss:        0.007178
Test - acc:         0.944500 loss:        0.219719
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.974060 loss:        0.078373
Test - acc:         0.927500 loss:        0.249588
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.981720 loss:        0.055450
Test - acc:         0.929600 loss:        0.246241
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.983880 loss:        0.050680
Test - acc:         0.933000 loss:        0.238438
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.986360 loss:        0.044507
Test - acc:         0.933100 loss:        0.235940
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.988060 loss:        0.041393
Test - acc:         0.933100 loss:        0.234663
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.987740 loss:        0.040337
Test - acc:         0.933600 loss:        0.233856
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.989620 loss:        0.036765
Test - acc:         0.933300 loss:        0.234268
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.989500 loss:        0.035758
Test - acc:         0.934300 loss:        0.235653
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.989800 loss:        0.034930
Test - acc:         0.935800 loss:        0.233759
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.990840 loss:        0.033130
Test - acc:         0.936700 loss:        0.231745
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.990640 loss:        0.032014
Test - acc:         0.936500 loss:        0.230040
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.991580 loss:        0.029792
Test - acc:         0.936800 loss:        0.230095
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.990980 loss:        0.031533
Test - acc:         0.937100 loss:        0.228927
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.991700 loss:        0.030085
Test - acc:         0.936800 loss:        0.229353
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.992440 loss:        0.028177
Test - acc:         0.935600 loss:        0.233161
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.992160 loss:        0.028228
Test - acc:         0.935200 loss:        0.232993
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.992520 loss:        0.026475
Test - acc:         0.937000 loss:        0.231910
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.992120 loss:        0.026014
Test - acc:         0.936100 loss:        0.234547
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.992460 loss:        0.026275
Test - acc:         0.936900 loss:        0.234251
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.993840 loss:        0.024261
Test - acc:         0.935700 loss:        0.237406
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.993980 loss:        0.022694
Test - acc:         0.937900 loss:        0.236073
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.993780 loss:        0.023644
Test - acc:         0.937400 loss:        0.237178
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.992840 loss:        0.025182
Test - acc:         0.936800 loss:        0.233197
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.993780 loss:        0.022688
Test - acc:         0.936900 loss:        0.234875
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.994420 loss:        0.021790
Test - acc:         0.937300 loss:        0.234002
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.993840 loss:        0.022889
Test - acc:         0.937600 loss:        0.234880
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.994560 loss:        0.021000
Test - acc:         0.936400 loss:        0.237672
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.995100 loss:        0.020321
Test - acc:         0.937600 loss:        0.236357
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.994320 loss:        0.020975
Test - acc:         0.937700 loss:        0.236786
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.993940 loss:        0.021971
Test - acc:         0.937100 loss:        0.238990
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.995180 loss:        0.019590
Test - acc:         0.934900 loss:        0.239173
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.994660 loss:        0.020529
Test - acc:         0.937200 loss:        0.238438
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.994820 loss:        0.019810
Test - acc:         0.938800 loss:        0.240314
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.995580 loss:        0.018462
Test - acc:         0.937000 loss:        0.237871
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.995900 loss:        0.017511
Test - acc:         0.937000 loss:        0.238800
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.995460 loss:        0.018913
Test - acc:         0.936900 loss:        0.238892
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.995060 loss:        0.018417
Test - acc:         0.939000 loss:        0.238146
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.995560 loss:        0.017551
Test - acc:         0.937100 loss:        0.243789
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.995720 loss:        0.017266
Test - acc:         0.937300 loss:        0.238046
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.995380 loss:        0.017980
Test - acc:         0.937900 loss:        0.237920
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.995640 loss:        0.017182
Test - acc:         0.937600 loss:        0.240302
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.996200 loss:        0.016198
Test - acc:         0.936700 loss:        0.241507
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.995780 loss:        0.016763
Test - acc:         0.937800 loss:        0.241368
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.995700 loss:        0.017262
Test - acc:         0.936000 loss:        0.241438
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.995640 loss:        0.017031
Test - acc:         0.938000 loss:        0.241058
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.996180 loss:        0.015788
Test - acc:         0.936700 loss:        0.242887
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.996400 loss:        0.015788
Test - acc:         0.937500 loss:        0.240513
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.995320 loss:        0.017228
Test - acc:         0.938000 loss:        0.241813
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.996640 loss:        0.015565
Test - acc:         0.936600 loss:        0.243745
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.995920 loss:        0.016042
Test - acc:         0.937600 loss:        0.243894
Sparsity :          0.9844
Wdecay :        0.000500
