Running --prune_bnorm --model resnet18 --noise --prune_criterion weight_div_flips --seed 42 --prune_freq 70 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=70_seed=42 --save_model=pre-finetune/resnet18_weight_div_flips_pf70_s42 --logdir=criterion_experiment_prune_bnorm/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 70,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": true,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_prune_bnorm/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf70_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11169152
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11169152
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.319760 loss:        2.027701
Test - acc:         0.373600 loss:        1.709656
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.483520 loss:        1.421054
Test - acc:         0.526700 loss:        1.263736
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.596320 loss:        1.129661
Test - acc:         0.595600 loss:        1.171379
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.673280 loss:        0.927933
Test - acc:         0.610100 loss:        1.134739
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.723460 loss:        0.790148
Test - acc:         0.705200 loss:        0.872062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.766400 loss:        0.675023
Test - acc:         0.746900 loss:        0.720768
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.790000 loss:        0.608257
Test - acc:         0.766300 loss:        0.692052
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.805140 loss:        0.568890
Test - acc:         0.777000 loss:        0.661023
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.812440 loss:        0.540974
Test - acc:         0.748500 loss:        0.765813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.823580 loss:        0.513555
Test - acc:         0.742700 loss:        0.756514
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.831620 loss:        0.490783
Test - acc:         0.778800 loss:        0.671311
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.834560 loss:        0.478143
Test - acc:         0.793000 loss:        0.626211
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.841480 loss:        0.462775
Test - acc:         0.792800 loss:        0.604948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838760 loss:        0.468699
Test - acc:         0.789100 loss:        0.624176
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.844780 loss:        0.450396
Test - acc:         0.816100 loss:        0.559841
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.846280 loss:        0.448807
Test - acc:         0.764600 loss:        0.719807
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.850540 loss:        0.438978
Test - acc:         0.818300 loss:        0.530640
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.850840 loss:        0.433470
Test - acc:         0.796600 loss:        0.604748
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.853620 loss:        0.425959
Test - acc:         0.841700 loss:        0.488544
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.854620 loss:        0.421329
Test - acc:         0.795500 loss:        0.623324
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.859220 loss:        0.417579
Test - acc:         0.823000 loss:        0.516899
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.856440 loss:        0.420470
Test - acc:         0.758600 loss:        0.712926
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.858640 loss:        0.411732
Test - acc:         0.793800 loss:        0.625225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.858100 loss:        0.413444
Test - acc:         0.793100 loss:        0.628817
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.862000 loss:        0.407803
Test - acc:         0.819100 loss:        0.573117
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.861660 loss:        0.407108
Test - acc:         0.805500 loss:        0.599522
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.864180 loss:        0.397832
Test - acc:         0.836000 loss:        0.468893
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.863120 loss:        0.402166
Test - acc:         0.833700 loss:        0.486745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.863140 loss:        0.396906
Test - acc:         0.855500 loss:        0.423829
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.867600 loss:        0.394886
Test - acc:         0.810100 loss:        0.579276
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.865620 loss:        0.394207
Test - acc:         0.853400 loss:        0.434255
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.866820 loss:        0.393427
Test - acc:         0.698200 loss:        1.116456
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.866560 loss:        0.390132
Test - acc:         0.833300 loss:        0.506263
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.866820 loss:        0.389891
Test - acc:         0.838100 loss:        0.489114
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.868660 loss:        0.383678
Test - acc:         0.817400 loss:        0.559010
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.384254
Test - acc:         0.822300 loss:        0.562526
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.866740 loss:        0.387204
Test - acc:         0.820700 loss:        0.539781
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.868140 loss:        0.390223
Test - acc:         0.839300 loss:        0.510206
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.866480 loss:        0.388159
Test - acc:         0.846800 loss:        0.458624
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.870960 loss:        0.379702
Test - acc:         0.817800 loss:        0.564097
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.387867
Test - acc:         0.835000 loss:        0.494873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.872200 loss:        0.374784
Test - acc:         0.814500 loss:        0.557098
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.873960 loss:        0.370231
Test - acc:         0.826900 loss:        0.516385
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.869500 loss:        0.379596
Test - acc:         0.837700 loss:        0.498690
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.868400 loss:        0.381868
Test - acc:         0.857200 loss:        0.420949
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.871940 loss:        0.377637
Test - acc:         0.825900 loss:        0.547199
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.870680 loss:        0.378822
Test - acc:         0.831400 loss:        0.497769
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.872980 loss:        0.375904
Test - acc:         0.809000 loss:        0.558676
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.872880 loss:        0.375953
Test - acc:         0.809400 loss:        0.604101
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.870100 loss:        0.383866
Test - acc:         0.825900 loss:        0.523767
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.872160 loss:        0.374283
Test - acc:         0.840700 loss:        0.498610
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.871900 loss:        0.377476
Test - acc:         0.834900 loss:        0.492628
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.873540 loss:        0.372013
Test - acc:         0.839500 loss:        0.512746
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.875160 loss:        0.367331
Test - acc:         0.783200 loss:        0.708540
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.872760 loss:        0.372317
Test - acc:         0.844700 loss:        0.471319
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.873920 loss:        0.373323
Test - acc:         0.804200 loss:        0.609845
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.872460 loss:        0.372738
Test - acc:         0.838400 loss:        0.494782
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.874980 loss:        0.368934
Test - acc:         0.849200 loss:        0.455239
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.874020 loss:        0.370070
Test - acc:         0.832900 loss:        0.496336
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.868960 loss:        0.383798
Test - acc:         0.833700 loss:        0.486506
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.875740 loss:        0.366422
Test - acc:         0.816900 loss:        0.564454
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.871340 loss:        0.373256
Test - acc:         0.770400 loss:        0.749775
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.874920 loss:        0.370402
Test - acc:         0.812500 loss:        0.563316
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.870020 loss:        0.377690
Test - acc:         0.829500 loss:        0.520252
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.872820 loss:        0.375011
Test - acc:         0.829000 loss:        0.514941
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.873400 loss:        0.369956
Test - acc:         0.845400 loss:        0.471184
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.874660 loss:        0.368661
Test - acc:         0.806800 loss:        0.643070
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.870700 loss:        0.379846
Test - acc:         0.828100 loss:        0.530396
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.873640 loss:        0.371382
Test - acc:         0.850100 loss:        0.459919
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.874240 loss:        0.370829
Test - acc:         0.812700 loss:        0.568211
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.882920 loss:        0.345264
Test - acc:         0.760300 loss:        0.807632
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.880120 loss:        0.349920
Test - acc:         0.810300 loss:        0.560293
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.879180 loss:        0.353662
Test - acc:         0.780000 loss:        0.672536
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.880820 loss:        0.350099
Test - acc:         0.819800 loss:        0.546140
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.880880 loss:        0.348067
Test - acc:         0.831900 loss:        0.522626
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.877700 loss:        0.359196
Test - acc:         0.811400 loss:        0.618459
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.879560 loss:        0.356258
Test - acc:         0.837700 loss:        0.482612
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.881380 loss:        0.350545
Test - acc:         0.821700 loss:        0.518174
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.878740 loss:        0.355828
Test - acc:         0.846200 loss:        0.457260
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.879980 loss:        0.350736
Test - acc:         0.811600 loss:        0.581014
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.879120 loss:        0.357236
Test - acc:         0.838000 loss:        0.475814
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.879620 loss:        0.354877
Test - acc:         0.845900 loss:        0.476346
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.880000 loss:        0.350522
Test - acc:         0.827000 loss:        0.521281
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.880740 loss:        0.349542
Test - acc:         0.767700 loss:        0.770745
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.879440 loss:        0.353024
Test - acc:         0.851500 loss:        0.439964
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.877040 loss:        0.355970
Test - acc:         0.864600 loss:        0.402180
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.881280 loss:        0.350369
Test - acc:         0.850000 loss:        0.454274
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.880000 loss:        0.351256
Test - acc:         0.831800 loss:        0.488669
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.879260 loss:        0.353967
Test - acc:         0.817600 loss:        0.536125
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.878040 loss:        0.353014
Test - acc:         0.809900 loss:        0.616452
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.879200 loss:        0.353678
Test - acc:         0.846100 loss:        0.451244
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.882540 loss:        0.346528
Test - acc:         0.836100 loss:        0.494715
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.880140 loss:        0.350804
Test - acc:         0.856000 loss:        0.432610
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.878420 loss:        0.357627
Test - acc:         0.842200 loss:        0.479927
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.881100 loss:        0.349322
Test - acc:         0.830300 loss:        0.509387
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.880200 loss:        0.349907
Test - acc:         0.828300 loss:        0.525905
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.881880 loss:        0.351391
Test - acc:         0.855900 loss:        0.440587
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.880240 loss:        0.349730
Test - acc:         0.804800 loss:        0.597363
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.878320 loss:        0.356294
Test - acc:         0.839900 loss:        0.485872
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.879940 loss:        0.352902
Test - acc:         0.843700 loss:        0.467703
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.880520 loss:        0.351151
Test - acc:         0.805500 loss:        0.582524
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.881140 loss:        0.348380
Test - acc:         0.852600 loss:        0.453140
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.880500 loss:        0.353342
Test - acc:         0.812500 loss:        0.589087
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.879320 loss:        0.353969
Test - acc:         0.825600 loss:        0.554614
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.880080 loss:        0.349556
Test - acc:         0.835800 loss:        0.520340
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.878280 loss:        0.355520
Test - acc:         0.826200 loss:        0.566160
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.881140 loss:        0.349624
Test - acc:         0.807000 loss:        0.639430
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.879440 loss:        0.353950
Test - acc:         0.784800 loss:        0.674365
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.880420 loss:        0.353259
Test - acc:         0.842600 loss:        0.481745
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.879540 loss:        0.351049
Test - acc:         0.847800 loss:        0.464107
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.880800 loss:        0.349246
Test - acc:         0.804300 loss:        0.618090
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.881460 loss:        0.347280
Test - acc:         0.840700 loss:        0.469999
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.877880 loss:        0.356294
Test - acc:         0.845500 loss:        0.479300
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.878360 loss:        0.352732
Test - acc:         0.822300 loss:        0.533281
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.883080 loss:        0.341982
Test - acc:         0.806900 loss:        0.568144
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.878180 loss:        0.358214
Test - acc:         0.814500 loss:        0.574211
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.881960 loss:        0.344867
Test - acc:         0.846200 loss:        0.472082
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.879860 loss:        0.351239
Test - acc:         0.794200 loss:        0.650153
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.882000 loss:        0.347030
Test - acc:         0.773900 loss:        0.734140
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.876740 loss:        0.357424
Test - acc:         0.865600 loss:        0.390062
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.881200 loss:        0.350463
Test - acc:         0.818800 loss:        0.543532
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.880320 loss:        0.352448
Test - acc:         0.769300 loss:        0.749695
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.881240 loss:        0.350770
Test - acc:         0.815400 loss:        0.561776
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.879300 loss:        0.351809
Test - acc:         0.846800 loss:        0.466510
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.881860 loss:        0.346036
Test - acc:         0.820300 loss:        0.550689
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.880820 loss:        0.349745
Test - acc:         0.857000 loss:        0.430004
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.879940 loss:        0.349651
Test - acc:         0.799700 loss:        0.660798
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.880580 loss:        0.349729
Test - acc:         0.801700 loss:        0.618677
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.878960 loss:        0.352669
Test - acc:         0.815900 loss:        0.579625
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.878060 loss:        0.349963
Test - acc:         0.808900 loss:        0.582762
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.880040 loss:        0.348400
Test - acc:         0.840600 loss:        0.477757
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.878560 loss:        0.351599
Test - acc:         0.866300 loss:        0.400110
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.880240 loss:        0.350436
Test - acc:         0.789000 loss:        0.663433
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.879680 loss:        0.351828
Test - acc:         0.836100 loss:        0.505411
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.880760 loss:        0.348977
Test - acc:         0.868900 loss:        0.392820
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.880800 loss:        0.346813
Test - acc:         0.842800 loss:        0.475887
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.879780 loss:        0.351850
Test - acc:         0.849500 loss:        0.450115
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.881400 loss:        0.349905
Test - acc:         0.840000 loss:        0.470493
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.878360 loss:        0.354460
Test - acc:         0.778500 loss:        0.717952
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.879320 loss:        0.354955
Test - acc:         0.750000 loss:        0.806923
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.890320 loss:        0.321549
Test - acc:         0.832800 loss:        0.518572
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.891520 loss:        0.320530
Test - acc:         0.849800 loss:        0.454563
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.888220 loss:        0.324073
Test - acc:         0.843000 loss:        0.475647
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.886780 loss:        0.330272
Test - acc:         0.811600 loss:        0.608503
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.883960 loss:        0.336033
Test - acc:         0.857300 loss:        0.428205
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.885000 loss:        0.334021
Test - acc:         0.800900 loss:        0.620891
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.886420 loss:        0.336068
Test - acc:         0.847000 loss:        0.482733
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.889520 loss:        0.327025
Test - acc:         0.799100 loss:        0.633756
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.885820 loss:        0.334640
Test - acc:         0.837800 loss:        0.477334
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.885980 loss:        0.330645
Test - acc:         0.838600 loss:        0.484890
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.935180 loss:        0.193645
Test - acc:         0.922800 loss:        0.225693
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.950680 loss:        0.146699
Test - acc:         0.926700 loss:        0.217366
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.956380 loss:        0.129834
Test - acc:         0.928500 loss:        0.208957
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.962040 loss:        0.113342
Test - acc:         0.928100 loss:        0.211047
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.963800 loss:        0.107012
Test - acc:         0.931700 loss:        0.207999
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.966300 loss:        0.098203
Test - acc:         0.933000 loss:        0.207613
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.969120 loss:        0.090345
Test - acc:         0.933100 loss:        0.207220
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.971820 loss:        0.083135
Test - acc:         0.932600 loss:        0.211509
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.973180 loss:        0.079929
Test - acc:         0.933100 loss:        0.216586
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.975220 loss:        0.073254
Test - acc:         0.935000 loss:        0.218434
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.976160 loss:        0.070808
Test - acc:         0.934400 loss:        0.214719
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.977720 loss:        0.067115
Test - acc:         0.929800 loss:        0.232744
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.978780 loss:        0.062782
Test - acc:         0.934500 loss:        0.223888
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.979300 loss:        0.062576
Test - acc:         0.930500 loss:        0.239642
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.980460 loss:        0.057849
Test - acc:         0.934900 loss:        0.224396
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.981220 loss:        0.056701
Test - acc:         0.931300 loss:        0.244515
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.980920 loss:        0.057265
Test - acc:         0.930800 loss:        0.237483
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.055635
Test - acc:         0.931800 loss:        0.239066
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.981780 loss:        0.055134
Test - acc:         0.929300 loss:        0.252415
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.981820 loss:        0.055163
Test - acc:         0.926500 loss:        0.249538
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.980920 loss:        0.056575
Test - acc:         0.924800 loss:        0.259927
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.981560 loss:        0.054685
Test - acc:         0.931500 loss:        0.240847
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.982560 loss:        0.053282
Test - acc:         0.922500 loss:        0.279942
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.981320 loss:        0.054878
Test - acc:         0.928900 loss:        0.246929
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.980720 loss:        0.057689
Test - acc:         0.925800 loss:        0.260725
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.981580 loss:        0.054906
Test - acc:         0.927300 loss:        0.257833
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.982360 loss:        0.053271
Test - acc:         0.925500 loss:        0.257427
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.979920 loss:        0.058210
Test - acc:         0.926200 loss:        0.261236
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.980080 loss:        0.058764
Test - acc:         0.925400 loss:        0.263739
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.982820 loss:        0.051755
Test - acc:         0.930000 loss:        0.251174
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.980280 loss:        0.056989
Test - acc:         0.923200 loss:        0.265824
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.980600 loss:        0.058196
Test - acc:         0.923000 loss:        0.275916
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.981860 loss:        0.055129
Test - acc:         0.926000 loss:        0.263682
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.980200 loss:        0.057394
Test - acc:         0.921400 loss:        0.266299
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.979880 loss:        0.058562
Test - acc:         0.924900 loss:        0.265317
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.979520 loss:        0.061056
Test - acc:         0.927200 loss:        0.256998
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.979360 loss:        0.059107
Test - acc:         0.922200 loss:        0.262825
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.058681
Test - acc:         0.922300 loss:        0.279871
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.979700 loss:        0.061569
Test - acc:         0.921800 loss:        0.279717
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.978460 loss:        0.061705
Test - acc:         0.916900 loss:        0.298325
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.978720 loss:        0.063351
Test - acc:         0.922700 loss:        0.269559
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.978580 loss:        0.062310
Test - acc:         0.918800 loss:        0.287142
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.978780 loss:        0.062526
Test - acc:         0.922400 loss:        0.273851
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.977320 loss:        0.064782
Test - acc:         0.921100 loss:        0.289228
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.978520 loss:        0.062909
Test - acc:         0.924100 loss:        0.281264
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.978800 loss:        0.062673
Test - acc:         0.917800 loss:        0.295134
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.979660 loss:        0.061804
Test - acc:         0.921900 loss:        0.274410
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.976740 loss:        0.068918
Test - acc:         0.918100 loss:        0.289568
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.977840 loss:        0.065891
Test - acc:         0.915800 loss:        0.292539
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.978100 loss:        0.065649
Test - acc:         0.922600 loss:        0.277607
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.978400 loss:        0.063455
Test - acc:         0.912500 loss:        0.310710
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.978220 loss:        0.063573
Test - acc:         0.918400 loss:        0.285641
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.979780 loss:        0.059881
Test - acc:         0.916600 loss:        0.295947
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.977400 loss:        0.066019
Test - acc:         0.924600 loss:        0.271213
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.977680 loss:        0.065217
Test - acc:         0.921800 loss:        0.264878
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.976620 loss:        0.067119
Test - acc:         0.920000 loss:        0.277845
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.978300 loss:        0.064508
Test - acc:         0.923000 loss:        0.277898
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.977680 loss:        0.064981
Test - acc:         0.908300 loss:        0.335181
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.977780 loss:        0.065060
Test - acc:         0.916000 loss:        0.305351
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.978340 loss:        0.063981
Test - acc:         0.917600 loss:        0.289098
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.983620 loss:        0.051374
Test - acc:         0.924500 loss:        0.261705
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.985380 loss:        0.044507
Test - acc:         0.927300 loss:        0.265202
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.984280 loss:        0.046566
Test - acc:         0.922200 loss:        0.277272
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.984880 loss:        0.045321
Test - acc:         0.926000 loss:        0.268762
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.984380 loss:        0.047743
Test - acc:         0.924400 loss:        0.276109
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.986000 loss:        0.043249
Test - acc:         0.928300 loss:        0.273946
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.981720 loss:        0.052105
Test - acc:         0.921600 loss:        0.277144
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.986080 loss:        0.043527
Test - acc:         0.924200 loss:        0.283785
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.984620 loss:        0.046751
Test - acc:         0.924900 loss:        0.280681
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.982640 loss:        0.051287
Test - acc:         0.919500 loss:        0.285012
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.983720 loss:        0.049001
Test - acc:         0.924600 loss:        0.287203
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.982020 loss:        0.053652
Test - acc:         0.920700 loss:        0.286044
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.982200 loss:        0.053488
Test - acc:         0.922200 loss:        0.274444
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.983640 loss:        0.049791
Test - acc:         0.921600 loss:        0.275615
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.982700 loss:        0.052626
Test - acc:         0.917100 loss:        0.290223
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.981420 loss:        0.054562
Test - acc:         0.928600 loss:        0.268073
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.981580 loss:        0.055143
Test - acc:         0.914600 loss:        0.305126
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.982480 loss:        0.052647
Test - acc:         0.916400 loss:        0.312844
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.982980 loss:        0.051730
Test - acc:         0.921200 loss:        0.294481
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.981600 loss:        0.054987
Test - acc:         0.918700 loss:        0.297521
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.981520 loss:        0.054841
Test - acc:         0.920200 loss:        0.282320
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.981720 loss:        0.051922
Test - acc:         0.923600 loss:        0.279642
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.981060 loss:        0.056213
Test - acc:         0.920800 loss:        0.284504
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.981780 loss:        0.054830
Test - acc:         0.922900 loss:        0.268357
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.980920 loss:        0.055931
Test - acc:         0.916000 loss:        0.294446
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.982120 loss:        0.052827
Test - acc:         0.920200 loss:        0.288718
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.981400 loss:        0.056109
Test - acc:         0.922200 loss:        0.280482
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.979540 loss:        0.060246
Test - acc:         0.923100 loss:        0.270774
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.981500 loss:        0.054363
Test - acc:         0.920100 loss:        0.276763
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.981900 loss:        0.053001
Test - acc:         0.919700 loss:        0.286631
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.055497
Test - acc:         0.921700 loss:        0.294188
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.982960 loss:        0.050608
Test - acc:         0.921300 loss:        0.285006
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.981100 loss:        0.055545
Test - acc:         0.920400 loss:        0.292413
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.980740 loss:        0.056760
Test - acc:         0.920800 loss:        0.277370
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.981700 loss:        0.055321
Test - acc:         0.923600 loss:        0.279540
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.982680 loss:        0.052117
Test - acc:         0.916100 loss:        0.299701
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.980880 loss:        0.056904
Test - acc:         0.921100 loss:        0.279868
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.982360 loss:        0.053520
Test - acc:         0.916300 loss:        0.286398
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.981820 loss:        0.053482
Test - acc:         0.918000 loss:        0.307761
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.980720 loss:        0.058289
Test - acc:         0.918300 loss:        0.288241
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.990620 loss:        0.031697
Test - acc:         0.935000 loss:        0.222183
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.995200 loss:        0.018767
Test - acc:         0.937800 loss:        0.216743
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.995640 loss:        0.016718
Test - acc:         0.939100 loss:        0.217463
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.996920 loss:        0.013232
Test - acc:         0.938900 loss:        0.217373
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.997300 loss:        0.012330
Test - acc:         0.939900 loss:        0.215723
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.997800 loss:        0.010764
Test - acc:         0.938900 loss:        0.216931
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.998080 loss:        0.010218
Test - acc:         0.940000 loss:        0.217143
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.998160 loss:        0.009955
Test - acc:         0.939400 loss:        0.218165
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.998020 loss:        0.009731
Test - acc:         0.941100 loss:        0.219255
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.998700 loss:        0.008260
Test - acc:         0.941100 loss:        0.217167
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.998700 loss:        0.008123
Test - acc:         0.942100 loss:        0.217696
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.998800 loss:        0.007522
Test - acc:         0.942300 loss:        0.215281
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.999000 loss:        0.007101
Test - acc:         0.942700 loss:        0.215013
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.999000 loss:        0.007062
Test - acc:         0.942600 loss:        0.216082
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.998580 loss:        0.007496
Test - acc:         0.941800 loss:        0.216239
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.998860 loss:        0.007414
Test - acc:         0.941400 loss:        0.215483
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.998840 loss:        0.006753
Test - acc:         0.941800 loss:        0.216879
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.998880 loss:        0.006547
Test - acc:         0.941800 loss:        0.217309
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.999220 loss:        0.006082
Test - acc:         0.941900 loss:        0.216137
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.999120 loss:        0.006130
Test - acc:         0.941700 loss:        0.215347
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.005697
Test - acc:         0.942700 loss:        0.214443
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.999120 loss:        0.006035
Test - acc:         0.943700 loss:        0.214580
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.999280 loss:        0.005463
Test - acc:         0.943100 loss:        0.216495
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.005288
Test - acc:         0.943900 loss:        0.214345
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.005255
Test - acc:         0.943300 loss:        0.213722
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.005152
Test - acc:         0.942900 loss:        0.215823
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.999280 loss:        0.005298
Test - acc:         0.943100 loss:        0.215407
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.005022
Test - acc:         0.943200 loss:        0.215147
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.999200 loss:        0.005249
Test - acc:         0.943300 loss:        0.214640
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.004756
Test - acc:         0.942100 loss:        0.220776
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.998720 loss:        0.008136
Test - acc:         0.942500 loss:        0.212983
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.998920 loss:        0.007629
Test - acc:         0.941800 loss:        0.211987
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.998900 loss:        0.007347
Test - acc:         0.943200 loss:        0.212385
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.998880 loss:        0.007145
Test - acc:         0.942700 loss:        0.211203
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.999060 loss:        0.006811
Test - acc:         0.941700 loss:        0.214202
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.006833
Test - acc:         0.941700 loss:        0.212540
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.999120 loss:        0.006403
Test - acc:         0.942200 loss:        0.212357
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.999160 loss:        0.006331
Test - acc:         0.941400 loss:        0.213972
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.999220 loss:        0.006278
Test - acc:         0.942700 loss:        0.211141
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.998900 loss:        0.006488
Test - acc:         0.942100 loss:        0.211926
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.999020 loss:        0.006274
Test - acc:         0.942300 loss:        0.212127
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.999280 loss:        0.005900
Test - acc:         0.943400 loss:        0.211362
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.005664
Test - acc:         0.942300 loss:        0.213638
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.005656
Test - acc:         0.941600 loss:        0.214933
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.999240 loss:        0.005712
Test - acc:         0.941600 loss:        0.213258
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.999220 loss:        0.005779
Test - acc:         0.943900 loss:        0.212440
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.999520 loss:        0.005331
Test - acc:         0.943900 loss:        0.211150
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.005217
Test - acc:         0.943000 loss:        0.211094
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.999320 loss:        0.005461
Test - acc:         0.942400 loss:        0.213002
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.005156
Test - acc:         0.940800 loss:        0.214620
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.005412
Test - acc:         0.942100 loss:        0.213062
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.999460 loss:        0.005182
Test - acc:         0.941700 loss:        0.212973
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.999340 loss:        0.005108
Test - acc:         0.941800 loss:        0.214501
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.005068
Test - acc:         0.941700 loss:        0.214560
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.004953
Test - acc:         0.941900 loss:        0.213293
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.999300 loss:        0.004874
Test - acc:         0.942800 loss:        0.212873
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.999360 loss:        0.005218
Test - acc:         0.942700 loss:        0.210212
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.004642
Test - acc:         0.942800 loss:        0.212867
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.999400 loss:        0.005204
Test - acc:         0.943800 loss:        0.215960
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.004649
Test - acc:         0.942600 loss:        0.212736
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.004519
Test - acc:         0.942500 loss:        0.212539
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.004339
Test - acc:         0.943600 loss:        0.212254
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.004344
Test - acc:         0.942000 loss:        0.214519
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.004394
Test - acc:         0.943200 loss:        0.213799
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.999380 loss:        0.004549
Test - acc:         0.943100 loss:        0.214171
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.999720 loss:        0.004270
Test - acc:         0.942200 loss:        0.214561
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.999580 loss:        0.004312
Test - acc:         0.943800 loss:        0.211986
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.004467
Test - acc:         0.943200 loss:        0.214084
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.004316
Test - acc:         0.943500 loss:        0.212426
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004349
Test - acc:         0.943300 loss:        0.216012
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.004423
Test - acc:         0.944900 loss:        0.209973
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.999440 loss:        0.004539
Test - acc:         0.943600 loss:        0.212568
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004409
Test - acc:         0.943700 loss:        0.209402
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.004150
Test - acc:         0.943200 loss:        0.212547
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.004505
Test - acc:         0.942800 loss:        0.212371
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.004098
Test - acc:         0.944400 loss:        0.210414
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003953
Test - acc:         0.943900 loss:        0.210721
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.004233
Test - acc:         0.943900 loss:        0.210721
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.004031
Test - acc:         0.943300 loss:        0.210483
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004434
Test - acc:         0.944100 loss:        0.211556
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.999740 loss:        0.003977
Test - acc:         0.943500 loss:        0.211208
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.999560 loss:        0.004396
Test - acc:         0.943800 loss:        0.213427
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.004121
Test - acc:         0.944200 loss:        0.210635
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.999760 loss:        0.003827
Test - acc:         0.944600 loss:        0.212146
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003941
Test - acc:         0.944200 loss:        0.210037
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.004044
Test - acc:         0.945000 loss:        0.209814
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.999700 loss:        0.003867
Test - acc:         0.944800 loss:        0.210009
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.003758
Test - acc:         0.944600 loss:        0.209477
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.999660 loss:        0.004024
Test - acc:         0.943900 loss:        0.209456
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.999500 loss:        0.004177
Test - acc:         0.943100 loss:        0.212253
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.999680 loss:        0.003724
Test - acc:         0.943200 loss:        0.210511
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003847
Test - acc:         0.942800 loss:        0.209558
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.999480 loss:        0.004171
Test - acc:         0.944700 loss:        0.209390
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.999620 loss:        0.003994
Test - acc:         0.944900 loss:        0.208748
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.999640 loss:        0.003854
Test - acc:         0.943000 loss:        0.212627
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.999600 loss:        0.003933
Test - acc:         0.944200 loss:        0.210245
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.999820 loss:        0.003622
Test - acc:         0.943600 loss:        0.210505
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.999780 loss:        0.003705
Test - acc:         0.944100 loss:        0.211531
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.999840 loss:        0.003694
Test - acc:         0.944800 loss:        0.211035
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.999540 loss:        0.003964
Test - acc:         0.943800 loss:        0.211302
Sparsity :          0.9375
Wdecay :        0.000500
