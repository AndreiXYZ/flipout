Running --prune_bnorm --model resnet18 --noise --prune_criterion weight_div_flips --seed 43 --prune_freq 39 --prune_rate 0.5 --comment=resnet18_crit=weight_div_flips_pf=39_seed=43 --save_model=pre-finetune/resnet18_weight_div_flips_pf39_s43 --logdir=criterion_experiment_prune_bnorm/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": true,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_prune_bnorm/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_weight_div_flips_pf39_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11169152
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11169152
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.275580 loss:        2.206162
Test - acc:         0.379300 loss:        1.773930
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.448080 loss:        1.503151
Test - acc:         0.529700 loss:        1.350102
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.561660 loss:        1.220748
Test - acc:         0.589900 loss:        1.153149
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.644920 loss:        1.002910
Test - acc:         0.641300 loss:        1.060353
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.705420 loss:        0.837933
Test - acc:         0.662400 loss:        0.967185
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.753280 loss:        0.707195
Test - acc:         0.719800 loss:        0.820684
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.778140 loss:        0.637992
Test - acc:         0.746700 loss:        0.735908
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.797340 loss:        0.584231
Test - acc:         0.764400 loss:        0.731834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.810600 loss:        0.549598
Test - acc:         0.739700 loss:        0.764843
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.816060 loss:        0.533603
Test - acc:         0.747800 loss:        0.758131
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.822860 loss:        0.513958
Test - acc:         0.768500 loss:        0.701588
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.827900 loss:        0.501282
Test - acc:         0.816000 loss:        0.537637
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.833580 loss:        0.483668
Test - acc:         0.810200 loss:        0.568396
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.836280 loss:        0.476076
Test - acc:         0.768400 loss:        0.720073
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.839420 loss:        0.464692
Test - acc:         0.759300 loss:        0.735929
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.845000 loss:        0.453000
Test - acc:         0.824500 loss:        0.526457
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.849680 loss:        0.444501
Test - acc:         0.826500 loss:        0.520368
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.849380 loss:        0.438538
Test - acc:         0.798100 loss:        0.637503
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.850040 loss:        0.440032
Test - acc:         0.829700 loss:        0.507356
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.854180 loss:        0.426594
Test - acc:         0.802500 loss:        0.589427
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.855560 loss:        0.423533
Test - acc:         0.786400 loss:        0.632288
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.855960 loss:        0.420786
Test - acc:         0.789600 loss:        0.646576
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.858360 loss:        0.416946
Test - acc:         0.752800 loss:        0.756219
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.860040 loss:        0.415164
Test - acc:         0.812200 loss:        0.558783
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.858620 loss:        0.414178
Test - acc:         0.814800 loss:        0.577834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.859180 loss:        0.410590
Test - acc:         0.813800 loss:        0.559850
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.860320 loss:        0.407069
Test - acc:         0.851000 loss:        0.442927
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.857900 loss:        0.414354
Test - acc:         0.838400 loss:        0.479358
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.861760 loss:        0.403122
Test - acc:         0.813600 loss:        0.552473
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.864300 loss:        0.400279
Test - acc:         0.798500 loss:        0.615302
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.864260 loss:        0.395054
Test - acc:         0.802800 loss:        0.596695
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.862920 loss:        0.397425
Test - acc:         0.813300 loss:        0.539908
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.863160 loss:        0.393080
Test - acc:         0.815100 loss:        0.580103
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.865860 loss:        0.394840
Test - acc:         0.824300 loss:        0.534873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.864560 loss:        0.393687
Test - acc:         0.828400 loss:        0.520443
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.867500 loss:        0.385755
Test - acc:         0.811500 loss:        0.577941
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.864660 loss:        0.392658
Test - acc:         0.724200 loss:        0.831120
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.866840 loss:        0.389900
Test - acc:         0.847400 loss:        0.461297
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.869240 loss:        0.384570
Test - acc:         0.804400 loss:        0.615804
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.878580 loss:        0.359454
Test - acc:         0.829700 loss:        0.492504
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.875460 loss:        0.361205
Test - acc:         0.832000 loss:        0.521761
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.877220 loss:        0.360297
Test - acc:         0.853900 loss:        0.440405
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.875180 loss:        0.366197
Test - acc:         0.825400 loss:        0.531022
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.875560 loss:        0.365201
Test - acc:         0.797800 loss:        0.653621
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.875240 loss:        0.363384
Test - acc:         0.858000 loss:        0.412969
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.877220 loss:        0.360144
Test - acc:         0.795900 loss:        0.604434
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.874280 loss:        0.368371
Test - acc:         0.818400 loss:        0.553813
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.876540 loss:        0.364572
Test - acc:         0.856200 loss:        0.434306
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.875680 loss:        0.362619
Test - acc:         0.828600 loss:        0.522009
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.877820 loss:        0.356301
Test - acc:         0.835500 loss:        0.486594
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.874840 loss:        0.361567
Test - acc:         0.821400 loss:        0.576077
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.878460 loss:        0.357212
Test - acc:         0.812800 loss:        0.573140
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.877720 loss:        0.359280
Test - acc:         0.827600 loss:        0.549184
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.876760 loss:        0.357482
Test - acc:         0.831400 loss:        0.499758
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.875920 loss:        0.362093
Test - acc:         0.862100 loss:        0.412901
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.875480 loss:        0.362236
Test - acc:         0.830500 loss:        0.504203
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.872860 loss:        0.367438
Test - acc:         0.795800 loss:        0.653890
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.874680 loss:        0.361249
Test - acc:         0.851000 loss:        0.446862
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.877380 loss:        0.358480
Test - acc:         0.796100 loss:        0.681053
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.877360 loss:        0.362828
Test - acc:         0.786800 loss:        0.642181
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.875960 loss:        0.360884
Test - acc:         0.811500 loss:        0.577493
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.876200 loss:        0.364551
Test - acc:         0.827000 loss:        0.545782
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.879700 loss:        0.349317
Test - acc:         0.810600 loss:        0.590901
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.878880 loss:        0.356407
Test - acc:         0.833000 loss:        0.519416
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.877800 loss:        0.360732
Test - acc:         0.810800 loss:        0.610512
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.880380 loss:        0.356651
Test - acc:         0.828300 loss:        0.502002
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.877600 loss:        0.360537
Test - acc:         0.858200 loss:        0.430006
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.877800 loss:        0.354808
Test - acc:         0.853900 loss:        0.446127
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.877180 loss:        0.358196
Test - acc:         0.748200 loss:        0.855743
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.876780 loss:        0.360753
Test - acc:         0.829200 loss:        0.547293
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.875740 loss:        0.363130
Test - acc:         0.825400 loss:        0.515184
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.879960 loss:        0.356039
Test - acc:         0.833200 loss:        0.503607
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.874560 loss:        0.364213
Test - acc:         0.829100 loss:        0.528843
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.879040 loss:        0.355850
Test - acc:         0.835200 loss:        0.501693
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.876920 loss:        0.359412
Test - acc:         0.856000 loss:        0.437837
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.877900 loss:        0.356550
Test - acc:         0.829300 loss:        0.565738
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.876640 loss:        0.357427
Test - acc:         0.840500 loss:        0.477977
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.877040 loss:        0.356325
Test - acc:         0.839700 loss:        0.487071
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.888640 loss:        0.323448
Test - acc:         0.850600 loss:        0.453667
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.889280 loss:        0.325237
Test - acc:         0.838300 loss:        0.506683
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.885820 loss:        0.333712
Test - acc:         0.791400 loss:        0.653933
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.883500 loss:        0.334521
Test - acc:         0.850100 loss:        0.463813
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.880680 loss:        0.346116
Test - acc:         0.841400 loss:        0.479661
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.885780 loss:        0.335535
Test - acc:         0.858000 loss:        0.421968
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.884960 loss:        0.336030
Test - acc:         0.788400 loss:        0.692488
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.885440 loss:        0.335506
Test - acc:         0.838700 loss:        0.486040
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.885720 loss:        0.332047
Test - acc:         0.805600 loss:        0.590527
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.884660 loss:        0.339058
Test - acc:         0.874900 loss:        0.376820
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.882980 loss:        0.342903
Test - acc:         0.844100 loss:        0.476628
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.883220 loss:        0.340672
Test - acc:         0.822800 loss:        0.541777
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.886120 loss:        0.335432
Test - acc:         0.828600 loss:        0.537764
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.884260 loss:        0.338849
Test - acc:         0.847200 loss:        0.464811
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.886580 loss:        0.331810
Test - acc:         0.810000 loss:        0.596596
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.885400 loss:        0.334381
Test - acc:         0.837700 loss:        0.502111
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.883740 loss:        0.340541
Test - acc:         0.813100 loss:        0.572849
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.887140 loss:        0.334382
Test - acc:         0.834100 loss:        0.480174
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.884880 loss:        0.335160
Test - acc:         0.825500 loss:        0.546907
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.884860 loss:        0.333503
Test - acc:         0.783300 loss:        0.706428
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.884120 loss:        0.335529
Test - acc:         0.836200 loss:        0.524093
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.885760 loss:        0.335197
Test - acc:         0.790600 loss:        0.650711
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.884580 loss:        0.332270
Test - acc:         0.838900 loss:        0.482347
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.886480 loss:        0.335961
Test - acc:         0.818300 loss:        0.525416
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.888240 loss:        0.332421
Test - acc:         0.815100 loss:        0.562067
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.884380 loss:        0.335670
Test - acc:         0.805100 loss:        0.657211
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.883600 loss:        0.337488
Test - acc:         0.790800 loss:        0.649926
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.885060 loss:        0.333917
Test - acc:         0.862200 loss:        0.416294
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.882480 loss:        0.340792
Test - acc:         0.807900 loss:        0.588021
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.886080 loss:        0.331604
Test - acc:         0.830200 loss:        0.529808
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.885300 loss:        0.334306
Test - acc:         0.820600 loss:        0.540930
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.884600 loss:        0.337463
Test - acc:         0.803900 loss:        0.578824
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.885000 loss:        0.336660
Test - acc:         0.805000 loss:        0.607222
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.884740 loss:        0.336444
Test - acc:         0.846200 loss:        0.456432
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.881820 loss:        0.339417
Test - acc:         0.857200 loss:        0.436256
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.885940 loss:        0.335286
Test - acc:         0.845800 loss:        0.469756
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.883640 loss:        0.335799
Test - acc:         0.851900 loss:        0.442227
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.885260 loss:        0.335691
Test - acc:         0.858500 loss:        0.413253
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.885380 loss:        0.336834
Test - acc:         0.815500 loss:        0.563909
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.896500 loss:        0.299967
Test - acc:         0.867500 loss:        0.400359
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.893860 loss:        0.306372
Test - acc:         0.789400 loss:        0.690181
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.890220 loss:        0.315955
Test - acc:         0.849200 loss:        0.466716
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.893320 loss:        0.311214
Test - acc:         0.832200 loss:        0.529429
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.892460 loss:        0.313017
Test - acc:         0.834100 loss:        0.502279
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.892700 loss:        0.311535
Test - acc:         0.868300 loss:        0.382460
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.893820 loss:        0.310331
Test - acc:         0.848600 loss:        0.444398
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.891440 loss:        0.314070
Test - acc:         0.826900 loss:        0.531978
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.893260 loss:        0.314836
Test - acc:         0.829800 loss:        0.521182
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.892060 loss:        0.314757
Test - acc:         0.841300 loss:        0.487096
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.892160 loss:        0.317702
Test - acc:         0.859200 loss:        0.441833
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.892920 loss:        0.307317
Test - acc:         0.810200 loss:        0.584082
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.892760 loss:        0.312909
Test - acc:         0.821200 loss:        0.581318
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.893500 loss:        0.309908
Test - acc:         0.862500 loss:        0.405320
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.891460 loss:        0.313870
Test - acc:         0.787700 loss:        0.677328
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.890880 loss:        0.318178
Test - acc:         0.829700 loss:        0.539672
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.894100 loss:        0.309445
Test - acc:         0.856900 loss:        0.430219
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.892600 loss:        0.315016
Test - acc:         0.819300 loss:        0.574868
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.894760 loss:        0.311810
Test - acc:         0.856600 loss:        0.427913
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.890320 loss:        0.321345
Test - acc:         0.825500 loss:        0.528037
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.894220 loss:        0.309199
Test - acc:         0.821000 loss:        0.558888
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.892940 loss:        0.313110
Test - acc:         0.789100 loss:        0.643020
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.891780 loss:        0.315540
Test - acc:         0.842300 loss:        0.479064
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.892940 loss:        0.314529
Test - acc:         0.834200 loss:        0.506687
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.893600 loss:        0.313113
Test - acc:         0.834300 loss:        0.514616
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.890680 loss:        0.316642
Test - acc:         0.833000 loss:        0.507123
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.890900 loss:        0.314246
Test - acc:         0.857500 loss:        0.431628
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.892840 loss:        0.313989
Test - acc:         0.819500 loss:        0.553470
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.892340 loss:        0.314030
Test - acc:         0.870200 loss:        0.390228
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.893140 loss:        0.311625
Test - acc:         0.821300 loss:        0.583106
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.891580 loss:        0.312086
Test - acc:         0.830700 loss:        0.517793
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.890380 loss:        0.315585
Test - acc:         0.841400 loss:        0.491990
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.892240 loss:        0.313746
Test - acc:         0.846200 loss:        0.484228
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.938900 loss:        0.182398
Test - acc:         0.923100 loss:        0.226150
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.953520 loss:        0.140849
Test - acc:         0.925400 loss:        0.219873
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.958120 loss:        0.123959
Test - acc:         0.930000 loss:        0.211264
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.960880 loss:        0.115189
Test - acc:         0.931700 loss:        0.206191
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.964160 loss:        0.106832
Test - acc:         0.933300 loss:        0.206732
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.967240 loss:        0.097643
Test - acc:         0.930600 loss:        0.212999
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.969600 loss:        0.090803
Test - acc:         0.932200 loss:        0.210896
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.971960 loss:        0.084643
Test - acc:         0.933700 loss:        0.210965
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.972560 loss:        0.082727
Test - acc:         0.933500 loss:        0.206099
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.974180 loss:        0.076823
Test - acc:         0.933100 loss:        0.208185
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.975680 loss:        0.072805
Test - acc:         0.931100 loss:        0.214957
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.976120 loss:        0.069172
Test - acc:         0.934700 loss:        0.210608
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.976400 loss:        0.067856
Test - acc:         0.933400 loss:        0.214698
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.978880 loss:        0.064113
Test - acc:         0.930300 loss:        0.227022
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.978200 loss:        0.064390
Test - acc:         0.933400 loss:        0.225394
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.979440 loss:        0.062168
Test - acc:         0.930800 loss:        0.226025
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.979740 loss:        0.060586
Test - acc:         0.933600 loss:        0.222231
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.981480 loss:        0.056592
Test - acc:         0.933900 loss:        0.221990
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.981520 loss:        0.055852
Test - acc:         0.931900 loss:        0.233741
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.980760 loss:        0.056432
Test - acc:         0.931100 loss:        0.238982
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.981040 loss:        0.056247
Test - acc:         0.933100 loss:        0.227148
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.980660 loss:        0.057018
Test - acc:         0.927900 loss:        0.244871
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.981600 loss:        0.055227
Test - acc:         0.930700 loss:        0.235089
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.981980 loss:        0.052328
Test - acc:         0.930100 loss:        0.240459
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.981640 loss:        0.053138
Test - acc:         0.929300 loss:        0.242855
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.982480 loss:        0.053225
Test - acc:         0.924000 loss:        0.254885
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.982240 loss:        0.052431
Test - acc:         0.932100 loss:        0.233868
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.982720 loss:        0.052241
Test - acc:         0.924100 loss:        0.254872
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.982440 loss:        0.051981
Test - acc:         0.929700 loss:        0.242085
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.980760 loss:        0.057316
Test - acc:         0.928800 loss:        0.246655
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.055233
Test - acc:         0.926700 loss:        0.248432
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.980680 loss:        0.056712
Test - acc:         0.926700 loss:        0.255075
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.981700 loss:        0.055325
Test - acc:         0.924400 loss:        0.261494
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.981360 loss:        0.056048
Test - acc:         0.925400 loss:        0.251032
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.980300 loss:        0.059185
Test - acc:         0.923300 loss:        0.268186
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.980960 loss:        0.057084
Test - acc:         0.924500 loss:        0.265059
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.980180 loss:        0.058553
Test - acc:         0.925000 loss:        0.267982
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.979800 loss:        0.058343
Test - acc:         0.924100 loss:        0.260564
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.979880 loss:        0.059020
Test - acc:         0.921500 loss:        0.273615
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.979880 loss:        0.060756
Test - acc:         0.920600 loss:        0.281144
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.979200 loss:        0.063974
Test - acc:         0.919800 loss:        0.267848
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.977940 loss:        0.065552
Test - acc:         0.924400 loss:        0.266517
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.979440 loss:        0.060544
Test - acc:         0.924800 loss:        0.255402
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.978520 loss:        0.063311
Test - acc:         0.927400 loss:        0.254955
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.977960 loss:        0.065192
Test - acc:         0.924100 loss:        0.266817
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.977840 loss:        0.065976
Test - acc:         0.924600 loss:        0.248666
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.980220 loss:        0.059700
Test - acc:         0.919100 loss:        0.282469
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.980000 loss:        0.059055
Test - acc:         0.924300 loss:        0.253351
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.980080 loss:        0.058164
Test - acc:         0.923400 loss:        0.265840
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.979480 loss:        0.060555
Test - acc:         0.923100 loss:        0.250289
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.980940 loss:        0.057542
Test - acc:         0.920300 loss:        0.280247
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.982520 loss:        0.054672
Test - acc:         0.923900 loss:        0.261884
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.981860 loss:        0.053941
Test - acc:         0.924600 loss:        0.270035
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.978760 loss:        0.063419
Test - acc:         0.924000 loss:        0.264405
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.981280 loss:        0.054684
Test - acc:         0.921900 loss:        0.278844
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.980820 loss:        0.056734
Test - acc:         0.921800 loss:        0.274861
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.981520 loss:        0.056702
Test - acc:         0.920500 loss:        0.276092
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.980760 loss:        0.056888
Test - acc:         0.919100 loss:        0.283888
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.979580 loss:        0.059207
Test - acc:         0.917700 loss:        0.281026
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.978520 loss:        0.063088
Test - acc:         0.922300 loss:        0.263669
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.979340 loss:        0.061800
Test - acc:         0.918100 loss:        0.271753
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.978220 loss:        0.064194
Test - acc:         0.921000 loss:        0.264483
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.980720 loss:        0.057819
Test - acc:         0.925300 loss:        0.259823
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.977980 loss:        0.064289
Test - acc:         0.917000 loss:        0.296955
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.978760 loss:        0.062552
Test - acc:         0.915300 loss:        0.300377
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.980560 loss:        0.056420
Test - acc:         0.925700 loss:        0.260787
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.979580 loss:        0.060824
Test - acc:         0.925400 loss:        0.266165
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.980400 loss:        0.057426
Test - acc:         0.920300 loss:        0.269815
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.977880 loss:        0.065231
Test - acc:         0.917400 loss:        0.281368
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.979580 loss:        0.060506
Test - acc:         0.922300 loss:        0.278679
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.978580 loss:        0.063837
Test - acc:         0.924900 loss:        0.261859
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.979580 loss:        0.061860
Test - acc:         0.924200 loss:        0.272207
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.978860 loss:        0.061761
Test - acc:         0.921800 loss:        0.265598
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.979280 loss:        0.062094
Test - acc:         0.921800 loss:        0.275591
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.979440 loss:        0.060221
Test - acc:         0.917700 loss:        0.291763
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.978720 loss:        0.062927
Test - acc:         0.911900 loss:        0.303441
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.979880 loss:        0.060419
Test - acc:         0.923900 loss:        0.268972
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.979720 loss:        0.061493
Test - acc:         0.917500 loss:        0.292671
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.979120 loss:        0.061745
Test - acc:         0.925700 loss:        0.257188
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.978640 loss:        0.062477
Test - acc:         0.919500 loss:        0.280568
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.977740 loss:        0.065190
Test - acc:         0.923800 loss:        0.265796
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.979040 loss:        0.061397
Test - acc:         0.915100 loss:        0.294448
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.977260 loss:        0.065175
Test - acc:         0.917000 loss:        0.301201
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.979920 loss:        0.060285
Test - acc:         0.922000 loss:        0.267946
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.968360 loss:        0.094631
Test - acc:         0.923800 loss:        0.253587
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.971780 loss:        0.081802
Test - acc:         0.920900 loss:        0.265594
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.972720 loss:        0.079971
Test - acc:         0.914500 loss:        0.296037
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.974480 loss:        0.074073
Test - acc:         0.921500 loss:        0.268251
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.972540 loss:        0.079905
Test - acc:         0.925800 loss:        0.254943
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.974340 loss:        0.074165
Test - acc:         0.927100 loss:        0.243102
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.974900 loss:        0.073455
Test - acc:         0.912300 loss:        0.298960
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.974220 loss:        0.075181
Test - acc:         0.913000 loss:        0.301226
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.973240 loss:        0.077185
Test - acc:         0.919600 loss:        0.271978
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.975740 loss:        0.071961
Test - acc:         0.924200 loss:        0.263775
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.974120 loss:        0.074800
Test - acc:         0.919200 loss:        0.269901
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.975700 loss:        0.071630
Test - acc:         0.920900 loss:        0.275962
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.976340 loss:        0.070277
Test - acc:         0.918700 loss:        0.278963
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.975580 loss:        0.073210
Test - acc:         0.918600 loss:        0.279951
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.973960 loss:        0.075719
Test - acc:         0.916000 loss:        0.282421
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.975620 loss:        0.073769
Test - acc:         0.920700 loss:        0.269707
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.984720 loss:        0.048629
Test - acc:         0.935700 loss:        0.218398
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.989000 loss:        0.036362
Test - acc:         0.938500 loss:        0.214412
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.991460 loss:        0.032223
Test - acc:         0.938700 loss:        0.213665
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.991780 loss:        0.030015
Test - acc:         0.938500 loss:        0.214925
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.992640 loss:        0.028658
Test - acc:         0.938000 loss:        0.215816
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.994000 loss:        0.025916
Test - acc:         0.938300 loss:        0.214255
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.993280 loss:        0.025350
Test - acc:         0.938200 loss:        0.214045
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.994020 loss:        0.023997
Test - acc:         0.940200 loss:        0.215200
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.994600 loss:        0.022855
Test - acc:         0.938900 loss:        0.215568
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.993940 loss:        0.024177
Test - acc:         0.938100 loss:        0.217927
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.994740 loss:        0.021860
Test - acc:         0.939700 loss:        0.216898
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.994800 loss:        0.021276
Test - acc:         0.937900 loss:        0.216924
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.994920 loss:        0.021014
Test - acc:         0.938800 loss:        0.217708
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.994800 loss:        0.021201
Test - acc:         0.938700 loss:        0.217490
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.995260 loss:        0.019784
Test - acc:         0.937400 loss:        0.217935
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.995940 loss:        0.019022
Test - acc:         0.938600 loss:        0.216784
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.995560 loss:        0.019358
Test - acc:         0.939300 loss:        0.219518
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.995920 loss:        0.018445
Test - acc:         0.938100 loss:        0.218982
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.996080 loss:        0.017766
Test - acc:         0.938800 loss:        0.217478
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.995820 loss:        0.018076
Test - acc:         0.939100 loss:        0.217507
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.996080 loss:        0.017302
Test - acc:         0.938700 loss:        0.218959
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.995760 loss:        0.017712
Test - acc:         0.938400 loss:        0.218029
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.996620 loss:        0.016423
Test - acc:         0.939500 loss:        0.217435
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.948580 loss:        0.151759
Test - acc:         0.916000 loss:        0.271279
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.960220 loss:        0.116878
Test - acc:         0.920100 loss:        0.256073
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.963120 loss:        0.107410
Test - acc:         0.921100 loss:        0.252248
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.967060 loss:        0.098011
Test - acc:         0.922800 loss:        0.244942
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.968880 loss:        0.093076
Test - acc:         0.923700 loss:        0.245977
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.970400 loss:        0.088788
Test - acc:         0.922900 loss:        0.245392
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.971020 loss:        0.085809
Test - acc:         0.925000 loss:        0.241881
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.973120 loss:        0.082105
Test - acc:         0.924500 loss:        0.242340
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.973860 loss:        0.080099
Test - acc:         0.926300 loss:        0.242473
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.974440 loss:        0.077428
Test - acc:         0.926200 loss:        0.239956
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.974460 loss:        0.076695
Test - acc:         0.927000 loss:        0.238788
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.975760 loss:        0.073858
Test - acc:         0.929200 loss:        0.237398
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.976600 loss:        0.073548
Test - acc:         0.927500 loss:        0.240769
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.976920 loss:        0.070328
Test - acc:         0.926700 loss:        0.242031
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.977380 loss:        0.069495
Test - acc:         0.927100 loss:        0.245220
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.978880 loss:        0.067707
Test - acc:         0.927000 loss:        0.240978
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.978600 loss:        0.066913
Test - acc:         0.926300 loss:        0.241902
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.979360 loss:        0.066008
Test - acc:         0.928700 loss:        0.240487
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.980900 loss:        0.062702
Test - acc:         0.929100 loss:        0.243039
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.979600 loss:        0.063313
Test - acc:         0.927600 loss:        0.242740
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.981240 loss:        0.060939
Test - acc:         0.928400 loss:        0.245234
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.980620 loss:        0.061160
Test - acc:         0.927400 loss:        0.245125
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.981080 loss:        0.059395
Test - acc:         0.928400 loss:        0.245851
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.981000 loss:        0.059749
Test - acc:         0.926700 loss:        0.243755
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.981500 loss:        0.059182
Test - acc:         0.927900 loss:        0.242489
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.981400 loss:        0.057745
Test - acc:         0.928200 loss:        0.246634
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.980380 loss:        0.058363
Test - acc:         0.927600 loss:        0.245629
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.981940 loss:        0.056093
Test - acc:         0.927900 loss:        0.247709
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.982220 loss:        0.056020
Test - acc:         0.927900 loss:        0.246377
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.982660 loss:        0.055767
Test - acc:         0.929900 loss:        0.243868
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.983100 loss:        0.054933
Test - acc:         0.929800 loss:        0.244587
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.982040 loss:        0.055287
Test - acc:         0.928400 loss:        0.244523
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.983260 loss:        0.054126
Test - acc:         0.929500 loss:        0.245363
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.984500 loss:        0.051717
Test - acc:         0.927600 loss:        0.250393
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.983520 loss:        0.051453
Test - acc:         0.928800 loss:        0.247543
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.982840 loss:        0.051979
Test - acc:         0.927900 loss:        0.252878
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.984060 loss:        0.051132
Test - acc:         0.930900 loss:        0.244987
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.984680 loss:        0.050405
Test - acc:         0.927900 loss:        0.249505
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.984080 loss:        0.050234
Test - acc:         0.929000 loss:        0.250069
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.859720 loss:        0.410294
Test - acc:         0.876600 loss:        0.364739
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.895280 loss:        0.305153
Test - acc:         0.887200 loss:        0.340320
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.908080 loss:        0.267856
Test - acc:         0.893900 loss:        0.321832
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
