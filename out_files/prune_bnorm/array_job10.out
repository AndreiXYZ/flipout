Running --prune_bnorm --model resnet18 --prune_criterion global_magnitude --seed 42 --prune_freq 32 --prune_rate 0.5 --comment=resnet18_crit=global_magnitude_pf=32_seed=42 --save_model=pre-finetune/resnet18_global_magnitude_pf32_s42 --logdir=criterion_experiment_prune_bnorm/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "global_magnitude",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": true,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_prune_bnorm/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_global_magnitude_pf32_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11169152
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11169152
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.314940 loss:        1.878041
Test - acc:         0.435800 loss:        1.523629
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.486540 loss:        1.393735
Test - acc:         0.544700 loss:        1.248046
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.596620 loss:        1.116283
Test - acc:         0.606000 loss:        1.122062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.668000 loss:        0.935051
Test - acc:         0.626100 loss:        1.067730
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.720100 loss:        0.804828
Test - acc:         0.702000 loss:        0.888096
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.761560 loss:        0.685870
Test - acc:         0.699000 loss:        0.853650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.787240 loss:        0.615040
Test - acc:         0.773100 loss:        0.658646
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.804960 loss:        0.565471
Test - acc:         0.792800 loss:        0.602051
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.813040 loss:        0.537862
Test - acc:         0.740800 loss:        0.815277
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.826920 loss:        0.505267
Test - acc:         0.694300 loss:        0.917833
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.833640 loss:        0.483014
Test - acc:         0.746800 loss:        0.775968
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.840240 loss:        0.464427
Test - acc:         0.760900 loss:        0.763097
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.847800 loss:        0.448224
Test - acc:         0.828900 loss:        0.508553
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.847600 loss:        0.444935
Test - acc:         0.774100 loss:        0.694553
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.852480 loss:        0.431459
Test - acc:         0.830600 loss:        0.505960
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.855420 loss:        0.421620
Test - acc:         0.841100 loss:        0.473837
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.856580 loss:        0.414233
Test - acc:         0.776600 loss:        0.692520
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.861980 loss:        0.403805
Test - acc:         0.838500 loss:        0.467352
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.864380 loss:        0.397561
Test - acc:         0.831300 loss:        0.485505
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.864280 loss:        0.392246
Test - acc:         0.830200 loss:        0.541550
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.868380 loss:        0.385116
Test - acc:         0.826100 loss:        0.520540
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.869220 loss:        0.381565
Test - acc:         0.826100 loss:        0.532683
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.871500 loss:        0.378418
Test - acc:         0.827400 loss:        0.509242
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.872600 loss:        0.371732
Test - acc:         0.807100 loss:        0.601565
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.873800 loss:        0.368222
Test - acc:         0.813700 loss:        0.570644
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.874760 loss:        0.369126
Test - acc:         0.825300 loss:        0.542713
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.877480 loss:        0.362290
Test - acc:         0.838900 loss:        0.472458
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.874340 loss:        0.366924
Test - acc:         0.840700 loss:        0.467696
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.878780 loss:        0.356626
Test - acc:         0.813000 loss:        0.609081
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.876760 loss:        0.359111
Test - acc:         0.780000 loss:        0.711481
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.877420 loss:        0.355311
Test - acc:         0.845400 loss:        0.458797
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.878700 loss:        0.354907
Test - acc:         0.770800 loss:        0.689277
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.889760 loss:        0.324866
Test - acc:         0.829900 loss:        0.508780
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.887100 loss:        0.330335
Test - acc:         0.803600 loss:        0.604159
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.890420 loss:        0.321384
Test - acc:         0.837100 loss:        0.508011
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.887060 loss:        0.326456
Test - acc:         0.849300 loss:        0.458060
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.885800 loss:        0.327395
Test - acc:         0.859500 loss:        0.422404
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.887580 loss:        0.326106
Test - acc:         0.840700 loss:        0.488959
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.890380 loss:        0.322937
Test - acc:         0.852600 loss:        0.443451
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.888600 loss:        0.323608
Test - acc:         0.754700 loss:        0.757831
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.889820 loss:        0.323769
Test - acc:         0.833100 loss:        0.559785
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.893380 loss:        0.314080
Test - acc:         0.857400 loss:        0.437625
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.893620 loss:        0.313346
Test - acc:         0.832200 loss:        0.519961
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.891760 loss:        0.315536
Test - acc:         0.823000 loss:        0.551003
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.892320 loss:        0.318761
Test - acc:         0.844500 loss:        0.464041
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.889640 loss:        0.320851
Test - acc:         0.840900 loss:        0.508363
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.893340 loss:        0.317458
Test - acc:         0.836900 loss:        0.494757
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.890500 loss:        0.319117
Test - acc:         0.783100 loss:        0.683723
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.895640 loss:        0.308367
Test - acc:         0.774300 loss:        0.724565
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.892200 loss:        0.316625
Test - acc:         0.844200 loss:        0.476489
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.895500 loss:        0.306425
Test - acc:         0.830700 loss:        0.536717
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.891900 loss:        0.314408
Test - acc:         0.858100 loss:        0.419089
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.896380 loss:        0.305043
Test - acc:         0.853400 loss:        0.436793
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.893600 loss:        0.311067
Test - acc:         0.799300 loss:        0.662430
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.895180 loss:        0.305289
Test - acc:         0.819900 loss:        0.555757
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.894220 loss:        0.308012
Test - acc:         0.846800 loss:        0.471584
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.894540 loss:        0.307755
Test - acc:         0.847900 loss:        0.436601
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.894440 loss:        0.310704
Test - acc:         0.869200 loss:        0.396501
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.895420 loss:        0.308491
Test - acc:         0.863900 loss:        0.395113
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.895800 loss:        0.306405
Test - acc:         0.841600 loss:        0.464526
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.897380 loss:        0.303768
Test - acc:         0.845100 loss:        0.480259
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.895360 loss:        0.306415
Test - acc:         0.794200 loss:        0.661013
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.897280 loss:        0.304162
Test - acc:         0.823600 loss:        0.551116
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.896040 loss:        0.306320
Test - acc:         0.853700 loss:        0.448329
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.906740 loss:        0.272989
Test - acc:         0.839800 loss:        0.473594
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.908600 loss:        0.266941
Test - acc:         0.826500 loss:        0.546816
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.903900 loss:        0.277849
Test - acc:         0.868000 loss:        0.402113
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.903740 loss:        0.279437
Test - acc:         0.834900 loss:        0.499099
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.903540 loss:        0.280758
Test - acc:         0.818100 loss:        0.563799
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.904580 loss:        0.278983
Test - acc:         0.869200 loss:        0.390796
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.902840 loss:        0.281340
Test - acc:         0.873700 loss:        0.379543
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.904000 loss:        0.277196
Test - acc:         0.867700 loss:        0.389170
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.906020 loss:        0.275645
Test - acc:         0.851600 loss:        0.445343
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.905160 loss:        0.280377
Test - acc:         0.868300 loss:        0.386010
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.904860 loss:        0.276807
Test - acc:         0.839800 loss:        0.508594
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.905380 loss:        0.275441
Test - acc:         0.769800 loss:        0.789669
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.904580 loss:        0.279130
Test - acc:         0.828500 loss:        0.542625
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.906200 loss:        0.270233
Test - acc:         0.853200 loss:        0.444614
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.904580 loss:        0.278766
Test - acc:         0.840000 loss:        0.482405
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.905300 loss:        0.276561
Test - acc:         0.875600 loss:        0.374167
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.904180 loss:        0.280897
Test - acc:         0.824200 loss:        0.564514
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.905660 loss:        0.276739
Test - acc:         0.871900 loss:        0.404789
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.906460 loss:        0.271935
Test - acc:         0.803100 loss:        0.640152
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.906280 loss:        0.273394
Test - acc:         0.838400 loss:        0.501502
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.904940 loss:        0.277615
Test - acc:         0.867000 loss:        0.398520
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.906100 loss:        0.273138
Test - acc:         0.878000 loss:        0.368049
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.905500 loss:        0.275187
Test - acc:         0.864200 loss:        0.405787
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.906980 loss:        0.272276
Test - acc:         0.870300 loss:        0.379449
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.903420 loss:        0.281510
Test - acc:         0.837900 loss:        0.479907
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.906640 loss:        0.273198
Test - acc:         0.868500 loss:        0.399570
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.903180 loss:        0.282528
Test - acc:         0.856200 loss:        0.431620
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.905460 loss:        0.275439
Test - acc:         0.836000 loss:        0.497722
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.908040 loss:        0.270130
Test - acc:         0.803700 loss:        0.635244
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.902080 loss:        0.282248
Test - acc:         0.847600 loss:        0.445753
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.906020 loss:        0.274918
Test - acc:         0.858300 loss:        0.435422
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.903860 loss:        0.276608
Test - acc:         0.827200 loss:        0.538309
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.919080 loss:        0.238093
Test - acc:         0.874700 loss:        0.398092
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.915800 loss:        0.243487
Test - acc:         0.849100 loss:        0.461439
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.915800 loss:        0.245323
Test - acc:         0.865800 loss:        0.411158
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.914500 loss:        0.248736
Test - acc:         0.867200 loss:        0.407550
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.914980 loss:        0.249761
Test - acc:         0.870500 loss:        0.398104
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.914180 loss:        0.252015
Test - acc:         0.845100 loss:        0.470592
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.914000 loss:        0.251378
Test - acc:         0.857200 loss:        0.438695
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.912980 loss:        0.250857
Test - acc:         0.857900 loss:        0.432771
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.913920 loss:        0.248505
Test - acc:         0.864400 loss:        0.434764
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.912820 loss:        0.253601
Test - acc:         0.846300 loss:        0.482611
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.913520 loss:        0.248231
Test - acc:         0.876700 loss:        0.381285
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.913060 loss:        0.251805
Test - acc:         0.819300 loss:        0.583367
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.913500 loss:        0.251235
Test - acc:         0.868600 loss:        0.403899
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.912080 loss:        0.254021
Test - acc:         0.884700 loss:        0.361493
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.914980 loss:        0.246968
Test - acc:         0.830800 loss:        0.506723
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.915740 loss:        0.245162
Test - acc:         0.850700 loss:        0.482685
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.914080 loss:        0.248550
Test - acc:         0.873500 loss:        0.387032
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.915240 loss:        0.249971
Test - acc:         0.841100 loss:        0.494468
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.914100 loss:        0.249237
Test - acc:         0.873800 loss:        0.382783
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.913260 loss:        0.254411
Test - acc:         0.872700 loss:        0.385422
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.914800 loss:        0.248456
Test - acc:         0.864900 loss:        0.404404
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.915380 loss:        0.247508
Test - acc:         0.858800 loss:        0.447001
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.913780 loss:        0.249412
Test - acc:         0.870400 loss:        0.391990
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.912640 loss:        0.253125
Test - acc:         0.832700 loss:        0.535294
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.915740 loss:        0.246675
Test - acc:         0.847700 loss:        0.479379
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.913480 loss:        0.249936
Test - acc:         0.871400 loss:        0.400433
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.914000 loss:        0.248850
Test - acc:         0.880200 loss:        0.374750
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.914620 loss:        0.249175
Test - acc:         0.809500 loss:        0.627179
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.915160 loss:        0.249032
Test - acc:         0.875100 loss:        0.387036
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.916000 loss:        0.247085
Test - acc:         0.836700 loss:        0.530779
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.914800 loss:        0.247554
Test - acc:         0.843900 loss:        0.517122
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.912680 loss:        0.251061
Test - acc:         0.856900 loss:        0.456777
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.924660 loss:        0.219275
Test - acc:         0.851300 loss:        0.482230
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.922980 loss:        0.225044
Test - acc:         0.870200 loss:        0.400710
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.921260 loss:        0.227951
Test - acc:         0.884000 loss:        0.351728
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.920840 loss:        0.228740
Test - acc:         0.877700 loss:        0.372399
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.918300 loss:        0.235920
Test - acc:         0.815100 loss:        0.595078
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.919060 loss:        0.234407
Test - acc:         0.857300 loss:        0.463575
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.919080 loss:        0.235107
Test - acc:         0.880700 loss:        0.361418
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.920220 loss:        0.229780
Test - acc:         0.831800 loss:        0.554916
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.917080 loss:        0.237892
Test - acc:         0.847000 loss:        0.471589
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.919480 loss:        0.235015
Test - acc:         0.860800 loss:        0.425069
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.916460 loss:        0.238566
Test - acc:         0.862600 loss:        0.417032
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.917260 loss:        0.238341
Test - acc:         0.804500 loss:        0.680322
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.918620 loss:        0.235669
Test - acc:         0.820400 loss:        0.591781
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.919660 loss:        0.232024
Test - acc:         0.870700 loss:        0.402854
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.918520 loss:        0.234905
Test - acc:         0.874600 loss:        0.384134
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.919620 loss:        0.231476
Test - acc:         0.858400 loss:        0.434013
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.918560 loss:        0.237047
Test - acc:         0.870300 loss:        0.405375
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.919880 loss:        0.234558
Test - acc:         0.864500 loss:        0.429240
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.918140 loss:        0.236050
Test - acc:         0.873600 loss:        0.384937
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.918540 loss:        0.234467
Test - acc:         0.875800 loss:        0.374542
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.919900 loss:        0.233677
Test - acc:         0.877400 loss:        0.374370
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.919300 loss:        0.231198
Test - acc:         0.872500 loss:        0.392648
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.959080 loss:        0.125099
Test - acc:         0.935100 loss:        0.185615
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.970420 loss:        0.091248
Test - acc:         0.937800 loss:        0.175667
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.974640 loss:        0.076792
Test - acc:         0.940100 loss:        0.169944
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.978980 loss:        0.065038
Test - acc:         0.942200 loss:        0.170734
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.980860 loss:        0.061592
Test - acc:         0.944800 loss:        0.166650
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.983000 loss:        0.053434
Test - acc:         0.945400 loss:        0.173561
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.984640 loss:        0.050384
Test - acc:         0.946500 loss:        0.167901
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.986100 loss:        0.046203
Test - acc:         0.944300 loss:        0.170451
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.987700 loss:        0.042005
Test - acc:         0.946500 loss:        0.179202
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.988700 loss:        0.038571
Test - acc:         0.946000 loss:        0.172123
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.987180 loss:        0.045553
Test - acc:         0.943900 loss:        0.179397
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.988520 loss:        0.040388
Test - acc:         0.945200 loss:        0.177547
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.989200 loss:        0.037590
Test - acc:         0.943600 loss:        0.179665
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.989500 loss:        0.036063
Test - acc:         0.946000 loss:        0.178608
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.990040 loss:        0.033875
Test - acc:         0.945400 loss:        0.180880
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.990540 loss:        0.033297
Test - acc:         0.944200 loss:        0.181585
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.990980 loss:        0.031825
Test - acc:         0.941400 loss:        0.197243
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.992060 loss:        0.029752
Test - acc:         0.946900 loss:        0.182473
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.991680 loss:        0.030799
Test - acc:         0.942300 loss:        0.193684
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.990880 loss:        0.031344
Test - acc:         0.944000 loss:        0.193986
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.992300 loss:        0.028154
Test - acc:         0.943500 loss:        0.185496
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.992640 loss:        0.026980
Test - acc:         0.945700 loss:        0.190245
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.991560 loss:        0.029110
Test - acc:         0.942200 loss:        0.201063
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.991820 loss:        0.027972
Test - acc:         0.942100 loss:        0.201589
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.992200 loss:        0.028145
Test - acc:         0.941300 loss:        0.203192
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.992420 loss:        0.027962
Test - acc:         0.939300 loss:        0.213434
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.991860 loss:        0.028070
Test - acc:         0.942100 loss:        0.204003
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.992100 loss:        0.027577
Test - acc:         0.940300 loss:        0.206352
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.992520 loss:        0.026799
Test - acc:         0.942600 loss:        0.205256
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.991180 loss:        0.030395
Test - acc:         0.942700 loss:        0.201226
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.990300 loss:        0.030790
Test - acc:         0.941100 loss:        0.205297
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.990940 loss:        0.029576
Test - acc:         0.937700 loss:        0.218219
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.990200 loss:        0.033332
Test - acc:         0.942400 loss:        0.205354
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.990580 loss:        0.031776
Test - acc:         0.938500 loss:        0.208283
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.990420 loss:        0.032244
Test - acc:         0.939100 loss:        0.204358
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.990200 loss:        0.033349
Test - acc:         0.937600 loss:        0.209182
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.988960 loss:        0.036110
Test - acc:         0.939900 loss:        0.207224
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.988620 loss:        0.037156
Test - acc:         0.939800 loss:        0.207249
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.989620 loss:        0.033118
Test - acc:         0.940700 loss:        0.210706
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.988520 loss:        0.036643
Test - acc:         0.930800 loss:        0.248966
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.988240 loss:        0.038379
Test - acc:         0.937200 loss:        0.220363
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.987360 loss:        0.039277
Test - acc:         0.938800 loss:        0.209871
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.964540 loss:        0.107543
Test - acc:         0.921700 loss:        0.258423
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.973480 loss:        0.081707
Test - acc:         0.925900 loss:        0.243705
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.975080 loss:        0.074961
Test - acc:         0.925400 loss:        0.249755
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.977520 loss:        0.069480
Test - acc:         0.925700 loss:        0.251113
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.977160 loss:        0.067222
Test - acc:         0.926600 loss:        0.245628
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.978740 loss:        0.063588
Test - acc:         0.919300 loss:        0.279074
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.978760 loss:        0.062620
Test - acc:         0.926100 loss:        0.252690
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.979500 loss:        0.062083
Test - acc:         0.930200 loss:        0.235545
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.979720 loss:        0.061808
Test - acc:         0.930200 loss:        0.244342
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.979480 loss:        0.061721
Test - acc:         0.927600 loss:        0.251030
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.981300 loss:        0.056498
Test - acc:         0.929400 loss:        0.247906
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.980680 loss:        0.058272
Test - acc:         0.924200 loss:        0.252428
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.980800 loss:        0.059073
Test - acc:         0.923600 loss:        0.267309
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.980400 loss:        0.059277
Test - acc:         0.921400 loss:        0.268138
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.981180 loss:        0.057281
Test - acc:         0.929700 loss:        0.247331
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.980920 loss:        0.057800
Test - acc:         0.920400 loss:        0.280975
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.979940 loss:        0.061517
Test - acc:         0.931000 loss:        0.238743
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.980620 loss:        0.057909
Test - acc:         0.925700 loss:        0.278353
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.980460 loss:        0.060145
Test - acc:         0.919800 loss:        0.275284
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.980920 loss:        0.059239
Test - acc:         0.931700 loss:        0.242028
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.980800 loss:        0.057442
Test - acc:         0.925900 loss:        0.260269
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.980940 loss:        0.057451
Test - acc:         0.927200 loss:        0.251714
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.982060 loss:        0.056289
Test - acc:         0.930000 loss:        0.245972
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.982340 loss:        0.053543
Test - acc:         0.924100 loss:        0.265261
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.980140 loss:        0.058991
Test - acc:         0.927800 loss:        0.248609
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.981160 loss:        0.056157
Test - acc:         0.922200 loss:        0.263805
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.980840 loss:        0.056743
Test - acc:         0.932000 loss:        0.244470
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.981360 loss:        0.057265
Test - acc:         0.926900 loss:        0.255237
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.981540 loss:        0.055834
Test - acc:         0.928700 loss:        0.246056
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.981760 loss:        0.055266
Test - acc:         0.924000 loss:        0.260076
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.980300 loss:        0.057889
Test - acc:         0.923500 loss:        0.274937
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.060282
Test - acc:         0.929100 loss:        0.228932
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.947220 loss:        0.157302
Test - acc:         0.904100 loss:        0.312087
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.955440 loss:        0.132869
Test - acc:         0.908100 loss:        0.298271
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.958420 loss:        0.122152
Test - acc:         0.910700 loss:        0.299061
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.960940 loss:        0.115116
Test - acc:         0.919200 loss:        0.269447
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.960880 loss:        0.114487
Test - acc:         0.912100 loss:        0.299830
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.961120 loss:        0.113572
Test - acc:         0.910400 loss:        0.286545
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.961960 loss:        0.110053
Test - acc:         0.917700 loss:        0.274643
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.965280 loss:        0.102814
Test - acc:         0.911500 loss:        0.291747
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.963200 loss:        0.107614
Test - acc:         0.920600 loss:        0.257308
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.963400 loss:        0.106069
Test - acc:         0.914900 loss:        0.285674
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.967080 loss:        0.099002
Test - acc:         0.914600 loss:        0.277351
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.963840 loss:        0.105195
Test - acc:         0.912800 loss:        0.286245
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.963700 loss:        0.102976
Test - acc:         0.919800 loss:        0.269109
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.966260 loss:        0.099076
Test - acc:         0.913800 loss:        0.291973
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.966000 loss:        0.099823
Test - acc:         0.915700 loss:        0.272368
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.968000 loss:        0.095148
Test - acc:         0.918600 loss:        0.269222
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.966080 loss:        0.100480
Test - acc:         0.914700 loss:        0.285197
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.965760 loss:        0.098149
Test - acc:         0.920800 loss:        0.263170
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.967080 loss:        0.097367
Test - acc:         0.919400 loss:        0.267181
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.967540 loss:        0.097492
Test - acc:         0.915800 loss:        0.282143
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.968440 loss:        0.092894
Test - acc:         0.914200 loss:        0.293115
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.969420 loss:        0.091431
Test - acc:         0.912300 loss:        0.303878
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.968340 loss:        0.094933
Test - acc:         0.908700 loss:        0.311321
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.968600 loss:        0.093449
Test - acc:         0.914100 loss:        0.288545
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.966980 loss:        0.096875
Test - acc:         0.922600 loss:        0.265785
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.968680 loss:        0.094348
Test - acc:         0.924900 loss:        0.252307
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.979320 loss:        0.065574
Test - acc:         0.933300 loss:        0.220275
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.984420 loss:        0.051363
Test - acc:         0.936000 loss:        0.213440
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.986020 loss:        0.048834
Test - acc:         0.936600 loss:        0.212158
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.987280 loss:        0.044893
Test - acc:         0.934900 loss:        0.210883
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.988780 loss:        0.041325
Test - acc:         0.937500 loss:        0.211882
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.989400 loss:        0.040026
Test - acc:         0.936300 loss:        0.212214
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.910860 loss:        0.284053
Test - acc:         0.897200 loss:        0.312748
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.930820 loss:        0.216870
Test - acc:         0.904400 loss:        0.290260
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.937800 loss:        0.194582
Test - acc:         0.905900 loss:        0.283923
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.941940 loss:        0.183012
Test - acc:         0.907300 loss:        0.276255
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.946480 loss:        0.169639
Test - acc:         0.912200 loss:        0.268395
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.945920 loss:        0.166264
Test - acc:         0.911000 loss:        0.267220
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.948940 loss:        0.160240
Test - acc:         0.912000 loss:        0.262376
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.950400 loss:        0.154873
Test - acc:         0.912700 loss:        0.263901
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.952000 loss:        0.150289
Test - acc:         0.914000 loss:        0.260478
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.951840 loss:        0.149197
Test - acc:         0.914400 loss:        0.261340
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.953940 loss:        0.145167
Test - acc:         0.914600 loss:        0.258577
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.955520 loss:        0.141520
Test - acc:         0.914400 loss:        0.256851
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.955600 loss:        0.137328
Test - acc:         0.915100 loss:        0.258717
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.957560 loss:        0.134627
Test - acc:         0.916500 loss:        0.253006
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.957740 loss:        0.131872
Test - acc:         0.916700 loss:        0.256180
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.956980 loss:        0.131598
Test - acc:         0.916600 loss:        0.254036
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.959080 loss:        0.128570
Test - acc:         0.916100 loss:        0.258864
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.959440 loss:        0.128083
Test - acc:         0.917700 loss:        0.255928
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.959300 loss:        0.126568
Test - acc:         0.915900 loss:        0.255399
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.959060 loss:        0.125467
Test - acc:         0.917200 loss:        0.255800
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.957320 loss:        0.128113
Test - acc:         0.917900 loss:        0.258080
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.958340 loss:        0.125245
Test - acc:         0.917000 loss:        0.256531
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.960800 loss:        0.120387
Test - acc:         0.916900 loss:        0.258280
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.961380 loss:        0.120072
Test - acc:         0.917700 loss:        0.253876
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.961440 loss:        0.117979
Test - acc:         0.919600 loss:        0.259184
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.961260 loss:        0.118345
Test - acc:         0.918900 loss:        0.257886
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.963020 loss:        0.114680
Test - acc:         0.916700 loss:        0.260340
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.962060 loss:        0.115776
Test - acc:         0.918700 loss:        0.255386
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.963900 loss:        0.113781
Test - acc:         0.916900 loss:        0.261848
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.961760 loss:        0.116304
Test - acc:         0.917200 loss:        0.262797
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.963660 loss:        0.113048
Test - acc:         0.918700 loss:        0.259731
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.963880 loss:        0.111825
Test - acc:         0.917200 loss:        0.262783
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.732360 loss:        0.808264
Test - acc:         0.786500 loss:        0.635097
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.798840 loss:        0.610724
Test - acc:         0.812500 loss:        0.566764
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.814660 loss:        0.560377
Test - acc:         0.824600 loss:        0.532248
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.826080 loss:        0.528136
Test - acc:         0.825900 loss:        0.520486
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.833060 loss:        0.499668
Test - acc:         0.835100 loss:        0.496532
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.836460 loss:        0.488174
Test - acc:         0.838500 loss:        0.485522
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.841840 loss:        0.474362
Test - acc:         0.840500 loss:        0.472338
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.846460 loss:        0.458770
Test - acc:         0.840900 loss:        0.472581
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.850220 loss:        0.451769
Test - acc:         0.843600 loss:        0.465486
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.852980 loss:        0.443771
Test - acc:         0.843600 loss:        0.463233
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.854440 loss:        0.436178
Test - acc:         0.847800 loss:        0.453413
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.857020 loss:        0.428284
Test - acc:         0.850600 loss:        0.445154
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.858000 loss:        0.424207
Test - acc:         0.852600 loss:        0.443618
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.858700 loss:        0.418529
Test - acc:         0.849600 loss:        0.444198
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.860400 loss:        0.410746
Test - acc:         0.852800 loss:        0.438460
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.862680 loss:        0.408850
Test - acc:         0.853000 loss:        0.438956
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.863820 loss:        0.405411
Test - acc:         0.852200 loss:        0.435652
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.866620 loss:        0.400597
Test - acc:         0.854700 loss:        0.430869
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.866060 loss:        0.399714
Test - acc:         0.857200 loss:        0.423520
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.867000 loss:        0.395098
Test - acc:         0.854600 loss:        0.430038
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.870120 loss:        0.390365
Test - acc:         0.858600 loss:        0.421973
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.868600 loss:        0.389713
Test - acc:         0.858800 loss:        0.424052
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.868820 loss:        0.386058
Test - acc:         0.857800 loss:        0.423719
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.871600 loss:        0.383258
Test - acc:         0.861700 loss:        0.412716
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.870280 loss:        0.382618
Test - acc:         0.861500 loss:        0.420557
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.871220 loss:        0.380599
Test - acc:         0.860500 loss:        0.416559
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.872780 loss:        0.378677
Test - acc:         0.860500 loss:        0.422574
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.874560 loss:        0.373489
Test - acc:         0.861300 loss:        0.417805
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.874940 loss:        0.371694
Test - acc:         0.860100 loss:        0.412001
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.875700 loss:        0.371210
Test - acc:         0.861500 loss:        0.414240
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.875380 loss:        0.368782
Test - acc:         0.858700 loss:        0.418947
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.875620 loss:        0.368547
Test - acc:         0.860100 loss:        0.415661
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.528300 loss:        1.366692
Test - acc:         0.612000 loss:        1.115965
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.620880 loss:        1.118470
Test - acc:         0.646000 loss:        1.021333
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.643860 loss:        1.048174
Test - acc:         0.657000 loss:        0.970635
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.664240 loss:        0.993272
Test - acc:         0.676200 loss:        0.936644
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.673820 loss:        0.960950
Test - acc:         0.677300 loss:        0.927894
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.683940 loss:        0.936878
Test - acc:         0.690900 loss:        0.892152
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.689840 loss:        0.913420
Test - acc:         0.694500 loss:        0.873313
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.698140 loss:        0.888152
Test - acc:         0.702200 loss:        0.839106
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.702600 loss:        0.873365
Test - acc:         0.707500 loss:        0.830611
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.706800 loss:        0.862176
Test - acc:         0.718900 loss:        0.808171
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.709940 loss:        0.846498
Test - acc:         0.718800 loss:        0.805563
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.716660 loss:        0.839296
Test - acc:         0.713600 loss:        0.812210
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.713160 loss:        0.834574
Test - acc:         0.725900 loss:        0.789334
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.722180 loss:        0.815738
Test - acc:         0.726200 loss:        0.784984
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.724660 loss:        0.809232
Test - acc:         0.731800 loss:        0.764398
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.725780 loss:        0.802921
Test - acc:         0.734600 loss:        0.770112
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.728280 loss:        0.796395
Test - acc:         0.735100 loss:        0.759464
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.732680 loss:        0.787355
Test - acc:         0.732600 loss:        0.766919
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.731740 loss:        0.785750
Test - acc:         0.732900 loss:        0.767519
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.734560 loss:        0.775277
Test - acc:         0.733500 loss:        0.757754
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.736600 loss:        0.770171
Test - acc:         0.743500 loss:        0.736231
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.737580 loss:        0.768657
Test - acc:         0.740600 loss:        0.742405
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.738760 loss:        0.765096
Test - acc:         0.742000 loss:        0.736389
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.738280 loss:        0.761593
Test - acc:         0.740000 loss:        0.742846
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.741600 loss:        0.750417
Test - acc:         0.744000 loss:        0.727503
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.742560 loss:        0.753181
Test - acc:         0.744400 loss:        0.727107
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.745320 loss:        0.745332
Test - acc:         0.746400 loss:        0.715553
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.744160 loss:        0.746791
Test - acc:         0.747500 loss:        0.721612
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.745720 loss:        0.743841
Test - acc:         0.748500 loss:        0.718957
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.747040 loss:        0.740720
Test - acc:         0.753200 loss:        0.713814
Sparsity :          0.9990
Wdecay :        0.000500
