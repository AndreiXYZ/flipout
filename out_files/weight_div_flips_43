Running weight div flips test.
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 5000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "test",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": null,
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.312320 loss:        2.021708
Test - acc:         0.425900 loss:        1.569443
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.491320 loss:        1.404705
Test - acc:         0.545000 loss:        1.303250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.605880 loss:        1.107495
Test - acc:         0.640500 loss:        0.993838
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.669940 loss:        0.934087
Test - acc:         0.697500 loss:        0.862543
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.725540 loss:        0.779656
Test - acc:         0.707300 loss:        0.843815
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.763880 loss:        0.673697
Test - acc:         0.746100 loss:        0.728321
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.787480 loss:        0.616630
Test - acc:         0.743700 loss:        0.783915
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.802560 loss:        0.576670
Test - acc:         0.734400 loss:        0.801148
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.811860 loss:        0.544244
Test - acc:         0.759400 loss:        0.735082
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.816500 loss:        0.531377
Test - acc:         0.794900 loss:        0.607953
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.822820 loss:        0.512541
Test - acc:         0.791300 loss:        0.623225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.827040 loss:        0.502584
Test - acc:         0.799400 loss:        0.608805
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.833520 loss:        0.483841
Test - acc:         0.819000 loss:        0.527694
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838680 loss:        0.474776
Test - acc:         0.813600 loss:        0.544646
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.837740 loss:        0.466880
Test - acc:         0.794600 loss:        0.632762
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.841640 loss:        0.458718
Test - acc:         0.805300 loss:        0.586747
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.847260 loss:        0.445931
Test - acc:         0.792400 loss:        0.621451
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.848800 loss:        0.440622
Test - acc:         0.837300 loss:        0.496310
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.849120 loss:        0.443121
Test - acc:         0.798600 loss:        0.595495
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.855160 loss:        0.426338
Test - acc:         0.795500 loss:        0.611692
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.854860 loss:        0.427016
Test - acc:         0.787300 loss:        0.651557
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.855320 loss:        0.424807
Test - acc:         0.804000 loss:        0.582887
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.854480 loss:        0.425782
Test - acc:         0.759100 loss:        0.753230
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.858040 loss:        0.418467
Test - acc:         0.809400 loss:        0.564949
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.857580 loss:        0.416681
Test - acc:         0.742200 loss:        0.856132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.857820 loss:        0.418899
Test - acc:         0.772900 loss:        0.668685
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.859860 loss:        0.410829
Test - acc:         0.838200 loss:        0.486422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.859360 loss:        0.413582
Test - acc:         0.811000 loss:        0.576767
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.860180 loss:        0.406462
Test - acc:         0.808300 loss:        0.576132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.860320 loss:        0.406117
Test - acc:         0.753000 loss:        0.733155
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.864100 loss:        0.397486
Test - acc:         0.718000 loss:        0.947536
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.862160 loss:        0.404211
Test - acc:         0.796000 loss:        0.588443
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.865800 loss:        0.394257
Test - acc:         0.839300 loss:        0.479164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.863400 loss:        0.399293
Test - acc:         0.833600 loss:        0.501613
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.865880 loss:        0.392880
Test - acc:         0.834600 loss:        0.493233
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.864740 loss:        0.393004
Test - acc:         0.812400 loss:        0.554357
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.866500 loss:        0.394667
Test - acc:         0.714100 loss:        0.879326
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.864860 loss:        0.394574
Test - acc:         0.799000 loss:        0.629274
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.868620 loss:        0.389635
Test - acc:         0.837200 loss:        0.485836
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.865480 loss:        0.395985
Test - acc:         0.845900 loss:        0.461872
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.868260 loss:        0.387592
Test - acc:         0.854300 loss:        0.445506
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.868160 loss:        0.388148
Test - acc:         0.823200 loss:        0.530462
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.866340 loss:        0.389087
Test - acc:         0.838800 loss:        0.478737
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.867760 loss:        0.388603
Test - acc:         0.809400 loss:        0.587819
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.865920 loss:        0.391427
Test - acc:         0.815500 loss:        0.545057
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.866060 loss:        0.390517
Test - acc:         0.815800 loss:        0.542245
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.868720 loss:        0.385067
Test - acc:         0.831700 loss:        0.530917
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.870920 loss:        0.379591
Test - acc:         0.843800 loss:        0.472578
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.868560 loss:        0.385460
Test - acc:         0.762100 loss:        0.775933
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.868500 loss:        0.380419
Test - acc:         0.844600 loss:        0.476790
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.882360 loss:        0.345141
Test - acc:         0.815500 loss:        0.559116
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.879960 loss:        0.349502
Test - acc:         0.771100 loss:        0.762099
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.877220 loss:        0.355496
Test - acc:         0.846600 loss:        0.469523
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.881160 loss:        0.352145
Test - acc:         0.835800 loss:        0.487353
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.873580 loss:        0.368303
Test - acc:         0.840100 loss:        0.483313
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.877600 loss:        0.359790
Test - acc:         0.798900 loss:        0.623281
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.876880 loss:        0.363677
Test - acc:         0.838100 loss:        0.506768
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.874780 loss:        0.362375
Test - acc:         0.802500 loss:        0.604303
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.876600 loss:        0.361378
Test - acc:         0.840200 loss:        0.483300
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.878800 loss:        0.357058
Test - acc:         0.804400 loss:        0.600504
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.875160 loss:        0.362419
Test - acc:         0.788800 loss:        0.658577
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.875620 loss:        0.360010
Test - acc:         0.831700 loss:        0.513150
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.877380 loss:        0.356635
Test - acc:         0.820700 loss:        0.569922
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.876800 loss:        0.360170
Test - acc:         0.813600 loss:        0.618710
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.879300 loss:        0.353926
Test - acc:         0.850200 loss:        0.440311
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.878660 loss:        0.355613
Test - acc:         0.829800 loss:        0.507991
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.875900 loss:        0.359266
Test - acc:         0.833000 loss:        0.509892
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.875880 loss:        0.360574
Test - acc:         0.839100 loss:        0.482018
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.878220 loss:        0.358895
Test - acc:         0.847500 loss:        0.455716
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.879820 loss:        0.354855
Test - acc:         0.832800 loss:        0.518961
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.877480 loss:        0.359986
Test - acc:         0.836200 loss:        0.517779
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.879220 loss:        0.356114
Test - acc:         0.846100 loss:        0.471431
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.877600 loss:        0.359035
Test - acc:         0.825200 loss:        0.509405
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.877060 loss:        0.358861
Test - acc:         0.828800 loss:        0.540161
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.878540 loss:        0.354742
Test - acc:         0.836300 loss:        0.492237
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.878860 loss:        0.354930
Test - acc:         0.841100 loss:        0.482805
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.876560 loss:        0.359874
Test - acc:         0.860800 loss:        0.409345
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.879440 loss:        0.352485
Test - acc:         0.856500 loss:        0.425302
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.878280 loss:        0.359740
Test - acc:         0.850100 loss:        0.471109
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.877120 loss:        0.360639
Test - acc:         0.785500 loss:        0.715395
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.879900 loss:        0.354590
Test - acc:         0.827900 loss:        0.547277
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.879260 loss:        0.358375
Test - acc:         0.846300 loss:        0.487659
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.875780 loss:        0.365429
Test - acc:         0.825900 loss:        0.524499
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.878780 loss:        0.355928
Test - acc:         0.846800 loss:        0.454414
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.877500 loss:        0.357387
Test - acc:         0.841400 loss:        0.480772
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.876340 loss:        0.357557
Test - acc:         0.828100 loss:        0.522494
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.880520 loss:        0.352386
Test - acc:         0.828300 loss:        0.532811
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.879020 loss:        0.355617
Test - acc:         0.838800 loss:        0.487489
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.876000 loss:        0.357051
Test - acc:         0.841900 loss:        0.480145
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.876500 loss:        0.357866
Test - acc:         0.794800 loss:        0.635942
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.878900 loss:        0.356809
Test - acc:         0.816200 loss:        0.588226
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.878080 loss:        0.356225
Test - acc:         0.838700 loss:        0.502517
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.876880 loss:        0.356521
Test - acc:         0.814000 loss:        0.566963
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.880860 loss:        0.349555
Test - acc:         0.801000 loss:        0.612915
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.876600 loss:        0.357347
Test - acc:         0.710400 loss:        1.046530
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.878180 loss:        0.357363
Test - acc:         0.736300 loss:        0.880066
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.878380 loss:        0.358529
Test - acc:         0.843500 loss:        0.468961
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.878900 loss:        0.356148
Test - acc:         0.817000 loss:        0.577623
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.877620 loss:        0.353846
Test - acc:         0.828700 loss:        0.532503
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.878320 loss:        0.355422
Test - acc:         0.822300 loss:        0.521002
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.888020 loss:        0.322835
Test - acc:         0.854100 loss:        0.437468
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.889460 loss:        0.324171
Test - acc:         0.856300 loss:        0.432332
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.886320 loss:        0.332072
Test - acc:         0.811500 loss:        0.609789
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.884860 loss:        0.333588
Test - acc:         0.858400 loss:        0.433345
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.888320 loss:        0.331681
Test - acc:         0.830000 loss:        0.511260
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.885680 loss:        0.334781
Test - acc:         0.855300 loss:        0.436844
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.882380 loss:        0.340000
Test - acc:         0.839300 loss:        0.479579
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.884240 loss:        0.335877
Test - acc:         0.837600 loss:        0.490870
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.887120 loss:        0.333994
Test - acc:         0.823000 loss:        0.548815
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.884100 loss:        0.335271
Test - acc:         0.844300 loss:        0.480054
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.883600 loss:        0.336080
Test - acc:         0.826700 loss:        0.549967
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.886320 loss:        0.335807
Test - acc:         0.846900 loss:        0.463184
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.883820 loss:        0.340465
Test - acc:         0.814600 loss:        0.586794
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.884000 loss:        0.338314
Test - acc:         0.862400 loss:        0.421082
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.884700 loss:        0.337831
Test - acc:         0.850600 loss:        0.459431
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.883540 loss:        0.338673
Test - acc:         0.859700 loss:        0.418924
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.885220 loss:        0.334826
Test - acc:         0.821700 loss:        0.547849
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.882980 loss:        0.338491
Test - acc:         0.844500 loss:        0.473888
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.885260 loss:        0.336277
Test - acc:         0.816000 loss:        0.563627
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.884080 loss:        0.339386
Test - acc:         0.828500 loss:        0.529467
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.883380 loss:        0.338685
Test - acc:         0.798500 loss:        0.644169
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.884540 loss:        0.339568
Test - acc:         0.852000 loss:        0.450589
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.886480 loss:        0.335527
Test - acc:         0.825600 loss:        0.530314
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.882800 loss:        0.336987
Test - acc:         0.850200 loss:        0.441106
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.885280 loss:        0.333364
Test - acc:         0.837900 loss:        0.471068
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.884880 loss:        0.334665
Test - acc:         0.821300 loss:        0.560662
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.885280 loss:        0.334019
Test - acc:         0.776000 loss:        0.730182
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.883860 loss:        0.337414
Test - acc:         0.835900 loss:        0.507751
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.888380 loss:        0.329782
Test - acc:         0.833100 loss:        0.491631
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.882980 loss:        0.339384
Test - acc:         0.816200 loss:        0.599367
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.882360 loss:        0.341192
Test - acc:         0.858800 loss:        0.414930
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.885200 loss:        0.337581
Test - acc:         0.837700 loss:        0.499854
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.886560 loss:        0.332788
Test - acc:         0.800000 loss:        0.648089
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.886000 loss:        0.335584
Test - acc:         0.856200 loss:        0.413877
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.884740 loss:        0.334973
Test - acc:         0.831100 loss:        0.547964
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.883640 loss:        0.337557
Test - acc:         0.832800 loss:        0.511291
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.883640 loss:        0.335487
Test - acc:         0.818200 loss:        0.574715
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.883100 loss:        0.337791
Test - acc:         0.805100 loss:        0.586700
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.887120 loss:        0.330943
Test - acc:         0.851500 loss:        0.444238
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.883480 loss:        0.339120
Test - acc:         0.842500 loss:        0.480132
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.884800 loss:        0.333258
Test - acc:         0.855300 loss:        0.439048
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.883160 loss:        0.337405
Test - acc:         0.860500 loss:        0.415224
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.887060 loss:        0.335393
Test - acc:         0.832400 loss:        0.514170
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.884680 loss:        0.336280
Test - acc:         0.861600 loss:        0.410953
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.887300 loss:        0.332080
Test - acc:         0.818200 loss:        0.577751
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.885240 loss:        0.334506
Test - acc:         0.860500 loss:        0.411777
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.885980 loss:        0.333910
Test - acc:         0.798800 loss:        0.656867
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.887920 loss:        0.333022
Test - acc:         0.830800 loss:        0.501223
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.882860 loss:        0.338916
Test - acc:         0.835400 loss:        0.495374
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.885080 loss:        0.337216
Test - acc:         0.835200 loss:        0.476127
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.931900 loss:        0.201377
Test - acc:         0.921900 loss:        0.230612
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.947320 loss:        0.153600
Test - acc:         0.925900 loss:        0.222925
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.953000 loss:        0.137928
Test - acc:         0.927900 loss:        0.217400
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.955780 loss:        0.129377
Test - acc:         0.929800 loss:        0.211576
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.960160 loss:        0.118110
Test - acc:         0.931200 loss:        0.211162
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.963080 loss:        0.108819
Test - acc:         0.932300 loss:        0.210935
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.966280 loss:        0.100578
Test - acc:         0.932700 loss:        0.213338
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.967460 loss:        0.095747
Test - acc:         0.933000 loss:        0.214974
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.968860 loss:        0.093092
Test - acc:         0.931600 loss:        0.210576
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.969360 loss:        0.087286
Test - acc:         0.935700 loss:        0.211991
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.971460 loss:        0.082958
Test - acc:         0.932100 loss:        0.212978
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.974100 loss:        0.077888
Test - acc:         0.933400 loss:        0.214691
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.974180 loss:        0.075784
Test - acc:         0.932100 loss:        0.217706
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.975960 loss:        0.071932
Test - acc:         0.932500 loss:        0.222809
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.976380 loss:        0.069664
Test - acc:         0.932300 loss:        0.226712
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.977800 loss:        0.066940
Test - acc:         0.929100 loss:        0.231243
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.977680 loss:        0.065162
Test - acc:         0.931700 loss:        0.231096
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.978860 loss:        0.061914
Test - acc:         0.931100 loss:        0.232407
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.979960 loss:        0.059842
Test - acc:         0.926900 loss:        0.246087
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.980480 loss:        0.058421
Test - acc:         0.930500 loss:        0.244877
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.981340 loss:        0.056197
Test - acc:         0.930700 loss:        0.247995
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.980900 loss:        0.056850
Test - acc:         0.928800 loss:        0.245031
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.979960 loss:        0.059359
Test - acc:         0.930300 loss:        0.251761
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.981380 loss:        0.055666
Test - acc:         0.931500 loss:        0.247071
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.057349
Test - acc:         0.930100 loss:        0.242720
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.979460 loss:        0.057703
Test - acc:         0.925800 loss:        0.264893
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.980660 loss:        0.057144
Test - acc:         0.927900 loss:        0.260315
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.980000 loss:        0.057321
Test - acc:         0.925100 loss:        0.262605
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.979880 loss:        0.057712
Test - acc:         0.928900 loss:        0.254318
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.058623
Test - acc:         0.928100 loss:        0.249722
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.981000 loss:        0.056995
Test - acc:         0.926800 loss:        0.256626
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.980600 loss:        0.056906
Test - acc:         0.924500 loss:        0.255146
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.981180 loss:        0.055417
Test - acc:         0.927500 loss:        0.261057
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.979680 loss:        0.060040
Test - acc:         0.922700 loss:        0.277968
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.978980 loss:        0.061209
Test - acc:         0.926900 loss:        0.271235
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.980200 loss:        0.059668
Test - acc:         0.923400 loss:        0.267012
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.979200 loss:        0.059833
Test - acc:         0.923300 loss:        0.276087
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.979760 loss:        0.060254
Test - acc:         0.921900 loss:        0.264749
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.979520 loss:        0.061516
Test - acc:         0.922100 loss:        0.275702
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.058293
Test - acc:         0.921900 loss:        0.277765
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.978080 loss:        0.063053
Test - acc:         0.922700 loss:        0.262487
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.977080 loss:        0.066082
Test - acc:         0.919800 loss:        0.286712
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.979680 loss:        0.061411
Test - acc:         0.923200 loss:        0.270976
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.978720 loss:        0.062902
Test - acc:         0.925400 loss:        0.261837
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.980500 loss:        0.060018
Test - acc:         0.917100 loss:        0.298266
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.977480 loss:        0.064953
Test - acc:         0.922800 loss:        0.278833
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.977940 loss:        0.063787
Test - acc:         0.923800 loss:        0.268694
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.978820 loss:        0.061217
Test - acc:         0.920900 loss:        0.286490
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.977300 loss:        0.066234
Test - acc:         0.920700 loss:        0.280979
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.976880 loss:        0.067983
Test - acc:         0.926000 loss:        0.265268
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.981200 loss:        0.056050
Test - acc:         0.923900 loss:        0.275493
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.984120 loss:        0.048674
Test - acc:         0.923900 loss:        0.275394
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.984040 loss:        0.048537
Test - acc:         0.924300 loss:        0.266764
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.983680 loss:        0.049726
Test - acc:         0.924000 loss:        0.267447
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.982160 loss:        0.053350
Test - acc:         0.924600 loss:        0.266010
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.983720 loss:        0.050649
Test - acc:         0.927100 loss:        0.256173
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.983360 loss:        0.050055
Test - acc:         0.922700 loss:        0.278681
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.984160 loss:        0.048068
Test - acc:         0.921900 loss:        0.281987
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.983460 loss:        0.049386
Test - acc:         0.921000 loss:        0.288412
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.981700 loss:        0.054972
Test - acc:         0.927100 loss:        0.254128
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.982920 loss:        0.051030
Test - acc:         0.916400 loss:        0.293188
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.981300 loss:        0.053954
Test - acc:         0.923100 loss:        0.275459
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.980140 loss:        0.057428
Test - acc:         0.919000 loss:        0.289284
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.982240 loss:        0.053931
Test - acc:         0.921400 loss:        0.289188
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.983240 loss:        0.051632
Test - acc:         0.920400 loss:        0.292090
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.981040 loss:        0.056331
Test - acc:         0.918400 loss:        0.302048
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.979240 loss:        0.059777
Test - acc:         0.915800 loss:        0.294352
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.982660 loss:        0.051768
Test - acc:         0.923000 loss:        0.267197
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.982460 loss:        0.052535
Test - acc:         0.926500 loss:        0.271324
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.980400 loss:        0.057389
Test - acc:         0.921000 loss:        0.274178
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.978820 loss:        0.062138
Test - acc:         0.923500 loss:        0.266750
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.979460 loss:        0.060118
Test - acc:         0.924900 loss:        0.270105
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.982320 loss:        0.054738
Test - acc:         0.923100 loss:        0.280417
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.979520 loss:        0.060149
Test - acc:         0.923000 loss:        0.279577
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.981180 loss:        0.056374
Test - acc:         0.923600 loss:        0.283374
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.980780 loss:        0.057476
Test - acc:         0.915500 loss:        0.311850
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.981380 loss:        0.055454
Test - acc:         0.920800 loss:        0.284735
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.980420 loss:        0.058369
Test - acc:         0.924200 loss:        0.269961
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.980940 loss:        0.056413
Test - acc:         0.922900 loss:        0.274276
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.982240 loss:        0.054719
Test - acc:         0.920900 loss:        0.286659
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.058255
Test - acc:         0.919200 loss:        0.286968
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.979740 loss:        0.059303
Test - acc:         0.918500 loss:        0.304599
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.981400 loss:        0.054850
Test - acc:         0.911100 loss:        0.333984
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.979880 loss:        0.059680
Test - acc:         0.923600 loss:        0.274514
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.980200 loss:        0.058650
Test - acc:         0.926000 loss:        0.267539
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.981160 loss:        0.055616
Test - acc:         0.915100 loss:        0.310821
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.980200 loss:        0.059518
Test - acc:         0.920200 loss:        0.306407
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.980940 loss:        0.057079
Test - acc:         0.918700 loss:        0.295648
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.980500 loss:        0.059976
Test - acc:         0.919700 loss:        0.294380
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.980460 loss:        0.057002
Test - acc:         0.926800 loss:        0.268371
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.982220 loss:        0.054628
Test - acc:         0.922300 loss:        0.280353
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.979520 loss:        0.060100
Test - acc:         0.919200 loss:        0.282450
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.981220 loss:        0.058433
Test - acc:         0.921600 loss:        0.289092
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.981440 loss:        0.055312
Test - acc:         0.922800 loss:        0.278243
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.981940 loss:        0.056009
Test - acc:         0.917400 loss:        0.294933
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.980380 loss:        0.058217
Test - acc:         0.921700 loss:        0.275387
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.055751
Test - acc:         0.919400 loss:        0.297238
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.979240 loss:        0.060091
Test - acc:         0.924200 loss:        0.288106
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.056831
Test - acc:         0.922100 loss:        0.277441
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.979940 loss:        0.057347
Test - acc:         0.925000 loss:        0.280739
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.985360 loss:        0.047740
Test - acc:         0.935700 loss:        0.223653
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.990200 loss:        0.033290
Test - acc:         0.937000 loss:        0.218658
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.992340 loss:        0.029522
Test - acc:         0.937300 loss:        0.218968
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.992980 loss:        0.026619
Test - acc:         0.940500 loss:        0.216742
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.994020 loss:        0.023922
Test - acc:         0.938700 loss:        0.215343
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.994600 loss:        0.021638
Test - acc:         0.938800 loss:        0.216362
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.994500 loss:        0.020606
Test - acc:         0.938500 loss:        0.216249
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.995540 loss:        0.019838
Test - acc:         0.939800 loss:        0.214844
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.995840 loss:        0.017994
Test - acc:         0.939100 loss:        0.216657
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.996180 loss:        0.017709
Test - acc:         0.938900 loss:        0.220567
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.995740 loss:        0.017229
Test - acc:         0.939600 loss:        0.219190
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.996460 loss:        0.016219
Test - acc:         0.939500 loss:        0.218936
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.996960 loss:        0.015502
Test - acc:         0.939100 loss:        0.222822
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.996780 loss:        0.015325
Test - acc:         0.939400 loss:        0.221089
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.996500 loss:        0.015553
Test - acc:         0.939600 loss:        0.221052
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.997400 loss:        0.013298
Test - acc:         0.939000 loss:        0.221338
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.997180 loss:        0.013331
Test - acc:         0.940200 loss:        0.222509
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.997140 loss:        0.013579
Test - acc:         0.940000 loss:        0.222347
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.996900 loss:        0.013996
Test - acc:         0.940300 loss:        0.221674
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.997200 loss:        0.013279
Test - acc:         0.940400 loss:        0.222208
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.997760 loss:        0.011946
Test - acc:         0.939700 loss:        0.224054
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.997940 loss:        0.011630
Test - acc:         0.939900 loss:        0.224476
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.998040 loss:        0.011665
Test - acc:         0.940400 loss:        0.222192
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.998140 loss:        0.011005
Test - acc:         0.941000 loss:        0.221951
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.997760 loss:        0.011515
Test - acc:         0.939100 loss:        0.226316
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.997620 loss:        0.012170
Test - acc:         0.940800 loss:        0.224518
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.997920 loss:        0.011386
Test - acc:         0.941200 loss:        0.223444
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.998020 loss:        0.010905
Test - acc:         0.940200 loss:        0.223449
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.998140 loss:        0.010540
Test - acc:         0.940100 loss:        0.225973
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.998360 loss:        0.009835
Test - acc:         0.940300 loss:        0.223498
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.998380 loss:        0.009834
Test - acc:         0.940900 loss:        0.222653
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.998220 loss:        0.010819
Test - acc:         0.941600 loss:        0.224394
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.998660 loss:        0.009196
Test - acc:         0.941900 loss:        0.224553
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.998380 loss:        0.010152
Test - acc:         0.942200 loss:        0.227222
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.998240 loss:        0.010049
Test - acc:         0.941400 loss:        0.227512
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.998240 loss:        0.009450
Test - acc:         0.941700 loss:        0.225940
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.998340 loss:        0.009538
Test - acc:         0.939900 loss:        0.228022
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.998660 loss:        0.008797
Test - acc:         0.941900 loss:        0.227990
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.998700 loss:        0.009036
Test - acc:         0.941200 loss:        0.227482
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.998480 loss:        0.008886
Test - acc:         0.941600 loss:        0.225682
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.998660 loss:        0.008733
Test - acc:         0.941500 loss:        0.226362
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.998860 loss:        0.008477
Test - acc:         0.940800 loss:        0.226980
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.998500 loss:        0.008282
Test - acc:         0.939900 loss:        0.226919
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.998720 loss:        0.008234
Test - acc:         0.941900 loss:        0.228840
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.998900 loss:        0.008148
Test - acc:         0.941300 loss:        0.227807
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.998820 loss:        0.008211
Test - acc:         0.940900 loss:        0.227156
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.998180 loss:        0.008965
Test - acc:         0.942300 loss:        0.227470
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.998700 loss:        0.008366
Test - acc:         0.941500 loss:        0.228359
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.998900 loss:        0.008220
Test - acc:         0.941900 loss:        0.227254
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.998960 loss:        0.007756
Test - acc:         0.940600 loss:        0.226859
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.976160 loss:        0.073442
Test - acc:         0.925900 loss:        0.251742
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.983440 loss:        0.054627
Test - acc:         0.928900 loss:        0.245435
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.985480 loss:        0.047884
Test - acc:         0.931800 loss:        0.240437
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.986900 loss:        0.044684
Test - acc:         0.931600 loss:        0.240125
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.987440 loss:        0.042540
Test - acc:         0.933000 loss:        0.239703
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.988840 loss:        0.039058
Test - acc:         0.932100 loss:        0.238702
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.989440 loss:        0.036998
Test - acc:         0.933200 loss:        0.239748
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.989600 loss:        0.036073
Test - acc:         0.933000 loss:        0.235286
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.991000 loss:        0.033906
Test - acc:         0.932800 loss:        0.240102
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.991000 loss:        0.033074
Test - acc:         0.932300 loss:        0.240518
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.991800 loss:        0.030735
Test - acc:         0.933800 loss:        0.239754
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.991960 loss:        0.029735
Test - acc:         0.932200 loss:        0.240512
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.991580 loss:        0.030416
Test - acc:         0.934900 loss:        0.240520
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.992120 loss:        0.029324
Test - acc:         0.932800 loss:        0.244314
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.992380 loss:        0.028722
Test - acc:         0.933200 loss:        0.238225
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.992600 loss:        0.026568
Test - acc:         0.933100 loss:        0.243848
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.992860 loss:        0.026360
Test - acc:         0.932700 loss:        0.242829
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.993220 loss:        0.026418
Test - acc:         0.935100 loss:        0.238461
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.993480 loss:        0.025561
Test - acc:         0.933900 loss:        0.239581
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.993840 loss:        0.023989
Test - acc:         0.935300 loss:        0.235820
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.993280 loss:        0.024723
Test - acc:         0.933300 loss:        0.241648
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.994420 loss:        0.023291
Test - acc:         0.933900 loss:        0.241305
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.994020 loss:        0.022803
Test - acc:         0.934100 loss:        0.242053
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.993860 loss:        0.023116
Test - acc:         0.935300 loss:        0.246208
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.994180 loss:        0.021809
Test - acc:         0.935200 loss:        0.244747
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.994980 loss:        0.021104
Test - acc:         0.934700 loss:        0.243417
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.994620 loss:        0.021922
Test - acc:         0.934500 loss:        0.246680
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.994680 loss:        0.021272
Test - acc:         0.934500 loss:        0.244903
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.995220 loss:        0.020461
Test - acc:         0.935600 loss:        0.245711
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.994780 loss:        0.020812
Test - acc:         0.934800 loss:        0.245298
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.995360 loss:        0.019080
Test - acc:         0.935700 loss:        0.247665
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.995260 loss:        0.020073
Test - acc:         0.934600 loss:        0.245426
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.994980 loss:        0.020008
Test - acc:         0.934700 loss:        0.248381
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.994980 loss:        0.019936
Test - acc:         0.934700 loss:        0.248517
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.995720 loss:        0.018862
Test - acc:         0.936000 loss:        0.245247
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.995540 loss:        0.018424
Test - acc:         0.936400 loss:        0.246316
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.995800 loss:        0.018701
Test - acc:         0.935500 loss:        0.245681
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.995300 loss:        0.018744
Test - acc:         0.936000 loss:        0.246224
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.996080 loss:        0.017187
Test - acc:         0.934400 loss:        0.246049
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.995500 loss:        0.018136
Test - acc:         0.934300 loss:        0.249901
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.995900 loss:        0.018075
Test - acc:         0.933800 loss:        0.251322
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.995880 loss:        0.017265
Test - acc:         0.936000 loss:        0.246417
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.995860 loss:        0.017362
Test - acc:         0.936000 loss:        0.244933
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.996200 loss:        0.017579
Test - acc:         0.936400 loss:        0.248737
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.996620 loss:        0.015790
Test - acc:         0.938300 loss:        0.246218
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.995560 loss:        0.017418
Test - acc:         0.936300 loss:        0.247050
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.995700 loss:        0.017492
Test - acc:         0.936200 loss:        0.252548
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.996000 loss:        0.017183
Test - acc:         0.936400 loss:        0.250754
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.996520 loss:        0.015433
Test - acc:         0.936300 loss:        0.249912
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.996120 loss:        0.016383
Test - acc:         0.935900 loss:        0.246852
Sparsity :          0.9844
Wdecay :        0.000500
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 5000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "test",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": null,
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.312320 loss:        2.021708
Test - acc:         0.425900 loss:        1.569443
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.491320 loss:        1.404705
Test - acc:         0.545000 loss:        1.303250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.605880 loss:        1.107495
Test - acc:         0.640500 loss:        0.993838
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.669940 loss:        0.934087
Test - acc:         0.697500 loss:        0.862543
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.725540 loss:        0.779656
Test - acc:         0.707300 loss:        0.843815
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.763880 loss:        0.673697
Test - acc:         0.746100 loss:        0.728321
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.787480 loss:        0.616630
Test - acc:         0.743700 loss:        0.783915
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.802560 loss:        0.576670
Test - acc:         0.734400 loss:        0.801148
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.811860 loss:        0.544244
Test - acc:         0.759400 loss:        0.735082
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.816500 loss:        0.531377
Test - acc:         0.794900 loss:        0.607953
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.822820 loss:        0.512541
Test - acc:         0.791300 loss:        0.623225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.827040 loss:        0.502584
Test - acc:         0.799400 loss:        0.608805
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.833520 loss:        0.483841
Test - acc:         0.819000 loss:        0.527694
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.838680 loss:        0.474776
Test - acc:         0.813600 loss:        0.544646
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.837740 loss:        0.466880
Test - acc:         0.794600 loss:        0.632762
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.841640 loss:        0.458718
Test - acc:         0.805300 loss:        0.586747
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.847260 loss:        0.445931
Test - acc:         0.792400 loss:        0.621451
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.848800 loss:        0.440622
Test - acc:         0.837300 loss:        0.496310
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.849120 loss:        0.443121
Test - acc:         0.798600 loss:        0.595495
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.855160 loss:        0.426338
Test - acc:         0.795500 loss:        0.611692
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.854860 loss:        0.427016
Test - acc:         0.787300 loss:        0.651557
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.855320 loss:        0.424807
Test - acc:         0.804000 loss:        0.582887
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.854480 loss:        0.425782
Test - acc:         0.759100 loss:        0.753230
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.858040 loss:        0.418467
Test - acc:         0.809400 loss:        0.564949
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.857580 loss:        0.416681
Test - acc:         0.742200 loss:        0.856132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.857820 loss:        0.418899
Test - acc:         0.772900 loss:        0.668685
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.859860 loss:        0.410829
Test - acc:         0.838200 loss:        0.486422
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.859360 loss:        0.413582
Test - acc:         0.811000 loss:        0.576767
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.860180 loss:        0.406462
Test - acc:         0.808300 loss:        0.576132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.860320 loss:        0.406117
Test - acc:         0.753000 loss:        0.733155
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.864100 loss:        0.397486
Test - acc:         0.718000 loss:        0.947536
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.862160 loss:        0.404211
Test - acc:         0.796000 loss:        0.588443
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.865800 loss:        0.394257
Test - acc:         0.839300 loss:        0.479164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.863400 loss:        0.399293
Test - acc:         0.833600 loss:        0.501613
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.865880 loss:        0.392880
Test - acc:         0.834600 loss:        0.493233
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.864740 loss:        0.393004
Test - acc:         0.812400 loss:        0.554357
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.866500 loss:        0.394667
Test - acc:         0.714100 loss:        0.879326
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.864860 loss:        0.394574
Test - acc:         0.799000 loss:        0.629274
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.868620 loss:        0.389635
Test - acc:         0.837200 loss:        0.485836
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.878180 loss:        0.358168
Test - acc:         0.792100 loss:        0.630136
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.877240 loss:        0.362263
Test - acc:         0.839600 loss:        0.497626
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.874400 loss:        0.364401
Test - acc:         0.828800 loss:        0.522448
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.874640 loss:        0.368021
Test - acc:         0.854800 loss:        0.445079
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.871760 loss:        0.371033
Test - acc:         0.843000 loss:        0.472864
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.873560 loss:        0.372486
Test - acc:         0.812800 loss:        0.562141
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.874900 loss:        0.364071
Test - acc:         0.794400 loss:        0.619897
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.875400 loss:        0.367198
Test - acc:         0.848000 loss:        0.456498
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.875000 loss:        0.365985
Test - acc:         0.841900 loss:        0.471741
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.873140 loss:        0.370019
Test - acc:         0.820100 loss:        0.567437
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.876640 loss:        0.360717
Test - acc:         0.855200 loss:        0.445132
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.875200 loss:        0.368818
Test - acc:         0.789500 loss:        0.682060
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.875040 loss:        0.361653
Test - acc:         0.850200 loss:        0.461269
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.875020 loss:        0.364028
Test - acc:         0.829900 loss:        0.559827
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.878680 loss:        0.357996
Test - acc:         0.816900 loss:        0.548347
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.877000 loss:        0.363744
Test - acc:         0.807300 loss:        0.592451
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.876700 loss:        0.363932
Test - acc:         0.830600 loss:        0.500539
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.874260 loss:        0.365628
Test - acc:         0.811400 loss:        0.578951
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.876460 loss:        0.361161
Test - acc:         0.820900 loss:        0.565580
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.875060 loss:        0.361196
Test - acc:         0.820000 loss:        0.572236
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.873840 loss:        0.368738
Test - acc:         0.817900 loss:        0.539696
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.875080 loss:        0.363420
Test - acc:         0.830200 loss:        0.507654
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.875360 loss:        0.362709
Test - acc:         0.821700 loss:        0.535644
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.878020 loss:        0.357893
Test - acc:         0.852800 loss:        0.451609
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.876240 loss:        0.362758
Test - acc:         0.810300 loss:        0.622596
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.876740 loss:        0.359566
Test - acc:         0.844100 loss:        0.464506
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.876100 loss:        0.361068
Test - acc:         0.813500 loss:        0.561866
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.874900 loss:        0.362965
Test - acc:         0.839400 loss:        0.480717
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.876560 loss:        0.360659
Test - acc:         0.854300 loss:        0.423635
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.877500 loss:        0.359107
Test - acc:         0.832100 loss:        0.498540
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.873560 loss:        0.365461
Test - acc:         0.770700 loss:        0.769537
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.875360 loss:        0.366708
Test - acc:         0.836300 loss:        0.485320
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.874620 loss:        0.359811
Test - acc:         0.815900 loss:        0.582901
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.876080 loss:        0.363583
Test - acc:         0.822300 loss:        0.531395
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.879480 loss:        0.358788
Test - acc:         0.825300 loss:        0.551160
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.876840 loss:        0.362655
Test - acc:         0.806800 loss:        0.589007
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.876880 loss:        0.361334
Test - acc:         0.853300 loss:        0.443945
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.877480 loss:        0.359145
Test - acc:         0.846000 loss:        0.472902
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.876840 loss:        0.359182
Test - acc:         0.826700 loss:        0.539869
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.889180 loss:        0.326216
Test - acc:         0.854600 loss:        0.436205
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.888160 loss:        0.331884
Test - acc:         0.826900 loss:        0.529635
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.885380 loss:        0.333079
Test - acc:         0.854100 loss:        0.423750
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.884420 loss:        0.337760
Test - acc:         0.851500 loss:        0.445693
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.883680 loss:        0.341221
Test - acc:         0.817900 loss:        0.548673
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.883240 loss:        0.340474
Test - acc:         0.838900 loss:        0.487363
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.882760 loss:        0.339153
Test - acc:         0.853100 loss:        0.440185
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.881920 loss:        0.340410
Test - acc:         0.828700 loss:        0.530169
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.884020 loss:        0.338021
Test - acc:         0.834400 loss:        0.499613
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.882720 loss:        0.340859
Test - acc:         0.849100 loss:        0.460248
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.883000 loss:        0.337959
Test - acc:         0.840400 loss:        0.480793
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.882540 loss:        0.340032
Test - acc:         0.819400 loss:        0.549731
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.883800 loss:        0.339574
Test - acc:         0.834500 loss:        0.500227
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.884900 loss:        0.336431
Test - acc:         0.840800 loss:        0.494863
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.882940 loss:        0.338047
Test - acc:         0.785500 loss:        0.682038
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.885640 loss:        0.335743
Test - acc:         0.818900 loss:        0.556926
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.883120 loss:        0.342034
Test - acc:         0.828700 loss:        0.521633
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.884380 loss:        0.334815
Test - acc:         0.791400 loss:        0.652909
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.885180 loss:        0.336087
Test - acc:         0.841900 loss:        0.468178
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.883180 loss:        0.340670
Test - acc:         0.748800 loss:        0.842076
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.885860 loss:        0.335694
Test - acc:         0.835000 loss:        0.514888
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.884260 loss:        0.334301
Test - acc:         0.836100 loss:        0.491180
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.882740 loss:        0.338898
Test - acc:         0.842700 loss:        0.472631
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.885000 loss:        0.335072
Test - acc:         0.831400 loss:        0.516223
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.884700 loss:        0.338444
Test - acc:         0.793000 loss:        0.614655
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.885260 loss:        0.334780
Test - acc:         0.837200 loss:        0.498376
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.881860 loss:        0.341597
Test - acc:         0.850400 loss:        0.470830
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.883960 loss:        0.338959
Test - acc:         0.832300 loss:        0.522806
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.881780 loss:        0.344930
Test - acc:         0.848200 loss:        0.458146
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.884860 loss:        0.334055
Test - acc:         0.776600 loss:        0.753971
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.885000 loss:        0.333818
Test - acc:         0.771900 loss:        0.803475
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.883280 loss:        0.337900
Test - acc:         0.828800 loss:        0.528208
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.884540 loss:        0.334841
Test - acc:         0.838900 loss:        0.485188
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.883680 loss:        0.339255
Test - acc:         0.846300 loss:        0.464590
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.882740 loss:        0.340320
Test - acc:         0.828400 loss:        0.540091
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.884640 loss:        0.338162
Test - acc:         0.861100 loss:        0.426538
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.884300 loss:        0.336058
Test - acc:         0.830800 loss:        0.523505
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.886300 loss:        0.335274
Test - acc:         0.862700 loss:        0.400328
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.886500 loss:        0.334746
Test - acc:         0.781900 loss:        0.689633
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.895880 loss:        0.301177
Test - acc:         0.862900 loss:        0.414635
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.896400 loss:        0.304611
Test - acc:         0.855000 loss:        0.446876
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.891320 loss:        0.314891
Test - acc:         0.854600 loss:        0.443255
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.893220 loss:        0.310842
Test - acc:         0.841100 loss:        0.471552
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.892380 loss:        0.316027
Test - acc:         0.866800 loss:        0.396496
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.891540 loss:        0.314373
Test - acc:         0.842300 loss:        0.481662
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.891660 loss:        0.315119
Test - acc:         0.854000 loss:        0.444977
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.893420 loss:        0.310405
Test - acc:         0.855400 loss:        0.430461
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.889380 loss:        0.317278
Test - acc:         0.796700 loss:        0.644882
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.892060 loss:        0.314579
Test - acc:         0.794800 loss:        0.687977
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.891400 loss:        0.317098
Test - acc:         0.848300 loss:        0.471596
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.891560 loss:        0.315557
Test - acc:         0.789800 loss:        0.648407
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.893300 loss:        0.310845
Test - acc:         0.843300 loss:        0.488786
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.890040 loss:        0.317525
Test - acc:         0.841200 loss:        0.488381
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.891740 loss:        0.314828
Test - acc:         0.846000 loss:        0.492039
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.890720 loss:        0.315939
Test - acc:         0.842600 loss:        0.472311
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.891180 loss:        0.316449
Test - acc:         0.866600 loss:        0.383689
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.893160 loss:        0.315529
Test - acc:         0.828000 loss:        0.544886
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.890940 loss:        0.316481
Test - acc:         0.842600 loss:        0.508240
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.893620 loss:        0.313022
Test - acc:         0.867100 loss:        0.395649
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.890900 loss:        0.315557
Test - acc:         0.842100 loss:        0.481506
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.890940 loss:        0.316737
Test - acc:         0.831200 loss:        0.498895
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.891360 loss:        0.316681
Test - acc:         0.837100 loss:        0.503249
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.890000 loss:        0.316032
Test - acc:         0.835300 loss:        0.522244
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.894020 loss:        0.309653
Test - acc:         0.874300 loss:        0.376034
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.889900 loss:        0.321321
Test - acc:         0.861200 loss:        0.416239
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.892800 loss:        0.312205
Test - acc:         0.861400 loss:        0.410046
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.890680 loss:        0.316319
Test - acc:         0.861400 loss:        0.412936
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.891840 loss:        0.315010
Test - acc:         0.831900 loss:        0.514471
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.892700 loss:        0.311441
Test - acc:         0.809100 loss:        0.679972
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.893320 loss:        0.312374
Test - acc:         0.853300 loss:        0.440725
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.891860 loss:        0.316563
Test - acc:         0.787900 loss:        0.702816
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.891060 loss:        0.317279
Test - acc:         0.839900 loss:        0.483277
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.936200 loss:        0.188754
Test - acc:         0.924100 loss:        0.228029
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.951540 loss:        0.145438
Test - acc:         0.926400 loss:        0.222885
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.955680 loss:        0.128166
Test - acc:         0.929100 loss:        0.214587
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.960860 loss:        0.116955
Test - acc:         0.931000 loss:        0.212889
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.962560 loss:        0.109632
Test - acc:         0.932100 loss:        0.207715
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.966060 loss:        0.100498
Test - acc:         0.932200 loss:        0.215915
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.968380 loss:        0.093514
Test - acc:         0.932500 loss:        0.211438
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.971240 loss:        0.087338
Test - acc:         0.931900 loss:        0.213857
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.971460 loss:        0.083019
Test - acc:         0.934300 loss:        0.210331
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.974040 loss:        0.079233
Test - acc:         0.929300 loss:        0.222325
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.974140 loss:        0.076420
Test - acc:         0.931600 loss:        0.221044
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.975780 loss:        0.071638
Test - acc:         0.934800 loss:        0.218092
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.977160 loss:        0.068907
Test - acc:         0.934100 loss:        0.218925
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.977680 loss:        0.066235
Test - acc:         0.932700 loss:        0.225754
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.978140 loss:        0.065182
Test - acc:         0.930400 loss:        0.234447
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.977920 loss:        0.064438
Test - acc:         0.932900 loss:        0.227747
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.979000 loss:        0.063246
Test - acc:         0.931700 loss:        0.226677
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.979360 loss:        0.060129
Test - acc:         0.931000 loss:        0.244090
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.980380 loss:        0.058425
Test - acc:         0.927700 loss:        0.239478
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.981840 loss:        0.055238
Test - acc:         0.929500 loss:        0.237313
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.981600 loss:        0.056006
Test - acc:         0.930200 loss:        0.249334
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.981480 loss:        0.055919
Test - acc:         0.928600 loss:        0.246881
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.981280 loss:        0.057130
Test - acc:         0.925900 loss:        0.246830
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.982800 loss:        0.051394
Test - acc:         0.929200 loss:        0.249511
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.981320 loss:        0.054734
Test - acc:         0.928500 loss:        0.248179
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.982040 loss:        0.053866
Test - acc:         0.929300 loss:        0.244682
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.056859
Test - acc:         0.929800 loss:        0.245778
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.983460 loss:        0.050189
Test - acc:         0.928500 loss:        0.251741
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.982180 loss:        0.052321
Test - acc:         0.929200 loss:        0.254886
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.981680 loss:        0.055067
Test - acc:         0.925700 loss:        0.251192
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.981380 loss:        0.054400
Test - acc:         0.927100 loss:        0.255318
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.980400 loss:        0.057864
Test - acc:         0.928000 loss:        0.254913
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.981520 loss:        0.055347
Test - acc:         0.924500 loss:        0.266585
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.979040 loss:        0.059922
Test - acc:         0.924600 loss:        0.259440
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.979800 loss:        0.059245
Test - acc:         0.924500 loss:        0.265559
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.978940 loss:        0.062352
Test - acc:         0.927900 loss:        0.254599
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.980640 loss:        0.059490
Test - acc:         0.924100 loss:        0.267184
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.060341
Test - acc:         0.917900 loss:        0.280972
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.980040 loss:        0.060640
Test - acc:         0.927400 loss:        0.254435
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.979280 loss:        0.060635
Test - acc:         0.922100 loss:        0.266670
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.977580 loss:        0.064780
Test - acc:         0.925800 loss:        0.247378
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.977500 loss:        0.065081
Test - acc:         0.920100 loss:        0.292178
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.977820 loss:        0.063402
Test - acc:         0.924700 loss:        0.262930
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.977580 loss:        0.065598
Test - acc:         0.924700 loss:        0.261072
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.978840 loss:        0.061871
Test - acc:         0.921300 loss:        0.285945
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.976400 loss:        0.068872
Test - acc:         0.927400 loss:        0.252046
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.980020 loss:        0.060532
Test - acc:         0.926500 loss:        0.248163
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.979440 loss:        0.061344
Test - acc:         0.923700 loss:        0.263518
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.979760 loss:        0.059426
Test - acc:         0.930000 loss:        0.247218
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.979820 loss:        0.059975
Test - acc:         0.922100 loss:        0.273619
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.979560 loss:        0.060428
Test - acc:         0.927600 loss:        0.255093
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.980240 loss:        0.057916
Test - acc:         0.918300 loss:        0.296948
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.979100 loss:        0.060233
Test - acc:         0.924700 loss:        0.267365
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.980240 loss:        0.058405
Test - acc:         0.923100 loss:        0.268776
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.978780 loss:        0.061729
Test - acc:         0.926300 loss:        0.268475
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.979700 loss:        0.059897
Test - acc:         0.923200 loss:        0.282003
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.981180 loss:        0.056648
Test - acc:         0.923000 loss:        0.275282
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.981540 loss:        0.056290
Test - acc:         0.922700 loss:        0.272424
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.979560 loss:        0.061135
Test - acc:         0.919700 loss:        0.285397
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.977460 loss:        0.066238
Test - acc:         0.922400 loss:        0.261410
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.061137
Test - acc:         0.923000 loss:        0.273559
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.979580 loss:        0.060800
Test - acc:         0.915100 loss:        0.300922
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.980020 loss:        0.060231
Test - acc:         0.924000 loss:        0.267098
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.978360 loss:        0.063771
Test - acc:         0.920000 loss:        0.288625
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.978880 loss:        0.062230
Test - acc:         0.921600 loss:        0.277558
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.978960 loss:        0.061557
Test - acc:         0.924400 loss:        0.263423
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.980540 loss:        0.059194
Test - acc:         0.922100 loss:        0.273964
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.979060 loss:        0.061610
Test - acc:         0.921400 loss:        0.267044
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.978700 loss:        0.062735
Test - acc:         0.925000 loss:        0.264259
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.978740 loss:        0.061388
Test - acc:         0.925600 loss:        0.261463
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.978000 loss:        0.064688
Test - acc:         0.922500 loss:        0.270174
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.978400 loss:        0.063032
Test - acc:         0.922100 loss:        0.272574
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.978140 loss:        0.065514
Test - acc:         0.926400 loss:        0.253744
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.977780 loss:        0.065196
Test - acc:         0.922600 loss:        0.267958
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.979340 loss:        0.061100
Test - acc:         0.921800 loss:        0.268546
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.980560 loss:        0.057555
Test - acc:         0.920800 loss:        0.278336
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.981060 loss:        0.057715
Test - acc:         0.921500 loss:        0.285051
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.980120 loss:        0.059621
Test - acc:         0.925000 loss:        0.265008
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.977360 loss:        0.067447
Test - acc:         0.915400 loss:        0.288337
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.977360 loss:        0.065955
Test - acc:         0.925000 loss:        0.261811
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.979380 loss:        0.060670
Test - acc:         0.920100 loss:        0.282348
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.978460 loss:        0.062834
Test - acc:         0.922700 loss:        0.269082
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.979620 loss:        0.060910
Test - acc:         0.922700 loss:        0.271007
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.979380 loss:        0.061342
Test - acc:         0.929000 loss:        0.246411
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.968740 loss:        0.093008
Test - acc:         0.916700 loss:        0.272325
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.971160 loss:        0.083703
Test - acc:         0.924900 loss:        0.257798
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.972160 loss:        0.080989
Test - acc:         0.920300 loss:        0.273564
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.974260 loss:        0.075087
Test - acc:         0.917400 loss:        0.280592
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.973920 loss:        0.078112
Test - acc:         0.921500 loss:        0.276095
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.974660 loss:        0.074551
Test - acc:         0.919700 loss:        0.279940
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.974640 loss:        0.076298
Test - acc:         0.923900 loss:        0.264794
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.974100 loss:        0.074352
Test - acc:         0.918100 loss:        0.283571
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.975640 loss:        0.070870
Test - acc:         0.919700 loss:        0.282118
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.975080 loss:        0.072925
Test - acc:         0.919200 loss:        0.282033
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.975200 loss:        0.073344
Test - acc:         0.919400 loss:        0.286392
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.974380 loss:        0.074881
Test - acc:         0.922800 loss:        0.276110
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.977260 loss:        0.068426
Test - acc:         0.922400 loss:        0.268090
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.975740 loss:        0.071035
Test - acc:         0.922500 loss:        0.274735
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.974400 loss:        0.073712
Test - acc:         0.917000 loss:        0.285958
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.975280 loss:        0.072041
Test - acc:         0.918700 loss:        0.292893
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.984680 loss:        0.048497
Test - acc:         0.932300 loss:        0.233367
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.987920 loss:        0.037955
Test - acc:         0.934200 loss:        0.227216
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.990900 loss:        0.033005
Test - acc:         0.935500 loss:        0.228439
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.991280 loss:        0.030989
Test - acc:         0.936200 loss:        0.228497
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.992380 loss:        0.028544
Test - acc:         0.936000 loss:        0.226325
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.993040 loss:        0.026929
Test - acc:         0.936600 loss:        0.227720
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.992760 loss:        0.025841
Test - acc:         0.937700 loss:        0.226237
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.993820 loss:        0.024667
Test - acc:         0.937200 loss:        0.224654
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.993660 loss:        0.024144
Test - acc:         0.936400 loss:        0.226787
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.993880 loss:        0.023348
Test - acc:         0.936300 loss:        0.229477
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.994420 loss:        0.022196
Test - acc:         0.937400 loss:        0.227492
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.994360 loss:        0.022314
Test - acc:         0.936300 loss:        0.228201
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.994680 loss:        0.021895
Test - acc:         0.937900 loss:        0.230715
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.994420 loss:        0.021840
Test - acc:         0.937200 loss:        0.225974
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.995680 loss:        0.019932
Test - acc:         0.937500 loss:        0.226378
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.995260 loss:        0.019143
Test - acc:         0.938300 loss:        0.226199
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.995100 loss:        0.019387
Test - acc:         0.937000 loss:        0.229811
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.996000 loss:        0.018851
Test - acc:         0.936800 loss:        0.232959
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.996100 loss:        0.018158
Test - acc:         0.938400 loss:        0.231540
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.995780 loss:        0.018622
Test - acc:         0.939500 loss:        0.230840
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.995940 loss:        0.018031
Test - acc:         0.936800 loss:        0.231664
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.996180 loss:        0.017367
Test - acc:         0.938000 loss:        0.233077
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.996140 loss:        0.017413
Test - acc:         0.938500 loss:        0.229612
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.949120 loss:        0.147346
Test - acc:         0.917700 loss:        0.266692
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.961360 loss:        0.113932
Test - acc:         0.921500 loss:        0.261278
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.965440 loss:        0.101990
Test - acc:         0.924800 loss:        0.251776
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.968860 loss:        0.093475
Test - acc:         0.923900 loss:        0.249427
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.969880 loss:        0.090319
Test - acc:         0.923400 loss:        0.248571
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.970720 loss:        0.087339
Test - acc:         0.926500 loss:        0.243762
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.971700 loss:        0.082388
Test - acc:         0.924700 loss:        0.250004
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.975100 loss:        0.076976
Test - acc:         0.927500 loss:        0.247384
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.974840 loss:        0.075971
Test - acc:         0.926700 loss:        0.243367
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.976000 loss:        0.073878
Test - acc:         0.927500 loss:        0.245522
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.975600 loss:        0.073681
Test - acc:         0.927200 loss:        0.245413
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.977240 loss:        0.069594
Test - acc:         0.929400 loss:        0.241888
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.977360 loss:        0.069320
Test - acc:         0.929600 loss:        0.241930
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.977740 loss:        0.068948
Test - acc:         0.928700 loss:        0.240738
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.979320 loss:        0.064203
Test - acc:         0.929400 loss:        0.243362
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.979420 loss:        0.064029
Test - acc:         0.928700 loss:        0.242410
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.979800 loss:        0.063001
Test - acc:         0.929900 loss:        0.240215
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.979760 loss:        0.063191
Test - acc:         0.928700 loss:        0.242774
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.980720 loss:        0.060309
Test - acc:         0.929800 loss:        0.241139
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.980980 loss:        0.059826
Test - acc:         0.929700 loss:        0.243012
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.980940 loss:        0.059177
Test - acc:         0.930200 loss:        0.242510
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.981220 loss:        0.058719
Test - acc:         0.928700 loss:        0.245743
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.982000 loss:        0.057209
Test - acc:         0.930200 loss:        0.249034
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.981060 loss:        0.057179
Test - acc:         0.930700 loss:        0.243204
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.982520 loss:        0.055470
Test - acc:         0.928500 loss:        0.246636
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.982420 loss:        0.056045
Test - acc:         0.927900 loss:        0.246122
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.982600 loss:        0.054614
Test - acc:         0.929500 loss:        0.245754
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.982360 loss:        0.053733
Test - acc:         0.928900 loss:        0.245749
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.983480 loss:        0.052217
Test - acc:         0.928200 loss:        0.247277
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.983580 loss:        0.052731
Test - acc:         0.928100 loss:        0.248410
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.984300 loss:        0.050060
Test - acc:         0.928900 loss:        0.246971
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.983280 loss:        0.051922
Test - acc:         0.928500 loss:        0.251285
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.984380 loss:        0.050364
Test - acc:         0.929000 loss:        0.247462
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.984800 loss:        0.049751
Test - acc:         0.928100 loss:        0.250304
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.984680 loss:        0.049220
Test - acc:         0.928400 loss:        0.247970
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.984260 loss:        0.049639
Test - acc:         0.927400 loss:        0.249263
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.985000 loss:        0.048213
Test - acc:         0.929000 loss:        0.249596
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.985420 loss:        0.047117
Test - acc:         0.928900 loss:        0.251221
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.985240 loss:        0.047949
Test - acc:         0.928500 loss:        0.254754
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.870340 loss:        0.376107
Test - acc:         0.881700 loss:        0.366942
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.903480 loss:        0.278353
Test - acc:         0.888100 loss:        0.345579
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.913280 loss:        0.254045
Test - acc:         0.888500 loss:        0.330900
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.917860 loss:        0.236519
Test - acc:         0.892000 loss:        0.323363
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.921740 loss:        0.225839
Test - acc:         0.894000 loss:        0.322846
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.922780 loss:        0.221109
Test - acc:         0.897400 loss:        0.314877
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.928520 loss:        0.208287
Test - acc:         0.899100 loss:        0.311225
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.929980 loss:        0.206356
Test - acc:         0.899000 loss:        0.308289
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.930480 loss:        0.201730
Test - acc:         0.900500 loss:        0.297833
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.931400 loss:        0.198654
Test - acc:         0.901600 loss:        0.296780
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.932940 loss:        0.194600
Test - acc:         0.902800 loss:        0.295860
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.933700 loss:        0.192101
Test - acc:         0.902300 loss:        0.296223
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.934860 loss:        0.188085
Test - acc:         0.901400 loss:        0.298530
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.937200 loss:        0.183029
Test - acc:         0.904800 loss:        0.295548
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.936600 loss:        0.182009
Test - acc:         0.901000 loss:        0.290977
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.937280 loss:        0.180618
Test - acc:         0.903800 loss:        0.291841
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.939600 loss:        0.176079
Test - acc:         0.905700 loss:        0.289918
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.939440 loss:        0.176292
Test - acc:         0.904700 loss:        0.290244
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.939640 loss:        0.173449
Test - acc:         0.905600 loss:        0.286615
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.941060 loss:        0.173438
Test - acc:         0.904300 loss:        0.289650
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.938720 loss:        0.174881
Test - acc:         0.905600 loss:        0.288161
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.940400 loss:        0.171718
Test - acc:         0.905200 loss:        0.292575
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.940720 loss:        0.167995
Test - acc:         0.907400 loss:        0.284236
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.941000 loss:        0.168831
Test - acc:         0.906300 loss:        0.287757
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.942820 loss:        0.164949
Test - acc:         0.908600 loss:        0.288555
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.942800 loss:        0.165543
Test - acc:         0.907100 loss:        0.286940
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.945040 loss:        0.162587
Test - acc:         0.907000 loss:        0.287970
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.944060 loss:        0.163328
Test - acc:         0.905400 loss:        0.288048
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.945800 loss:        0.158762
Test - acc:         0.907900 loss:        0.284386
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.944360 loss:        0.161176
Test - acc:         0.908600 loss:        0.279160
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.943440 loss:        0.161595
Test - acc:         0.908900 loss:        0.279900
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.946420 loss:        0.156466
Test - acc:         0.909400 loss:        0.285623
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.945960 loss:        0.157433
Test - acc:         0.908100 loss:        0.284620
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.944480 loss:        0.159677
Test - acc:         0.908400 loss:        0.285415
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.946040 loss:        0.156236
Test - acc:         0.906400 loss:        0.289670
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.945800 loss:        0.155878
Test - acc:         0.906400 loss:        0.290284
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.946280 loss:        0.154505
Test - acc:         0.908300 loss:        0.283413
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.946180 loss:        0.155050
Test - acc:         0.910700 loss:        0.283073
Sparsity :          0.9961
Wdecay :        0.000500
