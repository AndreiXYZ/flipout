Running weight div flips test.
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 5000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "test",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": null,
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.112220 loss:        2.707889
Test - acc:         0.115500 loss:        2.285625
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.171100 loss:        2.119530
Test - acc:         0.247800 loss:        1.889029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.260180 loss:        1.845003
Test - acc:         0.293800 loss:        1.793629
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.351700 loss:        1.666860
Test - acc:         0.365700 loss:        1.606198
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.442140 loss:        1.475611
Test - acc:         0.498800 loss:        1.346169
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.541180 loss:        1.260178
Test - acc:         0.453900 loss:        1.738315
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.619740 loss:        1.083854
Test - acc:         0.532000 loss:        1.515536
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.675980 loss:        0.929632
Test - acc:         0.621200 loss:        1.227844
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.710540 loss:        0.855181
Test - acc:         0.638900 loss:        1.146975
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.730440 loss:        0.803610
Test - acc:         0.680300 loss:        1.070751
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.752360 loss:        0.752724
Test - acc:         0.695200 loss:        0.963042
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.761480 loss:        0.725837
Test - acc:         0.731800 loss:        0.838521
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.771840 loss:        0.701123
Test - acc:         0.694700 loss:        0.991394
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.778780 loss:        0.676362
Test - acc:         0.667800 loss:        1.115059
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.781940 loss:        0.664950
Test - acc:         0.744500 loss:        0.805362
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.790840 loss:        0.641081
Test - acc:         0.779400 loss:        0.675063
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.795240 loss:        0.630031
Test - acc:         0.697300 loss:        0.917811
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.800100 loss:        0.615935
Test - acc:         0.719800 loss:        0.926714
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.800380 loss:        0.612391
Test - acc:         0.752900 loss:        0.813506
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.802400 loss:        0.604146
Test - acc:         0.770800 loss:        0.743486
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.807380 loss:        0.594119
Test - acc:         0.665200 loss:        1.083300
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.810160 loss:        0.585650
Test - acc:         0.778600 loss:        0.685493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.809940 loss:        0.585200
Test - acc:         0.780900 loss:        0.664840
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.811000 loss:        0.585315
Test - acc:         0.780700 loss:        0.692555
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.812760 loss:        0.576658
Test - acc:         0.723800 loss:        0.945873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.819780 loss:        0.555273
Test - acc:         0.759000 loss:        0.754278
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.816960 loss:        0.567006
Test - acc:         0.759200 loss:        0.769223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.818100 loss:        0.561920
Test - acc:         0.784400 loss:        0.689784
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.818680 loss:        0.560318
Test - acc:         0.781500 loss:        0.680860
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.818160 loss:        0.564143
Test - acc:         0.704200 loss:        0.952644
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.824040 loss:        0.548989
Test - acc:         0.768300 loss:        0.734637
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.822460 loss:        0.552491
Test - acc:         0.795300 loss:        0.651316
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.822260 loss:        0.549665
Test - acc:         0.776000 loss:        0.726616
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.824040 loss:        0.545305
Test - acc:         0.696300 loss:        1.066222
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.828700 loss:        0.532606
Test - acc:         0.763700 loss:        0.736834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.825980 loss:        0.536146
Test - acc:         0.797400 loss:        0.639977
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.825080 loss:        0.541546
Test - acc:         0.783400 loss:        0.693384
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.827300 loss:        0.528451
Test - acc:         0.736300 loss:        0.862552
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.824660 loss:        0.537154
Test - acc:         0.745800 loss:        0.838598
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.830840 loss:        0.527442
Test - acc:         0.776500 loss:        0.700459
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.827020 loss:        0.530133
Test - acc:         0.761700 loss:        0.742538
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.827200 loss:        0.537913
Test - acc:         0.782600 loss:        0.694255
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.831900 loss:        0.523550
Test - acc:         0.741600 loss:        0.814439
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.834320 loss:        0.514167
Test - acc:         0.719800 loss:        0.969886
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.830520 loss:        0.522136
Test - acc:         0.717100 loss:        1.180856
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.833200 loss:        0.516630
Test - acc:         0.786700 loss:        0.678516
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.830840 loss:        0.519828
Test - acc:         0.772900 loss:        0.756524
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.831600 loss:        0.518631
Test - acc:         0.815600 loss:        0.572250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.835020 loss:        0.510291
Test - acc:         0.822800 loss:        0.550194
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.833200 loss:        0.518434
Test - acc:         0.786100 loss:        0.652066
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.841020 loss:        0.483537
Test - acc:         0.756200 loss:        0.793195
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.840080 loss:        0.488435
Test - acc:         0.810300 loss:        0.591546
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.839500 loss:        0.490235
Test - acc:         0.800000 loss:        0.593393
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.840780 loss:        0.484494
Test - acc:         0.784300 loss:        0.690428
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.839660 loss:        0.491167
Test - acc:         0.802300 loss:        0.578553
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.840420 loss:        0.489654
Test - acc:         0.801100 loss:        0.625769
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.841500 loss:        0.481946
Test - acc:         0.762900 loss:        0.831159
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.839520 loss:        0.491092
Test - acc:         0.798400 loss:        0.644675
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.841280 loss:        0.484141
Test - acc:         0.830600 loss:        0.534453
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.841200 loss:        0.482206
Test - acc:         0.772200 loss:        0.698860
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.840780 loss:        0.485469
Test - acc:         0.777300 loss:        0.693231
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.840420 loss:        0.488838
Test - acc:         0.792800 loss:        0.663408
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.842900 loss:        0.479620
Test - acc:         0.810400 loss:        0.586828
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.842140 loss:        0.480127
Test - acc:         0.798000 loss:        0.628845
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.840340 loss:        0.487177
Test - acc:         0.762600 loss:        0.743514
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.842200 loss:        0.481575
Test - acc:         0.773700 loss:        0.729433
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.844820 loss:        0.474925
Test - acc:         0.783100 loss:        0.671673
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.839040 loss:        0.490899
Test - acc:         0.791800 loss:        0.658069
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.842540 loss:        0.480522
Test - acc:         0.781200 loss:        0.725541
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.844560 loss:        0.474809
Test - acc:         0.805500 loss:        0.625131
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.845220 loss:        0.471736
Test - acc:         0.755700 loss:        0.810482
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.844240 loss:        0.477090
Test - acc:         0.753900 loss:        0.816652
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.842000 loss:        0.474419
Test - acc:         0.765400 loss:        0.741216
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.844460 loss:        0.478305
Test - acc:         0.765200 loss:        0.715228
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.846420 loss:        0.464654
Test - acc:         0.728700 loss:        0.856734
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.841140 loss:        0.483832
Test - acc:         0.608800 loss:        1.367450
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.844560 loss:        0.474813
Test - acc:         0.691400 loss:        1.078154
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.843760 loss:        0.473205
Test - acc:         0.788900 loss:        0.662144
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.846140 loss:        0.464601
Test - acc:         0.778900 loss:        0.687760
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.845320 loss:        0.475913
Test - acc:         0.744300 loss:        0.877879
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.844120 loss:        0.469106
Test - acc:         0.795600 loss:        0.669519
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.843960 loss:        0.470624
Test - acc:         0.757700 loss:        0.771640
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.844920 loss:        0.468639
Test - acc:         0.714800 loss:        0.867655
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.849000 loss:        0.462391
Test - acc:         0.758700 loss:        0.800610
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.848600 loss:        0.462021
Test - acc:         0.792700 loss:        0.648172
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.848440 loss:        0.456589
Test - acc:         0.745100 loss:        0.848339
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.847280 loss:        0.466218
Test - acc:         0.802500 loss:        0.620471
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.848280 loss:        0.463283
Test - acc:         0.729400 loss:        0.913276
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.846320 loss:        0.464910
Test - acc:         0.823700 loss:        0.561797
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.844700 loss:        0.472937
Test - acc:         0.813700 loss:        0.567500
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.847040 loss:        0.463475
Test - acc:         0.825600 loss:        0.547839
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.847320 loss:        0.462988
Test - acc:         0.825200 loss:        0.559366
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.846900 loss:        0.462729
Test - acc:         0.813900 loss:        0.564043
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.846420 loss:        0.468295
Test - acc:         0.778300 loss:        0.735896
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.850140 loss:        0.460125
Test - acc:         0.788700 loss:        0.664368
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.849880 loss:        0.458160
Test - acc:         0.743900 loss:        0.821191
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.848400 loss:        0.460556
Test - acc:         0.742300 loss:        0.871123
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.849840 loss:        0.458243
Test - acc:         0.786900 loss:        0.633613
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.847880 loss:        0.462189
Test - acc:         0.744600 loss:        0.800608
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.847380 loss:        0.459675
Test - acc:         0.801500 loss:        0.625208
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.854260 loss:        0.437125
Test - acc:         0.814100 loss:        0.584983
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.851380 loss:        0.446665
Test - acc:         0.785300 loss:        0.654918
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.851540 loss:        0.446357
Test - acc:         0.776900 loss:        0.709105
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.853040 loss:        0.444645
Test - acc:         0.775200 loss:        0.688974
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.854320 loss:        0.440980
Test - acc:         0.731800 loss:        0.812359
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.848920 loss:        0.452082
Test - acc:         0.774600 loss:        0.693510
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.850820 loss:        0.447522
Test - acc:         0.832300 loss:        0.511948
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.854280 loss:        0.442268
Test - acc:         0.774600 loss:        0.716918
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.852620 loss:        0.445715
Test - acc:         0.807000 loss:        0.585701
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.854580 loss:        0.439129
Test - acc:         0.769300 loss:        0.745899
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.851040 loss:        0.447469
Test - acc:         0.778600 loss:        0.739204
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.851840 loss:        0.448670
Test - acc:         0.777400 loss:        0.706375
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.850760 loss:        0.447669
Test - acc:         0.760100 loss:        0.730936
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.852380 loss:        0.446701
Test - acc:         0.835900 loss:        0.491517
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.853980 loss:        0.443783
Test - acc:         0.801600 loss:        0.604950
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.851500 loss:        0.442990
Test - acc:         0.856700 loss:        0.445226
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.851400 loss:        0.447870
Test - acc:         0.824100 loss:        0.561453
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.854780 loss:        0.439699
Test - acc:         0.765000 loss:        0.827294
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.853280 loss:        0.440243
Test - acc:         0.828000 loss:        0.518880
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.851660 loss:        0.446216
Test - acc:         0.816600 loss:        0.558492
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.849940 loss:        0.451135
Test - acc:         0.793000 loss:        0.687773
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.851820 loss:        0.441620
Test - acc:         0.804500 loss:        0.583608
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.848720 loss:        0.448723
Test - acc:         0.834300 loss:        0.510561
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.852140 loss:        0.443811
Test - acc:         0.710200 loss:        1.005493
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.852000 loss:        0.446018
Test - acc:         0.806000 loss:        0.571708
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.853500 loss:        0.439727
Test - acc:         0.771100 loss:        0.702123
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.853440 loss:        0.439746
Test - acc:         0.794400 loss:        0.652555
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.853900 loss:        0.441563
Test - acc:         0.791300 loss:        0.658094
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.852120 loss:        0.442348
Test - acc:         0.729900 loss:        0.955953
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.851940 loss:        0.445858
Test - acc:         0.810500 loss:        0.584343
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.851460 loss:        0.443351
Test - acc:         0.771700 loss:        0.714450
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.853160 loss:        0.437178
Test - acc:         0.832100 loss:        0.514507
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.853160 loss:        0.438314
Test - acc:         0.767200 loss:        0.765749
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.850900 loss:        0.448823
Test - acc:         0.763600 loss:        0.729817
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.852520 loss:        0.439465
Test - acc:         0.762600 loss:        0.710778
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.852740 loss:        0.444799
Test - acc:         0.811300 loss:        0.572257
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.852840 loss:        0.441029
Test - acc:         0.789600 loss:        0.686620
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.856700 loss:        0.431543
Test - acc:         0.811800 loss:        0.576793
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.853880 loss:        0.437551
Test - acc:         0.799500 loss:        0.606211
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.856240 loss:        0.432320
Test - acc:         0.767700 loss:        0.741909
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.855720 loss:        0.433077
Test - acc:         0.797700 loss:        0.625517
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.854880 loss:        0.433971
Test - acc:         0.814800 loss:        0.575548
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.854080 loss:        0.440003
Test - acc:         0.817800 loss:        0.551828
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.848960 loss:        0.450269
Test - acc:         0.797800 loss:        0.637766
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.853540 loss:        0.438897
Test - acc:         0.808800 loss:        0.600105
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.855240 loss:        0.435695
Test - acc:         0.831600 loss:        0.534574
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.853160 loss:        0.437303
Test - acc:         0.777100 loss:        0.743505
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.850800 loss:        0.441198
Test - acc:         0.806800 loss:        0.581163
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.856840 loss:        0.431913
Test - acc:         0.835800 loss:        0.517996
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.855260 loss:        0.435683
Test - acc:         0.821600 loss:        0.547983
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.913160 loss:        0.260375
Test - acc:         0.901500 loss:        0.289773
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.929580 loss:        0.210790
Test - acc:         0.908600 loss:        0.277526
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.936400 loss:        0.188799
Test - acc:         0.910900 loss:        0.274315
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.941020 loss:        0.174003
Test - acc:         0.912600 loss:        0.262809
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.946020 loss:        0.159608
Test - acc:         0.915800 loss:        0.264554
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.946780 loss:        0.155756
Test - acc:         0.915100 loss:        0.273461
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.948940 loss:        0.147520
Test - acc:         0.915800 loss:        0.271475
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.952160 loss:        0.140575
Test - acc:         0.913300 loss:        0.272023
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.954460 loss:        0.131659
Test - acc:         0.912400 loss:        0.281858
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.958420 loss:        0.123644
Test - acc:         0.915000 loss:        0.272926
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.958720 loss:        0.119506
Test - acc:         0.914900 loss:        0.278945
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.959440 loss:        0.117499
Test - acc:         0.916400 loss:        0.280979
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.960800 loss:        0.113365
Test - acc:         0.912300 loss:        0.290707
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.964280 loss:        0.104362
Test - acc:         0.911000 loss:        0.302277
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.962880 loss:        0.107761
Test - acc:         0.912600 loss:        0.299306
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.964280 loss:        0.103683
Test - acc:         0.909300 loss:        0.301188
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.965080 loss:        0.100213
Test - acc:         0.913100 loss:        0.300629
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.966800 loss:        0.098304
Test - acc:         0.911700 loss:        0.306677
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.966460 loss:        0.097121
Test - acc:         0.906500 loss:        0.321905
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.967760 loss:        0.093857
Test - acc:         0.910100 loss:        0.303573
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.967220 loss:        0.094362
Test - acc:         0.908300 loss:        0.316002
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.966900 loss:        0.096140
Test - acc:         0.904900 loss:        0.337860
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.967400 loss:        0.096072
Test - acc:         0.904600 loss:        0.332494
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.966360 loss:        0.097739
Test - acc:         0.909400 loss:        0.322695
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.968020 loss:        0.091992
Test - acc:         0.905600 loss:        0.329185
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.966280 loss:        0.097604
Test - acc:         0.907500 loss:        0.321386
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.966880 loss:        0.095600
Test - acc:         0.904800 loss:        0.334905
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.967280 loss:        0.095125
Test - acc:         0.902900 loss:        0.342922
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.967100 loss:        0.094441
Test - acc:         0.904900 loss:        0.331233
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.966100 loss:        0.096943
Test - acc:         0.907200 loss:        0.322090
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.968240 loss:        0.092159
Test - acc:         0.903400 loss:        0.346086
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.964920 loss:        0.100573
Test - acc:         0.902100 loss:        0.347403
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.967400 loss:        0.094256
Test - acc:         0.903600 loss:        0.340290
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.967400 loss:        0.093030
Test - acc:         0.896000 loss:        0.369097
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.966440 loss:        0.096353
Test - acc:         0.906400 loss:        0.347113
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.966060 loss:        0.098681
Test - acc:         0.899600 loss:        0.345882
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.969480 loss:        0.089750
Test - acc:         0.897500 loss:        0.377946
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.966580 loss:        0.097439
Test - acc:         0.907800 loss:        0.328582
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.965700 loss:        0.098529
Test - acc:         0.900600 loss:        0.350448
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.967000 loss:        0.097038
Test - acc:         0.899000 loss:        0.345933
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.968060 loss:        0.095290
Test - acc:         0.900700 loss:        0.372906
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.966040 loss:        0.097817
Test - acc:         0.899400 loss:        0.365118
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.966500 loss:        0.097610
Test - acc:         0.903800 loss:        0.351736
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.965780 loss:        0.098989
Test - acc:         0.904100 loss:        0.337273
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.964820 loss:        0.102716
Test - acc:         0.906700 loss:        0.333858
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.966120 loss:        0.097506
Test - acc:         0.894600 loss:        0.370734
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.967760 loss:        0.093464
Test - acc:         0.901200 loss:        0.354981
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.966180 loss:        0.096647
Test - acc:         0.897000 loss:        0.358249
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.966720 loss:        0.097068
Test - acc:         0.901000 loss:        0.339794
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.967900 loss:        0.094725
Test - acc:         0.904700 loss:        0.337652
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.970860 loss:        0.083053
Test - acc:         0.908600 loss:        0.324383
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.972760 loss:        0.079852
Test - acc:         0.910000 loss:        0.326833
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.969640 loss:        0.085474
Test - acc:         0.904500 loss:        0.337822
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.973040 loss:        0.078047
Test - acc:         0.908700 loss:        0.336404
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.971380 loss:        0.083028
Test - acc:         0.894600 loss:        0.388829
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.970860 loss:        0.084405
Test - acc:         0.904800 loss:        0.346162
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.972280 loss:        0.081644
Test - acc:         0.909400 loss:        0.324277
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.972420 loss:        0.080594
Test - acc:         0.896200 loss:        0.377915
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.971600 loss:        0.083902
Test - acc:         0.901900 loss:        0.368616
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.972400 loss:        0.081628
Test - acc:         0.894500 loss:        0.392834
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.972660 loss:        0.080839
Test - acc:         0.907100 loss:        0.341097
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.971540 loss:        0.083759
Test - acc:         0.906800 loss:        0.327904
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.973340 loss:        0.078790
Test - acc:         0.904500 loss:        0.363443
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.971820 loss:        0.082357
Test - acc:         0.892900 loss:        0.386193
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.968320 loss:        0.090012
Test - acc:         0.907000 loss:        0.343864
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.972280 loss:        0.081378
Test - acc:         0.900700 loss:        0.352660
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.970180 loss:        0.087063
Test - acc:         0.887300 loss:        0.401556
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.970660 loss:        0.084803
Test - acc:         0.900400 loss:        0.368691
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.970660 loss:        0.086052
Test - acc:         0.903900 loss:        0.358188
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.971000 loss:        0.083326
Test - acc:         0.907800 loss:        0.332803
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.970200 loss:        0.085997
Test - acc:         0.905300 loss:        0.337822
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.971160 loss:        0.083618
Test - acc:         0.903200 loss:        0.347709
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.971140 loss:        0.085326
Test - acc:         0.902600 loss:        0.359896
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.970700 loss:        0.085336
Test - acc:         0.906800 loss:        0.351545
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.971000 loss:        0.082422
Test - acc:         0.899700 loss:        0.357450
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.969060 loss:        0.089438
Test - acc:         0.900100 loss:        0.360775
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.970400 loss:        0.085073
Test - acc:         0.906200 loss:        0.356094
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.970820 loss:        0.084401
Test - acc:         0.898400 loss:        0.373307
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.971440 loss:        0.085506
Test - acc:         0.891700 loss:        0.405432
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.973120 loss:        0.079648
Test - acc:         0.905000 loss:        0.346515
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.970540 loss:        0.085294
Test - acc:         0.902300 loss:        0.373354
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.971560 loss:        0.083709
Test - acc:         0.910100 loss:        0.333283
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.971020 loss:        0.083935
Test - acc:         0.891700 loss:        0.414361
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.969960 loss:        0.086688
Test - acc:         0.900900 loss:        0.359982
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.969820 loss:        0.086016
Test - acc:         0.896700 loss:        0.390081
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.971760 loss:        0.082270
Test - acc:         0.894900 loss:        0.372799
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.970620 loss:        0.086944
Test - acc:         0.901800 loss:        0.361988
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.968640 loss:        0.091878
Test - acc:         0.892700 loss:        0.389507
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.971660 loss:        0.082865
Test - acc:         0.901300 loss:        0.360620
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.971860 loss:        0.083290
Test - acc:         0.902800 loss:        0.358609
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.970580 loss:        0.085450
Test - acc:         0.893900 loss:        0.387008
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.974040 loss:        0.076226
Test - acc:         0.907000 loss:        0.336492
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.971820 loss:        0.082262
Test - acc:         0.893900 loss:        0.423085
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.969820 loss:        0.087340
Test - acc:         0.900200 loss:        0.382629
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.972240 loss:        0.083024
Test - acc:         0.905800 loss:        0.350483
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.972600 loss:        0.080649
Test - acc:         0.905800 loss:        0.360030
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.970600 loss:        0.086956
Test - acc:         0.906100 loss:        0.340043
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.971200 loss:        0.082956
Test - acc:         0.906600 loss:        0.333346
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.971540 loss:        0.082154
Test - acc:         0.886400 loss:        0.419301
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.971340 loss:        0.084975
Test - acc:         0.887600 loss:        0.423353
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.983040 loss:        0.053468
Test - acc:         0.921500 loss:        0.271880
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.988640 loss:        0.037464
Test - acc:         0.922000 loss:        0.268965
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.991100 loss:        0.030460
Test - acc:         0.925100 loss:        0.269257
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.991840 loss:        0.027441
Test - acc:         0.924100 loss:        0.273870
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.993120 loss:        0.024156
Test - acc:         0.925200 loss:        0.273937
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.993220 loss:        0.023304
Test - acc:         0.926800 loss:        0.274768
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.993480 loss:        0.022024
Test - acc:         0.927000 loss:        0.278351
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.994600 loss:        0.019505
Test - acc:         0.926800 loss:        0.278500
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.994960 loss:        0.017710
Test - acc:         0.927800 loss:        0.282504
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.995440 loss:        0.016189
Test - acc:         0.926400 loss:        0.285352
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.995620 loss:        0.015742
Test - acc:         0.926800 loss:        0.286026
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.995740 loss:        0.015304
Test - acc:         0.927800 loss:        0.288115
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.995740 loss:        0.014852
Test - acc:         0.927200 loss:        0.290647
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.996640 loss:        0.013169
Test - acc:         0.927400 loss:        0.291024
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.995920 loss:        0.014288
Test - acc:         0.927400 loss:        0.294202
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.996520 loss:        0.012913
Test - acc:         0.927300 loss:        0.297665
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.996320 loss:        0.013471
Test - acc:         0.928000 loss:        0.297265
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.996780 loss:        0.012223
Test - acc:         0.927000 loss:        0.300725
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.997220 loss:        0.010687
Test - acc:         0.928500 loss:        0.301767
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.996860 loss:        0.011020
Test - acc:         0.927400 loss:        0.303161
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.996940 loss:        0.010655
Test - acc:         0.926700 loss:        0.304243
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.997040 loss:        0.010523
Test - acc:         0.929400 loss:        0.304106
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.997180 loss:        0.010077
Test - acc:         0.928500 loss:        0.301710
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.997660 loss:        0.009269
Test - acc:         0.927200 loss:        0.308828
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.997900 loss:        0.008663
Test - acc:         0.927600 loss:        0.309584
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.997740 loss:        0.009104
Test - acc:         0.926900 loss:        0.314004
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.997980 loss:        0.008471
Test - acc:         0.926800 loss:        0.310698
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.997620 loss:        0.009165
Test - acc:         0.926700 loss:        0.312094
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.998040 loss:        0.008469
Test - acc:         0.927100 loss:        0.316397
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.997860 loss:        0.008115
Test - acc:         0.928500 loss:        0.312902
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.998000 loss:        0.007777
Test - acc:         0.926600 loss:        0.316787
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.997980 loss:        0.007877
Test - acc:         0.928200 loss:        0.313299
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.997900 loss:        0.007960
Test - acc:         0.927600 loss:        0.316796
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.998040 loss:        0.007802
Test - acc:         0.928500 loss:        0.311289
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.998460 loss:        0.006633
Test - acc:         0.927700 loss:        0.315636
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.998000 loss:        0.007308
Test - acc:         0.929000 loss:        0.313010
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.998300 loss:        0.007258
Test - acc:         0.928600 loss:        0.319402
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.998360 loss:        0.006203
Test - acc:         0.930000 loss:        0.315146
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.997960 loss:        0.007290
Test - acc:         0.927000 loss:        0.318027
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.998500 loss:        0.006431
Test - acc:         0.927800 loss:        0.317421
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.998080 loss:        0.006949
Test - acc:         0.927300 loss:        0.318008
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.998460 loss:        0.006476
Test - acc:         0.929400 loss:        0.315665
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.998380 loss:        0.006865
Test - acc:         0.927000 loss:        0.321894
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.998440 loss:        0.006266
Test - acc:         0.928800 loss:        0.322767
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.998500 loss:        0.006452
Test - acc:         0.928300 loss:        0.323824
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.998380 loss:        0.006105
Test - acc:         0.929200 loss:        0.327148
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.998380 loss:        0.006113
Test - acc:         0.928900 loss:        0.325894
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.998840 loss:        0.005351
Test - acc:         0.928300 loss:        0.329742
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.998440 loss:        0.005815
Test - acc:         0.928100 loss:        0.325441
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.998700 loss:        0.005518
Test - acc:         0.928600 loss:        0.331025
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.987660 loss:        0.038841
Test - acc:         0.921200 loss:        0.321652
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.990580 loss:        0.029198
Test - acc:         0.923000 loss:        0.313827
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.991740 loss:        0.025535
Test - acc:         0.923300 loss:        0.315470
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.993340 loss:        0.022664
Test - acc:         0.925700 loss:        0.319032
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.993380 loss:        0.021761
Test - acc:         0.924000 loss:        0.317091
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.993880 loss:        0.020408
Test - acc:         0.924700 loss:        0.322630
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.994520 loss:        0.018986
Test - acc:         0.924900 loss:        0.323443
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.994800 loss:        0.017743
Test - acc:         0.925600 loss:        0.330810
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.994500 loss:        0.017806
Test - acc:         0.923100 loss:        0.335525
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.995100 loss:        0.015667
Test - acc:         0.924300 loss:        0.336397
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.995140 loss:        0.016022
Test - acc:         0.923100 loss:        0.333061
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.995700 loss:        0.015510
Test - acc:         0.923900 loss:        0.334369
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.995880 loss:        0.014392
Test - acc:         0.925900 loss:        0.330603
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.995720 loss:        0.014540
Test - acc:         0.924300 loss:        0.337786
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.996040 loss:        0.013546
Test - acc:         0.924100 loss:        0.342639
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.995800 loss:        0.013757
Test - acc:         0.923800 loss:        0.334335
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.996080 loss:        0.013849
Test - acc:         0.925400 loss:        0.336150
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.996140 loss:        0.013597
Test - acc:         0.923500 loss:        0.336543
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.996040 loss:        0.013670
Test - acc:         0.923000 loss:        0.334615
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.996460 loss:        0.012223
Test - acc:         0.924500 loss:        0.342794
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.996700 loss:        0.011921
Test - acc:         0.924100 loss:        0.340193
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.996360 loss:        0.012025
Test - acc:         0.924100 loss:        0.343372
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.996860 loss:        0.010638
Test - acc:         0.924700 loss:        0.340631
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.997240 loss:        0.010548
Test - acc:         0.926000 loss:        0.343267
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.996880 loss:        0.011078
Test - acc:         0.926800 loss:        0.344522
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.997020 loss:        0.010311
Test - acc:         0.925400 loss:        0.344969
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.997180 loss:        0.010430
Test - acc:         0.924800 loss:        0.347876
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.997120 loss:        0.010714
Test - acc:         0.925700 loss:        0.348554
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.996780 loss:        0.011126
Test - acc:         0.924600 loss:        0.350941
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.996780 loss:        0.011543
Test - acc:         0.924800 loss:        0.351980
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.997200 loss:        0.009790
Test - acc:         0.924600 loss:        0.354464
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.997200 loss:        0.009569
Test - acc:         0.926200 loss:        0.354188
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.996840 loss:        0.010213
Test - acc:         0.926600 loss:        0.346207
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.997160 loss:        0.010095
Test - acc:         0.925800 loss:        0.354697
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.997320 loss:        0.009476
Test - acc:         0.926100 loss:        0.359347
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.997360 loss:        0.009373
Test - acc:         0.925700 loss:        0.356312
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.997280 loss:        0.009888
Test - acc:         0.925400 loss:        0.360855
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.997500 loss:        0.008853
Test - acc:         0.924400 loss:        0.359976
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.997340 loss:        0.009740
Test - acc:         0.925100 loss:        0.353060
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.997640 loss:        0.008606
Test - acc:         0.925100 loss:        0.357718
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.997060 loss:        0.009204
Test - acc:         0.924200 loss:        0.359077
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.997700 loss:        0.008219
Test - acc:         0.923200 loss:        0.361796
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.997780 loss:        0.007934
Test - acc:         0.926600 loss:        0.356287
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.997520 loss:        0.007918
Test - acc:         0.925900 loss:        0.356985
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.997900 loss:        0.007702
Test - acc:         0.925300 loss:        0.354030
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.997500 loss:        0.008816
Test - acc:         0.923100 loss:        0.359059
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.997960 loss:        0.007878
Test - acc:         0.925200 loss:        0.361412
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.997940 loss:        0.007734
Test - acc:         0.924900 loss:        0.358136
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.997900 loss:        0.007595
Test - acc:         0.925500 loss:        0.357862
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.998240 loss:        0.006785
Test - acc:         0.923600 loss:        0.360128
Sparsity :          0.9844
Wdecay :        0.000500
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 5000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "test",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": null,
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.112220 loss:        2.707889
Test - acc:         0.115500 loss:        2.285625
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.171100 loss:        2.119530
Test - acc:         0.247800 loss:        1.889029
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.260180 loss:        1.845003
Test - acc:         0.293800 loss:        1.793629
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.351700 loss:        1.666860
Test - acc:         0.365700 loss:        1.606198
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.442140 loss:        1.475611
Test - acc:         0.498800 loss:        1.346169
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.541180 loss:        1.260178
Test - acc:         0.453900 loss:        1.738315
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.619740 loss:        1.083854
Test - acc:         0.532000 loss:        1.515536
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.675980 loss:        0.929632
Test - acc:         0.621200 loss:        1.227844
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.710540 loss:        0.855181
Test - acc:         0.638900 loss:        1.146975
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.730440 loss:        0.803610
Test - acc:         0.680300 loss:        1.070751
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.752360 loss:        0.752724
Test - acc:         0.695200 loss:        0.963042
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.761480 loss:        0.725837
Test - acc:         0.731800 loss:        0.838521
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.771840 loss:        0.701123
Test - acc:         0.694700 loss:        0.991394
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.778780 loss:        0.676362
Test - acc:         0.667800 loss:        1.115059
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.781940 loss:        0.664950
Test - acc:         0.744500 loss:        0.805362
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.790840 loss:        0.641081
Test - acc:         0.779400 loss:        0.675063
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.795240 loss:        0.630031
Test - acc:         0.697300 loss:        0.917811
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.800100 loss:        0.615935
Test - acc:         0.719800 loss:        0.926714
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.800380 loss:        0.612391
Test - acc:         0.752900 loss:        0.813506
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.802400 loss:        0.604146
Test - acc:         0.770800 loss:        0.743486
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.807380 loss:        0.594119
Test - acc:         0.665200 loss:        1.083300
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.810160 loss:        0.585650
Test - acc:         0.778600 loss:        0.685493
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.809940 loss:        0.585200
Test - acc:         0.780900 loss:        0.664840
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.811000 loss:        0.585315
Test - acc:         0.780700 loss:        0.692555
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.812760 loss:        0.576658
Test - acc:         0.723800 loss:        0.945873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.819780 loss:        0.555273
Test - acc:         0.759000 loss:        0.754278
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.816960 loss:        0.567006
Test - acc:         0.759200 loss:        0.769223
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.818100 loss:        0.561920
Test - acc:         0.784400 loss:        0.689784
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.818680 loss:        0.560318
Test - acc:         0.781500 loss:        0.680860
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.818160 loss:        0.564143
Test - acc:         0.704200 loss:        0.952644
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.824040 loss:        0.548989
Test - acc:         0.768300 loss:        0.734637
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.822460 loss:        0.552491
Test - acc:         0.795300 loss:        0.651316
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.822260 loss:        0.549665
Test - acc:         0.776000 loss:        0.726616
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.824040 loss:        0.545305
Test - acc:         0.696300 loss:        1.066222
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.828700 loss:        0.532606
Test - acc:         0.763700 loss:        0.736834
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.825980 loss:        0.536146
Test - acc:         0.797400 loss:        0.639977
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.825080 loss:        0.541546
Test - acc:         0.783400 loss:        0.693384
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.827300 loss:        0.528451
Test - acc:         0.736300 loss:        0.862552
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.824660 loss:        0.537154
Test - acc:         0.745800 loss:        0.838598
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.838520 loss:        0.500924
Test - acc:         0.793700 loss:        0.631330
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.833920 loss:        0.508502
Test - acc:         0.806100 loss:        0.580948
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.832080 loss:        0.517167
Test - acc:         0.786800 loss:        0.694845
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.833800 loss:        0.511543
Test - acc:         0.755200 loss:        0.746825
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.834880 loss:        0.502361
Test - acc:         0.770500 loss:        0.730477
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.833940 loss:        0.511295
Test - acc:         0.621200 loss:        1.719833
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.835780 loss:        0.503326
Test - acc:         0.779700 loss:        0.702423
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.837240 loss:        0.499729
Test - acc:         0.792500 loss:        0.651938
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.834120 loss:        0.505959
Test - acc:         0.803200 loss:        0.604221
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.835300 loss:        0.503720
Test - acc:         0.811800 loss:        0.568039
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.837520 loss:        0.498859
Test - acc:         0.805200 loss:        0.607821
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.840480 loss:        0.483637
Test - acc:         0.737500 loss:        0.838422
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.836440 loss:        0.494476
Test - acc:         0.736700 loss:        0.846380
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.841820 loss:        0.485924
Test - acc:         0.726200 loss:        0.856878
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.839060 loss:        0.487366
Test - acc:         0.812700 loss:        0.614717
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.840420 loss:        0.491506
Test - acc:         0.783100 loss:        0.670789
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.837660 loss:        0.491081
Test - acc:         0.839200 loss:        0.492533
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.841660 loss:        0.476233
Test - acc:         0.783600 loss:        0.675041
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.840520 loss:        0.487575
Test - acc:         0.839800 loss:        0.483930
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.842320 loss:        0.481557
Test - acc:         0.787500 loss:        0.704280
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.839320 loss:        0.490351
Test - acc:         0.736900 loss:        0.846958
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.842600 loss:        0.478507
Test - acc:         0.782500 loss:        0.654619
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.840940 loss:        0.486297
Test - acc:         0.787400 loss:        0.694811
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.840780 loss:        0.481702
Test - acc:         0.800700 loss:        0.618277
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.845260 loss:        0.474919
Test - acc:         0.781000 loss:        0.680621
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.841980 loss:        0.484777
Test - acc:         0.781700 loss:        0.703865
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.843540 loss:        0.471767
Test - acc:         0.763800 loss:        0.783008
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.842560 loss:        0.472455
Test - acc:         0.804900 loss:        0.590880
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.840940 loss:        0.483821
Test - acc:         0.820500 loss:        0.550354
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.844260 loss:        0.469546
Test - acc:         0.817800 loss:        0.566994
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.845120 loss:        0.471098
Test - acc:         0.812000 loss:        0.563756
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.843160 loss:        0.475936
Test - acc:         0.764100 loss:        0.764917
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.843880 loss:        0.473634
Test - acc:         0.815000 loss:        0.580823
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.845820 loss:        0.470264
Test - acc:         0.703400 loss:        0.988886
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.845960 loss:        0.478989
Test - acc:         0.819100 loss:        0.553181
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.842940 loss:        0.470227
Test - acc:         0.807900 loss:        0.585234
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.844220 loss:        0.472235
Test - acc:         0.794500 loss:        0.654466
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.843820 loss:        0.475642
Test - acc:         0.741100 loss:        0.874841
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.841180 loss:        0.477375
Test - acc:         0.816700 loss:        0.553815
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.851180 loss:        0.449106
Test - acc:         0.696100 loss:        1.024187
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.849640 loss:        0.458864
Test - acc:         0.804800 loss:        0.592881
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.850140 loss:        0.452217
Test - acc:         0.773900 loss:        0.740048
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.848800 loss:        0.456138
Test - acc:         0.794100 loss:        0.655619
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.850380 loss:        0.450873
Test - acc:         0.708400 loss:        0.923635
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.850680 loss:        0.449223
Test - acc:         0.743800 loss:        0.876868
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.847960 loss:        0.458558
Test - acc:         0.810500 loss:        0.607446
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.848980 loss:        0.451987
Test - acc:         0.768300 loss:        0.749576
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.850320 loss:        0.451454
Test - acc:         0.822000 loss:        0.545829
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.849200 loss:        0.454119
Test - acc:         0.780700 loss:        0.735907
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.850600 loss:        0.447655
Test - acc:         0.804700 loss:        0.611056
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.851800 loss:        0.449298
Test - acc:         0.772200 loss:        0.829990
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.850440 loss:        0.451201
Test - acc:         0.806700 loss:        0.593920
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.847380 loss:        0.453625
Test - acc:         0.817300 loss:        0.539830
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.851360 loss:        0.449448
Test - acc:         0.829100 loss:        0.516443
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.850540 loss:        0.450356
Test - acc:         0.800400 loss:        0.617236
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.852920 loss:        0.448072
Test - acc:         0.784900 loss:        0.667497
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.851040 loss:        0.445437
Test - acc:         0.794000 loss:        0.674785
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.848580 loss:        0.451589
Test - acc:         0.755900 loss:        0.756093
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.857020 loss:        0.431719
Test - acc:         0.804000 loss:        0.636554
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.851700 loss:        0.445970
Test - acc:         0.815800 loss:        0.558447
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.854060 loss:        0.437733
Test - acc:         0.824700 loss:        0.546696
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.854360 loss:        0.436427
Test - acc:         0.788400 loss:        0.681248
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.850640 loss:        0.449280
Test - acc:         0.807800 loss:        0.602175
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.853220 loss:        0.446132
Test - acc:         0.792600 loss:        0.623759
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.854140 loss:        0.439625
Test - acc:         0.809500 loss:        0.564107
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.851500 loss:        0.448618
Test - acc:         0.797100 loss:        0.642940
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.853440 loss:        0.443259
Test - acc:         0.801700 loss:        0.622460
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.848740 loss:        0.449302
Test - acc:         0.834900 loss:        0.502266
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.857680 loss:        0.431279
Test - acc:         0.813300 loss:        0.586818
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.853000 loss:        0.445198
Test - acc:         0.808000 loss:        0.589023
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.849300 loss:        0.452487
Test - acc:         0.821600 loss:        0.561168
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.845920 loss:        0.460314
Test - acc:         0.792300 loss:        0.686650
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.850720 loss:        0.450326
Test - acc:         0.795600 loss:        0.613710
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.852100 loss:        0.446356
Test - acc:         0.774700 loss:        0.703341
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.850720 loss:        0.449928
Test - acc:         0.803600 loss:        0.605985
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.852660 loss:        0.444288
Test - acc:         0.808600 loss:        0.588697
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.850620 loss:        0.446633
Test - acc:         0.788700 loss:        0.659515
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.851560 loss:        0.445990
Test - acc:         0.822000 loss:        0.553085
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.862740 loss:        0.415564
Test - acc:         0.812800 loss:        0.586600
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.858920 loss:        0.424105
Test - acc:         0.838700 loss:        0.492357
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.854580 loss:        0.432912
Test - acc:         0.810500 loss:        0.580702
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.854460 loss:        0.433853
Test - acc:         0.814100 loss:        0.571883
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.855960 loss:        0.427720
Test - acc:         0.829200 loss:        0.518174
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.853700 loss:        0.436675
Test - acc:         0.815700 loss:        0.566449
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.854820 loss:        0.436617
Test - acc:         0.761600 loss:        0.774127
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.856820 loss:        0.429491
Test - acc:         0.799900 loss:        0.613546
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.857980 loss:        0.424947
Test - acc:         0.824700 loss:        0.535815
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.854340 loss:        0.428583
Test - acc:         0.825800 loss:        0.536535
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.856680 loss:        0.426938
Test - acc:         0.799900 loss:        0.643393
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.855940 loss:        0.434258
Test - acc:         0.823800 loss:        0.545562
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.857440 loss:        0.429044
Test - acc:         0.812500 loss:        0.603833
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.856960 loss:        0.430359
Test - acc:         0.747500 loss:        0.810703
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.857080 loss:        0.428132
Test - acc:         0.826100 loss:        0.524015
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.856460 loss:        0.428043
Test - acc:         0.754000 loss:        0.799920
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.854900 loss:        0.434300
Test - acc:         0.726200 loss:        0.837464
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.858220 loss:        0.424295
Test - acc:         0.793800 loss:        0.624840
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.857300 loss:        0.428206
Test - acc:         0.821600 loss:        0.559819
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.856920 loss:        0.430856
Test - acc:         0.839400 loss:        0.506966
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.858320 loss:        0.421351
Test - acc:         0.796200 loss:        0.612816
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.856860 loss:        0.429301
Test - acc:         0.806600 loss:        0.583199
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.858300 loss:        0.420374
Test - acc:         0.743400 loss:        0.902344
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.858740 loss:        0.424735
Test - acc:         0.769900 loss:        0.766384
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.857120 loss:        0.427111
Test - acc:         0.734500 loss:        0.837526
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.856800 loss:        0.430461
Test - acc:         0.840900 loss:        0.482658
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.856640 loss:        0.431404
Test - acc:         0.803500 loss:        0.606474
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.856460 loss:        0.427741
Test - acc:         0.805400 loss:        0.607660
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.859300 loss:        0.422644
Test - acc:         0.806100 loss:        0.605938
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.856520 loss:        0.430017
Test - acc:         0.798800 loss:        0.651280
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.857840 loss:        0.428083
Test - acc:         0.820600 loss:        0.555147
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.859780 loss:        0.423319
Test - acc:         0.811400 loss:        0.560862
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.856020 loss:        0.432429
Test - acc:         0.833200 loss:        0.505902
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.913700 loss:        0.254722
Test - acc:         0.902300 loss:        0.289470
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.931440 loss:        0.205268
Test - acc:         0.907300 loss:        0.270299
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.938160 loss:        0.184294
Test - acc:         0.914000 loss:        0.267303
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.942620 loss:        0.170286
Test - acc:         0.911700 loss:        0.265409
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.946880 loss:        0.154744
Test - acc:         0.912400 loss:        0.270854
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.949660 loss:        0.148411
Test - acc:         0.916700 loss:        0.260729
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.954060 loss:        0.137690
Test - acc:         0.916100 loss:        0.269793
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.955840 loss:        0.132143
Test - acc:         0.917100 loss:        0.261622
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.957600 loss:        0.126666
Test - acc:         0.917600 loss:        0.269176
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.959580 loss:        0.118161
Test - acc:         0.914500 loss:        0.272521
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.961560 loss:        0.114583
Test - acc:         0.916100 loss:        0.277109
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.961660 loss:        0.112033
Test - acc:         0.917300 loss:        0.268174
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.963020 loss:        0.107349
Test - acc:         0.913500 loss:        0.282356
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.963640 loss:        0.104263
Test - acc:         0.914200 loss:        0.282918
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.964960 loss:        0.101599
Test - acc:         0.913000 loss:        0.299220
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.965700 loss:        0.100661
Test - acc:         0.908500 loss:        0.316634
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.966140 loss:        0.098352
Test - acc:         0.907100 loss:        0.310887
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.966660 loss:        0.098112
Test - acc:         0.913200 loss:        0.286389
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.968380 loss:        0.092055
Test - acc:         0.904300 loss:        0.341735
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.968400 loss:        0.092864
Test - acc:         0.917100 loss:        0.285728
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.967220 loss:        0.093274
Test - acc:         0.909600 loss:        0.312954
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.967520 loss:        0.093552
Test - acc:         0.908100 loss:        0.321399
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.967420 loss:        0.093287
Test - acc:         0.909300 loss:        0.326453
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.967900 loss:        0.091639
Test - acc:         0.900900 loss:        0.355765
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.968720 loss:        0.090077
Test - acc:         0.908300 loss:        0.318380
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.967760 loss:        0.091065
Test - acc:         0.908400 loss:        0.323676
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.967340 loss:        0.092548
Test - acc:         0.901700 loss:        0.333056
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.967480 loss:        0.092129
Test - acc:         0.910000 loss:        0.327548
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.968260 loss:        0.092485
Test - acc:         0.910100 loss:        0.324545
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.968200 loss:        0.092816
Test - acc:         0.906600 loss:        0.319486
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.967900 loss:        0.094228
Test - acc:         0.898600 loss:        0.353926
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.968540 loss:        0.092298
Test - acc:         0.908200 loss:        0.327149
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.969560 loss:        0.089767
Test - acc:         0.904500 loss:        0.328611
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.965960 loss:        0.096437
Test - acc:         0.895500 loss:        0.367909
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.966640 loss:        0.096153
Test - acc:         0.904800 loss:        0.332556
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.967660 loss:        0.094417
Test - acc:         0.904300 loss:        0.343696
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.967420 loss:        0.093194
Test - acc:         0.903100 loss:        0.365996
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.967320 loss:        0.093564
Test - acc:         0.907700 loss:        0.326436
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.967520 loss:        0.094617
Test - acc:         0.894400 loss:        0.370154
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.966520 loss:        0.095580
Test - acc:         0.900700 loss:        0.356316
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.966140 loss:        0.097289
Test - acc:         0.902200 loss:        0.344326
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.967260 loss:        0.093958
Test - acc:         0.898300 loss:        0.357244
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.965440 loss:        0.099805
Test - acc:         0.898500 loss:        0.359792
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.966360 loss:        0.099206
Test - acc:         0.904800 loss:        0.339233
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.966200 loss:        0.098077
Test - acc:         0.904900 loss:        0.336048
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.970920 loss:        0.085443
Test - acc:         0.908800 loss:        0.317829
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.973880 loss:        0.076558
Test - acc:         0.907500 loss:        0.333804
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.972140 loss:        0.079926
Test - acc:         0.903600 loss:        0.355876
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.971960 loss:        0.080631
Test - acc:         0.906300 loss:        0.343310
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.973260 loss:        0.078011
Test - acc:         0.909000 loss:        0.338988
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.971960 loss:        0.080534
Test - acc:         0.898900 loss:        0.373353
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.972920 loss:        0.077831
Test - acc:         0.903300 loss:        0.360128
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.972500 loss:        0.079419
Test - acc:         0.884900 loss:        0.432909
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.972000 loss:        0.082187
Test - acc:         0.894000 loss:        0.390960
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.971280 loss:        0.083335
Test - acc:         0.903300 loss:        0.358224
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.971220 loss:        0.082651
Test - acc:         0.904400 loss:        0.348563
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.970340 loss:        0.084088
Test - acc:         0.899400 loss:        0.368600
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.970700 loss:        0.083302
Test - acc:         0.895200 loss:        0.386486
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.970920 loss:        0.083400
Test - acc:         0.901000 loss:        0.366789
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.971360 loss:        0.083800
Test - acc:         0.900800 loss:        0.366054
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.971180 loss:        0.081627
Test - acc:         0.899500 loss:        0.372745
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.972000 loss:        0.081689
Test - acc:         0.906000 loss:        0.327261
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.971060 loss:        0.082694
Test - acc:         0.903000 loss:        0.359956
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.971680 loss:        0.081986
Test - acc:         0.895400 loss:        0.386204
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.972680 loss:        0.080593
Test - acc:         0.906800 loss:        0.335151
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.970020 loss:        0.084401
Test - acc:         0.890200 loss:        0.404001
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.970620 loss:        0.085405
Test - acc:         0.900100 loss:        0.371779
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.969160 loss:        0.088108
Test - acc:         0.907400 loss:        0.333448
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.970600 loss:        0.084056
Test - acc:         0.904000 loss:        0.363108
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.972520 loss:        0.078551
Test - acc:         0.908300 loss:        0.345228
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.971120 loss:        0.085167
Test - acc:         0.901000 loss:        0.367481
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.971700 loss:        0.083900
Test - acc:         0.914000 loss:        0.308089
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.971180 loss:        0.083052
Test - acc:         0.896700 loss:        0.380324
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.970400 loss:        0.087197
Test - acc:         0.899100 loss:        0.365354
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.969980 loss:        0.087452
Test - acc:         0.902600 loss:        0.357300
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.971980 loss:        0.082225
Test - acc:         0.899100 loss:        0.378475
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.970340 loss:        0.085363
Test - acc:         0.901200 loss:        0.367687
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.969420 loss:        0.086142
Test - acc:         0.903800 loss:        0.353159
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.970320 loss:        0.086235
Test - acc:         0.904700 loss:        0.355391
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.970240 loss:        0.087126
Test - acc:         0.904800 loss:        0.343220
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.970860 loss:        0.084270
Test - acc:         0.899900 loss:        0.373417
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.970080 loss:        0.086584
Test - acc:         0.902700 loss:        0.360019
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.971240 loss:        0.082516
Test - acc:         0.893800 loss:        0.419511
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.970740 loss:        0.086549
Test - acc:         0.905300 loss:        0.338649
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.970320 loss:        0.087185
Test - acc:         0.902000 loss:        0.358983
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.972800 loss:        0.078762
Test - acc:         0.902000 loss:        0.352839
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.972080 loss:        0.081084
Test - acc:         0.907900 loss:        0.344633
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.972520 loss:        0.078579
Test - acc:         0.909200 loss:        0.325116
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.974880 loss:        0.074711
Test - acc:         0.905200 loss:        0.359544
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.972600 loss:        0.081042
Test - acc:         0.900200 loss:        0.373179
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.974400 loss:        0.075037
Test - acc:         0.898000 loss:        0.371866
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.974140 loss:        0.075476
Test - acc:         0.904400 loss:        0.355986
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.975400 loss:        0.070265
Test - acc:         0.904500 loss:        0.369552
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.972960 loss:        0.079278
Test - acc:         0.907800 loss:        0.342613
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.973920 loss:        0.075447
Test - acc:         0.910300 loss:        0.340205
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.974560 loss:        0.076450
Test - acc:         0.908200 loss:        0.341728
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.972000 loss:        0.079802
Test - acc:         0.907400 loss:        0.348061
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.974360 loss:        0.075169
Test - acc:         0.906200 loss:        0.341278
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.973020 loss:        0.076671
Test - acc:         0.903000 loss:        0.369674
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.973000 loss:        0.077797
Test - acc:         0.906700 loss:        0.347971
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.984760 loss:        0.046775
Test - acc:         0.924700 loss:        0.280837
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.989040 loss:        0.033506
Test - acc:         0.926700 loss:        0.286086
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.991540 loss:        0.027967
Test - acc:         0.927700 loss:        0.283971
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.991700 loss:        0.026080
Test - acc:         0.927100 loss:        0.289542
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.992520 loss:        0.023923
Test - acc:         0.926800 loss:        0.290047
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.993840 loss:        0.021240
Test - acc:         0.927100 loss:        0.291941
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.993520 loss:        0.021247
Test - acc:         0.927900 loss:        0.293418
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.994360 loss:        0.019327
Test - acc:         0.928300 loss:        0.294792
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.995120 loss:        0.017476
Test - acc:         0.927300 loss:        0.295949
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.995080 loss:        0.016724
Test - acc:         0.926700 loss:        0.300159
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.995620 loss:        0.015198
Test - acc:         0.927300 loss:        0.304795
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.995040 loss:        0.016922
Test - acc:         0.927200 loss:        0.300679
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.996000 loss:        0.014311
Test - acc:         0.928000 loss:        0.303916
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.995620 loss:        0.014582
Test - acc:         0.924900 loss:        0.306609
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.996140 loss:        0.013666
Test - acc:         0.927700 loss:        0.307075
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.996200 loss:        0.013129
Test - acc:         0.927200 loss:        0.306789
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.996120 loss:        0.013529
Test - acc:         0.926800 loss:        0.306664
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.996040 loss:        0.013072
Test - acc:         0.925200 loss:        0.313688
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.996920 loss:        0.012182
Test - acc:         0.926400 loss:        0.308252
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.996700 loss:        0.011685
Test - acc:         0.926700 loss:        0.310649
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.996800 loss:        0.011770
Test - acc:         0.926400 loss:        0.314992
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.996560 loss:        0.012020
Test - acc:         0.927600 loss:        0.316282
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.996980 loss:        0.011369
Test - acc:         0.926900 loss:        0.313432
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.974920 loss:        0.075893
Test - acc:         0.916300 loss:        0.318975
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.980000 loss:        0.059182
Test - acc:         0.917000 loss:        0.312861
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.982580 loss:        0.051099
Test - acc:         0.918100 loss:        0.311725
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.983240 loss:        0.049538
Test - acc:         0.917900 loss:        0.313909
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.984300 loss:        0.046241
Test - acc:         0.918200 loss:        0.310513
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.985220 loss:        0.043175
Test - acc:         0.918700 loss:        0.309289
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.986500 loss:        0.040314
Test - acc:         0.919900 loss:        0.311507
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.987200 loss:        0.039360
Test - acc:         0.919300 loss:        0.316696
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.987980 loss:        0.036420
Test - acc:         0.919400 loss:        0.314684
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.987700 loss:        0.037248
Test - acc:         0.919300 loss:        0.311932
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.988840 loss:        0.033578
Test - acc:         0.919500 loss:        0.317525
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.989080 loss:        0.033359
Test - acc:         0.919300 loss:        0.318866
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.989180 loss:        0.031779
Test - acc:         0.920800 loss:        0.315058
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.989360 loss:        0.032402
Test - acc:         0.919400 loss:        0.321557
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.989320 loss:        0.031799
Test - acc:         0.921500 loss:        0.320211
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.990060 loss:        0.030148
Test - acc:         0.920100 loss:        0.320378
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.990440 loss:        0.028408
Test - acc:         0.920600 loss:        0.323023
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.991040 loss:        0.027631
Test - acc:         0.919900 loss:        0.324044
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.990100 loss:        0.029778
Test - acc:         0.919000 loss:        0.324906
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.991720 loss:        0.025314
Test - acc:         0.920400 loss:        0.325981
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.991260 loss:        0.027470
Test - acc:         0.919200 loss:        0.330093
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.991920 loss:        0.025924
Test - acc:         0.921100 loss:        0.323162
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.991680 loss:        0.025599
Test - acc:         0.919500 loss:        0.327334
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.991540 loss:        0.025963
Test - acc:         0.918000 loss:        0.332316
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.991860 loss:        0.025150
Test - acc:         0.921400 loss:        0.329972
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.991960 loss:        0.024237
Test - acc:         0.921900 loss:        0.327442
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.992020 loss:        0.025207
Test - acc:         0.919100 loss:        0.331378
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.992580 loss:        0.022513
Test - acc:         0.921400 loss:        0.331228
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.992300 loss:        0.023528
Test - acc:         0.921200 loss:        0.336367
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.992700 loss:        0.023759
Test - acc:         0.920100 loss:        0.332759
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.992100 loss:        0.023968
Test - acc:         0.920800 loss:        0.338712
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.992740 loss:        0.023213
Test - acc:         0.922400 loss:        0.330242
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.993320 loss:        0.021498
Test - acc:         0.921600 loss:        0.336881
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.993920 loss:        0.019995
Test - acc:         0.920700 loss:        0.340247
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.992560 loss:        0.022078
Test - acc:         0.921700 loss:        0.341752
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.993600 loss:        0.020046
Test - acc:         0.919400 loss:        0.344570
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.994340 loss:        0.018950
Test - acc:         0.922200 loss:        0.339442
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.993340 loss:        0.021136
Test - acc:         0.921800 loss:        0.339503
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.994220 loss:        0.018304
Test - acc:         0.922200 loss:        0.341902
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.921180 loss:        0.239636
Test - acc:         0.896700 loss:        0.332396
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.938780 loss:        0.181112
Test - acc:         0.900600 loss:        0.317150
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.944540 loss:        0.162379
Test - acc:         0.905900 loss:        0.307769
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.947440 loss:        0.150059
Test - acc:         0.905300 loss:        0.305179
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.949700 loss:        0.144677
Test - acc:         0.908300 loss:        0.299500
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.953360 loss:        0.134546
Test - acc:         0.908600 loss:        0.299231
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.953620 loss:        0.132920
Test - acc:         0.910200 loss:        0.299306
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.955840 loss:        0.126946
Test - acc:         0.909100 loss:        0.307308
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.956900 loss:        0.121850
Test - acc:         0.911100 loss:        0.294311
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.959260 loss:        0.119734
Test - acc:         0.911400 loss:        0.298324
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.961440 loss:        0.112519
Test - acc:         0.909500 loss:        0.298042
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.962580 loss:        0.109036
Test - acc:         0.909900 loss:        0.300128
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.961460 loss:        0.112378
Test - acc:         0.913100 loss:        0.296682
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.964060 loss:        0.103088
Test - acc:         0.911100 loss:        0.299419
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.963740 loss:        0.104716
Test - acc:         0.910100 loss:        0.305262
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.963840 loss:        0.104259
Test - acc:         0.911100 loss:        0.302275
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.964540 loss:        0.102670
Test - acc:         0.912400 loss:        0.305130
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.966120 loss:        0.096727
Test - acc:         0.912200 loss:        0.308801
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.967180 loss:        0.095897
Test - acc:         0.912500 loss:        0.304823
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.967420 loss:        0.093826
Test - acc:         0.912100 loss:        0.304799
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.968320 loss:        0.092245
Test - acc:         0.913600 loss:        0.304951
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.968400 loss:        0.090642
Test - acc:         0.910900 loss:        0.308142
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.969660 loss:        0.087634
Test - acc:         0.912000 loss:        0.311941
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.970340 loss:        0.086413
Test - acc:         0.911900 loss:        0.313471
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.969620 loss:        0.089004
Test - acc:         0.912600 loss:        0.313989
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.970840 loss:        0.086791
Test - acc:         0.912800 loss:        0.306952
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.969380 loss:        0.086911
Test - acc:         0.912900 loss:        0.313681
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.971100 loss:        0.082684
Test - acc:         0.913300 loss:        0.315146
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.970100 loss:        0.085048
Test - acc:         0.913000 loss:        0.310292
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.970900 loss:        0.082515
Test - acc:         0.913500 loss:        0.316257
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.971980 loss:        0.080106
Test - acc:         0.910800 loss:        0.319394
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.971460 loss:        0.082348
Test - acc:         0.916200 loss:        0.312730
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.972280 loss:        0.079555
Test - acc:         0.911300 loss:        0.319034
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.972180 loss:        0.080449
Test - acc:         0.913400 loss:        0.312260
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.972140 loss:        0.080512
Test - acc:         0.915100 loss:        0.313009
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.973560 loss:        0.076925
Test - acc:         0.913000 loss:        0.321109
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.973420 loss:        0.076233
Test - acc:         0.914600 loss:        0.318604
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.973920 loss:        0.075773
Test - acc:         0.913100 loss:        0.320978
Sparsity :          0.9961
Wdecay :        0.000500
Running weight div flips test.
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 5000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 50,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "test",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": null,
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.106800 loss:        2.887146
Test - acc:         0.106900 loss:        2.342092
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.150740 loss:        2.225399
Test - acc:         0.217800 loss:        1.992225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.234640 loss:        1.934278
Test - acc:         0.264700 loss:        1.843442
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.300120 loss:        1.767933
Test - acc:         0.338200 loss:        1.686450
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.392020 loss:        1.587929
Test - acc:         0.437300 loss:        1.548025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.492220 loss:        1.385459
Test - acc:         0.509400 loss:        1.337186
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.563300 loss:        1.221598
Test - acc:         0.571100 loss:        1.229948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.623540 loss:        1.073808
Test - acc:         0.625300 loss:        1.131033
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.674300 loss:        0.946742
Test - acc:         0.649400 loss:        1.065826
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.715740 loss:        0.855450
Test - acc:         0.646100 loss:        1.036136
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.737360 loss:        0.797917
Test - acc:         0.710400 loss:        0.907650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.757520 loss:        0.740845
Test - acc:         0.739900 loss:        0.834118
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.767060 loss:        0.710992
Test - acc:         0.757100 loss:        0.765124
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.774240 loss:        0.687791
Test - acc:         0.616100 loss:        1.443478
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.778580 loss:        0.675666
Test - acc:         0.758400 loss:        0.744959
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.783520 loss:        0.664763
Test - acc:         0.749500 loss:        0.786861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.789340 loss:        0.647328
Test - acc:         0.641700 loss:        1.267682
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.797920 loss:        0.624745
Test - acc:         0.704000 loss:        1.001233
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.798240 loss:        0.618439
Test - acc:         0.726200 loss:        0.885273
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.806440 loss:        0.602265
Test - acc:         0.693200 loss:        1.160592
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.802280 loss:        0.607501
Test - acc:         0.719900 loss:        0.932802
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.807540 loss:        0.589088
Test - acc:         0.730100 loss:        0.917314
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.809820 loss:        0.588532
Test - acc:         0.715300 loss:        0.942745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.810920 loss:        0.585595
Test - acc:         0.714900 loss:        0.915837
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.811620 loss:        0.582650
Test - acc:         0.728800 loss:        0.960459
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.813400 loss:        0.579243
Test - acc:         0.747100 loss:        0.819809
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.818040 loss:        0.561499
Test - acc:         0.780800 loss:        0.700761
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.816380 loss:        0.563682
Test - acc:         0.758900 loss:        0.750426
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.818180 loss:        0.555751
Test - acc:         0.765400 loss:        0.744770
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.820560 loss:        0.551154
Test - acc:         0.754200 loss:        0.816667
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.820700 loss:        0.553704
Test - acc:         0.772100 loss:        0.741460
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.823620 loss:        0.541712
Test - acc:         0.776900 loss:        0.711410
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.823200 loss:        0.544783
Test - acc:         0.787900 loss:        0.659339
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.824960 loss:        0.537488
Test - acc:         0.739300 loss:        0.842265
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.824540 loss:        0.538636
Test - acc:         0.747200 loss:        0.830881
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.825260 loss:        0.538525
Test - acc:         0.721800 loss:        0.859321
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.826340 loss:        0.537777
Test - acc:         0.759300 loss:        0.778132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.825240 loss:        0.536022
Test - acc:         0.699400 loss:        0.966428
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.828320 loss:        0.533694
Test - acc:         0.748900 loss:        0.794782
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.829740 loss:        0.524597
Test - acc:         0.779400 loss:        0.698693
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.830560 loss:        0.526019
Test - acc:         0.765000 loss:        0.810277
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.829480 loss:        0.525207
Test - acc:         0.817700 loss:        0.535735
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.830560 loss:        0.524708
Test - acc:         0.800900 loss:        0.639905
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.832280 loss:        0.520831
Test - acc:         0.658700 loss:        1.128968
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.830180 loss:        0.520592
Test - acc:         0.752500 loss:        0.800119
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.830940 loss:        0.525489
Test - acc:         0.809500 loss:        0.578567
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.828380 loss:        0.522665
Test - acc:         0.742900 loss:        0.823478
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.833420 loss:        0.507824
Test - acc:         0.753900 loss:        0.753708
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.832900 loss:        0.511564
Test - acc:         0.644200 loss:        1.207186
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.831300 loss:        0.517602
Test - acc:         0.803600 loss:        0.605393
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.840680 loss:        0.484682
Test - acc:         0.747500 loss:        0.798628
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.840060 loss:        0.492501
Test - acc:         0.764000 loss:        0.805129
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.840860 loss:        0.488650
Test - acc:         0.821300 loss:        0.559254
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.838180 loss:        0.496718
Test - acc:         0.785300 loss:        0.657222
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.841440 loss:        0.484262
Test - acc:         0.722800 loss:        0.971121
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.839620 loss:        0.490442
Test - acc:         0.779800 loss:        0.680630
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.836900 loss:        0.496828
Test - acc:         0.747100 loss:        0.935585
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.837060 loss:        0.500181
Test - acc:         0.696300 loss:        1.163116
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.841360 loss:        0.486461
Test - acc:         0.792300 loss:        0.662626
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.841940 loss:        0.485859
Test - acc:         0.776900 loss:        0.729675
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.839860 loss:        0.484222
Test - acc:         0.751200 loss:        0.814169
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.838180 loss:        0.491284
Test - acc:         0.784600 loss:        0.689681
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.838660 loss:        0.491292
Test - acc:         0.796500 loss:        0.614007
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.837960 loss:        0.491256
Test - acc:         0.783800 loss:        0.684862
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.841280 loss:        0.481467
Test - acc:         0.699300 loss:        1.040882
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.844060 loss:        0.476336
Test - acc:         0.760400 loss:        0.798862
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.844620 loss:        0.477542
Test - acc:         0.745900 loss:        0.795079
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.840400 loss:        0.487205
Test - acc:         0.754200 loss:        0.814568
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.842400 loss:        0.479675
Test - acc:         0.769300 loss:        0.724195
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.841500 loss:        0.487010
Test - acc:         0.767300 loss:        0.781917
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.843960 loss:        0.479534
Test - acc:         0.770100 loss:        0.737989
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.841160 loss:        0.482660
Test - acc:         0.791800 loss:        0.641834
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.840740 loss:        0.488981
Test - acc:         0.786600 loss:        0.668088
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.844160 loss:        0.472232
Test - acc:         0.751800 loss:        0.805427
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.842120 loss:        0.478180
Test - acc:         0.702500 loss:        0.939316
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.843420 loss:        0.479109
Test - acc:         0.798100 loss:        0.658402
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.842700 loss:        0.478202
Test - acc:         0.800700 loss:        0.626107
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.844420 loss:        0.479731
Test - acc:         0.802200 loss:        0.645069
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.845200 loss:        0.472901
Test - acc:         0.769800 loss:        0.748393
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.843800 loss:        0.476746
Test - acc:         0.705700 loss:        0.987178
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.843860 loss:        0.474393
Test - acc:         0.749900 loss:        0.781490
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.844100 loss:        0.477959
Test - acc:         0.692100 loss:        1.096476
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.842860 loss:        0.481077
Test - acc:         0.751000 loss:        0.889433
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.845960 loss:        0.469100
Test - acc:         0.792900 loss:        0.676334
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.844740 loss:        0.476506
Test - acc:         0.764800 loss:        0.774133
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.846520 loss:        0.465874
Test - acc:         0.792300 loss:        0.628039
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.847480 loss:        0.466120
Test - acc:         0.792300 loss:        0.681373
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.846960 loss:        0.468219
Test - acc:         0.819500 loss:        0.574212
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.844800 loss:        0.473988
Test - acc:         0.811500 loss:        0.626596
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.843780 loss:        0.475404
Test - acc:         0.764500 loss:        0.703210
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.845420 loss:        0.466853
Test - acc:         0.720400 loss:        0.966700
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.844280 loss:        0.469405
Test - acc:         0.798100 loss:        0.624946
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.844920 loss:        0.468087
Test - acc:         0.799900 loss:        0.662255
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.846980 loss:        0.469482
Test - acc:         0.753300 loss:        0.775615
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.844300 loss:        0.473263
Test - acc:         0.840200 loss:        0.492104
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.845460 loss:        0.469536
Test - acc:         0.790900 loss:        0.662895
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.846260 loss:        0.468991
Test - acc:         0.790900 loss:        0.676335
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.845540 loss:        0.469960
Test - acc:         0.806600 loss:        0.622612
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.845500 loss:        0.466136
Test - acc:         0.769900 loss:        0.769612
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.848280 loss:        0.466600
Test - acc:         0.746200 loss:        0.796562
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.853260 loss:        0.449583
Test - acc:         0.820400 loss:        0.548691
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.853940 loss:        0.441957
Test - acc:         0.759700 loss:        0.793030
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.846600 loss:        0.461592
Test - acc:         0.750500 loss:        0.765572
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.850700 loss:        0.453222
Test - acc:         0.778300 loss:        0.721812
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.848800 loss:        0.456371
Test - acc:         0.742300 loss:        0.963459
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.851300 loss:        0.450523
Test - acc:         0.764800 loss:        0.728255
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.849420 loss:        0.453214
Test - acc:         0.812700 loss:        0.620422
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.845820 loss:        0.466733
Test - acc:         0.826600 loss:        0.527951
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.849800 loss:        0.457275
Test - acc:         0.802700 loss:        0.643683
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.853420 loss:        0.447632
Test - acc:         0.784800 loss:        0.703333
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.850780 loss:        0.454288
Test - acc:         0.759700 loss:        0.719518
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.851920 loss:        0.451615
Test - acc:         0.809500 loss:        0.605964
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.853480 loss:        0.444286
Test - acc:         0.786700 loss:        0.661662
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.845460 loss:        0.464172
Test - acc:         0.823700 loss:        0.540364
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.855800 loss:        0.439129
Test - acc:         0.764300 loss:        0.735919
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.850960 loss:        0.448927
Test - acc:         0.828700 loss:        0.523746
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.851220 loss:        0.452655
Test - acc:         0.757900 loss:        0.814522
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.850820 loss:        0.452457
Test - acc:         0.799400 loss:        0.625692
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.851280 loss:        0.446024
Test - acc:         0.741400 loss:        0.882777
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.846320 loss:        0.460180
Test - acc:         0.817200 loss:        0.578017
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.851400 loss:        0.445663
Test - acc:         0.760300 loss:        0.753012
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.851460 loss:        0.448551
Test - acc:         0.786700 loss:        0.681243
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.850000 loss:        0.450927
Test - acc:         0.817800 loss:        0.559806
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.853580 loss:        0.448178
Test - acc:         0.796800 loss:        0.639193
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.850380 loss:        0.451800
Test - acc:         0.793900 loss:        0.645495
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.852000 loss:        0.446023
Test - acc:         0.812600 loss:        0.578997
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.849100 loss:        0.455920
Test - acc:         0.796300 loss:        0.629870
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.852020 loss:        0.446286
Test - acc:         0.779900 loss:        0.713475
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.850080 loss:        0.450942
Test - acc:         0.758300 loss:        0.829801
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.851200 loss:        0.450290
Test - acc:         0.833900 loss:        0.536073
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.851280 loss:        0.452463
Test - acc:         0.792400 loss:        0.661743
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.851880 loss:        0.450405
Test - acc:         0.799900 loss:        0.628495
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.850180 loss:        0.450781
Test - acc:         0.838300 loss:        0.503563
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.854180 loss:        0.447335
Test - acc:         0.765800 loss:        0.709547
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.852980 loss:        0.449835
Test - acc:         0.828100 loss:        0.517559
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.850460 loss:        0.451554
Test - acc:         0.728500 loss:        0.871803
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.851300 loss:        0.450166
Test - acc:         0.715900 loss:        0.927501
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.854140 loss:        0.443576
Test - acc:         0.794000 loss:        0.676138
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.853820 loss:        0.444268
Test - acc:         0.779300 loss:        0.771739
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.852940 loss:        0.449008
Test - acc:         0.815500 loss:        0.564867
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.852240 loss:        0.444089
Test - acc:         0.772800 loss:        0.770490
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.851520 loss:        0.447487
Test - acc:         0.768200 loss:        0.710583
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.851900 loss:        0.447842
Test - acc:         0.780600 loss:        0.664423
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.852680 loss:        0.443489
Test - acc:         0.731000 loss:        0.904384
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.851140 loss:        0.445616
Test - acc:         0.744800 loss:        0.834712
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.850140 loss:        0.448324
Test - acc:         0.783100 loss:        0.703011
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.853960 loss:        0.448677
Test - acc:         0.790000 loss:        0.649956
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.852380 loss:        0.447923
Test - acc:         0.776100 loss:        0.734156
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.851580 loss:        0.448530
Test - acc:         0.808800 loss:        0.595334
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.852200 loss:        0.445330
Test - acc:         0.720500 loss:        0.921707
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.909580 loss:        0.273924
Test - acc:         0.899100 loss:        0.303620
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.926300 loss:        0.220421
Test - acc:         0.904700 loss:        0.296441
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.933320 loss:        0.202092
Test - acc:         0.910300 loss:        0.282177
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.937840 loss:        0.185118
Test - acc:         0.909700 loss:        0.280973
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.940120 loss:        0.175680
Test - acc:         0.909800 loss:        0.286284
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.945840 loss:        0.162002
Test - acc:         0.910100 loss:        0.280614
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.949820 loss:        0.152421
Test - acc:         0.907400 loss:        0.288837
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.951900 loss:        0.143489
Test - acc:         0.908600 loss:        0.288537
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.952240 loss:        0.143356
Test - acc:         0.913600 loss:        0.274622
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.954240 loss:        0.133681
Test - acc:         0.908200 loss:        0.299183
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.956440 loss:        0.126704
Test - acc:         0.910100 loss:        0.293224
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.958840 loss:        0.121131
Test - acc:         0.910100 loss:        0.290736
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.958800 loss:        0.121795
Test - acc:         0.912200 loss:        0.302842
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.961320 loss:        0.113507
Test - acc:         0.908100 loss:        0.315116
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.962500 loss:        0.110328
Test - acc:         0.910200 loss:        0.309403
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.961820 loss:        0.112013
Test - acc:         0.907600 loss:        0.312630
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.964540 loss:        0.104440
Test - acc:         0.909500 loss:        0.314372
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.962580 loss:        0.108440
Test - acc:         0.908600 loss:        0.313447
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.964180 loss:        0.104179
Test - acc:         0.908000 loss:        0.318500
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.964420 loss:        0.103145
Test - acc:         0.904700 loss:        0.328375
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.963960 loss:        0.103442
Test - acc:         0.903600 loss:        0.329608
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.965060 loss:        0.104455
Test - acc:         0.907500 loss:        0.316372
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.966440 loss:        0.098654
Test - acc:         0.907400 loss:        0.314998
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.966480 loss:        0.098409
Test - acc:         0.908800 loss:        0.314350
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.965960 loss:        0.100177
Test - acc:         0.904800 loss:        0.341465
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.964740 loss:        0.101571
Test - acc:         0.905700 loss:        0.332823
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.965540 loss:        0.101122
Test - acc:         0.905500 loss:        0.322833
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.964260 loss:        0.103885
Test - acc:         0.912100 loss:        0.307551
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.966440 loss:        0.097566
Test - acc:         0.904000 loss:        0.359081
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.964420 loss:        0.102662
Test - acc:         0.904100 loss:        0.337681
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.966820 loss:        0.097913
Test - acc:         0.906800 loss:        0.336209
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.965720 loss:        0.101016
Test - acc:         0.907400 loss:        0.323714
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.967320 loss:        0.094796
Test - acc:         0.900700 loss:        0.349679
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.962980 loss:        0.106009
Test - acc:         0.894800 loss:        0.370213
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.965680 loss:        0.099963
Test - acc:         0.898200 loss:        0.371100
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.964540 loss:        0.103646
Test - acc:         0.893200 loss:        0.380814
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.964500 loss:        0.103657
Test - acc:         0.899100 loss:        0.360525
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.966700 loss:        0.098205
Test - acc:         0.900400 loss:        0.342857
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.964540 loss:        0.102764
Test - acc:         0.894900 loss:        0.373727
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.963500 loss:        0.105258
Test - acc:         0.899900 loss:        0.356805
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.966420 loss:        0.096739
Test - acc:         0.893700 loss:        0.404722
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.964180 loss:        0.103901
Test - acc:         0.893900 loss:        0.383998
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.964780 loss:        0.102870
Test - acc:         0.902600 loss:        0.364389
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.963860 loss:        0.103647
Test - acc:         0.900900 loss:        0.357093
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.965020 loss:        0.100907
Test - acc:         0.887700 loss:        0.410170
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.963900 loss:        0.105072
Test - acc:         0.904400 loss:        0.360848
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.966400 loss:        0.097032
Test - acc:         0.899800 loss:        0.344392
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.965560 loss:        0.099986
Test - acc:         0.897700 loss:        0.376989
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.966120 loss:        0.099393
Test - acc:         0.901400 loss:        0.369633
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.963480 loss:        0.105946
Test - acc:         0.906600 loss:        0.331264
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.972300 loss:        0.082676
Test - acc:         0.904700 loss:        0.339120
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.971640 loss:        0.082190
Test - acc:         0.898700 loss:        0.389210
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.971500 loss:        0.083533
Test - acc:         0.902200 loss:        0.363257
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.970520 loss:        0.087397
Test - acc:         0.898900 loss:        0.369149
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.971200 loss:        0.084056
Test - acc:         0.898700 loss:        0.377657
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.971800 loss:        0.083756
Test - acc:         0.901300 loss:        0.367831
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.972580 loss:        0.081995
Test - acc:         0.900700 loss:        0.357982
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.968940 loss:        0.088310
Test - acc:         0.899000 loss:        0.359687
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.969900 loss:        0.087135
Test - acc:         0.905600 loss:        0.335966
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.971140 loss:        0.086280
Test - acc:         0.893200 loss:        0.396310
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.968740 loss:        0.090346
Test - acc:         0.905200 loss:        0.357556
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.970120 loss:        0.086914
Test - acc:         0.894800 loss:        0.387352
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.970520 loss:        0.086426
Test - acc:         0.895700 loss:        0.392488
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.968860 loss:        0.090939
Test - acc:         0.894300 loss:        0.397126
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.968940 loss:        0.091006
Test - acc:         0.899300 loss:        0.379958
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.969120 loss:        0.090247
Test - acc:         0.897000 loss:        0.373229
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.969640 loss:        0.087624
Test - acc:         0.899300 loss:        0.372557
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.968880 loss:        0.091003
Test - acc:         0.898700 loss:        0.356606
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.970760 loss:        0.088404
Test - acc:         0.887800 loss:        0.428297
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.968560 loss:        0.092460
Test - acc:         0.896300 loss:        0.377230
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.968200 loss:        0.092791
Test - acc:         0.898800 loss:        0.377475
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.967760 loss:        0.093614
Test - acc:         0.902000 loss:        0.363680
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.968180 loss:        0.092725
Test - acc:         0.893600 loss:        0.396163
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.972020 loss:        0.084662
Test - acc:         0.902200 loss:        0.367770
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.969700 loss:        0.090345
Test - acc:         0.898000 loss:        0.374016
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.968380 loss:        0.092423
Test - acc:         0.900700 loss:        0.359795
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.970480 loss:        0.087176
Test - acc:         0.899900 loss:        0.370566
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.968040 loss:        0.092506
Test - acc:         0.896600 loss:        0.359876
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.970880 loss:        0.086203
Test - acc:         0.900100 loss:        0.358440
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.969720 loss:        0.089953
Test - acc:         0.895600 loss:        0.382256
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.969920 loss:        0.088753
Test - acc:         0.899400 loss:        0.363468
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.967140 loss:        0.093897
Test - acc:         0.895300 loss:        0.367755
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.968060 loss:        0.092891
Test - acc:         0.901100 loss:        0.351882
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.972220 loss:        0.082589
Test - acc:         0.884800 loss:        0.433377
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.968780 loss:        0.088611
Test - acc:         0.895300 loss:        0.379141
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.967420 loss:        0.094452
Test - acc:         0.897900 loss:        0.355917
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.968940 loss:        0.090186
Test - acc:         0.903400 loss:        0.350380
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.970380 loss:        0.087656
Test - acc:         0.905000 loss:        0.358532
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.972560 loss:        0.083972
Test - acc:         0.893000 loss:        0.388447
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.969320 loss:        0.091249
Test - acc:         0.903200 loss:        0.359240
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.969260 loss:        0.086782
Test - acc:         0.899100 loss:        0.358376
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.971740 loss:        0.084741
Test - acc:         0.900500 loss:        0.373568
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.969680 loss:        0.089119
Test - acc:         0.897600 loss:        0.383864
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.971500 loss:        0.084811
Test - acc:         0.890800 loss:        0.410898
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.969700 loss:        0.088452
Test - acc:         0.897900 loss:        0.373487
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.970800 loss:        0.085407
Test - acc:         0.906900 loss:        0.351543
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.970120 loss:        0.087470
Test - acc:         0.888800 loss:        0.439684
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.969840 loss:        0.089604
Test - acc:         0.903700 loss:        0.350956
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.969540 loss:        0.089570
Test - acc:         0.895100 loss:        0.386803
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.969180 loss:        0.089986
Test - acc:         0.899500 loss:        0.357972
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.981420 loss:        0.058757
Test - acc:         0.919000 loss:        0.284414
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.987660 loss:        0.040292
Test - acc:         0.921300 loss:        0.283117
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.990020 loss:        0.032395
Test - acc:         0.922000 loss:        0.286917
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.991400 loss:        0.029892
Test - acc:         0.921800 loss:        0.291241
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.993000 loss:        0.025298
Test - acc:         0.922300 loss:        0.289565
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.992860 loss:        0.024543
Test - acc:         0.923300 loss:        0.293142
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.992900 loss:        0.023310
Test - acc:         0.924100 loss:        0.294449
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.994120 loss:        0.020404
Test - acc:         0.924000 loss:        0.299192
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.994980 loss:        0.018126
Test - acc:         0.922400 loss:        0.301926
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.995060 loss:        0.018911
Test - acc:         0.923800 loss:        0.304222
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.994920 loss:        0.017924
Test - acc:         0.924900 loss:        0.304372
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.995620 loss:        0.016480
Test - acc:         0.924400 loss:        0.304487
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.996100 loss:        0.014532
Test - acc:         0.924700 loss:        0.307921
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.995500 loss:        0.015389
Test - acc:         0.923400 loss:        0.309417
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.995520 loss:        0.014977
Test - acc:         0.925400 loss:        0.313476
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.995740 loss:        0.014935
Test - acc:         0.923900 loss:        0.310829
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.996480 loss:        0.012508
Test - acc:         0.924900 loss:        0.314512
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.996440 loss:        0.013072
Test - acc:         0.926700 loss:        0.314266
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.996560 loss:        0.012899
Test - acc:         0.924000 loss:        0.318505
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.996700 loss:        0.012276
Test - acc:         0.926000 loss:        0.318744
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.996480 loss:        0.011952
Test - acc:         0.925200 loss:        0.318563
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.996740 loss:        0.011244
Test - acc:         0.925700 loss:        0.319092
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.997260 loss:        0.011243
Test - acc:         0.925100 loss:        0.320925
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.996700 loss:        0.011137
Test - acc:         0.924800 loss:        0.326596
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.996980 loss:        0.010572
Test - acc:         0.925200 loss:        0.327733
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.997240 loss:        0.010225
Test - acc:         0.926200 loss:        0.322609
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.997100 loss:        0.010731
Test - acc:         0.924700 loss:        0.330739
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.997100 loss:        0.010513
Test - acc:         0.925700 loss:        0.329504
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.997360 loss:        0.009776
Test - acc:         0.924900 loss:        0.327599
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.997480 loss:        0.009405
Test - acc:         0.924800 loss:        0.330338
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.997600 loss:        0.008580
Test - acc:         0.926500 loss:        0.326567
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.997380 loss:        0.009388
Test - acc:         0.926000 loss:        0.327473
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.998020 loss:        0.007876
Test - acc:         0.925000 loss:        0.333708
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.998160 loss:        0.007242
Test - acc:         0.924800 loss:        0.335195
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.998260 loss:        0.007006
Test - acc:         0.924200 loss:        0.337705
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.998000 loss:        0.007869
Test - acc:         0.925800 loss:        0.336161
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.997940 loss:        0.007742
Test - acc:         0.926000 loss:        0.336301
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.997980 loss:        0.007513
Test - acc:         0.924700 loss:        0.340531
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.997680 loss:        0.008597
Test - acc:         0.926000 loss:        0.343549
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.997780 loss:        0.007976
Test - acc:         0.925200 loss:        0.341092
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.997940 loss:        0.007684
Test - acc:         0.926400 loss:        0.339711
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.998340 loss:        0.007067
Test - acc:         0.924700 loss:        0.341185
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.998240 loss:        0.006946
Test - acc:         0.924600 loss:        0.345433
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.998540 loss:        0.005914
Test - acc:         0.925000 loss:        0.348339
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.998180 loss:        0.006661
Test - acc:         0.923700 loss:        0.351068
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.998480 loss:        0.005961
Test - acc:         0.924100 loss:        0.350338
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.998620 loss:        0.006036
Test - acc:         0.924800 loss:        0.349201
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.997960 loss:        0.007057
Test - acc:         0.925800 loss:        0.350890
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.998640 loss:        0.005855
Test - acc:         0.925300 loss:        0.353188
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.998400 loss:        0.006471
Test - acc:         0.925100 loss:        0.353479
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.987060 loss:        0.040052
Test - acc:         0.919000 loss:        0.350236
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.990260 loss:        0.029391
Test - acc:         0.921800 loss:        0.338808
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.991100 loss:        0.026810
Test - acc:         0.922300 loss:        0.331423
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.991880 loss:        0.024504
Test - acc:         0.921700 loss:        0.333299
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.992520 loss:        0.023448
Test - acc:         0.922800 loss:        0.340394
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.993160 loss:        0.022451
Test - acc:         0.921900 loss:        0.335174
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.994020 loss:        0.019690
Test - acc:         0.922600 loss:        0.335284
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.994320 loss:        0.018129
Test - acc:         0.923300 loss:        0.334403
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.994300 loss:        0.018064
Test - acc:         0.922800 loss:        0.334438
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.994380 loss:        0.019011
Test - acc:         0.921700 loss:        0.340073
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.995140 loss:        0.016265
Test - acc:         0.922200 loss:        0.342423
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.995420 loss:        0.016285
Test - acc:         0.922600 loss:        0.339887
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.994720 loss:        0.016518
Test - acc:         0.922900 loss:        0.340358
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.995080 loss:        0.015659
Test - acc:         0.922400 loss:        0.340249
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.995640 loss:        0.015080
Test - acc:         0.923100 loss:        0.343912
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.995220 loss:        0.015190
Test - acc:         0.923500 loss:        0.342133
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.995800 loss:        0.014267
Test - acc:         0.923200 loss:        0.345841
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.995660 loss:        0.014555
Test - acc:         0.923800 loss:        0.346142
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.995460 loss:        0.014878
Test - acc:         0.923100 loss:        0.346960
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.996200 loss:        0.012851
Test - acc:         0.923400 loss:        0.344872
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.996560 loss:        0.012612
Test - acc:         0.922500 loss:        0.348687
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.995940 loss:        0.013244
Test - acc:         0.923500 loss:        0.349495
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.996380 loss:        0.012356
Test - acc:         0.922300 loss:        0.352522
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.995660 loss:        0.013582
Test - acc:         0.925100 loss:        0.344987
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.996520 loss:        0.012005
Test - acc:         0.924400 loss:        0.346960
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.996580 loss:        0.010986
Test - acc:         0.924100 loss:        0.345173
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.996660 loss:        0.011448
Test - acc:         0.924700 loss:        0.349336
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.997140 loss:        0.010314
Test - acc:         0.922300 loss:        0.349084
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.996320 loss:        0.010918
Test - acc:         0.923300 loss:        0.354154
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.996540 loss:        0.011532
Test - acc:         0.924300 loss:        0.346408
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.996640 loss:        0.011355
Test - acc:         0.923400 loss:        0.350911
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.996960 loss:        0.010364
Test - acc:         0.922000 loss:        0.360190
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.996820 loss:        0.010447
Test - acc:         0.923100 loss:        0.354055
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.997240 loss:        0.010200
Test - acc:         0.924000 loss:        0.360734
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.997360 loss:        0.009748
Test - acc:         0.925800 loss:        0.356405
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.997340 loss:        0.009879
Test - acc:         0.923900 loss:        0.353912
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.997000 loss:        0.009852
Test - acc:         0.924300 loss:        0.359310
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.997000 loss:        0.011016
Test - acc:         0.923400 loss:        0.357046
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.997320 loss:        0.009128
Test - acc:         0.923600 loss:        0.360600
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.997380 loss:        0.008934
Test - acc:         0.923500 loss:        0.360165
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.996980 loss:        0.010063
Test - acc:         0.922800 loss:        0.361781
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.997140 loss:        0.009578
Test - acc:         0.923000 loss:        0.359294
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.997020 loss:        0.010103
Test - acc:         0.922300 loss:        0.366493
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.996980 loss:        0.010292
Test - acc:         0.925400 loss:        0.362600
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.996640 loss:        0.010418
Test - acc:         0.922500 loss:        0.366843
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.997060 loss:        0.009516
Test - acc:         0.922400 loss:        0.360027
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.997620 loss:        0.008597
Test - acc:         0.922000 loss:        0.364538
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.996860 loss:        0.009826
Test - acc:         0.923000 loss:        0.365958
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.997320 loss:        0.009357
Test - acc:         0.923800 loss:        0.361548
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.997700 loss:        0.007864
Test - acc:         0.923900 loss:        0.362831
Sparsity :          0.9844
Wdecay :        0.000500
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 5000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "weight_div_flips",
    "prune_freq": 39,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "test",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": null,
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.106800 loss:        2.887146
Test - acc:         0.106900 loss:        2.342092
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.150740 loss:        2.225399
Test - acc:         0.217800 loss:        1.992225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.234640 loss:        1.934278
Test - acc:         0.264700 loss:        1.843442
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.300120 loss:        1.767933
Test - acc:         0.338200 loss:        1.686450
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.392020 loss:        1.587929
Test - acc:         0.437300 loss:        1.548025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.492220 loss:        1.385459
Test - acc:         0.509400 loss:        1.337186
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.563300 loss:        1.221598
Test - acc:         0.571100 loss:        1.229948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.623540 loss:        1.073808
Test - acc:         0.625300 loss:        1.131033
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.674300 loss:        0.946742
Test - acc:         0.649400 loss:        1.065826
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.715740 loss:        0.855450
Test - acc:         0.646100 loss:        1.036136
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.737360 loss:        0.797917
Test - acc:         0.710400 loss:        0.907650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.757520 loss:        0.740845
Test - acc:         0.739900 loss:        0.834118
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.767060 loss:        0.710992
Test - acc:         0.757100 loss:        0.765124
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.774240 loss:        0.687791
Test - acc:         0.616100 loss:        1.443478
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.778580 loss:        0.675666
Test - acc:         0.758400 loss:        0.744959
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.783520 loss:        0.664763
Test - acc:         0.749500 loss:        0.786861
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.789340 loss:        0.647328
Test - acc:         0.641700 loss:        1.267682
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.797920 loss:        0.624745
Test - acc:         0.704000 loss:        1.001233
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.798240 loss:        0.618439
Test - acc:         0.726200 loss:        0.885273
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.806440 loss:        0.602265
Test - acc:         0.693200 loss:        1.160592
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.802280 loss:        0.607501
Test - acc:         0.719900 loss:        0.932802
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.807540 loss:        0.589088
Test - acc:         0.730100 loss:        0.917314
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.809820 loss:        0.588532
Test - acc:         0.715300 loss:        0.942745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.810920 loss:        0.585595
Test - acc:         0.714900 loss:        0.915837
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.811620 loss:        0.582650
Test - acc:         0.728800 loss:        0.960459
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.813400 loss:        0.579243
Test - acc:         0.747100 loss:        0.819809
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.818040 loss:        0.561499
Test - acc:         0.780800 loss:        0.700761
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.816380 loss:        0.563682
Test - acc:         0.758900 loss:        0.750426
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.818180 loss:        0.555751
Test - acc:         0.765400 loss:        0.744770
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.820560 loss:        0.551154
Test - acc:         0.754200 loss:        0.816667
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.820700 loss:        0.553704
Test - acc:         0.772100 loss:        0.741460
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.823620 loss:        0.541712
Test - acc:         0.776900 loss:        0.711410
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.823200 loss:        0.544783
Test - acc:         0.787900 loss:        0.659339
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.824960 loss:        0.537488
Test - acc:         0.739300 loss:        0.842265
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.824540 loss:        0.538636
Test - acc:         0.747200 loss:        0.830881
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.825260 loss:        0.538525
Test - acc:         0.721800 loss:        0.859321
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.826340 loss:        0.537777
Test - acc:         0.759300 loss:        0.778132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.825240 loss:        0.536022
Test - acc:         0.699400 loss:        0.966428
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.828320 loss:        0.533694
Test - acc:         0.748900 loss:        0.794782
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.836320 loss:        0.499087
Test - acc:         0.729500 loss:        0.855307
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.832440 loss:        0.513287
Test - acc:         0.782000 loss:        0.709718
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.836080 loss:        0.504052
Test - acc:         0.745400 loss:        0.865146
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.833400 loss:        0.511349
Test - acc:         0.817800 loss:        0.586628
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.835380 loss:        0.501898
Test - acc:         0.771100 loss:        0.699043
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.835300 loss:        0.504877
Test - acc:         0.714100 loss:        1.061653
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.832880 loss:        0.512391
Test - acc:         0.767400 loss:        0.728133
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.836960 loss:        0.504519
Test - acc:         0.769600 loss:        0.747969
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.836680 loss:        0.497692
Test - acc:         0.788200 loss:        0.656999
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.835900 loss:        0.503736
Test - acc:         0.781500 loss:        0.673082
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.834240 loss:        0.502852
Test - acc:         0.812400 loss:        0.592255
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.839180 loss:        0.493193
Test - acc:         0.727700 loss:        0.903784
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.836960 loss:        0.497252
Test - acc:         0.788200 loss:        0.660316
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.837240 loss:        0.499564
Test - acc:         0.773800 loss:        0.724539
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.838540 loss:        0.495377
Test - acc:         0.764700 loss:        0.751486
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.841620 loss:        0.488102
Test - acc:         0.754200 loss:        0.839395
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.837680 loss:        0.495127
Test - acc:         0.823500 loss:        0.545658
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.838900 loss:        0.493543
Test - acc:         0.725600 loss:        0.879866
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.837780 loss:        0.497185
Test - acc:         0.743600 loss:        0.809461
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.839440 loss:        0.490495
Test - acc:         0.761700 loss:        0.746513
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.842280 loss:        0.485511
Test - acc:         0.801400 loss:        0.641118
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.839280 loss:        0.490537
Test - acc:         0.745300 loss:        0.764262
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.840680 loss:        0.485954
Test - acc:         0.814300 loss:        0.578464
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.841000 loss:        0.489255
Test - acc:         0.811800 loss:        0.610469
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.839580 loss:        0.492681
Test - acc:         0.788900 loss:        0.702279
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.840820 loss:        0.486564
Test - acc:         0.798300 loss:        0.601623
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.841200 loss:        0.484222
Test - acc:         0.775000 loss:        0.783575
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.843000 loss:        0.476134
Test - acc:         0.782800 loss:        0.657276
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.842480 loss:        0.478047
Test - acc:         0.796200 loss:        0.609311
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.841760 loss:        0.483928
Test - acc:         0.793100 loss:        0.662979
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.841380 loss:        0.482991
Test - acc:         0.776000 loss:        0.751783
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.841120 loss:        0.485708
Test - acc:         0.784300 loss:        0.668808
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.841800 loss:        0.483569
Test - acc:         0.720000 loss:        0.934742
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.842240 loss:        0.481047
Test - acc:         0.730600 loss:        0.875384
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.845020 loss:        0.474413
Test - acc:         0.787700 loss:        0.669649
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.841320 loss:        0.484041
Test - acc:         0.769400 loss:        0.733417
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.844600 loss:        0.473626
Test - acc:         0.765700 loss:        0.764672
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.841520 loss:        0.488669
Test - acc:         0.801900 loss:        0.599832
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.844980 loss:        0.473384
Test - acc:         0.809800 loss:        0.604742
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.848600 loss:        0.459726
Test - acc:         0.817700 loss:        0.550154
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.849000 loss:        0.459651
Test - acc:         0.785100 loss:        0.661864
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.847740 loss:        0.462899
Test - acc:         0.735600 loss:        0.845916
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.847920 loss:        0.463750
Test - acc:         0.805300 loss:        0.610880
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.846680 loss:        0.463993
Test - acc:         0.775800 loss:        0.735165
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.849100 loss:        0.458664
Test - acc:         0.735600 loss:        0.864216
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.847360 loss:        0.464843
Test - acc:         0.789700 loss:        0.667361
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.853780 loss:        0.448350
Test - acc:         0.755700 loss:        0.783828
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.847560 loss:        0.460446
Test - acc:         0.836100 loss:        0.513907
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.849120 loss:        0.461560
Test - acc:         0.819500 loss:        0.562867
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.851020 loss:        0.455570
Test - acc:         0.762800 loss:        0.722158
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.842960 loss:        0.485279
Test - acc:         0.765000 loss:        0.788321
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.847480 loss:        0.464428
Test - acc:         0.806200 loss:        0.603346
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.848440 loss:        0.461927
Test - acc:         0.813100 loss:        0.562712
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.848680 loss:        0.459443
Test - acc:         0.775100 loss:        0.692749
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.846200 loss:        0.464456
Test - acc:         0.812700 loss:        0.556049
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.849640 loss:        0.460477
Test - acc:         0.807700 loss:        0.594461
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.850360 loss:        0.456153
Test - acc:         0.820000 loss:        0.572033
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.847880 loss:        0.455437
Test - acc:         0.806600 loss:        0.580483
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.852060 loss:        0.451515
Test - acc:         0.753500 loss:        0.827404
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.849540 loss:        0.453869
Test - acc:         0.802000 loss:        0.603777
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.849720 loss:        0.454522
Test - acc:         0.770300 loss:        0.722861
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.849840 loss:        0.455271
Test - acc:         0.783400 loss:        0.691452
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.851840 loss:        0.451229
Test - acc:         0.806500 loss:        0.624262
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.847220 loss:        0.461198
Test - acc:         0.754800 loss:        0.832113
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.850080 loss:        0.452941
Test - acc:         0.812300 loss:        0.588843
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.849380 loss:        0.459133
Test - acc:         0.772300 loss:        0.767313
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.851860 loss:        0.448585
Test - acc:         0.792500 loss:        0.645001
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.850620 loss:        0.454272
Test - acc:         0.798500 loss:        0.642343
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.850060 loss:        0.451641
Test - acc:         0.814300 loss:        0.579837
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.850560 loss:        0.450537
Test - acc:         0.780100 loss:        0.674753
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.852300 loss:        0.451610
Test - acc:         0.789800 loss:        0.676790
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.850580 loss:        0.447844
Test - acc:         0.787400 loss:        0.692017
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.853100 loss:        0.449273
Test - acc:         0.744900 loss:        0.839062
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.849100 loss:        0.455811
Test - acc:         0.731600 loss:        0.962351
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.851480 loss:        0.452365
Test - acc:         0.801400 loss:        0.616867
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.856140 loss:        0.439683
Test - acc:         0.785800 loss:        0.683296
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.852180 loss:        0.447676
Test - acc:         0.816800 loss:        0.580019
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.850280 loss:        0.458368
Test - acc:         0.777600 loss:        0.719145
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.854720 loss:        0.437834
Test - acc:         0.818500 loss:        0.569121
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.855940 loss:        0.435876
Test - acc:         0.802200 loss:        0.654086
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.852620 loss:        0.442325
Test - acc:         0.829200 loss:        0.540575
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.852560 loss:        0.440905
Test - acc:         0.794200 loss:        0.690521
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.853920 loss:        0.438554
Test - acc:         0.810900 loss:        0.583385
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.855360 loss:        0.432260
Test - acc:         0.782300 loss:        0.662861
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.853120 loss:        0.440622
Test - acc:         0.821900 loss:        0.547724
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.853440 loss:        0.441154
Test - acc:         0.783300 loss:        0.682523
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.854160 loss:        0.437752
Test - acc:         0.751400 loss:        0.828304
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.855020 loss:        0.439605
Test - acc:         0.822100 loss:        0.550693
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.856080 loss:        0.436083
Test - acc:         0.810400 loss:        0.630978
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.854580 loss:        0.443322
Test - acc:         0.775100 loss:        0.723939
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.854620 loss:        0.439677
Test - acc:         0.832000 loss:        0.505648
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.854600 loss:        0.440073
Test - acc:         0.799700 loss:        0.600730
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.854960 loss:        0.437221
Test - acc:         0.818400 loss:        0.606302
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.853420 loss:        0.444329
Test - acc:         0.773100 loss:        0.720237
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.853740 loss:        0.436567
Test - acc:         0.785500 loss:        0.717762
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.855760 loss:        0.436357
Test - acc:         0.811400 loss:        0.590360
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.851040 loss:        0.447148
Test - acc:         0.759900 loss:        0.801240
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.851500 loss:        0.445518
Test - acc:         0.822600 loss:        0.531208
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.854820 loss:        0.437455
Test - acc:         0.849400 loss:        0.458644
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.853660 loss:        0.437727
Test - acc:         0.757200 loss:        0.861153
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.857400 loss:        0.429301
Test - acc:         0.822300 loss:        0.538466
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.853560 loss:        0.442743
Test - acc:         0.777000 loss:        0.731796
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.854000 loss:        0.437999
Test - acc:         0.776900 loss:        0.704583
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.856740 loss:        0.431025
Test - acc:         0.813800 loss:        0.564121
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.853880 loss:        0.436356
Test - acc:         0.809800 loss:        0.571485
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.855460 loss:        0.432750
Test - acc:         0.673000 loss:        1.233472
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.853200 loss:        0.440824
Test - acc:         0.780300 loss:        0.661906
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.854600 loss:        0.438990
Test - acc:         0.766600 loss:        0.724272
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.853340 loss:        0.440150
Test - acc:         0.793200 loss:        0.717511
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.848660 loss:        0.459581
Test - acc:         0.805300 loss:        0.628882
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.848820 loss:        0.464207
Test - acc:         0.777000 loss:        0.737794
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.907280 loss:        0.279955
Test - acc:         0.903300 loss:        0.297537
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.925040 loss:        0.223872
Test - acc:         0.907100 loss:        0.287971
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.932240 loss:        0.201424
Test - acc:         0.909400 loss:        0.274348
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.938600 loss:        0.186931
Test - acc:         0.912400 loss:        0.271108
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.942080 loss:        0.176143
Test - acc:         0.911900 loss:        0.276214
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.945220 loss:        0.164243
Test - acc:         0.913200 loss:        0.273927
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.949480 loss:        0.150717
Test - acc:         0.915700 loss:        0.265952
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.950880 loss:        0.142487
Test - acc:         0.912600 loss:        0.277136
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.953400 loss:        0.138206
Test - acc:         0.915000 loss:        0.271056
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.954160 loss:        0.131850
Test - acc:         0.915300 loss:        0.280091
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.957100 loss:        0.126763
Test - acc:         0.912400 loss:        0.283556
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.958560 loss:        0.122943
Test - acc:         0.912500 loss:        0.289988
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.960420 loss:        0.119735
Test - acc:         0.914000 loss:        0.288497
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.960000 loss:        0.117256
Test - acc:         0.907700 loss:        0.301142
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.962720 loss:        0.108858
Test - acc:         0.909300 loss:        0.306392
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.961860 loss:        0.111535
Test - acc:         0.910700 loss:        0.313287
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.961920 loss:        0.109610
Test - acc:         0.912900 loss:        0.311383
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.964280 loss:        0.106658
Test - acc:         0.911500 loss:        0.302049
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.963680 loss:        0.106721
Test - acc:         0.910200 loss:        0.304736
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.963360 loss:        0.107402
Test - acc:         0.915600 loss:        0.298970
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.964880 loss:        0.102395
Test - acc:         0.909400 loss:        0.302190
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.964340 loss:        0.104967
Test - acc:         0.911700 loss:        0.303789
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.966240 loss:        0.099304
Test - acc:         0.908500 loss:        0.314950
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.966700 loss:        0.096108
Test - acc:         0.910400 loss:        0.312517
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.965640 loss:        0.102126
Test - acc:         0.906600 loss:        0.329772
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.965560 loss:        0.099609
Test - acc:         0.911600 loss:        0.313137
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.965640 loss:        0.100544
Test - acc:         0.908600 loss:        0.311400
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.965280 loss:        0.099640
Test - acc:         0.907000 loss:        0.333582
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.966540 loss:        0.097801
Test - acc:         0.907000 loss:        0.338164
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.965520 loss:        0.099044
Test - acc:         0.902300 loss:        0.344265
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.965260 loss:        0.101447
Test - acc:         0.906500 loss:        0.317226
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.966260 loss:        0.097809
Test - acc:         0.906300 loss:        0.338041
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.964900 loss:        0.100405
Test - acc:         0.903700 loss:        0.334918
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.966120 loss:        0.099666
Test - acc:         0.899700 loss:        0.345048
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.965520 loss:        0.098659
Test - acc:         0.907000 loss:        0.329240
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.964360 loss:        0.103699
Test - acc:         0.890200 loss:        0.378369
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.963320 loss:        0.106220
Test - acc:         0.896500 loss:        0.356897
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.963780 loss:        0.104544
Test - acc:         0.903200 loss:        0.328988
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.965000 loss:        0.101883
Test - acc:         0.900500 loss:        0.350856
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.963760 loss:        0.105695
Test - acc:         0.903900 loss:        0.334053
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.965580 loss:        0.099706
Test - acc:         0.905200 loss:        0.337767
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.962960 loss:        0.106039
Test - acc:         0.897100 loss:        0.354589
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.965940 loss:        0.101968
Test - acc:         0.908100 loss:        0.335171
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.964980 loss:        0.101625
Test - acc:         0.906100 loss:        0.323490
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.964580 loss:        0.101518
Test - acc:         0.901700 loss:        0.344785
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.969080 loss:        0.089707
Test - acc:         0.906200 loss:        0.330392
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.971420 loss:        0.083162
Test - acc:         0.895800 loss:        0.390499
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.972520 loss:        0.082710
Test - acc:         0.905600 loss:        0.340163
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.968760 loss:        0.089013
Test - acc:         0.906200 loss:        0.338437
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.970960 loss:        0.086526
Test - acc:         0.899400 loss:        0.362659
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.970960 loss:        0.086337
Test - acc:         0.907000 loss:        0.327514
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.971220 loss:        0.083605
Test - acc:         0.899900 loss:        0.361693
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.971340 loss:        0.084434
Test - acc:         0.902900 loss:        0.356265
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.970700 loss:        0.086648
Test - acc:         0.895500 loss:        0.363282
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.969880 loss:        0.090562
Test - acc:         0.904100 loss:        0.330465
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.970480 loss:        0.086815
Test - acc:         0.898800 loss:        0.362909
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.971260 loss:        0.083259
Test - acc:         0.906600 loss:        0.347939
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.971980 loss:        0.083043
Test - acc:         0.900300 loss:        0.365487
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.969980 loss:        0.089328
Test - acc:         0.901800 loss:        0.359070
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.968980 loss:        0.091662
Test - acc:         0.899000 loss:        0.341056
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.968560 loss:        0.091091
Test - acc:         0.904400 loss:        0.330757
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.969460 loss:        0.089253
Test - acc:         0.891700 loss:        0.376024
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.970300 loss:        0.088061
Test - acc:         0.905100 loss:        0.342480
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.970500 loss:        0.086455
Test - acc:         0.898100 loss:        0.376708
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.967280 loss:        0.093579
Test - acc:         0.889300 loss:        0.403233
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.967580 loss:        0.093778
Test - acc:         0.908500 loss:        0.326666
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.969840 loss:        0.090271
Test - acc:         0.890700 loss:        0.398531
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.966800 loss:        0.093156
Test - acc:         0.898100 loss:        0.357010
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.968060 loss:        0.092182
Test - acc:         0.904500 loss:        0.320275
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.968860 loss:        0.092067
Test - acc:         0.899800 loss:        0.381265
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.967720 loss:        0.093944
Test - acc:         0.893700 loss:        0.379858
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.968820 loss:        0.090474
Test - acc:         0.904000 loss:        0.366321
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.968400 loss:        0.092382
Test - acc:         0.892900 loss:        0.399240
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.969220 loss:        0.089888
Test - acc:         0.900900 loss:        0.353374
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.968720 loss:        0.091917
Test - acc:         0.898200 loss:        0.349047
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.969980 loss:        0.089791
Test - acc:         0.905400 loss:        0.338528
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.969880 loss:        0.087470
Test - acc:         0.896200 loss:        0.380035
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.968680 loss:        0.093711
Test - acc:         0.901900 loss:        0.351940
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.970340 loss:        0.085642
Test - acc:         0.894100 loss:        0.389325
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.969300 loss:        0.090085
Test - acc:         0.894300 loss:        0.382905
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.968040 loss:        0.092382
Test - acc:         0.903800 loss:        0.351517
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.969920 loss:        0.089797
Test - acc:         0.900300 loss:        0.365613
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.968400 loss:        0.091919
Test - acc:         0.901400 loss:        0.355675
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.969820 loss:        0.089272
Test - acc:         0.896800 loss:        0.379951
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.967840 loss:        0.093906
Test - acc:         0.898700 loss:        0.373096
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.971120 loss:        0.085211
Test - acc:         0.903200 loss:        0.339243
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.971520 loss:        0.080142
Test - acc:         0.900200 loss:        0.341348
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.971840 loss:        0.083123
Test - acc:         0.910400 loss:        0.325204
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.972380 loss:        0.080203
Test - acc:         0.909800 loss:        0.329286
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.970260 loss:        0.086341
Test - acc:         0.904200 loss:        0.356245
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.973820 loss:        0.078076
Test - acc:         0.905800 loss:        0.353684
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.972820 loss:        0.080134
Test - acc:         0.905800 loss:        0.335688
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.971500 loss:        0.083203
Test - acc:         0.899600 loss:        0.377339
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.972160 loss:        0.081656
Test - acc:         0.901900 loss:        0.341525
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.971940 loss:        0.083806
Test - acc:         0.907400 loss:        0.342088
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.969800 loss:        0.088479
Test - acc:         0.898300 loss:        0.366054
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.972040 loss:        0.081733
Test - acc:         0.894000 loss:        0.386917
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.971440 loss:        0.081674
Test - acc:         0.905700 loss:        0.343529
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.971000 loss:        0.086862
Test - acc:         0.899100 loss:        0.355993
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.971800 loss:        0.082224
Test - acc:         0.900200 loss:        0.361681
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.982860 loss:        0.051482
Test - acc:         0.920500 loss:        0.284081
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.988320 loss:        0.036238
Test - acc:         0.922400 loss:        0.283476
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.990460 loss:        0.031810
Test - acc:         0.922900 loss:        0.285655
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.991680 loss:        0.027778
Test - acc:         0.924900 loss:        0.286917
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.992440 loss:        0.024744
Test - acc:         0.924600 loss:        0.287642
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.992600 loss:        0.024398
Test - acc:         0.925000 loss:        0.286074
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.993340 loss:        0.022112
Test - acc:         0.924900 loss:        0.291672
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.993640 loss:        0.021007
Test - acc:         0.925000 loss:        0.292822
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.994620 loss:        0.018949
Test - acc:         0.925200 loss:        0.297095
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.994720 loss:        0.018752
Test - acc:         0.924700 loss:        0.295672
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.994660 loss:        0.018180
Test - acc:         0.927400 loss:        0.296986
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.994760 loss:        0.017546
Test - acc:         0.925600 loss:        0.299607
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.995220 loss:        0.016814
Test - acc:         0.926300 loss:        0.298129
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.994820 loss:        0.017240
Test - acc:         0.925400 loss:        0.300228
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.995680 loss:        0.014935
Test - acc:         0.926000 loss:        0.300161
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.995780 loss:        0.015351
Test - acc:         0.926000 loss:        0.298203
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.995900 loss:        0.014177
Test - acc:         0.926000 loss:        0.302668
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.995720 loss:        0.013869
Test - acc:         0.924600 loss:        0.310997
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.996260 loss:        0.013245
Test - acc:         0.927100 loss:        0.304076
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.996660 loss:        0.012246
Test - acc:         0.927100 loss:        0.308127
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.996380 loss:        0.012872
Test - acc:         0.926700 loss:        0.308950
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.996380 loss:        0.012596
Test - acc:         0.926900 loss:        0.307861
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.996780 loss:        0.012402
Test - acc:         0.927000 loss:        0.312942
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.972500 loss:        0.082872
Test - acc:         0.914500 loss:        0.328612
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.977880 loss:        0.064218
Test - acc:         0.915300 loss:        0.312261
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.980300 loss:        0.059267
Test - acc:         0.917100 loss:        0.307711
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.981220 loss:        0.056300
Test - acc:         0.916900 loss:        0.302115
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.983140 loss:        0.051353
Test - acc:         0.916400 loss:        0.312279
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.984560 loss:        0.047838
Test - acc:         0.917000 loss:        0.309287
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.985440 loss:        0.044273
Test - acc:         0.916900 loss:        0.308312
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.984640 loss:        0.044950
Test - acc:         0.919800 loss:        0.310111
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.986280 loss:        0.041450
Test - acc:         0.919000 loss:        0.314293
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.986200 loss:        0.040276
Test - acc:         0.919700 loss:        0.309917
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.987380 loss:        0.037461
Test - acc:         0.918400 loss:        0.316014
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.987520 loss:        0.037601
Test - acc:         0.917900 loss:        0.312893
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.987900 loss:        0.036444
Test - acc:         0.919600 loss:        0.317834
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.988080 loss:        0.035911
Test - acc:         0.919200 loss:        0.313768
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.988380 loss:        0.034683
Test - acc:         0.918500 loss:        0.320263
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.988360 loss:        0.034558
Test - acc:         0.919600 loss:        0.319828
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.989180 loss:        0.032877
Test - acc:         0.921000 loss:        0.318132
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.989520 loss:        0.031122
Test - acc:         0.919900 loss:        0.325078
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.989960 loss:        0.029800
Test - acc:         0.918800 loss:        0.322945
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.989800 loss:        0.030406
Test - acc:         0.918100 loss:        0.325480
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.990220 loss:        0.029144
Test - acc:         0.919100 loss:        0.322684
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.989960 loss:        0.029894
Test - acc:         0.918400 loss:        0.324151
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.990400 loss:        0.028003
Test - acc:         0.917800 loss:        0.321116
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.990840 loss:        0.027467
Test - acc:         0.920800 loss:        0.326982
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.990800 loss:        0.028322
Test - acc:         0.919000 loss:        0.331462
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.991100 loss:        0.027176
Test - acc:         0.918800 loss:        0.327128
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.991280 loss:        0.026544
Test - acc:         0.919400 loss:        0.330944
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.991440 loss:        0.024879
Test - acc:         0.921300 loss:        0.330273
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.991380 loss:        0.026010
Test - acc:         0.917800 loss:        0.340154
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.991660 loss:        0.025136
Test - acc:         0.920000 loss:        0.330472
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.991420 loss:        0.025752
Test - acc:         0.920800 loss:        0.331507
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.991680 loss:        0.025710
Test - acc:         0.923000 loss:        0.325418
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.991360 loss:        0.024623
Test - acc:         0.921000 loss:        0.331468
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.992080 loss:        0.024630
Test - acc:         0.921700 loss:        0.335756
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.991500 loss:        0.025502
Test - acc:         0.920300 loss:        0.340375
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.992480 loss:        0.023285
Test - acc:         0.920400 loss:        0.335388
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.992760 loss:        0.023394
Test - acc:         0.920800 loss:        0.333461
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.992700 loss:        0.022206
Test - acc:         0.920200 loss:        0.335780
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.992500 loss:        0.022139
Test - acc:         0.920000 loss:        0.343000
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.916000 loss:        0.255342
Test - acc:         0.894400 loss:        0.351712
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.932000 loss:        0.199309
Test - acc:         0.896100 loss:        0.340059
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.937740 loss:        0.179686
Test - acc:         0.900200 loss:        0.327026
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.944120 loss:        0.165817
Test - acc:         0.900200 loss:        0.323001
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.945740 loss:        0.158981
Test - acc:         0.903200 loss:        0.318618
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.949600 loss:        0.148098
Test - acc:         0.903800 loss:        0.316640
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.950500 loss:        0.143038
Test - acc:         0.906400 loss:        0.317678
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.952280 loss:        0.138353
Test - acc:         0.907100 loss:        0.316083
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.954040 loss:        0.133198
Test - acc:         0.905600 loss:        0.317092
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.953660 loss:        0.134606
Test - acc:         0.902800 loss:        0.322646
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.955000 loss:        0.127799
Test - acc:         0.907500 loss:        0.314647
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.957760 loss:        0.123580
Test - acc:         0.906400 loss:        0.314205
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.957640 loss:        0.123559
Test - acc:         0.907300 loss:        0.318338
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.959180 loss:        0.118772
Test - acc:         0.908200 loss:        0.318659
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.959240 loss:        0.119061
Test - acc:         0.907000 loss:        0.317107
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.961340 loss:        0.112838
Test - acc:         0.908200 loss:        0.315078
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.961960 loss:        0.111259
Test - acc:         0.909300 loss:        0.317567
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.962040 loss:        0.110170
Test - acc:         0.910900 loss:        0.314303
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.962620 loss:        0.107766
Test - acc:         0.910700 loss:        0.316433
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.963660 loss:        0.105884
Test - acc:         0.909900 loss:        0.323684
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.963540 loss:        0.105792
Test - acc:         0.908600 loss:        0.314400
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.964080 loss:        0.102885
Test - acc:         0.909800 loss:        0.327772
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.965540 loss:        0.099312
Test - acc:         0.905900 loss:        0.327628
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.966220 loss:        0.099136
Test - acc:         0.906500 loss:        0.328220
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.965620 loss:        0.099255
Test - acc:         0.908100 loss:        0.321103
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.965460 loss:        0.098103
Test - acc:         0.906900 loss:        0.332267
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.967060 loss:        0.096175
Test - acc:         0.909000 loss:        0.322460
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.968120 loss:        0.094134
Test - acc:         0.908200 loss:        0.327724
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.966960 loss:        0.095699
Test - acc:         0.907700 loss:        0.323742
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.968100 loss:        0.091994
Test - acc:         0.910900 loss:        0.320513
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.968780 loss:        0.091415
Test - acc:         0.908100 loss:        0.323510
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.968780 loss:        0.090082
Test - acc:         0.906400 loss:        0.331338
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.968700 loss:        0.089919
Test - acc:         0.907300 loss:        0.325168
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.968400 loss:        0.089636
Test - acc:         0.908600 loss:        0.326144
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.969600 loss:        0.089140
Test - acc:         0.908200 loss:        0.325799
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.969260 loss:        0.089158
Test - acc:         0.908200 loss:        0.327804
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.968920 loss:        0.090032
Test - acc:         0.908200 loss:        0.324037
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.970520 loss:        0.087498
Test - acc:         0.908200 loss:        0.321892
Sparsity :          0.9961
Wdecay :        0.000500
