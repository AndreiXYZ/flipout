Running --model vgg19 --prune_criterion magnitude --seed 42 --prune_freq 32 --prune_rate 0.5 --comment=vgg19_crit=magnitude_pf=32_seed=42 --save_model=pre-finetune/vgg19_magnitude_pf32_s42 --logdir=criterion_experiment_no_bias/vgg19
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "magnitude",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_magnitude_pf32_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.103940 loss:        2.526355
Test - acc:         0.097900 loss:        2.302562
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.111000 loss:        2.296948
Test - acc:         0.135800 loss:        2.287671
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.185180 loss:        2.095249
Test - acc:         0.204100 loss:        1.924940
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.241660 loss:        1.887907
Test - acc:         0.264800 loss:        1.811172
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.326660 loss:        1.714085
Test - acc:         0.350900 loss:        1.675830
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.436780 loss:        1.479783
Test - acc:         0.408200 loss:        1.759592
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.525500 loss:        1.303626
Test - acc:         0.537000 loss:        1.378895
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.616800 loss:        1.083961
Test - acc:         0.590600 loss:        1.300951
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.669560 loss:        0.951760
Test - acc:         0.608900 loss:        1.110669
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.711520 loss:        0.844386
Test - acc:         0.650100 loss:        1.219713
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.741020 loss:        0.780390
Test - acc:         0.671600 loss:        1.031043
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.753580 loss:        0.740309
Test - acc:         0.644100 loss:        1.180330
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.772760 loss:        0.691439
Test - acc:         0.657200 loss:        1.078353
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.782560 loss:        0.664204
Test - acc:         0.571800 loss:        1.670740
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.784380 loss:        0.656515
Test - acc:         0.731100 loss:        0.940813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.795500 loss:        0.622433
Test - acc:         0.714400 loss:        0.970297
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.803000 loss:        0.609394
Test - acc:         0.781900 loss:        0.669261
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.807580 loss:        0.591968
Test - acc:         0.770600 loss:        0.724289
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.809360 loss:        0.589175
Test - acc:         0.803000 loss:        0.614238
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.814200 loss:        0.569209
Test - acc:         0.757800 loss:        0.758979
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.816600 loss:        0.563241
Test - acc:         0.763800 loss:        0.707813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.821260 loss:        0.551170
Test - acc:         0.761900 loss:        0.738491
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.823380 loss:        0.544565
Test - acc:         0.785600 loss:        0.672582
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.824980 loss:        0.539750
Test - acc:         0.739100 loss:        0.799185
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.825620 loss:        0.537203
Test - acc:         0.794200 loss:        0.634343
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.831920 loss:        0.515494
Test - acc:         0.784400 loss:        0.658191
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.834500 loss:        0.515579
Test - acc:         0.769900 loss:        0.790124
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.836880 loss:        0.507073
Test - acc:         0.761500 loss:        0.783315
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.835440 loss:        0.506329
Test - acc:         0.799000 loss:        0.616728
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.834040 loss:        0.505033
Test - acc:         0.684600 loss:        1.254283
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.839060 loss:        0.497706
Test - acc:         0.683600 loss:        1.059633
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.843760 loss:        0.483917
Test - acc:         0.768400 loss:        0.772606
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.855600 loss:        0.442645
Test - acc:         0.796200 loss:        0.654813
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.857700 loss:        0.435654
Test - acc:         0.743400 loss:        0.822314
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.859820 loss:        0.430847
Test - acc:         0.806300 loss:        0.607894
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.858000 loss:        0.435510
Test - acc:         0.798200 loss:        0.624232
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.861100 loss:        0.426692
Test - acc:         0.761200 loss:        0.803432
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.860360 loss:        0.428804
Test - acc:         0.803900 loss:        0.631567
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.858900 loss:        0.431492
Test - acc:         0.791400 loss:        0.665752
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.861520 loss:        0.422892
Test - acc:         0.811100 loss:        0.595657
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.861160 loss:        0.420404
Test - acc:         0.780700 loss:        0.715576
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.863080 loss:        0.422592
Test - acc:         0.805400 loss:        0.624187
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.865060 loss:        0.412213
Test - acc:         0.756800 loss:        0.787389
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.865160 loss:        0.411619
Test - acc:         0.793900 loss:        0.676006
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.860920 loss:        0.421101
Test - acc:         0.796400 loss:        0.722637
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.864720 loss:        0.415798
Test - acc:         0.793500 loss:        0.678351
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.868340 loss:        0.404599
Test - acc:         0.830600 loss:        0.548690
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.862580 loss:        0.414614
Test - acc:         0.791200 loss:        0.637598
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.863120 loss:        0.413856
Test - acc:         0.842400 loss:        0.477414
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.866560 loss:        0.407417
Test - acc:         0.779400 loss:        0.724538
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.865760 loss:        0.404477
Test - acc:         0.797200 loss:        0.673164
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.864860 loss:        0.407368
Test - acc:         0.807200 loss:        0.636733
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.867860 loss:        0.403714
Test - acc:         0.819300 loss:        0.564132
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.866180 loss:        0.405709
Test - acc:         0.843700 loss:        0.474537
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.870620 loss:        0.396141
Test - acc:         0.796000 loss:        0.680635
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.867800 loss:        0.400619
Test - acc:         0.785900 loss:        0.689903
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.868480 loss:        0.396992
Test - acc:         0.771700 loss:        0.836783
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.869600 loss:        0.395984
Test - acc:         0.807000 loss:        0.625133
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.868200 loss:        0.396421
Test - acc:         0.793500 loss:        0.643940
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.870240 loss:        0.398469
Test - acc:         0.763400 loss:        0.767651
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.869820 loss:        0.393667
Test - acc:         0.816100 loss:        0.568536
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.871200 loss:        0.394674
Test - acc:         0.774700 loss:        0.775752
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.869820 loss:        0.392057
Test - acc:         0.832300 loss:        0.514639
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.871540 loss:        0.388435
Test - acc:         0.757300 loss:        0.822921
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.886700 loss:        0.347449
Test - acc:         0.812300 loss:        0.624554
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.886480 loss:        0.344530
Test - acc:         0.802200 loss:        0.605612
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.886240 loss:        0.344476
Test - acc:         0.830500 loss:        0.526563
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.878180 loss:        0.360892
Test - acc:         0.820100 loss:        0.572182
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.883000 loss:        0.351663
Test - acc:         0.851200 loss:        0.462774
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.881200 loss:        0.353253
Test - acc:         0.804900 loss:        0.610247
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.883160 loss:        0.352645
Test - acc:         0.850400 loss:        0.456275
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.884680 loss:        0.346238
Test - acc:         0.812500 loss:        0.622265
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.886840 loss:        0.344778
Test - acc:         0.773500 loss:        0.759493
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.884240 loss:        0.349551
Test - acc:         0.835700 loss:        0.526779
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.881720 loss:        0.350268
Test - acc:         0.845000 loss:        0.492113
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.883040 loss:        0.349919
Test - acc:         0.780800 loss:        0.731904
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.885820 loss:        0.343292
Test - acc:         0.790900 loss:        0.694744
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.883880 loss:        0.349513
Test - acc:         0.792800 loss:        0.698193
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.884140 loss:        0.346918
Test - acc:         0.840600 loss:        0.494129
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.885960 loss:        0.344464
Test - acc:         0.837500 loss:        0.516348
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.884040 loss:        0.344102
Test - acc:         0.838400 loss:        0.493948
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.884100 loss:        0.349352
Test - acc:         0.850000 loss:        0.459897
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.883100 loss:        0.349365
Test - acc:         0.738400 loss:        0.813790
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.887440 loss:        0.340053
Test - acc:         0.804300 loss:        0.676320
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.885960 loss:        0.346225
Test - acc:         0.820600 loss:        0.574246
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.889600 loss:        0.335903
Test - acc:         0.800200 loss:        0.613916
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.884020 loss:        0.343107
Test - acc:         0.799900 loss:        0.630343
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.884960 loss:        0.341148
Test - acc:         0.819100 loss:        0.569484
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.886660 loss:        0.343286
Test - acc:         0.828500 loss:        0.541655
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.885600 loss:        0.343478
Test - acc:         0.787600 loss:        0.722516
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.887220 loss:        0.338606
Test - acc:         0.848900 loss:        0.474767
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.886320 loss:        0.340629
Test - acc:         0.839500 loss:        0.512779
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.887780 loss:        0.337618
Test - acc:         0.806400 loss:        0.604788
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.886520 loss:        0.339687
Test - acc:         0.830100 loss:        0.557319
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.888060 loss:        0.339656
Test - acc:         0.824400 loss:        0.564767
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.887520 loss:        0.339127
Test - acc:         0.832900 loss:        0.535212
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.890340 loss:        0.328037
Test - acc:         0.825700 loss:        0.540629
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.889460 loss:        0.324241
Test - acc:         0.842100 loss:        0.471723
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.889680 loss:        0.325921
Test - acc:         0.829400 loss:        0.554655
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.892080 loss:        0.323841
Test - acc:         0.837000 loss:        0.539777
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.891100 loss:        0.325705
Test - acc:         0.855800 loss:        0.479456
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.891260 loss:        0.321367
Test - acc:         0.804900 loss:        0.606780
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.892720 loss:        0.322489
Test - acc:         0.847200 loss:        0.506429
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.893600 loss:        0.320667
Test - acc:         0.832500 loss:        0.511271
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.893040 loss:        0.322623
Test - acc:         0.833000 loss:        0.557153
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.890340 loss:        0.325799
Test - acc:         0.796300 loss:        0.605776
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.892240 loss:        0.322697
Test - acc:         0.832800 loss:        0.491979
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.891380 loss:        0.322743
Test - acc:         0.848600 loss:        0.493074
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.893420 loss:        0.320484
Test - acc:         0.866900 loss:        0.414819
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.892380 loss:        0.324703
Test - acc:         0.819100 loss:        0.576298
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.891460 loss:        0.326290
Test - acc:         0.823000 loss:        0.619794
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.891740 loss:        0.318531
Test - acc:         0.839900 loss:        0.512768
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.892620 loss:        0.320377
Test - acc:         0.809300 loss:        0.600708
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.891020 loss:        0.324707
Test - acc:         0.823600 loss:        0.550806
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.889640 loss:        0.323054
Test - acc:         0.858700 loss:        0.456826
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.892360 loss:        0.320549
Test - acc:         0.712700 loss:        1.134220
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.890360 loss:        0.322835
Test - acc:         0.852200 loss:        0.497187
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.892900 loss:        0.318600
Test - acc:         0.794800 loss:        0.665109
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.893320 loss:        0.321586
Test - acc:         0.827400 loss:        0.540410
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.892020 loss:        0.321157
Test - acc:         0.868700 loss:        0.388941
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.891400 loss:        0.321193
Test - acc:         0.820400 loss:        0.615992
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.893560 loss:        0.324064
Test - acc:         0.855700 loss:        0.438657
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.890980 loss:        0.322140
Test - acc:         0.832600 loss:        0.564130
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.890640 loss:        0.323758
Test - acc:         0.704100 loss:        1.041341
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.890500 loss:        0.323446
Test - acc:         0.819200 loss:        0.539278
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.894200 loss:        0.315318
Test - acc:         0.851600 loss:        0.446847
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.891960 loss:        0.318759
Test - acc:         0.846900 loss:        0.481647
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.890060 loss:        0.325846
Test - acc:         0.821700 loss:        0.564613
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.882480 loss:        0.346877
Test - acc:         0.839500 loss:        0.526552
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.884320 loss:        0.340647
Test - acc:         0.820500 loss:        0.539750
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.884880 loss:        0.343329
Test - acc:         0.763200 loss:        0.766692
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.886600 loss:        0.337480
Test - acc:         0.854400 loss:        0.467344
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.886860 loss:        0.335275
Test - acc:         0.747200 loss:        0.853641
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.886900 loss:        0.337076
Test - acc:         0.842700 loss:        0.475593
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.888800 loss:        0.329758
Test - acc:         0.822600 loss:        0.535571
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.886760 loss:        0.333741
Test - acc:         0.805800 loss:        0.682384
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.888040 loss:        0.335949
Test - acc:         0.825300 loss:        0.571602
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.888040 loss:        0.334468
Test - acc:         0.821000 loss:        0.615580
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.884040 loss:        0.337518
Test - acc:         0.799600 loss:        0.626950
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.886240 loss:        0.333147
Test - acc:         0.754700 loss:        0.900050
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.888340 loss:        0.331354
Test - acc:         0.850100 loss:        0.470993
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.888800 loss:        0.329087
Test - acc:         0.840600 loss:        0.511098
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.886460 loss:        0.334495
Test - acc:         0.824000 loss:        0.532664
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.889020 loss:        0.334542
Test - acc:         0.808300 loss:        0.637321
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.888440 loss:        0.332604
Test - acc:         0.819900 loss:        0.551674
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.886780 loss:        0.331920
Test - acc:         0.843100 loss:        0.490980
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.888580 loss:        0.332819
Test - acc:         0.846300 loss:        0.480562
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.887180 loss:        0.333699
Test - acc:         0.844300 loss:        0.500568
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.885580 loss:        0.336628
Test - acc:         0.856500 loss:        0.436343
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.889720 loss:        0.327423
Test - acc:         0.801300 loss:        0.623168
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.931900 loss:        0.203059
Test - acc:         0.909300 loss:        0.271622
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.944320 loss:        0.164959
Test - acc:         0.916200 loss:        0.259456
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.949360 loss:        0.148571
Test - acc:         0.917400 loss:        0.256101
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.952620 loss:        0.138832
Test - acc:         0.918200 loss:        0.255818
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.955460 loss:        0.132977
Test - acc:         0.917500 loss:        0.258379
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.956980 loss:        0.127790
Test - acc:         0.920000 loss:        0.258570
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.958140 loss:        0.121625
Test - acc:         0.920800 loss:        0.252308
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.959100 loss:        0.118624
Test - acc:         0.918800 loss:        0.267120
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.961660 loss:        0.113565
Test - acc:         0.919400 loss:        0.264579
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.962300 loss:        0.108895
Test - acc:         0.919800 loss:        0.265841
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.927020 loss:        0.216066
Test - acc:         0.903300 loss:        0.307525
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.942980 loss:        0.164439
Test - acc:         0.905900 loss:        0.301732
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.946060 loss:        0.155459
Test - acc:         0.905100 loss:        0.301643
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.948240 loss:        0.150916
Test - acc:         0.910000 loss:        0.298498
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.948500 loss:        0.148760
Test - acc:         0.902500 loss:        0.310764
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.949160 loss:        0.147197
Test - acc:         0.904400 loss:        0.303437
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.950340 loss:        0.142419
Test - acc:         0.903700 loss:        0.304472
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.951080 loss:        0.143461
Test - acc:         0.903200 loss:        0.304981
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.951240 loss:        0.139673
Test - acc:         0.904700 loss:        0.323090
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.951540 loss:        0.140270
Test - acc:         0.905800 loss:        0.295306
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.950620 loss:        0.141500
Test - acc:         0.900100 loss:        0.317848
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.952360 loss:        0.138192
Test - acc:         0.905600 loss:        0.304777
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.951960 loss:        0.138846
Test - acc:         0.901300 loss:        0.316771
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.951640 loss:        0.140179
Test - acc:         0.904500 loss:        0.305357
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.951600 loss:        0.140651
Test - acc:         0.900700 loss:        0.311167
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.950420 loss:        0.142237
Test - acc:         0.893200 loss:        0.354516
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.951060 loss:        0.140408
Test - acc:         0.907600 loss:        0.305682
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.951720 loss:        0.139988
Test - acc:         0.904200 loss:        0.305651
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.951320 loss:        0.138418
Test - acc:         0.891000 loss:        0.351881
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.950880 loss:        0.141659
Test - acc:         0.899800 loss:        0.318892
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.952260 loss:        0.137168
Test - acc:         0.904300 loss:        0.315355
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.950680 loss:        0.143642
Test - acc:         0.894700 loss:        0.362245
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.951660 loss:        0.139582
Test - acc:         0.901300 loss:        0.325457
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.949100 loss:        0.145567
Test - acc:         0.889000 loss:        0.348068
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.949320 loss:        0.145112
Test - acc:         0.896700 loss:        0.335063
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.951120 loss:        0.142097
Test - acc:         0.896800 loss:        0.336050
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.949260 loss:        0.143920
Test - acc:         0.894300 loss:        0.367884
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.950020 loss:        0.143338
Test - acc:         0.903100 loss:        0.327136
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.950960 loss:        0.140527
Test - acc:         0.899200 loss:        0.329815
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.950280 loss:        0.142052
Test - acc:         0.898700 loss:        0.337502
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.950540 loss:        0.143352
Test - acc:         0.900400 loss:        0.324642
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.949240 loss:        0.146987
Test - acc:         0.890100 loss:        0.355101
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.844140 loss:        0.454223
Test - acc:         0.843800 loss:        0.469197
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.880600 loss:        0.347584
Test - acc:         0.854200 loss:        0.437855
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.886540 loss:        0.325607
Test - acc:         0.856900 loss:        0.446905
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.891640 loss:        0.314405
Test - acc:         0.857700 loss:        0.436726
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.894660 loss:        0.304673
Test - acc:         0.848100 loss:        0.456004
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.897620 loss:        0.300108
Test - acc:         0.807500 loss:        0.611989
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.898800 loss:        0.294060
Test - acc:         0.817300 loss:        0.591816
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.900300 loss:        0.290144
Test - acc:         0.859400 loss:        0.425990
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.900280 loss:        0.286293
Test - acc:         0.864800 loss:        0.412206
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.901220 loss:        0.286609
Test - acc:         0.868900 loss:        0.401778
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.902400 loss:        0.279486
Test - acc:         0.860700 loss:        0.424687
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.902820 loss:        0.280428
Test - acc:         0.855800 loss:        0.433404
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.901360 loss:        0.281128
Test - acc:         0.863000 loss:        0.419203
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.902160 loss:        0.280068
Test - acc:         0.864600 loss:        0.416544
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.904820 loss:        0.274949
Test - acc:         0.851400 loss:        0.471789
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.904960 loss:        0.277092
Test - acc:         0.849700 loss:        0.461253
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.904020 loss:        0.274948
Test - acc:         0.869100 loss:        0.394838
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.905420 loss:        0.269056
Test - acc:         0.866500 loss:        0.415239
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.904720 loss:        0.272546
Test - acc:         0.838600 loss:        0.516402
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.907400 loss:        0.267634
Test - acc:         0.846400 loss:        0.471261
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.904900 loss:        0.273637
Test - acc:         0.869600 loss:        0.391568
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.907040 loss:        0.266744
Test - acc:         0.852400 loss:        0.463701
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.906820 loss:        0.269387
Test - acc:         0.867600 loss:        0.404387
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.907680 loss:        0.268448
Test - acc:         0.862600 loss:        0.418863
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.905760 loss:        0.269740
Test - acc:         0.856100 loss:        0.437489
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.905680 loss:        0.270638
Test - acc:         0.874500 loss:        0.390403
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.906720 loss:        0.268038
Test - acc:         0.869200 loss:        0.419925
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.907620 loss:        0.265442
Test - acc:         0.879400 loss:        0.387502
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.906620 loss:        0.268598
Test - acc:         0.852600 loss:        0.465989
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.908320 loss:        0.265062
Test - acc:         0.861300 loss:        0.418450
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.907160 loss:        0.265653
Test - acc:         0.850100 loss:        0.469409
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.904560 loss:        0.269205
Test - acc:         0.846300 loss:        0.498632
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.793860 loss:        0.611857
Test - acc:         0.808000 loss:        0.574811
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.826920 loss:        0.506818
Test - acc:         0.760200 loss:        0.723741
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.834180 loss:        0.484020
Test - acc:         0.817700 loss:        0.553475
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.836220 loss:        0.477529
Test - acc:         0.810900 loss:        0.557735
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.840460 loss:        0.460901
Test - acc:         0.818600 loss:        0.547213
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.844040 loss:        0.460850
Test - acc:         0.829200 loss:        0.505620
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.841280 loss:        0.457953
Test - acc:         0.832400 loss:        0.514487
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.843900 loss:        0.451807
Test - acc:         0.820500 loss:        0.530982
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.845700 loss:        0.450990
Test - acc:         0.802700 loss:        0.599205
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.846460 loss:        0.443961
Test - acc:         0.822800 loss:        0.535125
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.847740 loss:        0.442651
Test - acc:         0.831800 loss:        0.504067
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.846700 loss:        0.445860
Test - acc:         0.820200 loss:        0.546921
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.848100 loss:        0.440194
Test - acc:         0.827900 loss:        0.509720
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.849060 loss:        0.440027
Test - acc:         0.831400 loss:        0.504697
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.849400 loss:        0.435997
Test - acc:         0.836300 loss:        0.490001
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.848340 loss:        0.436979
Test - acc:         0.819900 loss:        0.546224
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.848280 loss:        0.437790
Test - acc:         0.825100 loss:        0.530070
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.851360 loss:        0.433019
Test - acc:         0.825200 loss:        0.526618
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.849540 loss:        0.435071
Test - acc:         0.809100 loss:        0.600662
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.849140 loss:        0.437757
Test - acc:         0.834300 loss:        0.505317
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.849080 loss:        0.430402
Test - acc:         0.828300 loss:        0.508875
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.849100 loss:        0.433623
Test - acc:         0.808900 loss:        0.580039
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.851820 loss:        0.429448
Test - acc:         0.834800 loss:        0.510590
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.851100 loss:        0.429101
Test - acc:         0.829500 loss:        0.507185
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.851320 loss:        0.430905
Test - acc:         0.830600 loss:        0.509980
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.852540 loss:        0.426804
Test - acc:         0.814700 loss:        0.557971
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.868620 loss:        0.381923
Test - acc:         0.859800 loss:        0.419012
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.873880 loss:        0.361104
Test - acc:         0.862200 loss:        0.411738
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.878460 loss:        0.351686
Test - acc:         0.863600 loss:        0.411839
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.879920 loss:        0.347194
Test - acc:         0.863900 loss:        0.410409
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.881220 loss:        0.346387
Test - acc:         0.862400 loss:        0.409061
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.879740 loss:        0.346188
Test - acc:         0.863900 loss:        0.404665
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.620940 loss:        1.122998
Test - acc:         0.681600 loss:        0.924798
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.690680 loss:        0.894961
Test - acc:         0.714700 loss:        0.822239
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.710220 loss:        0.838686
Test - acc:         0.725900 loss:        0.803524
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.721960 loss:        0.809919
Test - acc:         0.737000 loss:        0.767281
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.727200 loss:        0.789008
Test - acc:         0.743700 loss:        0.752797
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.735080 loss:        0.769692
Test - acc:         0.745500 loss:        0.745381
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.741220 loss:        0.753972
Test - acc:         0.749800 loss:        0.730390
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.742880 loss:        0.750405
Test - acc:         0.753400 loss:        0.722148
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.746740 loss:        0.731293
Test - acc:         0.755600 loss:        0.704378
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.749040 loss:        0.725187
Test - acc:         0.761200 loss:        0.696023
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.753180 loss:        0.715469
Test - acc:         0.761100 loss:        0.693225
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.755060 loss:        0.712709
Test - acc:         0.763700 loss:        0.694124
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.756480 loss:        0.705098
Test - acc:         0.765000 loss:        0.685520
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.756700 loss:        0.702674
Test - acc:         0.763700 loss:        0.682652
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.761300 loss:        0.694205
Test - acc:         0.767900 loss:        0.678278
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.762080 loss:        0.691194
Test - acc:         0.768400 loss:        0.679891
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.763940 loss:        0.689734
Test - acc:         0.770600 loss:        0.674718
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.765760 loss:        0.681420
Test - acc:         0.767100 loss:        0.672535
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.765780 loss:        0.681031
Test - acc:         0.769900 loss:        0.664702
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.766120 loss:        0.679434
Test - acc:         0.770100 loss:        0.665252
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.766680 loss:        0.674575
Test - acc:         0.770800 loss:        0.668364
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.770440 loss:        0.668994
Test - acc:         0.768100 loss:        0.666990
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.770100 loss:        0.667774
Test - acc:         0.771600 loss:        0.658057
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.771860 loss:        0.664345
Test - acc:         0.772400 loss:        0.661922
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.773240 loss:        0.658622
Test - acc:         0.775300 loss:        0.657304
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.772700 loss:        0.658487
Test - acc:         0.776700 loss:        0.657217
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.771220 loss:        0.660916
Test - acc:         0.775300 loss:        0.653893
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.773800 loss:        0.656339
Test - acc:         0.775200 loss:        0.650468
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.773900 loss:        0.654454
Test - acc:         0.777300 loss:        0.646557
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.775980 loss:        0.645803
Test - acc:         0.773400 loss:        0.654287
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.775220 loss:        0.646256
Test - acc:         0.779800 loss:        0.644011
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.779220 loss:        0.644641
Test - acc:         0.777600 loss:        0.646653
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.425620 loss:        1.619100
Test - acc:         0.379600 loss:        1.787683
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.523960 loss:        1.328682
Test - acc:         0.522300 loss:        1.338614
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.554580 loss:        1.251587
Test - acc:         0.543100 loss:        1.269495
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.571060 loss:        1.207096
Test - acc:         0.498600 loss:        1.396488
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.583960 loss:        1.178197
Test - acc:         0.539200 loss:        1.284895
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.591480 loss:        1.153632
Test - acc:         0.590200 loss:        1.151847
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.603320 loss:        1.126022
Test - acc:         0.558200 loss:        1.235830
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.606420 loss:        1.112742
Test - acc:         0.551100 loss:        1.263642
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.614180 loss:        1.095468
Test - acc:         0.612300 loss:        1.103624
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.617140 loss:        1.086486
Test - acc:         0.623900 loss:        1.051967
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.620280 loss:        1.075636
Test - acc:         0.631000 loss:        1.036018
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.623280 loss:        1.065388
Test - acc:         0.631300 loss:        1.043324
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.626860 loss:        1.052021
Test - acc:         0.643300 loss:        1.007854
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.631260 loss:        1.042356
Test - acc:         0.596800 loss:        1.144835
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.632960 loss:        1.039102
Test - acc:         0.626200 loss:        1.057293
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.636140 loss:        1.029844
Test - acc:         0.603600 loss:        1.137744
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.640940 loss:        1.021691
Test - acc:         0.627200 loss:        1.046460
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.644940 loss:        1.013608
Test - acc:         0.631000 loss:        1.035777
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.645340 loss:        1.009252
Test - acc:         0.622100 loss:        1.080785
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.646040 loss:        1.004545
Test - acc:         0.619600 loss:        1.064432
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.648400 loss:        0.995535
Test - acc:         0.647400 loss:        1.012044
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.653160 loss:        0.987995
Test - acc:         0.612200 loss:        1.096902
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.656520 loss:        0.983528
Test - acc:         0.647100 loss:        0.993134
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.656200 loss:        0.979718
Test - acc:         0.648000 loss:        1.009299
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.657780 loss:        0.976890
Test - acc:         0.650700 loss:        0.986926
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.658920 loss:        0.973102
Test - acc:         0.663400 loss:        0.965671
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.661060 loss:        0.966453
Test - acc:         0.672100 loss:        0.935789
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.659160 loss:        0.965188
Test - acc:         0.632400 loss:        1.036603
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.662180 loss:        0.963218
Test - acc:         0.645300 loss:        1.006052
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.666760 loss:        0.953485
Test - acc:         0.615200 loss:        1.092830
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.667980 loss:        0.948861
Test - acc:         0.666900 loss:        0.936395
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.667940 loss:        0.944885
Test - acc:         0.661900 loss:        0.958894
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.274800 loss:        2.157099
Test - acc:         0.100000 loss:     2263.851953
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.329320 loss:        1.904272
Test - acc:         0.100000 loss:     1074.734863
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.343260 loss:        1.850062
Test - acc:         0.252800 loss:        2.233450
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.353720 loss:        1.811617
Test - acc:         0.100000 loss:      180.716937
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.360740 loss:        1.787008
Test - acc:         0.100000 loss:     2475.468701
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.362240 loss:        1.768881
Test - acc:         0.100000 loss:     4977.794629
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.367020 loss:        1.757982
Test - acc:         0.100000 loss:      662.357788
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.372040 loss:        1.739719
Test - acc:         0.100000 loss:       41.480715
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.373580 loss:        1.731979
Test - acc:         0.100000 loss:      120.752945
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.375180 loss:        1.718495
Test - acc:         0.100000 loss:     2613.019824
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.377340 loss:        1.709653
Test - acc:         0.100000 loss:        9.264272
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.376540 loss:        1.703818
Test - acc:         0.100000 loss:      105.697975
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.383260 loss:        1.694776
Test - acc:         0.100000 loss:       99.208133
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.383680 loss:        1.685660
Test - acc:         0.100000 loss:        6.229454
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.380400 loss:        1.679083
Test - acc:         0.100000 loss:        5.125977
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.379960 loss:        1.676603
Test - acc:         0.100000 loss:      794.908105
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.384880 loss:        1.669664
Test - acc:         0.100000 loss:      161.893256
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.389940 loss:        1.666087
Test - acc:         0.100000 loss:        4.572580
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.389200 loss:        1.660885
Test - acc:         0.100000 loss:       44.979230
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.389880 loss:        1.658395
Test - acc:         0.100000 loss:       69.637415
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.388000 loss:        1.653894
Test - acc:         0.238900 loss:        2.308723
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.385160 loss:        1.653138
Test - acc:         0.100000 loss:      727.879358
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.399220 loss:        1.651145
Test - acc:         0.214600 loss:        2.521038
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.391520 loss:        1.645715
Test - acc:         0.100000 loss:      134.090729
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.396940 loss:        1.640393
Test - acc:         0.137100 loss:        3.222733
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.389460 loss:        1.636520
Test - acc:         0.100000 loss:      126.727094
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.395180 loss:        1.632789
Test - acc:         0.100000 loss:      110.826166
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.395480 loss:        1.631063
Test - acc:         0.100000 loss:      110.326915
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.398000 loss:        1.629382
Test - acc:         0.100000 loss:      134.613980
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.392980 loss:        1.628211
Test - acc:         0.100000 loss:     1402.879761
Sparsity :          0.9990
Wdecay :        0.000500
