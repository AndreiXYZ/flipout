Running --model vgg19 --prune_criterion magnitude --seed 44 --prune_freq 32 --prune_rate 0.5 --comment=vgg19_crit=magnitude_pf=32_seed=44 --save_model=pre-finetune/vgg19_magnitude_pf32_s44 --logdir=criterion_experiment_no_bias/vgg19
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "magnitude",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_magnitude_pf32_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.121220 loss:        2.493571
Test - acc:         0.162400 loss:        2.247057
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.238540 loss:        1.988574
Test - acc:         0.291200 loss:        1.816521
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.309940 loss:        1.770952
Test - acc:         0.338500 loss:        1.698639
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.379240 loss:        1.597609
Test - acc:         0.373000 loss:        1.624368
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.485060 loss:        1.383755
Test - acc:         0.492900 loss:        1.383657
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.595460 loss:        1.144366
Test - acc:         0.548400 loss:        1.317607
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.663920 loss:        0.966378
Test - acc:         0.628100 loss:        1.085081
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.708800 loss:        0.854189
Test - acc:         0.628000 loss:        1.141725
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.733740 loss:        0.788849
Test - acc:         0.738400 loss:        0.793110
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.751300 loss:        0.746240
Test - acc:         0.596200 loss:        1.291147
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.771620 loss:        0.698481
Test - acc:         0.701500 loss:        0.996400
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.777780 loss:        0.677521
Test - acc:         0.672600 loss:        1.052496
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.785620 loss:        0.657763
Test - acc:         0.678700 loss:        1.063080
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.790880 loss:        0.638954
Test - acc:         0.713700 loss:        0.933064
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.796840 loss:        0.622712
Test - acc:         0.778500 loss:        0.679065
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.803920 loss:        0.610713
Test - acc:         0.757800 loss:        0.748650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.807580 loss:        0.596009
Test - acc:         0.752600 loss:        0.749324
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.809320 loss:        0.586204
Test - acc:         0.688000 loss:        0.984244
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.813800 loss:        0.574660
Test - acc:         0.712500 loss:        0.908072
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.822480 loss:        0.554705
Test - acc:         0.751600 loss:        0.800250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.822680 loss:        0.542792
Test - acc:         0.747900 loss:        0.776715
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.825960 loss:        0.537973
Test - acc:         0.783600 loss:        0.670969
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.827120 loss:        0.530511
Test - acc:         0.725800 loss:        0.980620
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.829620 loss:        0.524179
Test - acc:         0.759400 loss:        0.772408
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.834520 loss:        0.512033
Test - acc:         0.744100 loss:        0.875782
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.835800 loss:        0.505285
Test - acc:         0.759700 loss:        0.725445
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.839820 loss:        0.494703
Test - acc:         0.784400 loss:        0.703626
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.840580 loss:        0.488501
Test - acc:         0.780800 loss:        0.733794
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.842180 loss:        0.488092
Test - acc:         0.826500 loss:        0.554136
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.843940 loss:        0.479515
Test - acc:         0.802400 loss:        0.607688
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.845500 loss:        0.478120
Test - acc:         0.758200 loss:        0.820148
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.844480 loss:        0.475611
Test - acc:         0.762400 loss:        0.770402
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.864480 loss:        0.418535
Test - acc:         0.807800 loss:        0.606292
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.859920 loss:        0.428011
Test - acc:         0.758500 loss:        0.794359
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.864940 loss:        0.412555
Test - acc:         0.764000 loss:        0.786891
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.863160 loss:        0.419130
Test - acc:         0.803000 loss:        0.619444
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.862620 loss:        0.421952
Test - acc:         0.750000 loss:        0.763114
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.863740 loss:        0.419179
Test - acc:         0.760800 loss:        0.836779
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.862420 loss:        0.422701
Test - acc:         0.795700 loss:        0.629670
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.867940 loss:        0.408582
Test - acc:         0.726200 loss:        1.001010
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.865680 loss:        0.410758
Test - acc:         0.807800 loss:        0.595439
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.866200 loss:        0.409278
Test - acc:         0.729600 loss:        0.936559
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.864460 loss:        0.411330
Test - acc:         0.797200 loss:        0.677256
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.863960 loss:        0.414350
Test - acc:         0.807300 loss:        0.590467
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.867640 loss:        0.403602
Test - acc:         0.821200 loss:        0.561418
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.868660 loss:        0.400876
Test - acc:         0.801700 loss:        0.622487
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.867460 loss:        0.401541
Test - acc:         0.799200 loss:        0.676977
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.869400 loss:        0.394057
Test - acc:         0.822200 loss:        0.563041
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.869320 loss:        0.394956
Test - acc:         0.775800 loss:        0.760014
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.868720 loss:        0.397644
Test - acc:         0.837100 loss:        0.504266
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.868400 loss:        0.394986
Test - acc:         0.754700 loss:        0.778054
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.871820 loss:        0.387086
Test - acc:         0.795900 loss:        0.645454
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.872160 loss:        0.388457
Test - acc:         0.813600 loss:        0.584110
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.869400 loss:        0.395858
Test - acc:         0.784600 loss:        0.685290
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.870080 loss:        0.390591
Test - acc:         0.824600 loss:        0.525089
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.875300 loss:        0.381417
Test - acc:         0.789100 loss:        0.687471
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.873360 loss:        0.387835
Test - acc:         0.801900 loss:        0.676456
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.870540 loss:        0.391408
Test - acc:         0.773100 loss:        0.709929
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.871780 loss:        0.383284
Test - acc:         0.796500 loss:        0.613569
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.873640 loss:        0.382358
Test - acc:         0.688100 loss:        1.132952
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.871600 loss:        0.382521
Test - acc:         0.823800 loss:        0.550235
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.872440 loss:        0.384234
Test - acc:         0.799500 loss:        0.661197
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.876760 loss:        0.374978
Test - acc:         0.823600 loss:        0.544164
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.874400 loss:        0.378852
Test - acc:         0.793400 loss:        0.694395
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.886700 loss:        0.337425
Test - acc:         0.850600 loss:        0.460270
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.891080 loss:        0.331362
Test - acc:         0.841600 loss:        0.497099
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.884360 loss:        0.342979
Test - acc:         0.831700 loss:        0.516794
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.886420 loss:        0.340418
Test - acc:         0.801200 loss:        0.640671
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.888740 loss:        0.339642
Test - acc:         0.842900 loss:        0.486713
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.885700 loss:        0.342709
Test - acc:         0.819500 loss:        0.551016
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.886820 loss:        0.342566
Test - acc:         0.837900 loss:        0.480466
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.887440 loss:        0.339728
Test - acc:         0.841200 loss:        0.515004
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.885800 loss:        0.340135
Test - acc:         0.837600 loss:        0.542635
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.887900 loss:        0.338195
Test - acc:         0.819100 loss:        0.559796
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.889680 loss:        0.333908
Test - acc:         0.818900 loss:        0.549575
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.886520 loss:        0.339445
Test - acc:         0.846400 loss:        0.465575
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.887200 loss:        0.337925
Test - acc:         0.850600 loss:        0.474123
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.887360 loss:        0.338234
Test - acc:         0.803300 loss:        0.639199
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.888460 loss:        0.339441
Test - acc:         0.772800 loss:        0.723664
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.886760 loss:        0.339161
Test - acc:         0.831900 loss:        0.543828
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.890720 loss:        0.329897
Test - acc:         0.843000 loss:        0.487359
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.885860 loss:        0.341374
Test - acc:         0.837800 loss:        0.494949
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.889660 loss:        0.330525
Test - acc:         0.841600 loss:        0.501157
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.887780 loss:        0.334067
Test - acc:         0.759200 loss:        0.883666
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.889220 loss:        0.333780
Test - acc:         0.867100 loss:        0.417458
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.886200 loss:        0.336807
Test - acc:         0.826400 loss:        0.548102
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.886140 loss:        0.339620
Test - acc:         0.834500 loss:        0.485503
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.890820 loss:        0.326865
Test - acc:         0.806000 loss:        0.619334
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.890040 loss:        0.329140
Test - acc:         0.803900 loss:        0.615669
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.889740 loss:        0.330339
Test - acc:         0.833500 loss:        0.517733
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.888140 loss:        0.334459
Test - acc:         0.781500 loss:        0.743128
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.887920 loss:        0.335680
Test - acc:         0.840900 loss:        0.523613
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.889280 loss:        0.329714
Test - acc:         0.850100 loss:        0.481256
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.887640 loss:        0.336113
Test - acc:         0.749400 loss:        0.802804
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.889700 loss:        0.330147
Test - acc:         0.797600 loss:        0.652704
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.888580 loss:        0.336750
Test - acc:         0.807800 loss:        0.597881
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.895380 loss:        0.313055
Test - acc:         0.812600 loss:        0.570844
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.895100 loss:        0.313948
Test - acc:         0.816800 loss:        0.559150
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.894360 loss:        0.313876
Test - acc:         0.847700 loss:        0.497610
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.893440 loss:        0.318296
Test - acc:         0.850800 loss:        0.463616
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.893980 loss:        0.316673
Test - acc:         0.846100 loss:        0.492837
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.891320 loss:        0.321321
Test - acc:         0.798100 loss:        0.608506
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.895460 loss:        0.310499
Test - acc:         0.834100 loss:        0.512953
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.893440 loss:        0.316665
Test - acc:         0.852500 loss:        0.460748
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.896000 loss:        0.312667
Test - acc:         0.851600 loss:        0.446020
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.892040 loss:        0.320791
Test - acc:         0.794000 loss:        0.623834
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.894540 loss:        0.313089
Test - acc:         0.817900 loss:        0.585377
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.895960 loss:        0.313592
Test - acc:         0.839200 loss:        0.531059
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.892640 loss:        0.316566
Test - acc:         0.824100 loss:        0.560166
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.894160 loss:        0.316679
Test - acc:         0.765100 loss:        0.757534
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.889460 loss:        0.323674
Test - acc:         0.803000 loss:        0.646694
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.894400 loss:        0.315828
Test - acc:         0.810000 loss:        0.669537
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.894520 loss:        0.314617
Test - acc:         0.835100 loss:        0.577015
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.893000 loss:        0.316391
Test - acc:         0.847700 loss:        0.463377
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.894460 loss:        0.311720
Test - acc:         0.801100 loss:        0.682779
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.892960 loss:        0.317694
Test - acc:         0.815600 loss:        0.612508
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.892040 loss:        0.323750
Test - acc:         0.818300 loss:        0.629248
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.892580 loss:        0.323075
Test - acc:         0.841800 loss:        0.527589
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.893520 loss:        0.315350
Test - acc:         0.844300 loss:        0.508083
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.891900 loss:        0.323572
Test - acc:         0.832400 loss:        0.517188
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.893460 loss:        0.316135
Test - acc:         0.856400 loss:        0.433733
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.891500 loss:        0.321131
Test - acc:         0.822300 loss:        0.561924
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.893500 loss:        0.314046
Test - acc:         0.854600 loss:        0.449453
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.891560 loss:        0.325269
Test - acc:         0.830900 loss:        0.532481
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.893820 loss:        0.314073
Test - acc:         0.739300 loss:        0.913182
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.891140 loss:        0.321917
Test - acc:         0.861200 loss:        0.418419
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.894460 loss:        0.311663
Test - acc:         0.834600 loss:        0.515189
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.894820 loss:        0.313356
Test - acc:         0.841000 loss:        0.485615
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.882900 loss:        0.344904
Test - acc:         0.809300 loss:        0.588412
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.889960 loss:        0.325237
Test - acc:         0.843500 loss:        0.488548
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.888020 loss:        0.329073
Test - acc:         0.843700 loss:        0.472045
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.887200 loss:        0.331549
Test - acc:         0.810400 loss:        0.629307
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.889500 loss:        0.328826
Test - acc:         0.773000 loss:        0.756122
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.890340 loss:        0.324889
Test - acc:         0.823100 loss:        0.556151
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.887980 loss:        0.330321
Test - acc:         0.760700 loss:        0.821091
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.886140 loss:        0.334066
Test - acc:         0.786900 loss:        0.677090
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.890880 loss:        0.325791
Test - acc:         0.819200 loss:        0.571801
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.889340 loss:        0.330410
Test - acc:         0.860700 loss:        0.434359
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.888100 loss:        0.332116
Test - acc:         0.800200 loss:        0.656514
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.886600 loss:        0.333096
Test - acc:         0.820300 loss:        0.533679
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.888720 loss:        0.326637
Test - acc:         0.818600 loss:        0.596542
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.887740 loss:        0.332493
Test - acc:         0.809200 loss:        0.585366
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.890840 loss:        0.322512
Test - acc:         0.837000 loss:        0.533948
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.887400 loss:        0.332644
Test - acc:         0.833400 loss:        0.536901
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.889700 loss:        0.324783
Test - acc:         0.845300 loss:        0.481953
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.887100 loss:        0.330246
Test - acc:         0.803000 loss:        0.647397
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.889240 loss:        0.326070
Test - acc:         0.670300 loss:        1.361333
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.888880 loss:        0.326980
Test - acc:         0.814800 loss:        0.626605
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.888020 loss:        0.328641
Test - acc:         0.852800 loss:        0.456061
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.890660 loss:        0.321000
Test - acc:         0.779900 loss:        0.692611
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.933580 loss:        0.197181
Test - acc:         0.912900 loss:        0.272717
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.946080 loss:        0.159437
Test - acc:         0.915500 loss:        0.267291
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.951540 loss:        0.144178
Test - acc:         0.915200 loss:        0.264321
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.954600 loss:        0.135858
Test - acc:         0.918900 loss:        0.263850
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.956000 loss:        0.128248
Test - acc:         0.917200 loss:        0.266300
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.959380 loss:        0.120750
Test - acc:         0.917800 loss:        0.269182
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.959980 loss:        0.117650
Test - acc:         0.914000 loss:        0.275248
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.961280 loss:        0.112675
Test - acc:         0.916200 loss:        0.272046
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.962460 loss:        0.106477
Test - acc:         0.916300 loss:        0.280682
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.965040 loss:        0.101963
Test - acc:         0.917300 loss:        0.280164
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.933320 loss:        0.191475
Test - acc:         0.904300 loss:        0.312757
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.945720 loss:        0.156999
Test - acc:         0.903500 loss:        0.309783
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.949820 loss:        0.147139
Test - acc:         0.899600 loss:        0.322518
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.950540 loss:        0.141797
Test - acc:         0.908100 loss:        0.302885
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.953220 loss:        0.134019
Test - acc:         0.903800 loss:        0.328123
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.951360 loss:        0.138910
Test - acc:         0.905800 loss:        0.303448
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.953840 loss:        0.133918
Test - acc:         0.902300 loss:        0.327369
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.953400 loss:        0.134342
Test - acc:         0.900400 loss:        0.334624
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.954900 loss:        0.130781
Test - acc:         0.904100 loss:        0.320439
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.955500 loss:        0.127392
Test - acc:         0.906000 loss:        0.316344
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.955700 loss:        0.129139
Test - acc:         0.906400 loss:        0.311007
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.955820 loss:        0.129061
Test - acc:         0.899500 loss:        0.332989
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.956360 loss:        0.127137
Test - acc:         0.904900 loss:        0.325110
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.954660 loss:        0.128868
Test - acc:         0.901100 loss:        0.335101
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.954220 loss:        0.132367
Test - acc:         0.906400 loss:        0.315823
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.954260 loss:        0.130663
Test - acc:         0.904300 loss:        0.323681
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.955540 loss:        0.127439
Test - acc:         0.905400 loss:        0.324118
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.954220 loss:        0.132799
Test - acc:         0.899500 loss:        0.346607
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.954800 loss:        0.131447
Test - acc:         0.898000 loss:        0.345214
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.955280 loss:        0.128684
Test - acc:         0.899500 loss:        0.335082
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.951480 loss:        0.135704
Test - acc:         0.899600 loss:        0.353860
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.954940 loss:        0.130446
Test - acc:         0.902200 loss:        0.333401
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.953440 loss:        0.135501
Test - acc:         0.901600 loss:        0.322478
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.952680 loss:        0.134278
Test - acc:         0.902400 loss:        0.324005
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.953040 loss:        0.132990
Test - acc:         0.902700 loss:        0.326198
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.954400 loss:        0.132234
Test - acc:         0.895500 loss:        0.354677
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.952220 loss:        0.137093
Test - acc:         0.895000 loss:        0.362232
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.952780 loss:        0.135343
Test - acc:         0.894900 loss:        0.354705
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.953720 loss:        0.133122
Test - acc:         0.894900 loss:        0.355055
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.951080 loss:        0.137603
Test - acc:         0.904300 loss:        0.322565
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.952440 loss:        0.133816
Test - acc:         0.899800 loss:        0.326169
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.951360 loss:        0.139432
Test - acc:         0.898600 loss:        0.341209
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.863440 loss:        0.397784
Test - acc:         0.841600 loss:        0.480152
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.892460 loss:        0.309937
Test - acc:         0.870300 loss:        0.407954
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.900320 loss:        0.285304
Test - acc:         0.864900 loss:        0.425451
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.904480 loss:        0.274571
Test - acc:         0.859600 loss:        0.428447
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.905160 loss:        0.272970
Test - acc:         0.864600 loss:        0.431083
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.907440 loss:        0.262256
Test - acc:         0.861800 loss:        0.439768
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.909740 loss:        0.259693
Test - acc:         0.854900 loss:        0.458579
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.908220 loss:        0.258402
Test - acc:         0.868800 loss:        0.402145
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.912660 loss:        0.252913
Test - acc:         0.871200 loss:        0.399914
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.912320 loss:        0.250896
Test - acc:         0.868400 loss:        0.409991
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.912160 loss:        0.251101
Test - acc:         0.864900 loss:        0.432637
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.910840 loss:        0.250824
Test - acc:         0.840600 loss:        0.505491
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.913640 loss:        0.243899
Test - acc:         0.870700 loss:        0.398450
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.914260 loss:        0.247166
Test - acc:         0.867700 loss:        0.414237
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.913580 loss:        0.245850
Test - acc:         0.876600 loss:        0.391722
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.914420 loss:        0.244446
Test - acc:         0.874100 loss:        0.389332
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.915000 loss:        0.245686
Test - acc:         0.874200 loss:        0.391042
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.914760 loss:        0.243016
Test - acc:         0.861000 loss:        0.431595
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.915560 loss:        0.243119
Test - acc:         0.877700 loss:        0.379066
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.914320 loss:        0.245077
Test - acc:         0.874100 loss:        0.383515
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.916040 loss:        0.243836
Test - acc:         0.873200 loss:        0.388847
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.914420 loss:        0.241160
Test - acc:         0.872100 loss:        0.390044
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.915040 loss:        0.243184
Test - acc:         0.872700 loss:        0.406295
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.915020 loss:        0.240551
Test - acc:         0.858400 loss:        0.456799
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.915840 loss:        0.240059
Test - acc:         0.871000 loss:        0.403198
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.914940 loss:        0.241681
Test - acc:         0.857300 loss:        0.454392
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.917120 loss:        0.235407
Test - acc:         0.867900 loss:        0.398313
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.917280 loss:        0.237551
Test - acc:         0.876400 loss:        0.385858
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.916660 loss:        0.234413
Test - acc:         0.862500 loss:        0.433364
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.917300 loss:        0.238040
Test - acc:         0.851100 loss:        0.488551
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.916200 loss:        0.239979
Test - acc:         0.879400 loss:        0.373103
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.916460 loss:        0.236844
Test - acc:         0.871600 loss:        0.405725
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.806640 loss:        0.569957
Test - acc:         0.819900 loss:        0.543449
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.837760 loss:        0.465993
Test - acc:         0.819300 loss:        0.552990
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.844480 loss:        0.451268
Test - acc:         0.827900 loss:        0.512288
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.849200 loss:        0.434480
Test - acc:         0.836500 loss:        0.495295
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.851820 loss:        0.427450
Test - acc:         0.830900 loss:        0.523971
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.855640 loss:        0.422326
Test - acc:         0.841000 loss:        0.465241
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.857060 loss:        0.411951
Test - acc:         0.833300 loss:        0.505408
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.856820 loss:        0.413936
Test - acc:         0.830400 loss:        0.510303
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.857820 loss:        0.410530
Test - acc:         0.823500 loss:        0.528507
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.857740 loss:        0.407490
Test - acc:         0.827500 loss:        0.504826
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.858240 loss:        0.405455
Test - acc:         0.836300 loss:        0.490625
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.860660 loss:        0.401114
Test - acc:         0.832800 loss:        0.502479
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.860760 loss:        0.398848
Test - acc:         0.842000 loss:        0.483960
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.859620 loss:        0.398998
Test - acc:         0.842000 loss:        0.474908
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.861420 loss:        0.396514
Test - acc:         0.833800 loss:        0.492352
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.862100 loss:        0.395834
Test - acc:         0.838800 loss:        0.480133
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.862920 loss:        0.397457
Test - acc:         0.829700 loss:        0.505000
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.862980 loss:        0.394098
Test - acc:         0.831500 loss:        0.493717
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.862240 loss:        0.389875
Test - acc:         0.826100 loss:        0.527740
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.863040 loss:        0.394941
Test - acc:         0.845100 loss:        0.461593
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.862440 loss:        0.393503
Test - acc:         0.842600 loss:        0.464967
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.864660 loss:        0.390035
Test - acc:         0.838500 loss:        0.476896
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.862800 loss:        0.392872
Test - acc:         0.828400 loss:        0.505036
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.865160 loss:        0.388890
Test - acc:         0.820000 loss:        0.536962
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.865460 loss:        0.390202
Test - acc:         0.833500 loss:        0.503112
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.866620 loss:        0.386666
Test - acc:         0.832500 loss:        0.500500
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.882280 loss:        0.338016
Test - acc:         0.863300 loss:        0.401202
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.889120 loss:        0.318722
Test - acc:         0.866000 loss:        0.399598
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.891900 loss:        0.309048
Test - acc:         0.866300 loss:        0.398116
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.894520 loss:        0.305629
Test - acc:         0.866500 loss:        0.395034
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.895320 loss:        0.300625
Test - acc:         0.866400 loss:        0.397309
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.895080 loss:        0.302076
Test - acc:         0.868300 loss:        0.395801
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.601160 loss:        1.183725
Test - acc:         0.676200 loss:        0.936866
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.694420 loss:        0.877191
Test - acc:         0.720100 loss:        0.808954
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.722820 loss:        0.799572
Test - acc:         0.738300 loss:        0.767572
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.735980 loss:        0.763630
Test - acc:         0.745900 loss:        0.742196
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.744940 loss:        0.735521
Test - acc:         0.738000 loss:        0.761645
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.750240 loss:        0.718268
Test - acc:         0.718300 loss:        0.835323
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.756980 loss:        0.703514
Test - acc:         0.761400 loss:        0.693537
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.759140 loss:        0.692931
Test - acc:         0.759500 loss:        0.702437
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.764860 loss:        0.679181
Test - acc:         0.738700 loss:        0.768905
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.767680 loss:        0.671767
Test - acc:         0.755900 loss:        0.704365
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.770580 loss:        0.660137
Test - acc:         0.772500 loss:        0.661606
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.772020 loss:        0.655437
Test - acc:         0.750000 loss:        0.728070
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.774340 loss:        0.648230
Test - acc:         0.769400 loss:        0.672070
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.777760 loss:        0.640774
Test - acc:         0.758600 loss:        0.703849
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.776300 loss:        0.642516
Test - acc:         0.774100 loss:        0.654611
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.780260 loss:        0.632367
Test - acc:         0.775800 loss:        0.644711
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.782780 loss:        0.626721
Test - acc:         0.787200 loss:        0.628666
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.783000 loss:        0.622465
Test - acc:         0.781300 loss:        0.639251
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.782840 loss:        0.624112
Test - acc:         0.775100 loss:        0.662402
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.782900 loss:        0.622792
Test - acc:         0.784200 loss:        0.634265
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.785180 loss:        0.616844
Test - acc:         0.770500 loss:        0.666152
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.789220 loss:        0.603258
Test - acc:         0.786100 loss:        0.622002
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.787100 loss:        0.609624
Test - acc:         0.773900 loss:        0.646841
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.788800 loss:        0.611063
Test - acc:         0.787800 loss:        0.624471
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.791140 loss:        0.604842
Test - acc:         0.787900 loss:        0.617064
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.787800 loss:        0.604238
Test - acc:         0.768600 loss:        0.680440
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.791060 loss:        0.601157
Test - acc:         0.788600 loss:        0.618070
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.793620 loss:        0.593686
Test - acc:         0.789300 loss:        0.621489
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.793240 loss:        0.595143
Test - acc:         0.788400 loss:        0.620749
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.792880 loss:        0.594122
Test - acc:         0.788200 loss:        0.614309
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.793860 loss:        0.589066
Test - acc:         0.793100 loss:        0.610562
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.797220 loss:        0.586865
Test - acc:         0.790700 loss:        0.608764
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.345360 loss:        2.010197
Test - acc:         0.288000 loss:        2.218011
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.411360 loss:        1.718140
Test - acc:         0.100000 loss:       11.591981
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.434340 loss:        1.626820
Test - acc:         0.100000 loss:       17.423660
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.448020 loss:        1.577504
Test - acc:         0.300500 loss:        2.363659
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.462760 loss:        1.530712
Test - acc:         0.100000 loss:       16.278953
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.469780 loss:        1.505271
Test - acc:         0.103600 loss:       19.060287
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.473620 loss:        1.485553
Test - acc:         0.131000 loss:       11.702630
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.480180 loss:        1.461266
Test - acc:         0.101400 loss:        8.991942
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.487080 loss:        1.436625
Test - acc:         0.241900 loss:        2.670917
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.489740 loss:        1.426378
Test - acc:         0.376100 loss:        1.873297
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.492620 loss:        1.413045
Test - acc:         0.100000 loss:       11.552436
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.497060 loss:        1.400032
Test - acc:         0.100000 loss:       18.277841
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.499960 loss:        1.383779
Test - acc:         0.176700 loss:        4.660115
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.505920 loss:        1.376828
Test - acc:         0.100000 loss:       16.385812
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.510660 loss:        1.359351
Test - acc:         0.100600 loss:       11.396938
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.511880 loss:        1.354398
Test - acc:         0.216800 loss:        3.059127
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.516980 loss:        1.343425
Test - acc:         0.331700 loss:        2.282138
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.514940 loss:        1.335226
Test - acc:         0.100100 loss:       51.035241
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.543960 loss:        1.324616
Test - acc:         0.105200 loss:       12.159101
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.518440 loss:        1.319508
Test - acc:         0.100400 loss:       12.132732
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.522720 loss:        1.315016
Test - acc:         0.101900 loss:        7.803353
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.556600 loss:        1.303353
Test - acc:         0.100000 loss:       18.329996
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.544500 loss:        1.302499
Test - acc:         0.160600 loss:        4.990553
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.532680 loss:        1.292888
Test - acc:         0.100000 loss:       19.637682
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.562680 loss:        1.288518
Test - acc:         0.100000 loss:       15.383172
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.531120 loss:        1.284004
Test - acc:         0.348100 loss:        2.231327
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.546880 loss:        1.279733
Test - acc:         0.100000 loss:       20.989529
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.542300 loss:        1.276577
Test - acc:         0.100000 loss:       17.587131
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.565820 loss:        1.265661
Test - acc:         0.559400 loss:        1.292893
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.545280 loss:        1.264030
Test - acc:         0.102800 loss:        9.393392
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.563180 loss:        1.258221
Test - acc:         0.291000 loss:        2.905254
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.563840 loss:        1.259020
Test - acc:         0.205100 loss:        3.945657
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.237740 loss:        2.376267
Test - acc:         0.099700 loss:        2.875553
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.301600 loss:        2.120856
Test - acc:         0.100000 loss:      113.450432
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.306580 loss:        1.995611
Test - acc:         0.094200 loss:        2.554033
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.308680 loss:        1.925234
Test - acc:         0.100000 loss:        2.838976
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.309580 loss:        1.882465
Test - acc:         0.096800 loss:        2.503105
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.314520 loss:        1.855337
Test - acc:         0.100000 loss:        3.808559
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.324080 loss:        1.841602
Test - acc:         0.100000 loss:        3.728092
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.319240 loss:        1.821168
Test - acc:         0.100000 loss:      843.906726
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.328320 loss:        1.817644
Test - acc:         0.100000 loss:        2.746984
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.334180 loss:        1.807765
Test - acc:         0.101500 loss:        2.541439
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.326880 loss:        1.801315
Test - acc:         0.100000 loss:      878.931152
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.337360 loss:        1.797217
Test - acc:         0.100000 loss:      597.465845
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.339540 loss:        1.785551
Test - acc:         0.099300 loss:        2.584359
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.350580 loss:        1.785176
Test - acc:         0.100100 loss:        2.840351
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.335280 loss:        1.781116
Test - acc:         0.109500 loss:        2.742289
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.356300 loss:        1.776255
Test - acc:         0.100000 loss:      239.403915
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.346940 loss:        1.771960
Test - acc:         0.100000 loss:      182.836853
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.343900 loss:        1.761884
Test - acc:         0.100000 loss:      502.143164
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.358140 loss:        1.759576
Test - acc:         0.121500 loss:        2.583100
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.337680 loss:        1.758270
Test - acc:         0.100500 loss:        3.479132
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.334920 loss:        1.754668
Test - acc:         0.100000 loss:      668.531702
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.342760 loss:        1.748914
Test - acc:         0.100000 loss:        7.569454
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.356000 loss:        1.747685
Test - acc:         0.100000 loss:      310.938489
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.344260 loss:        1.746681
Test - acc:         0.100000 loss:      211.125897
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.354120 loss:        1.742373
Test - acc:         0.177300 loss:        2.856025
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.348180 loss:        1.741724
Test - acc:         0.100000 loss:        9.358170
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.343040 loss:        1.742666
Test - acc:         0.102100 loss:        3.311282
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.342240 loss:        1.735090
Test - acc:         0.102800 loss:        3.184667
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.358440 loss:        1.737493
Test - acc:         0.100000 loss:     1314.878979
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.360060 loss:        1.728392
Test - acc:         0.112500 loss:        2.620779
Sparsity :          0.9990
Wdecay :        0.000500
