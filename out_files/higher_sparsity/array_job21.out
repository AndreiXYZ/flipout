Running --model vgg19 --prune_criterion global_magnitude --seed 43 --prune_freq 32 --prune_rate 0.5 --comment=vgg19_crit=global_magnitude_pf=32_seed=43 --save_model=pre-finetune/vgg19_global_magnitude_pf32_s43 --logdir=criterion_experiment_no_bias/vgg19
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "global_magnitude",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_global_magnitude_pf32_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.102040 loss:        2.593170
Test - acc:         0.101800 loss:        2.305669
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.102980 loss:        2.302027
Test - acc:         0.108700 loss:        2.299553
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.107760 loss:        2.295210
Test - acc:         0.102100 loss:        2.288999
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.170220 loss:        2.125674
Test - acc:         0.234300 loss:        1.914818
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.283940 loss:        1.804644
Test - acc:         0.265700 loss:        1.923784
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.401960 loss:        1.553583
Test - acc:         0.376900 loss:        1.801617
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.539940 loss:        1.245976
Test - acc:         0.572700 loss:        1.260381
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.632400 loss:        1.036366
Test - acc:         0.586200 loss:        1.257524
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.681460 loss:        0.918227
Test - acc:         0.610400 loss:        1.206541
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.713460 loss:        0.850217
Test - acc:         0.668700 loss:        0.947266
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.739980 loss:        0.792240
Test - acc:         0.681800 loss:        1.044388
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.758800 loss:        0.739895
Test - acc:         0.697800 loss:        0.921571
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.770220 loss:        0.715436
Test - acc:         0.645300 loss:        1.166721
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.777960 loss:        0.693221
Test - acc:         0.740500 loss:        0.807132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.784440 loss:        0.673860
Test - acc:         0.720600 loss:        0.904886
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.789860 loss:        0.663452
Test - acc:         0.728000 loss:        0.842933
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.799320 loss:        0.628781
Test - acc:         0.682700 loss:        0.994196
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.805000 loss:        0.609107
Test - acc:         0.763000 loss:        0.748730
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.807880 loss:        0.599333
Test - acc:         0.706800 loss:        0.948992
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.812460 loss:        0.583509
Test - acc:         0.789700 loss:        0.668061
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.817240 loss:        0.569957
Test - acc:         0.753900 loss:        0.833114
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.819180 loss:        0.563694
Test - acc:         0.770800 loss:        0.724838
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.820740 loss:        0.554781
Test - acc:         0.700100 loss:        0.967560
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.825820 loss:        0.543488
Test - acc:         0.723000 loss:        0.915873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.830920 loss:        0.527130
Test - acc:         0.757800 loss:        0.837092
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.830200 loss:        0.531166
Test - acc:         0.776600 loss:        0.708030
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.832860 loss:        0.517132
Test - acc:         0.768400 loss:        0.819225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.831800 loss:        0.523846
Test - acc:         0.802900 loss:        0.601694
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.838100 loss:        0.499992
Test - acc:         0.759300 loss:        0.806052
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.838440 loss:        0.504548
Test - acc:         0.713300 loss:        0.926746
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.839900 loss:        0.498613
Test - acc:         0.756200 loss:        0.786542
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.839700 loss:        0.498225
Test - acc:         0.805800 loss:        0.605070
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.844220 loss:        0.479851
Test - acc:         0.747000 loss:        0.816520
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.848160 loss:        0.475932
Test - acc:         0.799400 loss:        0.665387
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.845020 loss:        0.478053
Test - acc:         0.773600 loss:        0.743800
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.845680 loss:        0.475752
Test - acc:         0.788800 loss:        0.640144
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.845780 loss:        0.476055
Test - acc:         0.670000 loss:        1.227205
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.848540 loss:        0.471464
Test - acc:         0.690400 loss:        1.045429
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.845960 loss:        0.475024
Test - acc:         0.790600 loss:        0.648501
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.847880 loss:        0.462254
Test - acc:         0.772800 loss:        0.702246
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.848000 loss:        0.467910
Test - acc:         0.713700 loss:        1.006632
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.853000 loss:        0.458314
Test - acc:         0.813000 loss:        0.557321
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.853300 loss:        0.454527
Test - acc:         0.766100 loss:        0.769870
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.856300 loss:        0.445550
Test - acc:         0.784900 loss:        0.684586
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.853660 loss:        0.456266
Test - acc:         0.785300 loss:        0.689104
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.854860 loss:        0.451268
Test - acc:         0.778300 loss:        0.737470
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.855580 loss:        0.444567
Test - acc:         0.760100 loss:        0.815471
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.854860 loss:        0.446377
Test - acc:         0.786600 loss:        0.649624
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.855240 loss:        0.446030
Test - acc:         0.644100 loss:        1.233038
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.855160 loss:        0.449827
Test - acc:         0.787600 loss:        0.671821
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.857520 loss:        0.437071
Test - acc:         0.781700 loss:        0.701069
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.856760 loss:        0.441773
Test - acc:         0.819600 loss:        0.592383
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.859420 loss:        0.439534
Test - acc:         0.778400 loss:        0.835084
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.859660 loss:        0.435370
Test - acc:         0.805000 loss:        0.618677
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.858700 loss:        0.437297
Test - acc:         0.784900 loss:        0.706795
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.857940 loss:        0.435896
Test - acc:         0.795400 loss:        0.658917
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.860120 loss:        0.431726
Test - acc:         0.813500 loss:        0.588500
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.861380 loss:        0.427242
Test - acc:         0.793500 loss:        0.700507
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.860580 loss:        0.427241
Test - acc:         0.824300 loss:        0.556870
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.861400 loss:        0.426412
Test - acc:         0.775700 loss:        0.730236
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.864060 loss:        0.417434
Test - acc:         0.797200 loss:        0.692413
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.862620 loss:        0.422497
Test - acc:         0.807600 loss:        0.645463
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.863640 loss:        0.422387
Test - acc:         0.846000 loss:        0.477983
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.862300 loss:        0.424175
Test - acc:         0.717200 loss:        1.010971
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.864260 loss:        0.415895
Test - acc:         0.722700 loss:        0.979467
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.866520 loss:        0.411808
Test - acc:         0.790600 loss:        0.674060
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.867320 loss:        0.405137
Test - acc:         0.758600 loss:        0.779966
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.863620 loss:        0.412271
Test - acc:         0.825200 loss:        0.534699
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.866560 loss:        0.408371
Test - acc:         0.740000 loss:        0.897831
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.864520 loss:        0.413858
Test - acc:         0.805500 loss:        0.641159
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.866600 loss:        0.408848
Test - acc:         0.787400 loss:        0.659154
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.867320 loss:        0.406409
Test - acc:         0.842600 loss:        0.488698
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.868140 loss:        0.403929
Test - acc:         0.743500 loss:        0.848399
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.869920 loss:        0.396704
Test - acc:         0.759500 loss:        0.837808
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.867320 loss:        0.405169
Test - acc:         0.816800 loss:        0.591740
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.870460 loss:        0.395258
Test - acc:         0.774300 loss:        0.763240
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.867860 loss:        0.403641
Test - acc:         0.825400 loss:        0.534450
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.868700 loss:        0.400235
Test - acc:         0.777900 loss:        0.817892
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.867740 loss:        0.402531
Test - acc:         0.816300 loss:        0.574587
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.868480 loss:        0.398545
Test - acc:         0.824500 loss:        0.552499
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.864220 loss:        0.409835
Test - acc:         0.803700 loss:        0.621245
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.870360 loss:        0.400044
Test - acc:         0.791400 loss:        0.687755
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.871800 loss:        0.390485
Test - acc:         0.791400 loss:        0.743418
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.871560 loss:        0.390992
Test - acc:         0.779800 loss:        0.709696
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.869820 loss:        0.394656
Test - acc:         0.795300 loss:        0.643543
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.870820 loss:        0.388783
Test - acc:         0.770900 loss:        0.764307
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.872320 loss:        0.389223
Test - acc:         0.805400 loss:        0.627628
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.871140 loss:        0.391704
Test - acc:         0.770600 loss:        0.733198
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.872040 loss:        0.385991
Test - acc:         0.838600 loss:        0.491637
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.873360 loss:        0.387549
Test - acc:         0.817100 loss:        0.560186
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.870380 loss:        0.390869
Test - acc:         0.739800 loss:        0.876878
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.873080 loss:        0.385574
Test - acc:         0.810000 loss:        0.594512
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.876060 loss:        0.377873
Test - acc:         0.809000 loss:        0.590588
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.871900 loss:        0.386418
Test - acc:         0.779100 loss:        0.676997
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.874640 loss:        0.379136
Test - acc:         0.826100 loss:        0.540450
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.872200 loss:        0.384861
Test - acc:         0.761700 loss:        0.849541
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.878000 loss:        0.367304
Test - acc:         0.838900 loss:        0.491202
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.878940 loss:        0.364247
Test - acc:         0.790600 loss:        0.651871
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.876120 loss:        0.368442
Test - acc:         0.852700 loss:        0.460903
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.876380 loss:        0.366999
Test - acc:         0.847200 loss:        0.461996
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.879620 loss:        0.363211
Test - acc:         0.825700 loss:        0.543831
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.879560 loss:        0.359065
Test - acc:         0.816400 loss:        0.560128
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.879780 loss:        0.363686
Test - acc:         0.810900 loss:        0.605546
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.878620 loss:        0.362302
Test - acc:         0.803800 loss:        0.597827
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.877600 loss:        0.364423
Test - acc:         0.805100 loss:        0.638923
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.878580 loss:        0.366277
Test - acc:         0.802700 loss:        0.587137
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.877860 loss:        0.363502
Test - acc:         0.831800 loss:        0.502712
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.877420 loss:        0.366694
Test - acc:         0.839300 loss:        0.494483
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.877920 loss:        0.363844
Test - acc:         0.801600 loss:        0.645704
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.881060 loss:        0.356568
Test - acc:         0.798600 loss:        0.641218
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.878900 loss:        0.361415
Test - acc:         0.804500 loss:        0.631991
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.878860 loss:        0.364448
Test - acc:         0.801100 loss:        0.653649
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.880880 loss:        0.361314
Test - acc:         0.769500 loss:        0.793527
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.878720 loss:        0.361633
Test - acc:         0.791600 loss:        0.661988
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.879940 loss:        0.358249
Test - acc:         0.832400 loss:        0.505297
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.880620 loss:        0.359796
Test - acc:         0.834700 loss:        0.506940
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.879140 loss:        0.361225
Test - acc:         0.760700 loss:        0.849915
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.879460 loss:        0.361238
Test - acc:         0.839900 loss:        0.488654
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.882600 loss:        0.353676
Test - acc:         0.841800 loss:        0.484904
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.879740 loss:        0.360478
Test - acc:         0.817000 loss:        0.577483
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.880700 loss:        0.357692
Test - acc:         0.793200 loss:        0.678537
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.879420 loss:        0.361285
Test - acc:         0.869100 loss:        0.394355
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.883140 loss:        0.354026
Test - acc:         0.831700 loss:        0.512073
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.881220 loss:        0.360358
Test - acc:         0.787600 loss:        0.691470
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.881140 loss:        0.356226
Test - acc:         0.812500 loss:        0.600604
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.881300 loss:        0.352261
Test - acc:         0.775100 loss:        0.742338
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.881260 loss:        0.356772
Test - acc:         0.750200 loss:        0.897338
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.880580 loss:        0.356404
Test - acc:         0.824500 loss:        0.547876
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.891440 loss:        0.324174
Test - acc:         0.772400 loss:        0.704963
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.892000 loss:        0.320510
Test - acc:         0.838700 loss:        0.504024
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.891840 loss:        0.322978
Test - acc:         0.823300 loss:        0.571961
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.889220 loss:        0.330963
Test - acc:         0.821200 loss:        0.561593
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.888780 loss:        0.334272
Test - acc:         0.854500 loss:        0.469864
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.888660 loss:        0.331259
Test - acc:         0.837100 loss:        0.521236
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.890700 loss:        0.324265
Test - acc:         0.821600 loss:        0.579746
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.889780 loss:        0.331772
Test - acc:         0.848200 loss:        0.466355
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.887540 loss:        0.334146
Test - acc:         0.837300 loss:        0.532949
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.889060 loss:        0.333141
Test - acc:         0.845300 loss:        0.470170
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.889080 loss:        0.329059
Test - acc:         0.730900 loss:        1.171321
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.888780 loss:        0.332882
Test - acc:         0.729700 loss:        1.044310
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.890100 loss:        0.328670
Test - acc:         0.821300 loss:        0.620304
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.888540 loss:        0.330967
Test - acc:         0.837700 loss:        0.498538
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.891220 loss:        0.326124
Test - acc:         0.808500 loss:        0.571652
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.889180 loss:        0.332733
Test - acc:         0.803500 loss:        0.641963
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.888920 loss:        0.330839
Test - acc:         0.831200 loss:        0.523684
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.887920 loss:        0.331277
Test - acc:         0.797800 loss:        0.629146
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.890120 loss:        0.331848
Test - acc:         0.822200 loss:        0.541402
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.885420 loss:        0.338929
Test - acc:         0.825600 loss:        0.541693
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.889240 loss:        0.329251
Test - acc:         0.795700 loss:        0.632151
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.889220 loss:        0.329316
Test - acc:         0.788000 loss:        0.740836
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.941820 loss:        0.172578
Test - acc:         0.921300 loss:        0.242103
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.958460 loss:        0.125742
Test - acc:         0.925300 loss:        0.233946
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.965140 loss:        0.104988
Test - acc:         0.924800 loss:        0.231899
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.968480 loss:        0.095017
Test - acc:         0.928100 loss:        0.230134
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.971460 loss:        0.086036
Test - acc:         0.929800 loss:        0.231706
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.975200 loss:        0.075742
Test - acc:         0.928900 loss:        0.241491
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.977660 loss:        0.066819
Test - acc:         0.926900 loss:        0.244979
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.980440 loss:        0.059452
Test - acc:         0.927600 loss:        0.250284
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.978840 loss:        0.061756
Test - acc:         0.927400 loss:        0.253136
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.057274
Test - acc:         0.924400 loss:        0.263959
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.982620 loss:        0.052025
Test - acc:         0.925400 loss:        0.259775
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.985100 loss:        0.045660
Test - acc:         0.927200 loss:        0.264135
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.984880 loss:        0.044967
Test - acc:         0.928500 loss:        0.259326
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.985500 loss:        0.042595
Test - acc:         0.928200 loss:        0.258959
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.986540 loss:        0.040053
Test - acc:         0.928600 loss:        0.267632
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.985700 loss:        0.043135
Test - acc:         0.925000 loss:        0.276990
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.988040 loss:        0.037021
Test - acc:         0.923400 loss:        0.283281
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.987580 loss:        0.038297
Test - acc:         0.929000 loss:        0.267353
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.987700 loss:        0.037855
Test - acc:         0.926900 loss:        0.274986
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.987320 loss:        0.036846
Test - acc:         0.925800 loss:        0.277000
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.987400 loss:        0.038039
Test - acc:         0.926000 loss:        0.280898
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.988260 loss:        0.035515
Test - acc:         0.922100 loss:        0.291559
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.987440 loss:        0.038111
Test - acc:         0.926600 loss:        0.283434
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.986760 loss:        0.038497
Test - acc:         0.923500 loss:        0.285739
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.986980 loss:        0.038171
Test - acc:         0.921800 loss:        0.298188
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.986420 loss:        0.040417
Test - acc:         0.921500 loss:        0.286894
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.987580 loss:        0.037721
Test - acc:         0.923400 loss:        0.292457
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.986340 loss:        0.040279
Test - acc:         0.923900 loss:        0.291672
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.986540 loss:        0.039392
Test - acc:         0.922600 loss:        0.283994
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.985380 loss:        0.043706
Test - acc:         0.920600 loss:        0.292955
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.985220 loss:        0.044648
Test - acc:         0.921200 loss:        0.310401
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.985140 loss:        0.042884
Test - acc:         0.918700 loss:        0.302542
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.985900 loss:        0.041895
Test - acc:         0.918300 loss:        0.298926
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.984240 loss:        0.046650
Test - acc:         0.912700 loss:        0.321514
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.985600 loss:        0.041905
Test - acc:         0.917300 loss:        0.301294
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.982880 loss:        0.049330
Test - acc:         0.917000 loss:        0.325426
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.983060 loss:        0.049580
Test - acc:         0.921700 loss:        0.287698
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.983520 loss:        0.048023
Test - acc:         0.914600 loss:        0.310344
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.983540 loss:        0.048284
Test - acc:         0.918400 loss:        0.317734
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.983180 loss:        0.049130
Test - acc:         0.922600 loss:        0.291219
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.983200 loss:        0.049980
Test - acc:         0.921500 loss:        0.290261
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.983660 loss:        0.048358
Test - acc:         0.918300 loss:        0.286708
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.971640 loss:        0.082370
Test - acc:         0.921700 loss:        0.293655
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.978140 loss:        0.065113
Test - acc:         0.914700 loss:        0.323405
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.980720 loss:        0.057261
Test - acc:         0.916700 loss:        0.301387
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.981560 loss:        0.053537
Test - acc:         0.920200 loss:        0.304222
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.983180 loss:        0.050734
Test - acc:         0.919100 loss:        0.294317
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.982120 loss:        0.053441
Test - acc:         0.919400 loss:        0.314936
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.983720 loss:        0.048838
Test - acc:         0.914200 loss:        0.322421
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.979780 loss:        0.058412
Test - acc:         0.909400 loss:        0.338699
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.980700 loss:        0.056835
Test - acc:         0.908700 loss:        0.353313
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.983180 loss:        0.049837
Test - acc:         0.918100 loss:        0.313087
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.981780 loss:        0.052650
Test - acc:         0.913100 loss:        0.321335
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.981300 loss:        0.053780
Test - acc:         0.911300 loss:        0.331185
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.982820 loss:        0.050005
Test - acc:         0.912000 loss:        0.338447
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.980680 loss:        0.056765
Test - acc:         0.908800 loss:        0.340720
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.983400 loss:        0.049987
Test - acc:         0.914000 loss:        0.328017
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.982120 loss:        0.053789
Test - acc:         0.907900 loss:        0.368448
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.980560 loss:        0.057057
Test - acc:         0.897900 loss:        0.399369
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.981600 loss:        0.054233
Test - acc:         0.921300 loss:        0.303993
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.980600 loss:        0.056818
Test - acc:         0.908900 loss:        0.348796
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.981500 loss:        0.055003
Test - acc:         0.911400 loss:        0.316115
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.981980 loss:        0.052004
Test - acc:         0.917200 loss:        0.310036
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.982600 loss:        0.050430
Test - acc:         0.913500 loss:        0.328204
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.982540 loss:        0.051764
Test - acc:         0.907100 loss:        0.359205
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.979900 loss:        0.059210
Test - acc:         0.916800 loss:        0.297174
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.981240 loss:        0.055085
Test - acc:         0.909700 loss:        0.345002
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.982520 loss:        0.051737
Test - acc:         0.911900 loss:        0.330017
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.981080 loss:        0.056132
Test - acc:         0.910700 loss:        0.340632
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.981080 loss:        0.055552
Test - acc:         0.912800 loss:        0.313714
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.982180 loss:        0.053009
Test - acc:         0.915300 loss:        0.308690
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.981840 loss:        0.053626
Test - acc:         0.916300 loss:        0.317012
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.980160 loss:        0.058392
Test - acc:         0.912300 loss:        0.337786
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.982480 loss:        0.053051
Test - acc:         0.912800 loss:        0.314564
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.961480 loss:        0.113652
Test - acc:         0.910300 loss:        0.308978
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.970260 loss:        0.087806
Test - acc:         0.893500 loss:        0.380749
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.971180 loss:        0.086480
Test - acc:         0.908700 loss:        0.306424
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.971180 loss:        0.082557
Test - acc:         0.902300 loss:        0.365688
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.973200 loss:        0.080151
Test - acc:         0.908400 loss:        0.331211
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.973920 loss:        0.076626
Test - acc:         0.906000 loss:        0.334238
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.975560 loss:        0.071786
Test - acc:         0.909700 loss:        0.321897
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.974160 loss:        0.075420
Test - acc:         0.906400 loss:        0.331449
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.974680 loss:        0.075229
Test - acc:         0.901900 loss:        0.364502
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.975020 loss:        0.072642
Test - acc:         0.886900 loss:        0.435191
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.973800 loss:        0.075955
Test - acc:         0.911000 loss:        0.329610
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.975620 loss:        0.070817
Test - acc:         0.906200 loss:        0.342779
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.976420 loss:        0.071009
Test - acc:         0.908300 loss:        0.328385
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.977280 loss:        0.068022
Test - acc:         0.912700 loss:        0.333801
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.977060 loss:        0.067436
Test - acc:         0.912900 loss:        0.316182
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.974680 loss:        0.074618
Test - acc:         0.908000 loss:        0.320926
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.975680 loss:        0.070597
Test - acc:         0.910400 loss:        0.330724
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.977240 loss:        0.067781
Test - acc:         0.901700 loss:        0.369123
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.974920 loss:        0.072024
Test - acc:         0.909500 loss:        0.327929
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.976000 loss:        0.069119
Test - acc:         0.914400 loss:        0.312633
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.978400 loss:        0.065103
Test - acc:         0.913100 loss:        0.320140
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.976680 loss:        0.067705
Test - acc:         0.900500 loss:        0.372288
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.976300 loss:        0.069998
Test - acc:         0.897900 loss:        0.377694
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.976060 loss:        0.070958
Test - acc:         0.909400 loss:        0.333172
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.975840 loss:        0.068785
Test - acc:         0.908100 loss:        0.337732
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.977020 loss:        0.067140
Test - acc:         0.904700 loss:        0.352987
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.986100 loss:        0.042991
Test - acc:         0.929300 loss:        0.259255
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.991360 loss:        0.028606
Test - acc:         0.930700 loss:        0.251202
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.992920 loss:        0.023843
Test - acc:         0.931800 loss:        0.255149
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.993280 loss:        0.022660
Test - acc:         0.931400 loss:        0.255743
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.994280 loss:        0.020263
Test - acc:         0.931000 loss:        0.251372
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.995160 loss:        0.017717
Test - acc:         0.932000 loss:        0.258190
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.957180 loss:        0.124884
Test - acc:         0.908300 loss:        0.327575
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.966480 loss:        0.094768
Test - acc:         0.913100 loss:        0.312013
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.971020 loss:        0.083745
Test - acc:         0.913700 loss:        0.309145
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.974280 loss:        0.077415
Test - acc:         0.914400 loss:        0.303557
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.976400 loss:        0.068805
Test - acc:         0.916400 loss:        0.297969
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.977760 loss:        0.066397
Test - acc:         0.916500 loss:        0.297820
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.977760 loss:        0.065133
Test - acc:         0.915600 loss:        0.296253
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.979560 loss:        0.061163
Test - acc:         0.915900 loss:        0.301982
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.981140 loss:        0.056745
Test - acc:         0.919700 loss:        0.299816
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.981080 loss:        0.055150
Test - acc:         0.919300 loss:        0.295596
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.982260 loss:        0.053836
Test - acc:         0.920100 loss:        0.300677
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.981180 loss:        0.056209
Test - acc:         0.920400 loss:        0.299285
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.982620 loss:        0.051786
Test - acc:         0.919000 loss:        0.299449
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.984640 loss:        0.046718
Test - acc:         0.919600 loss:        0.298259
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.983520 loss:        0.048490
Test - acc:         0.921000 loss:        0.293815
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.985100 loss:        0.045672
Test - acc:         0.918600 loss:        0.300009
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.984960 loss:        0.045056
Test - acc:         0.920100 loss:        0.296062
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.985780 loss:        0.043655
Test - acc:         0.920500 loss:        0.300144
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.984520 loss:        0.045056
Test - acc:         0.919500 loss:        0.298479
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.986820 loss:        0.041027
Test - acc:         0.921600 loss:        0.295970
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.986520 loss:        0.040893
Test - acc:         0.922800 loss:        0.294103
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.987140 loss:        0.040019
Test - acc:         0.922800 loss:        0.300680
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.986320 loss:        0.041089
Test - acc:         0.921000 loss:        0.302574
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.987820 loss:        0.038360
Test - acc:         0.919200 loss:        0.304199
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.987400 loss:        0.038026
Test - acc:         0.921500 loss:        0.304746
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.987300 loss:        0.037718
Test - acc:         0.924000 loss:        0.305155
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.988360 loss:        0.036089
Test - acc:         0.921900 loss:        0.300212
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.989100 loss:        0.034401
Test - acc:         0.921600 loss:        0.307157
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.988580 loss:        0.034826
Test - acc:         0.922000 loss:        0.302622
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.989120 loss:        0.034319
Test - acc:         0.922600 loss:        0.304039
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.988840 loss:        0.034643
Test - acc:         0.921800 loss:        0.310601
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.989100 loss:        0.033762
Test - acc:         0.923300 loss:        0.306616
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.850580 loss:        0.461045
Test - acc:         0.860600 loss:        0.452317
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.891000 loss:        0.318694
Test - acc:         0.871200 loss:        0.407389
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.904420 loss:        0.278290
Test - acc:         0.881300 loss:        0.384844
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.913000 loss:        0.254228
Test - acc:         0.881000 loss:        0.373663
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.917640 loss:        0.238966
Test - acc:         0.882500 loss:        0.371963
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.923320 loss:        0.225958
Test - acc:         0.885600 loss:        0.360694
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.924900 loss:        0.219115
Test - acc:         0.889300 loss:        0.355616
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.927400 loss:        0.211681
Test - acc:         0.890600 loss:        0.346963
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.929480 loss:        0.205069
Test - acc:         0.891100 loss:        0.351949
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.931940 loss:        0.198499
Test - acc:         0.893000 loss:        0.345580
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.931940 loss:        0.194464
Test - acc:         0.892600 loss:        0.346325
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.935760 loss:        0.188090
Test - acc:         0.893900 loss:        0.348084
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.935020 loss:        0.187854
Test - acc:         0.893900 loss:        0.341691
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.936900 loss:        0.184442
Test - acc:         0.895600 loss:        0.342680
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.935980 loss:        0.183902
Test - acc:         0.895800 loss:        0.340115
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.936220 loss:        0.182988
Test - acc:         0.895400 loss:        0.343418
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.940820 loss:        0.172438
Test - acc:         0.896500 loss:        0.335926
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.940020 loss:        0.173787
Test - acc:         0.896600 loss:        0.338825
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.940840 loss:        0.169752
Test - acc:         0.899400 loss:        0.334445
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.941620 loss:        0.167861
Test - acc:         0.897100 loss:        0.336003
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.943400 loss:        0.162755
Test - acc:         0.900200 loss:        0.327836
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.942320 loss:        0.166457
Test - acc:         0.897700 loss:        0.342664
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.943340 loss:        0.161695
Test - acc:         0.900100 loss:        0.335772
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.943220 loss:        0.161184
Test - acc:         0.896700 loss:        0.338683
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.943620 loss:        0.161621
Test - acc:         0.898700 loss:        0.335506
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.945740 loss:        0.156507
Test - acc:         0.898200 loss:        0.338834
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.946720 loss:        0.154576
Test - acc:         0.901700 loss:        0.338472
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.944020 loss:        0.160625
Test - acc:         0.900500 loss:        0.338244
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.946140 loss:        0.157558
Test - acc:         0.902200 loss:        0.335393
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.945680 loss:        0.155530
Test - acc:         0.899800 loss:        0.335117
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.947620 loss:        0.150449
Test - acc:         0.901600 loss:        0.335213
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.945940 loss:        0.155779
Test - acc:         0.900100 loss:        0.340195
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.722640 loss:        0.848088
Test - acc:         0.775600 loss:        0.686061
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.793320 loss:        0.631392
Test - acc:         0.795500 loss:        0.631491
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.810960 loss:        0.575141
Test - acc:         0.810200 loss:        0.590495
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.817840 loss:        0.545684
Test - acc:         0.820100 loss:        0.551897
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.827380 loss:        0.520980
Test - acc:         0.828600 loss:        0.532075
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.832160 loss:        0.504357
Test - acc:         0.827500 loss:        0.541236
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.836160 loss:        0.492862
Test - acc:         0.832600 loss:        0.520393
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.839320 loss:        0.479685
Test - acc:         0.831900 loss:        0.516606
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.842560 loss:        0.473988
Test - acc:         0.830700 loss:        0.504881
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.844440 loss:        0.464051
Test - acc:         0.839200 loss:        0.499952
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.846600 loss:        0.458518
Test - acc:         0.835300 loss:        0.494832
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.849200 loss:        0.451458
Test - acc:         0.836200 loss:        0.496054
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.849460 loss:        0.452107
Test - acc:         0.841000 loss:        0.492274
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.849480 loss:        0.444313
Test - acc:         0.840800 loss:        0.487050
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.850400 loss:        0.440950
Test - acc:         0.842700 loss:        0.489873
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.852820 loss:        0.437802
Test - acc:         0.838800 loss:        0.490432
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.855540 loss:        0.431662
Test - acc:         0.843700 loss:        0.480846
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.854560 loss:        0.429404
Test - acc:         0.845900 loss:        0.476947
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.856680 loss:        0.424972
Test - acc:         0.846500 loss:        0.474832
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.856980 loss:        0.423301
Test - acc:         0.840400 loss:        0.484637
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.860000 loss:        0.418035
Test - acc:         0.842700 loss:        0.476941
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.861160 loss:        0.417560
Test - acc:         0.847000 loss:        0.478878
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.857860 loss:        0.419212
Test - acc:         0.847400 loss:        0.467938
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.860240 loss:        0.410942
Test - acc:         0.850800 loss:        0.466766
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.859820 loss:        0.412789
Test - acc:         0.844400 loss:        0.481682
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.860640 loss:        0.414180
Test - acc:         0.843500 loss:        0.482770
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.860080 loss:        0.410469
Test - acc:         0.847200 loss:        0.472202
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.864020 loss:        0.405310
Test - acc:         0.848000 loss:        0.475828
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.863540 loss:        0.404560
Test - acc:         0.848800 loss:        0.467355
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.864040 loss:        0.405739
Test - acc:         0.846400 loss:        0.470527
Sparsity :          0.9990
Wdecay :        0.000500
