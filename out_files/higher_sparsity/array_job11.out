Running --model resnet18 --prune_criterion global_magnitude --seed 44 --prune_freq 32 --prune_rate 0.5 --comment=resnet18_crit=global_magnitude_pf=32_seed=44 --save_model=pre-finetune/resnet18_global_magnitude_pf32_s44 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "global_magnitude",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_global_magnitude_pf32_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.326500 loss:        1.844461
Test - acc:         0.423100 loss:        1.566802
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.523060 loss:        1.311480
Test - acc:         0.573700 loss:        1.162164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.631560 loss:        1.037143
Test - acc:         0.605500 loss:        1.154360
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.690180 loss:        0.879254
Test - acc:         0.698600 loss:        0.870134
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.743000 loss:        0.739316
Test - acc:         0.741600 loss:        0.760918
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.775920 loss:        0.646051
Test - acc:         0.761800 loss:        0.712189
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.798040 loss:        0.583349
Test - acc:         0.699400 loss:        0.960318
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.808880 loss:        0.550098
Test - acc:         0.719800 loss:        0.895329
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.822400 loss:        0.517782
Test - acc:         0.798300 loss:        0.592408
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.828280 loss:        0.498775
Test - acc:         0.803500 loss:        0.578936
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.838380 loss:        0.470293
Test - acc:         0.802200 loss:        0.613507
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.840980 loss:        0.464587
Test - acc:         0.794200 loss:        0.611622
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.847140 loss:        0.446681
Test - acc:         0.812200 loss:        0.584225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.851420 loss:        0.433831
Test - acc:         0.745900 loss:        0.817999
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.852860 loss:        0.428770
Test - acc:         0.797000 loss:        0.603769
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.856840 loss:        0.417291
Test - acc:         0.793900 loss:        0.644432
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.862140 loss:        0.409136
Test - acc:         0.824900 loss:        0.529044
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.861320 loss:        0.401354
Test - acc:         0.815400 loss:        0.555978
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.863660 loss:        0.396668
Test - acc:         0.817300 loss:        0.531897
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.866860 loss:        0.390623
Test - acc:         0.770700 loss:        0.716770
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.868780 loss:        0.385096
Test - acc:         0.832300 loss:        0.500326
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870100 loss:        0.382342
Test - acc:         0.813700 loss:        0.540815
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.869500 loss:        0.384136
Test - acc:         0.837000 loss:        0.476577
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.872000 loss:        0.375453
Test - acc:         0.842100 loss:        0.462574
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.874400 loss:        0.369435
Test - acc:         0.822800 loss:        0.543520
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.873880 loss:        0.369378
Test - acc:         0.853400 loss:        0.432042
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.876600 loss:        0.359764
Test - acc:         0.824500 loss:        0.514843
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.877660 loss:        0.357625
Test - acc:         0.769200 loss:        0.763895
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.878520 loss:        0.356873
Test - acc:         0.815700 loss:        0.576099
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.878700 loss:        0.357575
Test - acc:         0.829500 loss:        0.503405
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.877880 loss:        0.360216
Test - acc:         0.826900 loss:        0.531613
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.881500 loss:        0.347606
Test - acc:         0.833600 loss:        0.533857
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.890760 loss:        0.318491
Test - acc:         0.824100 loss:        0.552886
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.889480 loss:        0.324037
Test - acc:         0.856100 loss:        0.422169
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.888100 loss:        0.329303
Test - acc:         0.856100 loss:        0.420451
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.890540 loss:        0.321694
Test - acc:         0.843300 loss:        0.479861
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.890240 loss:        0.321553
Test - acc:         0.824400 loss:        0.540163
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.891620 loss:        0.318801
Test - acc:         0.836900 loss:        0.522339
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.891320 loss:        0.322315
Test - acc:         0.839200 loss:        0.488896
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.891180 loss:        0.319746
Test - acc:         0.853800 loss:        0.462505
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.891520 loss:        0.318237
Test - acc:         0.843900 loss:        0.474039
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.893020 loss:        0.312754
Test - acc:         0.830800 loss:        0.520859
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.892160 loss:        0.316203
Test - acc:         0.809100 loss:        0.648582
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.892940 loss:        0.311420
Test - acc:         0.845200 loss:        0.449886
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.892340 loss:        0.316859
Test - acc:         0.838600 loss:        0.502370
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.893220 loss:        0.310805
Test - acc:         0.834600 loss:        0.513960
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.893440 loss:        0.310015
Test - acc:         0.810900 loss:        0.613465
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.893260 loss:        0.317376
Test - acc:         0.858700 loss:        0.414746
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.893540 loss:        0.310142
Test - acc:         0.835200 loss:        0.519956
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.892380 loss:        0.312367
Test - acc:         0.844200 loss:        0.484752
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.893240 loss:        0.311386
Test - acc:         0.846700 loss:        0.475147
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.896800 loss:        0.305891
Test - acc:         0.831100 loss:        0.501377
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.892420 loss:        0.314230
Test - acc:         0.847100 loss:        0.474305
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.894960 loss:        0.308198
Test - acc:         0.811700 loss:        0.568069
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.894540 loss:        0.309265
Test - acc:         0.827800 loss:        0.521625
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.895280 loss:        0.306946
Test - acc:         0.834500 loss:        0.502015
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.895760 loss:        0.305621
Test - acc:         0.843000 loss:        0.478524
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.895020 loss:        0.308953
Test - acc:         0.852500 loss:        0.452075
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.892940 loss:        0.311122
Test - acc:         0.805300 loss:        0.610851
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.897660 loss:        0.302354
Test - acc:         0.870300 loss:        0.395069
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.895920 loss:        0.304744
Test - acc:         0.863500 loss:        0.404552
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.893820 loss:        0.308232
Test - acc:         0.857100 loss:        0.446988
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.896780 loss:        0.301914
Test - acc:         0.839900 loss:        0.479700
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.895020 loss:        0.308516
Test - acc:         0.856500 loss:        0.438979
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.906660 loss:        0.270835
Test - acc:         0.844000 loss:        0.476309
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.904280 loss:        0.274064
Test - acc:         0.859100 loss:        0.431379
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.907620 loss:        0.271904
Test - acc:         0.818100 loss:        0.598206
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.902180 loss:        0.285241
Test - acc:         0.861100 loss:        0.414768
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.903600 loss:        0.283210
Test - acc:         0.870700 loss:        0.392829
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.907020 loss:        0.270739
Test - acc:         0.858200 loss:        0.426386
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.905120 loss:        0.279044
Test - acc:         0.858800 loss:        0.412392
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.903840 loss:        0.279284
Test - acc:         0.865600 loss:        0.410315
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.907180 loss:        0.271913
Test - acc:         0.868100 loss:        0.399897
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.905100 loss:        0.279721
Test - acc:         0.834300 loss:        0.503324
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.904700 loss:        0.279046
Test - acc:         0.842700 loss:        0.477547
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.906320 loss:        0.274061
Test - acc:         0.818100 loss:        0.568726
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.906900 loss:        0.274642
Test - acc:         0.850200 loss:        0.481861
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.903700 loss:        0.279197
Test - acc:         0.837400 loss:        0.505392
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.906140 loss:        0.272418
Test - acc:         0.864400 loss:        0.408947
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.904320 loss:        0.277976
Test - acc:         0.839300 loss:        0.550102
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.906920 loss:        0.272330
Test - acc:         0.828000 loss:        0.522762
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.903900 loss:        0.278193
Test - acc:         0.847900 loss:        0.467814
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.903500 loss:        0.276714
Test - acc:         0.818200 loss:        0.622577
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.904400 loss:        0.277663
Test - acc:         0.852600 loss:        0.465400
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.907100 loss:        0.272848
Test - acc:         0.851100 loss:        0.450125
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.904260 loss:        0.279191
Test - acc:         0.851400 loss:        0.451405
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.906140 loss:        0.273211
Test - acc:         0.872000 loss:        0.380951
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.907060 loss:        0.269658
Test - acc:         0.858200 loss:        0.442198
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.905980 loss:        0.274272
Test - acc:         0.794100 loss:        0.692568
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.904100 loss:        0.277399
Test - acc:         0.751700 loss:        0.912419
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.904080 loss:        0.277870
Test - acc:         0.833700 loss:        0.545731
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.904340 loss:        0.279365
Test - acc:         0.850400 loss:        0.465288
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.907980 loss:        0.272349
Test - acc:         0.848500 loss:        0.478269
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.905340 loss:        0.277468
Test - acc:         0.870200 loss:        0.400673
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.907440 loss:        0.272645
Test - acc:         0.871800 loss:        0.379107
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.909220 loss:        0.269042
Test - acc:         0.857100 loss:        0.447925
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.919800 loss:        0.235118
Test - acc:         0.861500 loss:        0.423025
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.915800 loss:        0.247226
Test - acc:         0.806700 loss:        0.647608
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.916760 loss:        0.245766
Test - acc:         0.863500 loss:        0.412987
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.915020 loss:        0.246970
Test - acc:         0.859200 loss:        0.426851
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.914780 loss:        0.247167
Test - acc:         0.841900 loss:        0.527165
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.913980 loss:        0.250567
Test - acc:         0.852700 loss:        0.462133
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.914540 loss:        0.247661
Test - acc:         0.856900 loss:        0.451365
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.914820 loss:        0.250852
Test - acc:         0.831900 loss:        0.560755
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.916380 loss:        0.243256
Test - acc:         0.829300 loss:        0.528421
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.912380 loss:        0.252179
Test - acc:         0.884100 loss:        0.357235
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.914120 loss:        0.251019
Test - acc:         0.843400 loss:        0.545224
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.914260 loss:        0.248611
Test - acc:         0.849100 loss:        0.483509
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.913580 loss:        0.247826
Test - acc:         0.846500 loss:        0.475717
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.915900 loss:        0.244063
Test - acc:         0.889300 loss:        0.338079
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.914520 loss:        0.248837
Test - acc:         0.857600 loss:        0.430080
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.912660 loss:        0.252621
Test - acc:         0.871900 loss:        0.387600
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.916640 loss:        0.245247
Test - acc:         0.858600 loss:        0.447774
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.913280 loss:        0.250174
Test - acc:         0.874500 loss:        0.386896
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.914940 loss:        0.248082
Test - acc:         0.877800 loss:        0.364616
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.916100 loss:        0.244830
Test - acc:         0.871700 loss:        0.400502
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.915000 loss:        0.246573
Test - acc:         0.856000 loss:        0.437171
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.912220 loss:        0.251370
Test - acc:         0.831500 loss:        0.521791
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.915280 loss:        0.247180
Test - acc:         0.880200 loss:        0.365552
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.915140 loss:        0.247303
Test - acc:         0.841500 loss:        0.498001
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.916440 loss:        0.244841
Test - acc:         0.877700 loss:        0.374314
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.915340 loss:        0.248204
Test - acc:         0.874900 loss:        0.386218
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.912880 loss:        0.253928
Test - acc:         0.875800 loss:        0.374299
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.913740 loss:        0.250435
Test - acc:         0.855000 loss:        0.460307
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.915340 loss:        0.246955
Test - acc:         0.894400 loss:        0.330968
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.914840 loss:        0.248132
Test - acc:         0.867600 loss:        0.401741
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.916000 loss:        0.246335
Test - acc:         0.870800 loss:        0.392203
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.913020 loss:        0.251985
Test - acc:         0.859400 loss:        0.417862
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.923740 loss:        0.221041
Test - acc:         0.868200 loss:        0.411204
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.923600 loss:        0.221739
Test - acc:         0.819500 loss:        0.588082
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.920660 loss:        0.231982
Test - acc:         0.881800 loss:        0.347793
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.919420 loss:        0.231807
Test - acc:         0.874300 loss:        0.373482
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.919100 loss:        0.232683
Test - acc:         0.880400 loss:        0.366680
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.918960 loss:        0.230771
Test - acc:         0.880800 loss:        0.366473
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.919740 loss:        0.233985
Test - acc:         0.851600 loss:        0.463330
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.919280 loss:        0.233823
Test - acc:         0.895300 loss:        0.311198
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.919700 loss:        0.232016
Test - acc:         0.877200 loss:        0.372677
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.919340 loss:        0.234664
Test - acc:         0.874500 loss:        0.391701
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.920780 loss:        0.230759
Test - acc:         0.874000 loss:        0.401418
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.920800 loss:        0.233014
Test - acc:         0.847000 loss:        0.496076
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.918060 loss:        0.233583
Test - acc:         0.854300 loss:        0.460464
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.917980 loss:        0.237583
Test - acc:         0.865200 loss:        0.444695
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.921580 loss:        0.229336
Test - acc:         0.873000 loss:        0.384763
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.919400 loss:        0.233097
Test - acc:         0.878200 loss:        0.383809
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.920180 loss:        0.233381
Test - acc:         0.866000 loss:        0.419437
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.920180 loss:        0.229372
Test - acc:         0.859000 loss:        0.431836
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.919960 loss:        0.232989
Test - acc:         0.856400 loss:        0.452052
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.920580 loss:        0.228924
Test - acc:         0.845900 loss:        0.504413
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.917860 loss:        0.239976
Test - acc:         0.869100 loss:        0.410088
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.920380 loss:        0.231760
Test - acc:         0.865400 loss:        0.413771
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.957820 loss:        0.130327
Test - acc:         0.935100 loss:        0.186078
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.971200 loss:        0.090082
Test - acc:         0.938700 loss:        0.179805
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.975360 loss:        0.076228
Test - acc:         0.939300 loss:        0.176316
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.978560 loss:        0.066430
Test - acc:         0.939900 loss:        0.175263
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.060134
Test - acc:         0.942700 loss:        0.174420
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.983180 loss:        0.054800
Test - acc:         0.943500 loss:        0.171059
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.985020 loss:        0.049039
Test - acc:         0.943100 loss:        0.178751
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.986580 loss:        0.043733
Test - acc:         0.944000 loss:        0.179163
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.987100 loss:        0.041034
Test - acc:         0.942900 loss:        0.181069
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.988780 loss:        0.038506
Test - acc:         0.942600 loss:        0.185070
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.987640 loss:        0.042791
Test - acc:         0.940000 loss:        0.190237
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.988560 loss:        0.040062
Test - acc:         0.941300 loss:        0.184945
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.990020 loss:        0.035471
Test - acc:         0.942800 loss:        0.183926
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.989320 loss:        0.035890
Test - acc:         0.939500 loss:        0.186266
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.990680 loss:        0.032645
Test - acc:         0.943500 loss:        0.185920
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.990900 loss:        0.032746
Test - acc:         0.941500 loss:        0.187173
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.992200 loss:        0.028977
Test - acc:         0.942800 loss:        0.188836
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.991860 loss:        0.028927
Test - acc:         0.942000 loss:        0.192328
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.992200 loss:        0.027570
Test - acc:         0.941000 loss:        0.197260
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.991800 loss:        0.029732
Test - acc:         0.940800 loss:        0.205903
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.991220 loss:        0.029885
Test - acc:         0.942900 loss:        0.192145
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.992240 loss:        0.027536
Test - acc:         0.942900 loss:        0.198975
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.992440 loss:        0.027117
Test - acc:         0.942100 loss:        0.200931
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.993260 loss:        0.025383
Test - acc:         0.941500 loss:        0.198601
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.991620 loss:        0.027980
Test - acc:         0.943400 loss:        0.196176
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.991040 loss:        0.029039
Test - acc:         0.941100 loss:        0.215538
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.992000 loss:        0.028150
Test - acc:         0.943700 loss:        0.198456
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.991680 loss:        0.027879
Test - acc:         0.938400 loss:        0.216069
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.991560 loss:        0.028568
Test - acc:         0.943100 loss:        0.204751
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.991160 loss:        0.028997
Test - acc:         0.940000 loss:        0.216172
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.991340 loss:        0.029466
Test - acc:         0.939700 loss:        0.218988
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.991020 loss:        0.030341
Test - acc:         0.940600 loss:        0.222872
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.990800 loss:        0.031220
Test - acc:         0.940400 loss:        0.211577
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.991220 loss:        0.030222
Test - acc:         0.935200 loss:        0.224583
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.990020 loss:        0.033166
Test - acc:         0.938100 loss:        0.223164
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.988800 loss:        0.034115
Test - acc:         0.935900 loss:        0.225259
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.991000 loss:        0.031172
Test - acc:         0.933000 loss:        0.236804
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.989760 loss:        0.032818
Test - acc:         0.934000 loss:        0.232693
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.989220 loss:        0.034637
Test - acc:         0.932300 loss:        0.243400
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.988500 loss:        0.037083
Test - acc:         0.935400 loss:        0.227594
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.987880 loss:        0.037649
Test - acc:         0.934100 loss:        0.236021
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.987600 loss:        0.038695
Test - acc:         0.935200 loss:        0.231505
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.966340 loss:        0.100944
Test - acc:         0.921500 loss:        0.270136
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.974380 loss:        0.078450
Test - acc:         0.924300 loss:        0.253997
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.976560 loss:        0.071004
Test - acc:         0.919800 loss:        0.276197
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.978060 loss:        0.067651
Test - acc:         0.922300 loss:        0.270494
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.977720 loss:        0.066076
Test - acc:         0.924000 loss:        0.256932
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.978160 loss:        0.064182
Test - acc:         0.925000 loss:        0.261290
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.978820 loss:        0.064352
Test - acc:         0.924300 loss:        0.263302
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.979580 loss:        0.059895
Test - acc:         0.927400 loss:        0.262861
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.979640 loss:        0.060979
Test - acc:         0.917500 loss:        0.282416
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.980800 loss:        0.058897
Test - acc:         0.922600 loss:        0.269284
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.981200 loss:        0.057476
Test - acc:         0.926600 loss:        0.264563
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.980920 loss:        0.058271
Test - acc:         0.927300 loss:        0.255869
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.980940 loss:        0.058373
Test - acc:         0.909900 loss:        0.327212
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.980440 loss:        0.059285
Test - acc:         0.924100 loss:        0.264819
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.979500 loss:        0.060750
Test - acc:         0.914100 loss:        0.303527
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.978840 loss:        0.062363
Test - acc:         0.924500 loss:        0.259250
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.979800 loss:        0.059716
Test - acc:         0.927100 loss:        0.259947
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.979760 loss:        0.060878
Test - acc:         0.928900 loss:        0.250676
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.981740 loss:        0.054782
Test - acc:         0.926200 loss:        0.261253
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.981000 loss:        0.056893
Test - acc:         0.928700 loss:        0.264958
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.980260 loss:        0.059193
Test - acc:         0.927800 loss:        0.251357
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.982080 loss:        0.054912
Test - acc:         0.923400 loss:        0.262692
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.981300 loss:        0.057127
Test - acc:         0.927300 loss:        0.251422
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.981280 loss:        0.057345
Test - acc:         0.924000 loss:        0.271169
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.980900 loss:        0.058408
Test - acc:         0.926400 loss:        0.260478
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.980480 loss:        0.058991
Test - acc:         0.930000 loss:        0.251902
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.980420 loss:        0.059421
Test - acc:         0.930900 loss:        0.247951
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.982360 loss:        0.054807
Test - acc:         0.929400 loss:        0.248240
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.981600 loss:        0.054521
Test - acc:         0.927100 loss:        0.264279
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.981340 loss:        0.056456
Test - acc:         0.927500 loss:        0.270548
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.981500 loss:        0.056050
Test - acc:         0.921200 loss:        0.279622
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.980880 loss:        0.058734
Test - acc:         0.927100 loss:        0.269978
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.950120 loss:        0.149032
Test - acc:         0.905400 loss:        0.308002
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.959380 loss:        0.120880
Test - acc:         0.908500 loss:        0.309506
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.960020 loss:        0.119153
Test - acc:         0.904200 loss:        0.318694
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.962580 loss:        0.112464
Test - acc:         0.907400 loss:        0.310884
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.963040 loss:        0.106906
Test - acc:         0.918300 loss:        0.271970
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.964120 loss:        0.105543
Test - acc:         0.919900 loss:        0.272227
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.964820 loss:        0.100091
Test - acc:         0.911200 loss:        0.284485
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.965960 loss:        0.099406
Test - acc:         0.914800 loss:        0.285529
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.966800 loss:        0.098480
Test - acc:         0.920100 loss:        0.263229
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.967240 loss:        0.096508
Test - acc:         0.917200 loss:        0.281309
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.967300 loss:        0.097344
Test - acc:         0.917200 loss:        0.276953
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.967860 loss:        0.094491
Test - acc:         0.913500 loss:        0.284205
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.967180 loss:        0.097493
Test - acc:         0.908100 loss:        0.310170
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.968280 loss:        0.094095
Test - acc:         0.918600 loss:        0.264670
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.969440 loss:        0.089504
Test - acc:         0.916500 loss:        0.270369
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.969820 loss:        0.090089
Test - acc:         0.909100 loss:        0.305889
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.968580 loss:        0.092588
Test - acc:         0.919600 loss:        0.276179
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.968040 loss:        0.094948
Test - acc:         0.916300 loss:        0.278060
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.969400 loss:        0.089214
Test - acc:         0.918100 loss:        0.280252
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.968560 loss:        0.091563
Test - acc:         0.911200 loss:        0.314798
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.968560 loss:        0.091669
Test - acc:         0.917700 loss:        0.274744
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.968420 loss:        0.093201
Test - acc:         0.922600 loss:        0.262053
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.970800 loss:        0.087228
Test - acc:         0.911300 loss:        0.316336
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.969180 loss:        0.088955
Test - acc:         0.914200 loss:        0.297889
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.971040 loss:        0.088244
Test - acc:         0.921900 loss:        0.271595
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.969420 loss:        0.088477
Test - acc:         0.915300 loss:        0.290278
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.982340 loss:        0.059266
Test - acc:         0.931200 loss:        0.227404
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.986500 loss:        0.048039
Test - acc:         0.933800 loss:        0.221719
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.987120 loss:        0.044947
Test - acc:         0.935600 loss:        0.218902
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.987580 loss:        0.041801
Test - acc:         0.935900 loss:        0.218701
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.989600 loss:        0.038513
Test - acc:         0.935100 loss:        0.219278
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.988960 loss:        0.038628
Test - acc:         0.936000 loss:        0.217814
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.923340 loss:        0.239262
Test - acc:         0.899200 loss:        0.299103
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.941720 loss:        0.183854
Test - acc:         0.907400 loss:        0.282042
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.947000 loss:        0.165260
Test - acc:         0.905800 loss:        0.278445
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.951760 loss:        0.154596
Test - acc:         0.911600 loss:        0.269591
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.953380 loss:        0.147338
Test - acc:         0.909800 loss:        0.266717
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.954620 loss:        0.141798
Test - acc:         0.914200 loss:        0.262340
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.955940 loss:        0.136753
Test - acc:         0.914200 loss:        0.259574
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.958760 loss:        0.132574
Test - acc:         0.916700 loss:        0.258510
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.960540 loss:        0.126466
Test - acc:         0.916400 loss:        0.255588
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.960860 loss:        0.123535
Test - acc:         0.917200 loss:        0.254768
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.960980 loss:        0.124083
Test - acc:         0.918000 loss:        0.253685
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.962540 loss:        0.117679
Test - acc:         0.919100 loss:        0.252291
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.963660 loss:        0.114486
Test - acc:         0.919400 loss:        0.255446
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.963140 loss:        0.113435
Test - acc:         0.920000 loss:        0.250164
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.963720 loss:        0.111975
Test - acc:         0.919900 loss:        0.251995
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.965160 loss:        0.109691
Test - acc:         0.921200 loss:        0.251618
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.965040 loss:        0.109873
Test - acc:         0.920100 loss:        0.250432
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.965780 loss:        0.106636
Test - acc:         0.919800 loss:        0.250251
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.966060 loss:        0.106121
Test - acc:         0.918600 loss:        0.251458
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.965800 loss:        0.104770
Test - acc:         0.920100 loss:        0.253746
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.967120 loss:        0.104421
Test - acc:         0.919900 loss:        0.251806
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.967220 loss:        0.102191
Test - acc:         0.920200 loss:        0.252369
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.967860 loss:        0.100990
Test - acc:         0.918200 loss:        0.254940
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.966760 loss:        0.101420
Test - acc:         0.920600 loss:        0.252978
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.967920 loss:        0.099595
Test - acc:         0.922600 loss:        0.252021
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.968180 loss:        0.099726
Test - acc:         0.921900 loss:        0.254301
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.967600 loss:        0.099271
Test - acc:         0.921300 loss:        0.258163
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.968980 loss:        0.098734
Test - acc:         0.922500 loss:        0.253382
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.968800 loss:        0.095862
Test - acc:         0.921400 loss:        0.255521
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.969300 loss:        0.094490
Test - acc:         0.922600 loss:        0.252309
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.969580 loss:        0.094684
Test - acc:         0.920900 loss:        0.255630
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.970560 loss:        0.093016
Test - acc:         0.920000 loss:        0.261860
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.781420 loss:        0.663245
Test - acc:         0.819000 loss:        0.552097
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.836800 loss:        0.493239
Test - acc:         0.837600 loss:        0.488378
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.851760 loss:        0.447159
Test - acc:         0.844300 loss:        0.467430
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.862280 loss:        0.419419
Test - acc:         0.852300 loss:        0.445827
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.866480 loss:        0.402570
Test - acc:         0.858300 loss:        0.430443
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.872920 loss:        0.386169
Test - acc:         0.859000 loss:        0.423345
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.873980 loss:        0.376697
Test - acc:         0.861600 loss:        0.416440
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.878440 loss:        0.367082
Test - acc:         0.865100 loss:        0.408588
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.880780 loss:        0.360509
Test - acc:         0.866500 loss:        0.405925
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.882600 loss:        0.349514
Test - acc:         0.868000 loss:        0.406036
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.884340 loss:        0.345980
Test - acc:         0.866300 loss:        0.405172
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.886020 loss:        0.341099
Test - acc:         0.869600 loss:        0.395060
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.886420 loss:        0.339931
Test - acc:         0.869700 loss:        0.398273
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.887320 loss:        0.333652
Test - acc:         0.868000 loss:        0.390234
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.889380 loss:        0.328473
Test - acc:         0.870900 loss:        0.391876
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.891040 loss:        0.325396
Test - acc:         0.870100 loss:        0.387768
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.891960 loss:        0.322861
Test - acc:         0.871300 loss:        0.390046
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.892660 loss:        0.320690
Test - acc:         0.868700 loss:        0.387893
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.894760 loss:        0.316597
Test - acc:         0.875800 loss:        0.379088
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.895040 loss:        0.311950
Test - acc:         0.875800 loss:        0.385127
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.895620 loss:        0.311269
Test - acc:         0.877000 loss:        0.381654
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.896300 loss:        0.308878
Test - acc:         0.876500 loss:        0.381689
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.896500 loss:        0.306726
Test - acc:         0.875100 loss:        0.384195
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.897720 loss:        0.302374
Test - acc:         0.877300 loss:        0.379611
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.897800 loss:        0.305057
Test - acc:         0.876300 loss:        0.375755
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.898220 loss:        0.301009
Test - acc:         0.876000 loss:        0.377051
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.898020 loss:        0.300489
Test - acc:         0.876300 loss:        0.377777
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.899560 loss:        0.295473
Test - acc:         0.875000 loss:        0.375736
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.900380 loss:        0.297942
Test - acc:         0.876200 loss:        0.377384
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.899540 loss:        0.295824
Test - acc:         0.876900 loss:        0.374467
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.900040 loss:        0.295421
Test - acc:         0.876600 loss:        0.372006
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.900720 loss:        0.294258
Test - acc:         0.880300 loss:        0.374364
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.662220 loss:        0.984605
Test - acc:         0.729600 loss:        0.797472
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.728240 loss:        0.799401
Test - acc:         0.746300 loss:        0.746068
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.746280 loss:        0.750753
Test - acc:         0.760800 loss:        0.697200
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.757480 loss:        0.716260
Test - acc:         0.764000 loss:        0.682543
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.766180 loss:        0.694543
Test - acc:         0.773200 loss:        0.667485
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.771080 loss:        0.677758
Test - acc:         0.774400 loss:        0.660315
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.776940 loss:        0.662565
Test - acc:         0.778400 loss:        0.645448
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.779160 loss:        0.652924
Test - acc:         0.781100 loss:        0.633779
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.782400 loss:        0.642140
Test - acc:         0.783600 loss:        0.628867
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.785100 loss:        0.633026
Test - acc:         0.790400 loss:        0.618397
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.787800 loss:        0.626481
Test - acc:         0.786900 loss:        0.616650
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.789420 loss:        0.618739
Test - acc:         0.793100 loss:        0.605625
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.791780 loss:        0.614556
Test - acc:         0.793800 loss:        0.603177
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.793220 loss:        0.606593
Test - acc:         0.795700 loss:        0.608479
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.794600 loss:        0.602806
Test - acc:         0.797400 loss:        0.598849
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.794780 loss:        0.599681
Test - acc:         0.796700 loss:        0.597316
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.798640 loss:        0.590406
Test - acc:         0.797900 loss:        0.594320
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.801000 loss:        0.588089
Test - acc:         0.800700 loss:        0.589926
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.801700 loss:        0.585576
Test - acc:         0.803900 loss:        0.585283
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.801560 loss:        0.584704
Test - acc:         0.805700 loss:        0.581338
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.804540 loss:        0.580630
Test - acc:         0.801800 loss:        0.585545
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.801940 loss:        0.577441
Test - acc:         0.803100 loss:        0.583722
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.805320 loss:        0.573840
Test - acc:         0.803500 loss:        0.577328
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.804020 loss:        0.574377
Test - acc:         0.806000 loss:        0.573840
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.806360 loss:        0.568148
Test - acc:         0.803200 loss:        0.576402
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.807500 loss:        0.566727
Test - acc:         0.809900 loss:        0.569081
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.808160 loss:        0.565199
Test - acc:         0.808800 loss:        0.567409
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.808360 loss:        0.560105
Test - acc:         0.807400 loss:        0.575370
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.809040 loss:        0.560539
Test - acc:         0.807300 loss:        0.570963
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.810360 loss:        0.555930
Test - acc:         0.806300 loss:        0.580339
Sparsity :          0.9990
Wdecay :        0.000500
