Running --model vgg19 --prune_criterion random --seed 43 --prune_freq 32 --prune_rate 0.5 --comment=vgg19_crit=random_pf=32_seed=43 --save_model=pre-finetune/vgg19_random_pf32_s43 --logdir=criterion_experiment_no_bias/vgg19
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "random",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_random_pf32_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.102040 loss:        2.593170
Test - acc:         0.101800 loss:        2.305669
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.102980 loss:        2.302027
Test - acc:         0.108700 loss:        2.299553
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.107760 loss:        2.295210
Test - acc:         0.102100 loss:        2.288999
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.170220 loss:        2.125674
Test - acc:         0.234300 loss:        1.914818
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.283940 loss:        1.804644
Test - acc:         0.265700 loss:        1.923784
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.401960 loss:        1.553583
Test - acc:         0.376900 loss:        1.801617
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.539940 loss:        1.245976
Test - acc:         0.572700 loss:        1.260381
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.632400 loss:        1.036366
Test - acc:         0.586200 loss:        1.257524
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.681460 loss:        0.918227
Test - acc:         0.610400 loss:        1.206541
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.713460 loss:        0.850217
Test - acc:         0.668700 loss:        0.947266
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.739980 loss:        0.792240
Test - acc:         0.681800 loss:        1.044388
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.758800 loss:        0.739895
Test - acc:         0.697800 loss:        0.921571
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.770220 loss:        0.715436
Test - acc:         0.645300 loss:        1.166721
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.777960 loss:        0.693221
Test - acc:         0.740500 loss:        0.807132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.784440 loss:        0.673860
Test - acc:         0.720600 loss:        0.904886
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.789860 loss:        0.663452
Test - acc:         0.728000 loss:        0.842933
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.799320 loss:        0.628781
Test - acc:         0.682700 loss:        0.994196
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.805000 loss:        0.609107
Test - acc:         0.763000 loss:        0.748730
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.807880 loss:        0.599333
Test - acc:         0.706800 loss:        0.948992
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.812460 loss:        0.583509
Test - acc:         0.789700 loss:        0.668061
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.817240 loss:        0.569957
Test - acc:         0.753900 loss:        0.833114
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.819180 loss:        0.563694
Test - acc:         0.770800 loss:        0.724838
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.820740 loss:        0.554781
Test - acc:         0.700100 loss:        0.967560
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.825820 loss:        0.543488
Test - acc:         0.723000 loss:        0.915873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.830920 loss:        0.527130
Test - acc:         0.757800 loss:        0.837092
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.830200 loss:        0.531166
Test - acc:         0.776600 loss:        0.708030
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.832860 loss:        0.517132
Test - acc:         0.768400 loss:        0.819225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.831800 loss:        0.523846
Test - acc:         0.802900 loss:        0.601694
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.838100 loss:        0.499992
Test - acc:         0.759300 loss:        0.806052
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.838440 loss:        0.504548
Test - acc:         0.713300 loss:        0.926746
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.839900 loss:        0.498613
Test - acc:         0.756200 loss:        0.786542
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.839700 loss:        0.498225
Test - acc:         0.805800 loss:        0.605070
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.773120 loss:        0.705832
Test - acc:         0.694000 loss:        0.981913
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.811920 loss:        0.581865
Test - acc:         0.677700 loss:        1.089172
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.822000 loss:        0.553487
Test - acc:         0.799600 loss:        0.635415
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.825420 loss:        0.536816
Test - acc:         0.728500 loss:        0.844539
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.829140 loss:        0.525841
Test - acc:         0.602700 loss:        1.575160
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.832460 loss:        0.520430
Test - acc:         0.601500 loss:        1.848679
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.836260 loss:        0.505867
Test - acc:         0.752500 loss:        0.894478
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.839000 loss:        0.494996
Test - acc:         0.774000 loss:        0.696307
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.839840 loss:        0.495272
Test - acc:         0.678600 loss:        1.102216
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.839640 loss:        0.490439
Test - acc:         0.750000 loss:        0.798253
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.842180 loss:        0.481691
Test - acc:         0.815600 loss:        0.562399
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.842860 loss:        0.484457
Test - acc:         0.759200 loss:        0.779700
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.845020 loss:        0.481085
Test - acc:         0.771100 loss:        0.719233
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.842500 loss:        0.479674
Test - acc:         0.798900 loss:        0.625755
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.847320 loss:        0.468200
Test - acc:         0.784600 loss:        0.702223
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.847360 loss:        0.466322
Test - acc:         0.736800 loss:        0.861694
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.847720 loss:        0.469702
Test - acc:         0.752100 loss:        0.781816
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.848120 loss:        0.466506
Test - acc:         0.791100 loss:        0.627899
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.851640 loss:        0.454543
Test - acc:         0.728500 loss:        0.964310
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.849380 loss:        0.461751
Test - acc:         0.779100 loss:        0.719837
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.850320 loss:        0.457671
Test - acc:         0.782700 loss:        0.677253
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.850080 loss:        0.459539
Test - acc:         0.831700 loss:        0.534999
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.856040 loss:        0.442878
Test - acc:         0.700400 loss:        1.101224
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.853400 loss:        0.447052
Test - acc:         0.753000 loss:        0.777412
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.853100 loss:        0.452285
Test - acc:         0.783000 loss:        0.727748
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.853820 loss:        0.450235
Test - acc:         0.799200 loss:        0.643627
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.855920 loss:        0.439824
Test - acc:         0.846300 loss:        0.460904
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.856000 loss:        0.439163
Test - acc:         0.748100 loss:        0.865280
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.858100 loss:        0.438484
Test - acc:         0.810800 loss:        0.566663
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.857200 loss:        0.436246
Test - acc:         0.802000 loss:        0.613786
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.858600 loss:        0.435637
Test - acc:         0.822000 loss:        0.537782
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.860420 loss:        0.428467
Test - acc:         0.824300 loss:        0.558265
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.754620 loss:        0.742534
Test - acc:         0.694500 loss:        0.988479
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.809520 loss:        0.575731
Test - acc:         0.763600 loss:        0.786834
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.821700 loss:        0.537084
Test - acc:         0.761000 loss:        0.731064
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.830420 loss:        0.507921
Test - acc:         0.812800 loss:        0.568096
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.833200 loss:        0.502816
Test - acc:         0.790100 loss:        0.650869
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.834880 loss:        0.499263
Test - acc:         0.801400 loss:        0.653027
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.837360 loss:        0.490938
Test - acc:         0.796200 loss:        0.596069
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.839940 loss:        0.480026
Test - acc:         0.816400 loss:        0.554994
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.840320 loss:        0.478364
Test - acc:         0.795700 loss:        0.649682
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.844580 loss:        0.465231
Test - acc:         0.752600 loss:        0.761892
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.843040 loss:        0.472410
Test - acc:         0.763400 loss:        0.717895
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.845240 loss:        0.468795
Test - acc:         0.794300 loss:        0.646082
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.843660 loss:        0.472353
Test - acc:         0.818300 loss:        0.562158
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.844500 loss:        0.467066
Test - acc:         0.782400 loss:        0.699624
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.844420 loss:        0.465165
Test - acc:         0.812000 loss:        0.578192
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.843780 loss:        0.465270
Test - acc:         0.816600 loss:        0.538593
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.845640 loss:        0.460900
Test - acc:         0.785200 loss:        0.671394
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.849680 loss:        0.452525
Test - acc:         0.786600 loss:        0.680368
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.849580 loss:        0.451927
Test - acc:         0.814100 loss:        0.553088
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.847960 loss:        0.451606
Test - acc:         0.809100 loss:        0.582333
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.848780 loss:        0.452636
Test - acc:         0.801300 loss:        0.604409
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.851180 loss:        0.445302
Test - acc:         0.779400 loss:        0.699054
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.851720 loss:        0.442107
Test - acc:         0.795500 loss:        0.666108
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.850640 loss:        0.447748
Test - acc:         0.764200 loss:        0.780985
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.853380 loss:        0.444394
Test - acc:         0.769400 loss:        0.740244
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.850860 loss:        0.448111
Test - acc:         0.792800 loss:        0.639902
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.850720 loss:        0.447146
Test - acc:         0.807300 loss:        0.587240
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.850960 loss:        0.442527
Test - acc:         0.788500 loss:        0.680149
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.851660 loss:        0.439942
Test - acc:         0.776400 loss:        0.690171
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.851420 loss:        0.441064
Test - acc:         0.806300 loss:        0.621775
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.851820 loss:        0.444240
Test - acc:         0.824900 loss:        0.522592
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.853440 loss:        0.437686
Test - acc:         0.822200 loss:        0.537251
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.672980 loss:        0.947950
Test - acc:         0.748400 loss:        0.776790
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.771020 loss:        0.673631
Test - acc:         0.716000 loss:        0.878637
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.793420 loss:        0.617118
Test - acc:         0.783400 loss:        0.665344
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.802340 loss:        0.584220
Test - acc:         0.766000 loss:        0.688796
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.804280 loss:        0.580631
Test - acc:         0.778600 loss:        0.694238
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.811000 loss:        0.557065
Test - acc:         0.761400 loss:        0.715469
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.814980 loss:        0.557655
Test - acc:         0.735300 loss:        0.822315
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.816120 loss:        0.540613
Test - acc:         0.773400 loss:        0.683892
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.816820 loss:        0.545607
Test - acc:         0.752500 loss:        0.751143
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.818760 loss:        0.534810
Test - acc:         0.765200 loss:        0.700530
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.820000 loss:        0.531988
Test - acc:         0.806400 loss:        0.563346
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.821460 loss:        0.527388
Test - acc:         0.752900 loss:        0.755142
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.823560 loss:        0.523110
Test - acc:         0.780200 loss:        0.659067
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.822320 loss:        0.525904
Test - acc:         0.760700 loss:        0.736821
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.825800 loss:        0.514167
Test - acc:         0.751700 loss:        0.866657
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.827100 loss:        0.514590
Test - acc:         0.756000 loss:        0.730671
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.825900 loss:        0.514366
Test - acc:         0.771100 loss:        0.711221
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.827420 loss:        0.510038
Test - acc:         0.774900 loss:        0.699899
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.829580 loss:        0.504574
Test - acc:         0.794900 loss:        0.627185
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.827260 loss:        0.506087
Test - acc:         0.787300 loss:        0.635395
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.829440 loss:        0.502600
Test - acc:         0.768600 loss:        0.694457
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.829980 loss:        0.502558
Test - acc:         0.809300 loss:        0.577258
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.830040 loss:        0.504272
Test - acc:         0.747200 loss:        0.799431
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.830800 loss:        0.506645
Test - acc:         0.746400 loss:        0.819146
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.828820 loss:        0.501952
Test - acc:         0.788300 loss:        0.653274
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.831040 loss:        0.498231
Test - acc:         0.782700 loss:        0.636781
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.833660 loss:        0.492694
Test - acc:         0.785600 loss:        0.645627
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.834700 loss:        0.493602
Test - acc:         0.732700 loss:        0.847993
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.832440 loss:        0.490968
Test - acc:         0.745100 loss:        0.834479
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.836600 loss:        0.486213
Test - acc:         0.681600 loss:        0.983607
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.832860 loss:        0.495434
Test - acc:         0.768200 loss:        0.712034
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.835460 loss:        0.485203
Test - acc:         0.776700 loss:        0.682985
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.478780 loss:        1.411307
Test - acc:         0.584200 loss:        1.247482
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.638160 loss:        1.018359
Test - acc:         0.589900 loss:        1.180008
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.685800 loss:        0.894403
Test - acc:         0.652500 loss:        1.033318
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.714240 loss:        0.825504
Test - acc:         0.688700 loss:        0.927548
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.731700 loss:        0.776146
Test - acc:         0.613700 loss:        1.168466
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.744620 loss:        0.747209
Test - acc:         0.697800 loss:        0.870366
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.751460 loss:        0.722953
Test - acc:         0.689000 loss:        0.938264
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.756820 loss:        0.711964
Test - acc:         0.722500 loss:        0.818573
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.760820 loss:        0.698000
Test - acc:         0.724900 loss:        0.819117
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.767420 loss:        0.682203
Test - acc:         0.710800 loss:        0.881050
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.767780 loss:        0.678471
Test - acc:         0.619300 loss:        1.258005
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.770780 loss:        0.668379
Test - acc:         0.743400 loss:        0.764892
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.773600 loss:        0.659425
Test - acc:         0.661500 loss:        1.017633
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.775940 loss:        0.654637
Test - acc:         0.716900 loss:        0.843151
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.777680 loss:        0.647702
Test - acc:         0.757400 loss:        0.738129
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.777220 loss:        0.644348
Test - acc:         0.741000 loss:        0.801440
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.783160 loss:        0.635296
Test - acc:         0.715700 loss:        0.863764
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.780760 loss:        0.635941
Test - acc:         0.725500 loss:        0.785902
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.782640 loss:        0.633283
Test - acc:         0.585500 loss:        1.313228
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.786040 loss:        0.626664
Test - acc:         0.635200 loss:        1.316340
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.786520 loss:        0.619641
Test - acc:         0.731400 loss:        0.837462
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.787100 loss:        0.623459
Test - acc:         0.737500 loss:        0.763219
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.841160 loss:        0.461384
Test - acc:         0.844400 loss:        0.447401
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.859880 loss:        0.408353
Test - acc:         0.848800 loss:        0.434961
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.869180 loss:        0.384629
Test - acc:         0.852100 loss:        0.419461
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.873680 loss:        0.369706
Test - acc:         0.858600 loss:        0.413784
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.876060 loss:        0.359305
Test - acc:         0.856300 loss:        0.416443
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.879600 loss:        0.348176
Test - acc:         0.863600 loss:        0.402439
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.884740 loss:        0.336926
Test - acc:         0.860800 loss:        0.407116
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.886640 loss:        0.327362
Test - acc:         0.862900 loss:        0.404987
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.888320 loss:        0.324666
Test - acc:         0.860200 loss:        0.416756
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.889720 loss:        0.316510
Test - acc:         0.862300 loss:        0.402873
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.383640 loss:        1.673304
Test - acc:         0.506000 loss:        1.355187
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.544460 loss:        1.267881
Test - acc:         0.537600 loss:        1.377869
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.614840 loss:        1.093591
Test - acc:         0.606500 loss:        1.127493
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.650160 loss:        0.992682
Test - acc:         0.633900 loss:        1.022215
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.676060 loss:        0.924934
Test - acc:         0.686500 loss:        0.908693
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.696360 loss:        0.864858
Test - acc:         0.696700 loss:        0.858356
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.711200 loss:        0.821034
Test - acc:         0.711300 loss:        0.842288
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.725580 loss:        0.787092
Test - acc:         0.703200 loss:        0.845495
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.737240 loss:        0.752207
Test - acc:         0.702200 loss:        0.870769
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.746760 loss:        0.729849
Test - acc:         0.741800 loss:        0.754158
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.752120 loss:        0.708526
Test - acc:         0.703800 loss:        0.876817
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.760120 loss:        0.687591
Test - acc:         0.751600 loss:        0.712693
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.766340 loss:        0.669690
Test - acc:         0.742700 loss:        0.742832
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.775360 loss:        0.650150
Test - acc:         0.742300 loss:        0.756703
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.775880 loss:        0.643594
Test - acc:         0.757700 loss:        0.695259
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.780380 loss:        0.632730
Test - acc:         0.770900 loss:        0.668928
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.784880 loss:        0.621934
Test - acc:         0.769000 loss:        0.684034
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.790320 loss:        0.609351
Test - acc:         0.762900 loss:        0.674356
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.792300 loss:        0.594812
Test - acc:         0.779400 loss:        0.648778
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.795140 loss:        0.588201
Test - acc:         0.776700 loss:        0.660535
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.800340 loss:        0.580416
Test - acc:         0.780300 loss:        0.636539
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.798940 loss:        0.573065
Test - acc:         0.784100 loss:        0.635667
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.803420 loss:        0.564768
Test - acc:         0.785900 loss:        0.632266
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.806680 loss:        0.558821
Test - acc:         0.732500 loss:        0.789408
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.807900 loss:        0.553621
Test - acc:         0.775400 loss:        0.658433
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.808920 loss:        0.551194
Test - acc:         0.780700 loss:        0.658155
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.810220 loss:        0.546270
Test - acc:         0.754000 loss:        0.738977
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.815580 loss:        0.535765
Test - acc:         0.766000 loss:        0.683180
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.815440 loss:        0.531627
Test - acc:         0.780500 loss:        0.654481
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.816260 loss:        0.530986
Test - acc:         0.790200 loss:        0.610416
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.819720 loss:        0.521970
Test - acc:         0.790800 loss:        0.621917
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.818320 loss:        0.524856
Test - acc:         0.789400 loss:        0.617269
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.345060 loss:        1.789385
Test - acc:         0.378600 loss:        1.678988
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.477600 loss:        1.439803
Test - acc:         0.412200 loss:        1.661602
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.530120 loss:        1.308540
Test - acc:         0.510500 loss:        1.360546
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.565940 loss:        1.218175
Test - acc:         0.523500 loss:        1.347760
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.587260 loss:        1.165790
Test - acc:         0.572800 loss:        1.204936
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.599220 loss:        1.138096
Test - acc:         0.582300 loss:        1.198553
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.619100 loss:        1.084753
Test - acc:         0.568000 loss:        1.204051
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.629040 loss:        1.054474
Test - acc:         0.600500 loss:        1.121101
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.640560 loss:        1.015303
Test - acc:         0.661300 loss:        0.979592
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.649260 loss:        0.993117
Test - acc:         0.662700 loss:        0.974057
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.659180 loss:        0.966526
Test - acc:         0.648400 loss:        1.015351
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.664900 loss:        0.953351
Test - acc:         0.673100 loss:        0.938283
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.670180 loss:        0.937170
Test - acc:         0.644800 loss:        0.995987
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.678100 loss:        0.917704
Test - acc:         0.669600 loss:        0.940238
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.684580 loss:        0.897508
Test - acc:         0.678300 loss:        0.933258
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.689620 loss:        0.886149
Test - acc:         0.700700 loss:        0.861595
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.692040 loss:        0.878127
Test - acc:         0.693300 loss:        0.880456
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.696980 loss:        0.865465
Test - acc:         0.690000 loss:        0.901694
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.698400 loss:        0.855494
Test - acc:         0.687400 loss:        0.900079
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.709720 loss:        0.834617
Test - acc:         0.679300 loss:        0.928328
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.707000 loss:        0.838074
Test - acc:         0.715600 loss:        0.817526
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.714000 loss:        0.817995
Test - acc:         0.695100 loss:        0.880271
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.714420 loss:        0.815569
Test - acc:         0.687800 loss:        0.918994
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.717200 loss:        0.808845
Test - acc:         0.701800 loss:        0.862651
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.717820 loss:        0.804822
Test - acc:         0.715400 loss:        0.821592
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.723720 loss:        0.797593
Test - acc:         0.706500 loss:        0.861146
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.727000 loss:        0.786860
Test - acc:         0.701400 loss:        0.873547
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.726360 loss:        0.785742
Test - acc:         0.697000 loss:        0.876460
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.728880 loss:        0.782349
Test - acc:         0.733000 loss:        0.775966
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.732440 loss:        0.773220
Test - acc:         0.714200 loss:        0.825970
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.730100 loss:        0.772561
Test - acc:         0.729900 loss:        0.784750
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.736680 loss:        0.757049
Test - acc:         0.721000 loss:        0.806825
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.235060 loss:        2.062186
Test - acc:         0.219700 loss:        2.168981
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.335260 loss:        1.792119
Test - acc:         0.207200 loss:        2.287354
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.377420 loss:        1.694209
Test - acc:         0.198800 loss:        2.564594
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.402060 loss:        1.632983
Test - acc:         0.284700 loss:        2.041042
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.435260 loss:        1.548523
Test - acc:         0.128300 loss:        3.091281
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.462420 loss:        1.491569
Test - acc:         0.257500 loss:        2.445275
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.514460 loss:        1.377898
Test - acc:         0.313800 loss:        2.233899
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.536720 loss:        1.324415
Test - acc:         0.397900 loss:        1.862868
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.546900 loss:        1.297617
Test - acc:         0.322400 loss:        1.949822
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.558460 loss:        1.264395
Test - acc:         0.432200 loss:        1.760874
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.569220 loss:        1.240060
Test - acc:         0.328900 loss:        2.076523
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.573660 loss:        1.228365
Test - acc:         0.272900 loss:        2.514418
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.580280 loss:        1.211271
Test - acc:         0.349900 loss:        2.141376
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.582860 loss:        1.201011
Test - acc:         0.443900 loss:        1.641193
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.591040 loss:        1.179191
Test - acc:         0.424600 loss:        1.758747
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.591440 loss:        1.181507
Test - acc:         0.569000 loss:        1.243536
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.591860 loss:        1.165341
Test - acc:         0.553500 loss:        1.302981
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.602180 loss:        1.147234
Test - acc:         0.495000 loss:        1.587250
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.608680 loss:        1.135739
Test - acc:         0.604400 loss:        1.129533
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.609620 loss:        1.125379
Test - acc:         0.549700 loss:        1.320301
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.610680 loss:        1.125179
Test - acc:         0.578000 loss:        1.218533
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.615260 loss:        1.112635
Test - acc:         0.534500 loss:        1.365439
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.615480 loss:        1.105883
Test - acc:         0.562300 loss:        1.281548
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.619140 loss:        1.095451
Test - acc:         0.622000 loss:        1.091705
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.623660 loss:        1.091475
Test - acc:         0.620200 loss:        1.115047
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.624880 loss:        1.083385
Test - acc:         0.630300 loss:        1.075385
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.650700 loss:        1.011884
Test - acc:         0.661100 loss:        0.980376
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.657920 loss:        0.987834
Test - acc:         0.671000 loss:        0.957969
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.664260 loss:        0.974754
Test - acc:         0.674100 loss:        0.946891
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.661640 loss:        0.972785
Test - acc:         0.678900 loss:        0.942222
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.666940 loss:        0.966270
Test - acc:         0.677100 loss:        0.948348
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.666500 loss:        0.960476
Test - acc:         0.675800 loss:        0.941924
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.101040 loss:        2.955601
Test - acc:         0.100000 loss:        2.483488
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.424390
Test - acc:         0.100000 loss:        2.380795
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.356888
Test - acc:         0.100000 loss:        2.338754
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.327617
Test - acc:         0.100000 loss:        2.319471
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.099960 loss:        2.314281
Test - acc:         0.100000 loss:        2.311026
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.099480 loss:        2.307945
Test - acc:         0.100000 loss:        2.306842
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.100480 loss:        2.305342
Test - acc:         0.100000 loss:        2.304234
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.099040 loss:        2.303953
Test - acc:         0.100000 loss:        2.304691
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.100660 loss:        2.303382
Test - acc:         0.100000 loss:        2.303368
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.098660 loss:        2.302921
Test - acc:         0.100000 loss:        2.302858
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.098540 loss:        2.302912
Test - acc:         0.100000 loss:        2.302743
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.100380 loss:        2.302805
Test - acc:         0.100000 loss:        2.302911
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.099300 loss:        2.302777
Test - acc:         0.100000 loss:        2.302637
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.097220 loss:        2.302844
Test - acc:         0.100000 loss:        2.302638
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.098360 loss:        2.302831
Test - acc:         0.100000 loss:        2.302643
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.098820 loss:        2.302789
Test - acc:         0.100000 loss:        2.302729
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.098240 loss:        2.302778
Test - acc:         0.100000 loss:        2.302732
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.099780 loss:        2.302766
Test - acc:         0.100000 loss:        2.302646
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.097640 loss:        2.302780
Test - acc:         0.100000 loss:        2.302721
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.096340 loss:        2.302823
Test - acc:         0.100000 loss:        2.302601
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.098600 loss:        2.302826
Test - acc:         0.100000 loss:        2.302619
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.098340 loss:        2.302794
Test - acc:         0.100000 loss:        2.302640
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.097820 loss:        2.302782
Test - acc:         0.100000 loss:        2.302702
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.101840 loss:        2.302719
Test - acc:         0.100000 loss:        2.302817
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.097640 loss:        2.302860
Test - acc:         0.100000 loss:        2.302629
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.099940 loss:        2.302802
Test - acc:         0.100000 loss:        2.302665
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.098120 loss:        2.302814
Test - acc:         0.100000 loss:        2.302620
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.097380 loss:        2.302803
Test - acc:         0.100000 loss:        2.302622
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.098460 loss:        2.302823
Test - acc:         0.100000 loss:        2.302727
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.097900 loss:        2.302828
Test - acc:         0.100000 loss:        2.302742
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.098160 loss:        2.302833
Test - acc:         0.100000 loss:        2.302618
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.098500 loss:        2.302777
Test - acc:         0.100000 loss:        2.302683
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.545142
Test - acc:         0.100000 loss:        2.432883
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.380896
Test - acc:         0.100000 loss:        2.342784
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.329012
Test - acc:         0.100000 loss:        2.320826
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.315871
Test - acc:         0.100000 loss:        2.311788
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.309265
Test - acc:         0.100000 loss:        2.307119
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.305891
Test - acc:         0.100000 loss:        2.304836
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.304229
Test - acc:         0.100000 loss:        2.303629
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.303422
Test - acc:         0.100000 loss:        2.303079
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.302995
Test - acc:         0.100000 loss:        2.302813
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.099980 loss:        2.302820
Test - acc:         0.100000 loss:        2.302707
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.099640 loss:        2.302714
Test - acc:         0.100000 loss:        2.302676
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.100520 loss:        2.302705
Test - acc:         0.100000 loss:        2.302611
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.098520 loss:        2.302677
Test - acc:         0.100000 loss:        2.302599
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.098280 loss:        2.302664
Test - acc:         0.100000 loss:        2.302641
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.098100 loss:        2.302655
Test - acc:         0.100000 loss:        2.302589
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.099380 loss:        2.302637
Test - acc:         0.100000 loss:        2.302606
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.098120 loss:        2.302670
Test - acc:         0.100000 loss:        2.302595
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.096760 loss:        2.302645
Test - acc:         0.100000 loss:        2.302586
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.098840 loss:        2.302662
Test - acc:         0.100000 loss:        2.302594
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.097840 loss:        2.302644
Test - acc:         0.100000 loss:        2.302589
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.096660 loss:        2.302643
Test - acc:         0.100000 loss:        2.302595
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.098460 loss:        2.302675
Test - acc:         0.100000 loss:        2.302586
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.099900 loss:        2.302633
Test - acc:         0.100000 loss:        2.302587
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.098120 loss:        2.302643
Test - acc:         0.100000 loss:        2.302600
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.099460 loss:        2.302635
Test - acc:         0.100000 loss:        2.302603
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.098600 loss:        2.302649
Test - acc:         0.100000 loss:        2.302588
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.097760 loss:        2.302650
Test - acc:         0.100000 loss:        2.302635
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.098380 loss:        2.302667
Test - acc:         0.100000 loss:        2.302593
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.096140 loss:        2.302640
Test - acc:         0.100000 loss:        2.302586
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.099760 loss:        2.302645
Test - acc:         0.100000 loss:        2.302587
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.097040 loss:        2.302641
Test - acc:         0.100000 loss:        2.302595
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.097580 loss:        2.302663
Test - acc:         0.100000 loss:        2.302591
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.305502
Test - acc:         0.100000 loss:        2.304357
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.303831
Test - acc:         0.100000 loss:        2.303356
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.303139
Test - acc:         0.100000 loss:        2.302918
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.302844
Test - acc:         0.100000 loss:        2.302737
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.302718
Test - acc:         0.100000 loss:        2.302650
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.302663
Test - acc:         0.100000 loss:        2.302614
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.302637
Test - acc:         0.100000 loss:        2.302600
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.302629
Test - acc:         0.100000 loss:        2.302591
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.302622
Test - acc:         0.100000 loss:        2.302589
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.099280 loss:        2.302622
Test - acc:         0.100000 loss:        2.302586
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.098600 loss:        2.302617
Test - acc:         0.100000 loss:        2.302587
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.097660 loss:        2.302617
Test - acc:         0.100000 loss:        2.302586
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.099040 loss:        2.302618
Test - acc:         0.100000 loss:        2.302585
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.096380 loss:        2.302614
Test - acc:         0.100000 loss:        2.302585
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.097420 loss:        2.302618
Test - acc:         0.100000 loss:        2.302585
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.095960 loss:        2.302618
Test - acc:         0.100000 loss:        2.302585
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.097960 loss:        2.302617
Test - acc:         0.100000 loss:        2.302585
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.097000 loss:        2.302616
Test - acc:         0.100000 loss:        2.302586
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.098460 loss:        2.302619
Test - acc:         0.100000 loss:        2.302585
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.097100 loss:        2.302616
Test - acc:         0.100000 loss:        2.302585
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.097800 loss:        2.302621
Test - acc:         0.100000 loss:        2.302585
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.095840 loss:        2.302613
Test - acc:         0.100000 loss:        2.302586
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.097100 loss:        2.302620
Test - acc:         0.100000 loss:        2.302586
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.096940 loss:        2.302620
Test - acc:         0.100000 loss:        2.302585
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.098220 loss:        2.302615
Test - acc:         0.100000 loss:        2.302586
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.094520 loss:        2.302622
Test - acc:         0.100000 loss:        2.302585
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.098180 loss:        2.302617
Test - acc:         0.100000 loss:        2.302586
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.097060 loss:        2.302617
Test - acc:         0.100000 loss:        2.302586
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.096480 loss:        2.302615
Test - acc:         0.100000 loss:        2.302586
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.098140 loss:        2.302616
Test - acc:         0.100000 loss:        2.302585
Sparsity :          0.9990
Wdecay :        0.000500
