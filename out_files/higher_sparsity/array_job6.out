Running --model resnet18 --prune_criterion global_magnitude --seed 43 --prune_freq 32 --prune_rate 0.5 --comment=resnet18_crit=global_magnitude_pf=32_seed=43 --save_model=pre-finetune/resnet18_global_magnitude_pf32_s43 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "global_magnitude",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_global_magnitude_pf32_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.348320 loss:        1.810961
Test - acc:         0.439400 loss:        1.484592
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.526380 loss:        1.296187
Test - acc:         0.587300 loss:        1.162469
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.635500 loss:        1.021202
Test - acc:         0.626700 loss:        1.042242
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.702640 loss:        0.843171
Test - acc:         0.711600 loss:        0.860480
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.752060 loss:        0.713263
Test - acc:         0.752700 loss:        0.721025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.783660 loss:        0.623389
Test - acc:         0.733700 loss:        0.786503
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.801860 loss:        0.570900
Test - acc:         0.757000 loss:        0.762860
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.814640 loss:        0.539573
Test - acc:         0.704900 loss:        0.941443
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.825480 loss:        0.505140
Test - acc:         0.784800 loss:        0.633475
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.830340 loss:        0.493756
Test - acc:         0.779500 loss:        0.657745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.838340 loss:        0.474250
Test - acc:         0.804200 loss:        0.593576
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.844280 loss:        0.458721
Test - acc:         0.834400 loss:        0.502315
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.846500 loss:        0.449426
Test - acc:         0.817100 loss:        0.554424
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.847180 loss:        0.443512
Test - acc:         0.812100 loss:        0.573417
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.853420 loss:        0.425940
Test - acc:         0.804500 loss:        0.581330
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.857080 loss:        0.415865
Test - acc:         0.827800 loss:        0.517806
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.861940 loss:        0.404304
Test - acc:         0.815300 loss:        0.557021
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.863300 loss:        0.400604
Test - acc:         0.797600 loss:        0.647381
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.864700 loss:        0.398190
Test - acc:         0.820300 loss:        0.561645
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.869280 loss:        0.387392
Test - acc:         0.815000 loss:        0.584442
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.870620 loss:        0.382326
Test - acc:         0.818900 loss:        0.536924
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870120 loss:        0.380016
Test - acc:         0.783000 loss:        0.719661
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.869920 loss:        0.379140
Test - acc:         0.822700 loss:        0.551593
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.874520 loss:        0.374324
Test - acc:         0.836900 loss:        0.490705
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.874720 loss:        0.370254
Test - acc:         0.827900 loss:        0.513918
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.876980 loss:        0.362354
Test - acc:         0.850800 loss:        0.456385
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.873500 loss:        0.368452
Test - acc:         0.823200 loss:        0.540426
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.876640 loss:        0.360789
Test - acc:         0.848400 loss:        0.446691
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.876560 loss:        0.360781
Test - acc:         0.803700 loss:        0.617252
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.879380 loss:        0.354437
Test - acc:         0.703300 loss:        1.070219
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.879920 loss:        0.350535
Test - acc:         0.835700 loss:        0.514332
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.877980 loss:        0.357668
Test - acc:         0.852200 loss:        0.417868
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.888780 loss:        0.324647
Test - acc:         0.829900 loss:        0.510132
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.889180 loss:        0.324182
Test - acc:         0.747500 loss:        0.897443
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.888200 loss:        0.327397
Test - acc:         0.832400 loss:        0.525111
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.888900 loss:        0.325737
Test - acc:         0.816900 loss:        0.572499
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.889840 loss:        0.324237
Test - acc:         0.788100 loss:        0.662711
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.888700 loss:        0.325051
Test - acc:         0.848400 loss:        0.450679
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.892480 loss:        0.319040
Test - acc:         0.815000 loss:        0.578478
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.888580 loss:        0.324116
Test - acc:         0.823900 loss:        0.517175
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.890460 loss:        0.321604
Test - acc:         0.842300 loss:        0.486451
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.891640 loss:        0.317102
Test - acc:         0.835100 loss:        0.511859
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.889840 loss:        0.322217
Test - acc:         0.844300 loss:        0.492476
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.891780 loss:        0.314699
Test - acc:         0.863300 loss:        0.414877
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.891300 loss:        0.315961
Test - acc:         0.837600 loss:        0.495656
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.892640 loss:        0.312831
Test - acc:         0.719800 loss:        0.982036
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.890800 loss:        0.316265
Test - acc:         0.861900 loss:        0.419590
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.893140 loss:        0.314166
Test - acc:         0.875600 loss:        0.382064
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.893920 loss:        0.312220
Test - acc:         0.847400 loss:        0.482028
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.892440 loss:        0.309825
Test - acc:         0.822400 loss:        0.559441
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.892060 loss:        0.311934
Test - acc:         0.843000 loss:        0.491681
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.893780 loss:        0.312493
Test - acc:         0.807800 loss:        0.610157
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.894460 loss:        0.309498
Test - acc:         0.849400 loss:        0.480667
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.895000 loss:        0.306139
Test - acc:         0.804000 loss:        0.592523
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.892620 loss:        0.311369
Test - acc:         0.825400 loss:        0.541198
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.895980 loss:        0.307083
Test - acc:         0.861400 loss:        0.415611
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.893320 loss:        0.312332
Test - acc:         0.786800 loss:        0.683276
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.893140 loss:        0.312610
Test - acc:         0.840000 loss:        0.483315
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.896420 loss:        0.301863
Test - acc:         0.865200 loss:        0.411449
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.894840 loss:        0.308165
Test - acc:         0.831400 loss:        0.506888
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.892980 loss:        0.310953
Test - acc:         0.796200 loss:        0.636939
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.895260 loss:        0.305173
Test - acc:         0.832900 loss:        0.520408
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.894140 loss:        0.307915
Test - acc:         0.849900 loss:        0.466161
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.894580 loss:        0.307068
Test - acc:         0.867200 loss:        0.424172
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.908800 loss:        0.265514
Test - acc:         0.845700 loss:        0.490286
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.906500 loss:        0.273623
Test - acc:         0.869100 loss:        0.402264
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.905140 loss:        0.277907
Test - acc:         0.876800 loss:        0.368744
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.904560 loss:        0.274076
Test - acc:         0.866500 loss:        0.403668
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.905440 loss:        0.278496
Test - acc:         0.834800 loss:        0.508601
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.903420 loss:        0.280323
Test - acc:         0.855500 loss:        0.470609
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.903660 loss:        0.278944
Test - acc:         0.882300 loss:        0.360367
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.905520 loss:        0.277329
Test - acc:         0.850000 loss:        0.463701
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.903860 loss:        0.280149
Test - acc:         0.847400 loss:        0.504303
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.905100 loss:        0.276090
Test - acc:         0.841300 loss:        0.532940
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.904180 loss:        0.275822
Test - acc:         0.863600 loss:        0.413835
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.903920 loss:        0.278346
Test - acc:         0.859000 loss:        0.442886
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.906880 loss:        0.275053
Test - acc:         0.855100 loss:        0.447052
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.906640 loss:        0.271880
Test - acc:         0.872900 loss:        0.392970
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.902380 loss:        0.280755
Test - acc:         0.851200 loss:        0.460487
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.905120 loss:        0.278943
Test - acc:         0.852900 loss:        0.453061
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.907460 loss:        0.272471
Test - acc:         0.856700 loss:        0.446068
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.905460 loss:        0.273656
Test - acc:         0.858200 loss:        0.443652
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.905980 loss:        0.276603
Test - acc:         0.871800 loss:        0.383796
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.907140 loss:        0.269994
Test - acc:         0.880300 loss:        0.361227
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.905520 loss:        0.278432
Test - acc:         0.878400 loss:        0.376825
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.907160 loss:        0.273503
Test - acc:         0.816300 loss:        0.603857
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.906460 loss:        0.271603
Test - acc:         0.832100 loss:        0.528165
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.904720 loss:        0.280794
Test - acc:         0.857200 loss:        0.442170
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.904440 loss:        0.278893
Test - acc:         0.818800 loss:        0.582345
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.903960 loss:        0.280631
Test - acc:         0.858200 loss:        0.422985
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.907000 loss:        0.270132
Test - acc:         0.831000 loss:        0.541739
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.905600 loss:        0.276182
Test - acc:         0.871200 loss:        0.380259
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.906560 loss:        0.273456
Test - acc:         0.837700 loss:        0.515397
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.905620 loss:        0.275376
Test - acc:         0.849800 loss:        0.469903
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.904740 loss:        0.277990
Test - acc:         0.856200 loss:        0.432299
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.905760 loss:        0.272889
Test - acc:         0.834100 loss:        0.494356
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.921400 loss:        0.233124
Test - acc:         0.867900 loss:        0.402227
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.916840 loss:        0.246137
Test - acc:         0.863000 loss:        0.412006
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.915140 loss:        0.242367
Test - acc:         0.874700 loss:        0.383700
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.914060 loss:        0.247444
Test - acc:         0.846300 loss:        0.445858
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.914380 loss:        0.247990
Test - acc:         0.858600 loss:        0.433220
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.914120 loss:        0.248782
Test - acc:         0.850400 loss:        0.442869
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.915620 loss:        0.246988
Test - acc:         0.835100 loss:        0.528657
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.912280 loss:        0.254047
Test - acc:         0.818700 loss:        0.618595
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.914240 loss:        0.250056
Test - acc:         0.851500 loss:        0.454090
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.914600 loss:        0.249117
Test - acc:         0.871700 loss:        0.386120
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.913240 loss:        0.253047
Test - acc:         0.862000 loss:        0.410509
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.914400 loss:        0.248558
Test - acc:         0.841200 loss:        0.509090
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.915300 loss:        0.247236
Test - acc:         0.868600 loss:        0.403826
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.915500 loss:        0.244927
Test - acc:         0.857300 loss:        0.438430
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.914080 loss:        0.251947
Test - acc:         0.849100 loss:        0.470366
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.914540 loss:        0.251470
Test - acc:         0.851800 loss:        0.447218
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.912480 loss:        0.253738
Test - acc:         0.873700 loss:        0.389341
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.911980 loss:        0.252597
Test - acc:         0.862700 loss:        0.421139
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.914740 loss:        0.250123
Test - acc:         0.865600 loss:        0.408089
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.913100 loss:        0.250175
Test - acc:         0.871300 loss:        0.402676
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.912500 loss:        0.254851
Test - acc:         0.862700 loss:        0.416452
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.912300 loss:        0.252196
Test - acc:         0.876100 loss:        0.380034
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.913920 loss:        0.250739
Test - acc:         0.864000 loss:        0.397933
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.913500 loss:        0.252289
Test - acc:         0.842500 loss:        0.494927
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.913920 loss:        0.248978
Test - acc:         0.865000 loss:        0.413269
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.914200 loss:        0.249362
Test - acc:         0.872900 loss:        0.386849
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.913040 loss:        0.250875
Test - acc:         0.863700 loss:        0.429839
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.913600 loss:        0.251525
Test - acc:         0.877900 loss:        0.362611
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.915380 loss:        0.246806
Test - acc:         0.873500 loss:        0.396710
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.913000 loss:        0.253373
Test - acc:         0.873700 loss:        0.399047
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.913300 loss:        0.249648
Test - acc:         0.842000 loss:        0.496202
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.912880 loss:        0.252604
Test - acc:         0.853100 loss:        0.474565
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.922740 loss:        0.222397
Test - acc:         0.880800 loss:        0.377056
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.923360 loss:        0.222212
Test - acc:         0.866400 loss:        0.405519
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.920280 loss:        0.231951
Test - acc:         0.825700 loss:        0.564018
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.919020 loss:        0.231633
Test - acc:         0.811200 loss:        0.678848
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.917020 loss:        0.237980
Test - acc:         0.843000 loss:        0.488851
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.922440 loss:        0.230139
Test - acc:         0.880700 loss:        0.364063
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.919100 loss:        0.234208
Test - acc:         0.829000 loss:        0.553689
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.920240 loss:        0.231854
Test - acc:         0.860100 loss:        0.426410
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.919940 loss:        0.233302
Test - acc:         0.868000 loss:        0.424322
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.918940 loss:        0.234242
Test - acc:         0.837500 loss:        0.546001
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.920280 loss:        0.232754
Test - acc:         0.869900 loss:        0.393270
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.920320 loss:        0.232564
Test - acc:         0.873800 loss:        0.384856
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.920560 loss:        0.233822
Test - acc:         0.829800 loss:        0.564598
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.917880 loss:        0.239672
Test - acc:         0.879900 loss:        0.378967
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.919840 loss:        0.235212
Test - acc:         0.877600 loss:        0.367787
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.918500 loss:        0.235577
Test - acc:         0.853500 loss:        0.470022
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.918100 loss:        0.234874
Test - acc:         0.864000 loss:        0.415443
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.919320 loss:        0.233042
Test - acc:         0.888100 loss:        0.323791
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.920700 loss:        0.232300
Test - acc:         0.873300 loss:        0.403336
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.918020 loss:        0.234904
Test - acc:         0.887400 loss:        0.338334
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.917920 loss:        0.236934
Test - acc:         0.872300 loss:        0.394049
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.919920 loss:        0.232261
Test - acc:         0.873200 loss:        0.389727
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.959080 loss:        0.123708
Test - acc:         0.934200 loss:        0.190941
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.970940 loss:        0.091197
Test - acc:         0.937600 loss:        0.183513
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.974720 loss:        0.076269
Test - acc:         0.938800 loss:        0.180314
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.977280 loss:        0.068948
Test - acc:         0.938600 loss:        0.184083
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.979980 loss:        0.062212
Test - acc:         0.941800 loss:        0.179897
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.982620 loss:        0.055266
Test - acc:         0.941800 loss:        0.179852
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.984980 loss:        0.048343
Test - acc:         0.940200 loss:        0.187201
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.985700 loss:        0.046058
Test - acc:         0.940800 loss:        0.184154
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.987080 loss:        0.042255
Test - acc:         0.941800 loss:        0.178262
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.988240 loss:        0.038978
Test - acc:         0.941100 loss:        0.195580
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.986360 loss:        0.046181
Test - acc:         0.940200 loss:        0.189907
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.988540 loss:        0.040816
Test - acc:         0.941800 loss:        0.187557
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.988840 loss:        0.037962
Test - acc:         0.943400 loss:        0.190458
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.989960 loss:        0.035212
Test - acc:         0.941700 loss:        0.194765
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.990080 loss:        0.034267
Test - acc:         0.941200 loss:        0.197000
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.990100 loss:        0.033324
Test - acc:         0.937100 loss:        0.200053
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.990680 loss:        0.032133
Test - acc:         0.941700 loss:        0.196905
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.990640 loss:        0.031839
Test - acc:         0.939200 loss:        0.205169
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.991040 loss:        0.030829
Test - acc:         0.941100 loss:        0.198334
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.992180 loss:        0.028003
Test - acc:         0.943100 loss:        0.203389
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.991200 loss:        0.029084
Test - acc:         0.942800 loss:        0.198934
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.990780 loss:        0.029908
Test - acc:         0.942600 loss:        0.201050
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.991520 loss:        0.028933
Test - acc:         0.940500 loss:        0.210909
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.993220 loss:        0.025256
Test - acc:         0.941100 loss:        0.205811
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.993000 loss:        0.025458
Test - acc:         0.941500 loss:        0.207797
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.992220 loss:        0.027118
Test - acc:         0.939400 loss:        0.209934
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.991720 loss:        0.028926
Test - acc:         0.942600 loss:        0.204147
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.991300 loss:        0.029752
Test - acc:         0.938500 loss:        0.216870
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.992020 loss:        0.027182
Test - acc:         0.940200 loss:        0.214300
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.992100 loss:        0.028294
Test - acc:         0.941400 loss:        0.209456
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.992360 loss:        0.028253
Test - acc:         0.937000 loss:        0.228012
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.990060 loss:        0.031921
Test - acc:         0.934000 loss:        0.235684
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.989600 loss:        0.032800
Test - acc:         0.934300 loss:        0.243625
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.990120 loss:        0.032630
Test - acc:         0.935800 loss:        0.232874
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.989600 loss:        0.033613
Test - acc:         0.932500 loss:        0.256206
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.989940 loss:        0.033588
Test - acc:         0.936700 loss:        0.228625
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.989760 loss:        0.034033
Test - acc:         0.934700 loss:        0.247869
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.989600 loss:        0.034084
Test - acc:         0.933100 loss:        0.225316
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.988840 loss:        0.037019
Test - acc:         0.933900 loss:        0.228777
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.988600 loss:        0.036767
Test - acc:         0.934200 loss:        0.231515
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.987020 loss:        0.040650
Test - acc:         0.930200 loss:        0.232728
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.988320 loss:        0.038616
Test - acc:         0.935400 loss:        0.231783
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.967460 loss:        0.097895
Test - acc:         0.923800 loss:        0.262628
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.974040 loss:        0.078807
Test - acc:         0.919400 loss:        0.265142
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.975500 loss:        0.072902
Test - acc:         0.924000 loss:        0.259704
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.977580 loss:        0.067947
Test - acc:         0.921000 loss:        0.267875
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.975060 loss:        0.074634
Test - acc:         0.925500 loss:        0.251127
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.978680 loss:        0.065737
Test - acc:         0.926000 loss:        0.259399
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.977660 loss:        0.067629
Test - acc:         0.926800 loss:        0.251885
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.980120 loss:        0.061133
Test - acc:         0.927300 loss:        0.253660
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.979400 loss:        0.061661
Test - acc:         0.925300 loss:        0.269134
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.979880 loss:        0.060482
Test - acc:         0.925300 loss:        0.266641
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.981340 loss:        0.057306
Test - acc:         0.926900 loss:        0.265072
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.061282
Test - acc:         0.920900 loss:        0.288384
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.979920 loss:        0.061034
Test - acc:         0.923500 loss:        0.270015
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.980520 loss:        0.060995
Test - acc:         0.925500 loss:        0.255065
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.981540 loss:        0.056164
Test - acc:         0.924300 loss:        0.269297
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.059068
Test - acc:         0.919600 loss:        0.287267
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.982340 loss:        0.054372
Test - acc:         0.924700 loss:        0.270833
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.979720 loss:        0.060914
Test - acc:         0.912800 loss:        0.315750
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.979560 loss:        0.061426
Test - acc:         0.918900 loss:        0.272490
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.981480 loss:        0.055909
Test - acc:         0.924100 loss:        0.271000
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.981140 loss:        0.056828
Test - acc:         0.924300 loss:        0.271953
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.980300 loss:        0.058418
Test - acc:         0.923800 loss:        0.274036
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.982120 loss:        0.056173
Test - acc:         0.928200 loss:        0.258258
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.982260 loss:        0.054122
Test - acc:         0.928400 loss:        0.253502
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.982020 loss:        0.054925
Test - acc:         0.921900 loss:        0.287969
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.982500 loss:        0.053923
Test - acc:         0.929700 loss:        0.263084
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.980580 loss:        0.057276
Test - acc:         0.919600 loss:        0.304094
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.981880 loss:        0.055489
Test - acc:         0.921900 loss:        0.277165
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.980280 loss:        0.059016
Test - acc:         0.926800 loss:        0.258815
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.056724
Test - acc:         0.923900 loss:        0.265815
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.981480 loss:        0.055284
Test - acc:         0.933400 loss:        0.243812
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.981000 loss:        0.055888
Test - acc:         0.925600 loss:        0.262498
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.946200 loss:        0.156445
Test - acc:         0.898200 loss:        0.345629
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.957940 loss:        0.122554
Test - acc:         0.891300 loss:        0.367817
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.959920 loss:        0.118027
Test - acc:         0.912400 loss:        0.284300
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.961300 loss:        0.112003
Test - acc:         0.919600 loss:        0.261849
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.962500 loss:        0.110194
Test - acc:         0.916100 loss:        0.279255
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.965500 loss:        0.102171
Test - acc:         0.919000 loss:        0.281956
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.963580 loss:        0.107578
Test - acc:         0.917900 loss:        0.286185
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.964840 loss:        0.104313
Test - acc:         0.914400 loss:        0.291737
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.966560 loss:        0.098659
Test - acc:         0.913700 loss:        0.290894
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.967160 loss:        0.095997
Test - acc:         0.917300 loss:        0.288871
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.966560 loss:        0.098057
Test - acc:         0.920700 loss:        0.280354
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.967520 loss:        0.094527
Test - acc:         0.919800 loss:        0.271195
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.965840 loss:        0.099484
Test - acc:         0.910700 loss:        0.304702
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.968260 loss:        0.092556
Test - acc:         0.912900 loss:        0.297406
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.968100 loss:        0.094326
Test - acc:         0.917000 loss:        0.275112
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.967160 loss:        0.095245
Test - acc:         0.915100 loss:        0.291966
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.968480 loss:        0.092292
Test - acc:         0.925300 loss:        0.253348
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.968500 loss:        0.091018
Test - acc:         0.910700 loss:        0.298620
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.967160 loss:        0.095486
Test - acc:         0.917400 loss:        0.289861
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.968920 loss:        0.092038
Test - acc:         0.906500 loss:        0.332078
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.967920 loss:        0.093523
Test - acc:         0.915300 loss:        0.290265
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.968740 loss:        0.090966
Test - acc:         0.920800 loss:        0.262423
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.970460 loss:        0.087075
Test - acc:         0.921500 loss:        0.271998
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.970340 loss:        0.088070
Test - acc:         0.914100 loss:        0.299060
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.969360 loss:        0.090170
Test - acc:         0.915500 loss:        0.293632
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.969720 loss:        0.091193
Test - acc:         0.916700 loss:        0.284011
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.980200 loss:        0.062678
Test - acc:         0.933400 loss:        0.219752
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.985440 loss:        0.049226
Test - acc:         0.935500 loss:        0.215190
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.986480 loss:        0.046523
Test - acc:         0.936200 loss:        0.215259
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.987640 loss:        0.042853
Test - acc:         0.937000 loss:        0.215197
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.988520 loss:        0.040422
Test - acc:         0.938400 loss:        0.214777
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.989420 loss:        0.038293
Test - acc:         0.938300 loss:        0.216469
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.925640 loss:        0.235874
Test - acc:         0.901900 loss:        0.296409
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.942840 loss:        0.184736
Test - acc:         0.906900 loss:        0.280475
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.947760 loss:        0.166641
Test - acc:         0.910500 loss:        0.272450
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.951660 loss:        0.155731
Test - acc:         0.912200 loss:        0.269258
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.954740 loss:        0.147111
Test - acc:         0.911900 loss:        0.266432
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.955440 loss:        0.141336
Test - acc:         0.914700 loss:        0.265997
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.956040 loss:        0.137815
Test - acc:         0.913300 loss:        0.265251
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.956920 loss:        0.133082
Test - acc:         0.913400 loss:        0.264561
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.957780 loss:        0.130417
Test - acc:         0.916100 loss:        0.263365
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.958540 loss:        0.128015
Test - acc:         0.915400 loss:        0.262981
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.960360 loss:        0.122131
Test - acc:         0.915100 loss:        0.264000
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.961500 loss:        0.120902
Test - acc:         0.917300 loss:        0.258587
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.961700 loss:        0.119038
Test - acc:         0.917100 loss:        0.258327
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.962180 loss:        0.118314
Test - acc:         0.918400 loss:        0.258092
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.963200 loss:        0.113992
Test - acc:         0.919200 loss:        0.255941
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.964660 loss:        0.110791
Test - acc:         0.918700 loss:        0.254586
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.963920 loss:        0.110124
Test - acc:         0.920900 loss:        0.254294
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.965000 loss:        0.108721
Test - acc:         0.919000 loss:        0.257251
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.964420 loss:        0.109236
Test - acc:         0.919000 loss:        0.256735
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.966020 loss:        0.107603
Test - acc:         0.920700 loss:        0.255275
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.966100 loss:        0.106523
Test - acc:         0.920700 loss:        0.255312
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.967620 loss:        0.102297
Test - acc:         0.919300 loss:        0.257896
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.966860 loss:        0.103163
Test - acc:         0.919200 loss:        0.258531
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.966900 loss:        0.101719
Test - acc:         0.919800 loss:        0.256195
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.967700 loss:        0.101051
Test - acc:         0.919800 loss:        0.257139
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.966980 loss:        0.100727
Test - acc:         0.920800 loss:        0.257701
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.969040 loss:        0.097921
Test - acc:         0.921800 loss:        0.254731
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.967520 loss:        0.100209
Test - acc:         0.922300 loss:        0.252033
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.968120 loss:        0.098277
Test - acc:         0.920000 loss:        0.255641
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.970060 loss:        0.094390
Test - acc:         0.921200 loss:        0.253986
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.967020 loss:        0.098374
Test - acc:         0.920500 loss:        0.257650
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.970280 loss:        0.093768
Test - acc:         0.922200 loss:        0.255598
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.775120 loss:        0.671241
Test - acc:         0.822200 loss:        0.537872
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.833420 loss:        0.499190
Test - acc:         0.841800 loss:        0.481024
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.848600 loss:        0.455408
Test - acc:         0.847500 loss:        0.460333
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.858960 loss:        0.423736
Test - acc:         0.850800 loss:        0.445736
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.866320 loss:        0.406582
Test - acc:         0.854400 loss:        0.435519
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.868100 loss:        0.393802
Test - acc:         0.856100 loss:        0.433890
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.873740 loss:        0.381874
Test - acc:         0.863200 loss:        0.415563
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.873320 loss:        0.374462
Test - acc:         0.863900 loss:        0.408900
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.876740 loss:        0.367384
Test - acc:         0.863600 loss:        0.405172
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.879000 loss:        0.360798
Test - acc:         0.870000 loss:        0.396756
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.881540 loss:        0.354550
Test - acc:         0.866500 loss:        0.396568
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.881940 loss:        0.349444
Test - acc:         0.872100 loss:        0.391569
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.883740 loss:        0.346842
Test - acc:         0.870900 loss:        0.390692
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.887440 loss:        0.339958
Test - acc:         0.871900 loss:        0.383097
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.886840 loss:        0.336867
Test - acc:         0.872700 loss:        0.385795
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.887460 loss:        0.331969
Test - acc:         0.873200 loss:        0.378454
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.889160 loss:        0.328934
Test - acc:         0.873900 loss:        0.384191
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.889620 loss:        0.326186
Test - acc:         0.875100 loss:        0.378153
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.891780 loss:        0.320243
Test - acc:         0.874600 loss:        0.382215
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.894240 loss:        0.318569
Test - acc:         0.875700 loss:        0.374542
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.893200 loss:        0.316625
Test - acc:         0.873300 loss:        0.382085
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.893660 loss:        0.316776
Test - acc:         0.876300 loss:        0.367562
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.895080 loss:        0.310558
Test - acc:         0.876100 loss:        0.379272
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.892380 loss:        0.313484
Test - acc:         0.877200 loss:        0.374308
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.893680 loss:        0.312389
Test - acc:         0.877400 loss:        0.370757
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.896380 loss:        0.309145
Test - acc:         0.877900 loss:        0.365370
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.896060 loss:        0.309003
Test - acc:         0.877100 loss:        0.371240
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.895780 loss:        0.307993
Test - acc:         0.876400 loss:        0.374525
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.897300 loss:        0.303870
Test - acc:         0.877300 loss:        0.371651
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.896940 loss:        0.302543
Test - acc:         0.881000 loss:        0.359591
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.899340 loss:        0.301433
Test - acc:         0.880100 loss:        0.367215
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.898760 loss:        0.298566
Test - acc:         0.878200 loss:        0.366332
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.649620 loss:        1.024605
Test - acc:         0.704500 loss:        0.844996
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.721640 loss:        0.828008
Test - acc:         0.728500 loss:        0.781352
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.740720 loss:        0.770793
Test - acc:         0.751400 loss:        0.731965
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.751920 loss:        0.742192
Test - acc:         0.758100 loss:        0.710714
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.758900 loss:        0.716098
Test - acc:         0.766500 loss:        0.685109
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.768160 loss:        0.692842
Test - acc:         0.769900 loss:        0.676520
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.770480 loss:        0.683630
Test - acc:         0.768900 loss:        0.667185
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.774300 loss:        0.669353
Test - acc:         0.777000 loss:        0.655602
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.775560 loss:        0.663650
Test - acc:         0.778600 loss:        0.648977
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.779920 loss:        0.655009
Test - acc:         0.785500 loss:        0.639802
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.783540 loss:        0.642976
Test - acc:         0.785600 loss:        0.632666
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.786360 loss:        0.636921
Test - acc:         0.784000 loss:        0.633051
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.782820 loss:        0.638466
Test - acc:         0.791200 loss:        0.620279
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.789620 loss:        0.629847
Test - acc:         0.791900 loss:        0.614903
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.790220 loss:        0.622556
Test - acc:         0.784300 loss:        0.630021
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.790980 loss:        0.619108
Test - acc:         0.797700 loss:        0.607733
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.790780 loss:        0.614249
Test - acc:         0.798700 loss:        0.609863
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.792240 loss:        0.609594
Test - acc:         0.796300 loss:        0.605103
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.794660 loss:        0.604819
Test - acc:         0.791200 loss:        0.616361
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.796880 loss:        0.599869
Test - acc:         0.800500 loss:        0.597033
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.798360 loss:        0.595796
Test - acc:         0.794600 loss:        0.603623
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.798120 loss:        0.592934
Test - acc:         0.797400 loss:        0.599021
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.801300 loss:        0.590839
Test - acc:         0.801600 loss:        0.589714
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.798340 loss:        0.592272
Test - acc:         0.799300 loss:        0.592891
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.798860 loss:        0.586740
Test - acc:         0.801800 loss:        0.584444
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.801760 loss:        0.584865
Test - acc:         0.802600 loss:        0.584658
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.802680 loss:        0.579609
Test - acc:         0.796600 loss:        0.600079
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.802660 loss:        0.580263
Test - acc:         0.798700 loss:        0.584605
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.804960 loss:        0.575596
Test - acc:         0.805000 loss:        0.579095
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.803980 loss:        0.574935
Test - acc:         0.801800 loss:        0.587701
Sparsity :          0.9990
Wdecay :        0.000500
