Running --model vgg19 --prune_criterion magnitude --seed 43 --prune_freq 32 --prune_rate 0.5 --comment=vgg19_crit=magnitude_pf=32_seed=43 --save_model=pre-finetune/vgg19_magnitude_pf32_s43 --logdir=criterion_experiment_no_bias/vgg19
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "magnitude",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_magnitude_pf32_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.102040 loss:        2.593170
Test - acc:         0.101800 loss:        2.305669
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.102980 loss:        2.302027
Test - acc:         0.108700 loss:        2.299553
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.107760 loss:        2.295210
Test - acc:         0.102100 loss:        2.288999
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.170220 loss:        2.125674
Test - acc:         0.234300 loss:        1.914818
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.283940 loss:        1.804644
Test - acc:         0.265700 loss:        1.923784
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.401960 loss:        1.553583
Test - acc:         0.376900 loss:        1.801617
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.539940 loss:        1.245976
Test - acc:         0.572700 loss:        1.260381
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.632400 loss:        1.036366
Test - acc:         0.586200 loss:        1.257524
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.681460 loss:        0.918227
Test - acc:         0.610400 loss:        1.206541
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.713460 loss:        0.850217
Test - acc:         0.668700 loss:        0.947266
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.739980 loss:        0.792240
Test - acc:         0.681800 loss:        1.044388
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.758800 loss:        0.739895
Test - acc:         0.697800 loss:        0.921571
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.770220 loss:        0.715436
Test - acc:         0.645300 loss:        1.166721
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.777960 loss:        0.693221
Test - acc:         0.740500 loss:        0.807132
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.784440 loss:        0.673860
Test - acc:         0.720600 loss:        0.904886
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.789860 loss:        0.663452
Test - acc:         0.728000 loss:        0.842933
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.799320 loss:        0.628781
Test - acc:         0.682700 loss:        0.994196
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.805000 loss:        0.609107
Test - acc:         0.763000 loss:        0.748730
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.807880 loss:        0.599333
Test - acc:         0.706800 loss:        0.948992
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.812460 loss:        0.583509
Test - acc:         0.789700 loss:        0.668061
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.817240 loss:        0.569957
Test - acc:         0.753900 loss:        0.833114
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.819180 loss:        0.563694
Test - acc:         0.770800 loss:        0.724838
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.820740 loss:        0.554781
Test - acc:         0.700100 loss:        0.967560
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.825820 loss:        0.543488
Test - acc:         0.723000 loss:        0.915873
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.830920 loss:        0.527130
Test - acc:         0.757800 loss:        0.837092
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.830200 loss:        0.531166
Test - acc:         0.776600 loss:        0.708030
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.832860 loss:        0.517132
Test - acc:         0.768400 loss:        0.819225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.831800 loss:        0.523846
Test - acc:         0.802900 loss:        0.601694
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.838100 loss:        0.499992
Test - acc:         0.759300 loss:        0.806052
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.838440 loss:        0.504548
Test - acc:         0.713300 loss:        0.926746
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.839900 loss:        0.498613
Test - acc:         0.756200 loss:        0.786542
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.839700 loss:        0.498225
Test - acc:         0.805800 loss:        0.605070
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.859240 loss:        0.432826
Test - acc:         0.754400 loss:        0.857785
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.860380 loss:        0.431022
Test - acc:         0.785200 loss:        0.686856
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.857360 loss:        0.440179
Test - acc:         0.790300 loss:        0.665747
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.858780 loss:        0.436101
Test - acc:         0.745100 loss:        0.840512
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.860320 loss:        0.435965
Test - acc:         0.803700 loss:        0.640997
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.855740 loss:        0.447515
Test - acc:         0.662000 loss:        1.333744
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.858460 loss:        0.436251
Test - acc:         0.821600 loss:        0.550091
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.859700 loss:        0.425160
Test - acc:         0.811300 loss:        0.603175
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.860760 loss:        0.428654
Test - acc:         0.719600 loss:        0.963025
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.862220 loss:        0.426073
Test - acc:         0.824200 loss:        0.555913
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.864580 loss:        0.419862
Test - acc:         0.805700 loss:        0.661958
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.865500 loss:        0.417260
Test - acc:         0.728800 loss:        0.978620
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.864740 loss:        0.421473
Test - acc:         0.716200 loss:        1.000283
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.860920 loss:        0.427864
Test - acc:         0.786900 loss:        0.680429
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.864840 loss:        0.418854
Test - acc:         0.797800 loss:        0.674745
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.865480 loss:        0.417851
Test - acc:         0.773600 loss:        0.752041
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.865540 loss:        0.417082
Test - acc:         0.803300 loss:        0.634594
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.864520 loss:        0.420390
Test - acc:         0.820100 loss:        0.533589
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.866920 loss:        0.408980
Test - acc:         0.752500 loss:        0.791958
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.865500 loss:        0.414460
Test - acc:         0.794100 loss:        0.693722
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.869120 loss:        0.408076
Test - acc:         0.798100 loss:        0.670194
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.866940 loss:        0.409537
Test - acc:         0.798000 loss:        0.671630
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.870540 loss:        0.402187
Test - acc:         0.774700 loss:        0.788090
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.868620 loss:        0.406761
Test - acc:         0.772700 loss:        0.761588
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.867960 loss:        0.402625
Test - acc:         0.767700 loss:        0.831970
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.868720 loss:        0.408142
Test - acc:         0.806200 loss:        0.605459
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.868100 loss:        0.404599
Test - acc:         0.793200 loss:        0.674417
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.871280 loss:        0.398049
Test - acc:         0.856000 loss:        0.436712
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.869060 loss:        0.400781
Test - acc:         0.804500 loss:        0.655541
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.868580 loss:        0.400806
Test - acc:         0.793400 loss:        0.704748
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.869280 loss:        0.405824
Test - acc:         0.814000 loss:        0.572686
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.869900 loss:        0.402862
Test - acc:         0.780700 loss:        0.768932
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.882660 loss:        0.357034
Test - acc:         0.816900 loss:        0.608780
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.883260 loss:        0.356450
Test - acc:         0.686700 loss:        1.074061
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.880860 loss:        0.361035
Test - acc:         0.779900 loss:        0.740485
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.881000 loss:        0.363016
Test - acc:         0.846600 loss:        0.495253
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.884100 loss:        0.353070
Test - acc:         0.809600 loss:        0.641397
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.879700 loss:        0.363911
Test - acc:         0.812200 loss:        0.647195
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.882620 loss:        0.359979
Test - acc:         0.820100 loss:        0.559105
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.882440 loss:        0.358986
Test - acc:         0.823500 loss:        0.537709
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.883380 loss:        0.353882
Test - acc:         0.829400 loss:        0.562727
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.883400 loss:        0.355240
Test - acc:         0.825000 loss:        0.565066
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.884500 loss:        0.355714
Test - acc:         0.836300 loss:        0.525858
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.884040 loss:        0.353934
Test - acc:         0.847800 loss:        0.501140
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.883760 loss:        0.352741
Test - acc:         0.832500 loss:        0.522373
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.883940 loss:        0.356270
Test - acc:         0.827100 loss:        0.593468
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.885900 loss:        0.350180
Test - acc:         0.782200 loss:        0.699464
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.882360 loss:        0.354713
Test - acc:         0.799400 loss:        0.677287
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.882780 loss:        0.353885
Test - acc:         0.752900 loss:        0.861033
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.885720 loss:        0.347055
Test - acc:         0.830000 loss:        0.570367
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.885340 loss:        0.352342
Test - acc:         0.833000 loss:        0.521074
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.885140 loss:        0.347558
Test - acc:         0.819000 loss:        0.575755
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.882900 loss:        0.351748
Test - acc:         0.813200 loss:        0.590292
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.884740 loss:        0.350941
Test - acc:         0.812400 loss:        0.610021
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.887040 loss:        0.341786
Test - acc:         0.812100 loss:        0.665111
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.884100 loss:        0.348868
Test - acc:         0.849800 loss:        0.466192
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.885880 loss:        0.345322
Test - acc:         0.702200 loss:        1.165562
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.884100 loss:        0.350092
Test - acc:         0.819200 loss:        0.537098
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.885900 loss:        0.344492
Test - acc:         0.840100 loss:        0.491278
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.885640 loss:        0.349188
Test - acc:         0.820700 loss:        0.566353
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.885440 loss:        0.344368
Test - acc:         0.780200 loss:        0.703361
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.885480 loss:        0.345314
Test - acc:         0.839800 loss:        0.500705
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.885400 loss:        0.350132
Test - acc:         0.793000 loss:        0.640338
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.883600 loss:        0.350068
Test - acc:         0.848500 loss:        0.481438
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.888840 loss:        0.337453
Test - acc:         0.849000 loss:        0.487765
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.891920 loss:        0.325299
Test - acc:         0.794800 loss:        0.668467
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.890820 loss:        0.328872
Test - acc:         0.842500 loss:        0.505418
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.892420 loss:        0.322240
Test - acc:         0.826700 loss:        0.539340
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.892860 loss:        0.325023
Test - acc:         0.818300 loss:        0.590483
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.890980 loss:        0.326062
Test - acc:         0.846200 loss:        0.490019
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.890680 loss:        0.331879
Test - acc:         0.842000 loss:        0.485327
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.889660 loss:        0.324983
Test - acc:         0.865800 loss:        0.399246
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.889820 loss:        0.330174
Test - acc:         0.833000 loss:        0.546408
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.892780 loss:        0.322524
Test - acc:         0.817600 loss:        0.563382
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.891600 loss:        0.326459
Test - acc:         0.820700 loss:        0.552940
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.891440 loss:        0.329779
Test - acc:         0.836200 loss:        0.496214
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.891460 loss:        0.330712
Test - acc:         0.823300 loss:        0.559932
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.892240 loss:        0.323115
Test - acc:         0.815500 loss:        0.618850
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.891820 loss:        0.322185
Test - acc:         0.822400 loss:        0.557079
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.889520 loss:        0.331127
Test - acc:         0.815100 loss:        0.593121
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.892440 loss:        0.323829
Test - acc:         0.795100 loss:        0.737637
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.892180 loss:        0.324908
Test - acc:         0.827500 loss:        0.552490
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.894660 loss:        0.320671
Test - acc:         0.825100 loss:        0.581755
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.893560 loss:        0.319677
Test - acc:         0.834000 loss:        0.501545
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.891080 loss:        0.326092
Test - acc:         0.817300 loss:        0.586391
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.890520 loss:        0.324332
Test - acc:         0.840300 loss:        0.517291
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.892300 loss:        0.324367
Test - acc:         0.793100 loss:        0.706137
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.891780 loss:        0.326493
Test - acc:         0.818700 loss:        0.622124
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.891180 loss:        0.325603
Test - acc:         0.822400 loss:        0.598688
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.890140 loss:        0.329017
Test - acc:         0.810400 loss:        0.589085
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.891220 loss:        0.323221
Test - acc:         0.845100 loss:        0.491189
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.891300 loss:        0.323009
Test - acc:         0.826100 loss:        0.531583
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.891140 loss:        0.323586
Test - acc:         0.816800 loss:        0.557068
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.892120 loss:        0.322163
Test - acc:         0.785400 loss:        0.748234
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.890400 loss:        0.329895
Test - acc:         0.808700 loss:        0.615723
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.893620 loss:        0.321534
Test - acc:         0.790900 loss:        0.689496
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.881200 loss:        0.351029
Test - acc:         0.753500 loss:        0.892411
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.887040 loss:        0.336217
Test - acc:         0.847800 loss:        0.483723
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.889160 loss:        0.329852
Test - acc:         0.844300 loss:        0.485782
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.886620 loss:        0.336900
Test - acc:         0.814500 loss:        0.665094
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.886740 loss:        0.338089
Test - acc:         0.827300 loss:        0.545335
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.888980 loss:        0.330641
Test - acc:         0.851700 loss:        0.450918
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.887160 loss:        0.335799
Test - acc:         0.832800 loss:        0.551783
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.885920 loss:        0.342485
Test - acc:         0.600600 loss:        1.864152
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.883400 loss:        0.348436
Test - acc:         0.755000 loss:        0.753951
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.887480 loss:        0.333886
Test - acc:         0.817500 loss:        0.565071
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.888000 loss:        0.334547
Test - acc:         0.798500 loss:        0.676036
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.888040 loss:        0.334705
Test - acc:         0.848200 loss:        0.501825
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.889060 loss:        0.332818
Test - acc:         0.754600 loss:        0.986440
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.887940 loss:        0.336775
Test - acc:         0.783300 loss:        0.655195
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.887880 loss:        0.334206
Test - acc:         0.843000 loss:        0.480516
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.887720 loss:        0.337635
Test - acc:         0.735000 loss:        0.894010
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.887960 loss:        0.331689
Test - acc:         0.832800 loss:        0.585891
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.887860 loss:        0.333437
Test - acc:         0.805000 loss:        0.613658
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.888300 loss:        0.335887
Test - acc:         0.830200 loss:        0.540065
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.886540 loss:        0.336503
Test - acc:         0.824700 loss:        0.581246
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.890140 loss:        0.330750
Test - acc:         0.831500 loss:        0.532888
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.890100 loss:        0.331413
Test - acc:         0.835500 loss:        0.501725
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.933980 loss:        0.198361
Test - acc:         0.914900 loss:        0.259181
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.945460 loss:        0.162250
Test - acc:         0.917900 loss:        0.256836
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.950220 loss:        0.146876
Test - acc:         0.918400 loss:        0.254097
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.954080 loss:        0.137728
Test - acc:         0.919700 loss:        0.254341
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.956020 loss:        0.131228
Test - acc:         0.918300 loss:        0.260721
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.958980 loss:        0.123583
Test - acc:         0.917100 loss:        0.259451
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.959880 loss:        0.120548
Test - acc:         0.921400 loss:        0.251157
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.962200 loss:        0.113163
Test - acc:         0.920600 loss:        0.255128
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.963380 loss:        0.109186
Test - acc:         0.919600 loss:        0.257768
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.964260 loss:        0.104799
Test - acc:         0.920200 loss:        0.256307
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.929600 loss:        0.204806
Test - acc:         0.902800 loss:        0.298138
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.944280 loss:        0.163672
Test - acc:         0.906000 loss:        0.296287
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.946560 loss:        0.156645
Test - acc:         0.908400 loss:        0.292982
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.948160 loss:        0.150502
Test - acc:         0.909400 loss:        0.285626
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.948860 loss:        0.145430
Test - acc:         0.906600 loss:        0.298567
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.949600 loss:        0.143872
Test - acc:         0.906100 loss:        0.298032
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.951360 loss:        0.141192
Test - acc:         0.908700 loss:        0.293168
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.952200 loss:        0.140998
Test - acc:         0.911300 loss:        0.292233
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.953440 loss:        0.135681
Test - acc:         0.903900 loss:        0.308568
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.954140 loss:        0.135360
Test - acc:         0.910900 loss:        0.292597
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.951500 loss:        0.138267
Test - acc:         0.903100 loss:        0.304773
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.952040 loss:        0.139441
Test - acc:         0.906900 loss:        0.300153
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.953320 loss:        0.135698
Test - acc:         0.905000 loss:        0.304329
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.953580 loss:        0.134478
Test - acc:         0.901200 loss:        0.313257
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.953580 loss:        0.135547
Test - acc:         0.906000 loss:        0.300605
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.952700 loss:        0.138158
Test - acc:         0.907700 loss:        0.302456
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.952500 loss:        0.137439
Test - acc:         0.901000 loss:        0.316658
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.951580 loss:        0.139714
Test - acc:         0.902300 loss:        0.322581
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.953460 loss:        0.134752
Test - acc:         0.904000 loss:        0.312706
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.950960 loss:        0.140458
Test - acc:         0.906600 loss:        0.307467
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.951940 loss:        0.139004
Test - acc:         0.901400 loss:        0.323987
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.953180 loss:        0.138167
Test - acc:         0.905500 loss:        0.308417
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.952400 loss:        0.138842
Test - acc:         0.898600 loss:        0.330582
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.950560 loss:        0.140492
Test - acc:         0.903400 loss:        0.313975
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.952260 loss:        0.138769
Test - acc:         0.900700 loss:        0.318264
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.952420 loss:        0.138636
Test - acc:         0.902500 loss:        0.315825
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.950260 loss:        0.143528
Test - acc:         0.896900 loss:        0.329786
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.951020 loss:        0.138393
Test - acc:         0.901500 loss:        0.324238
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.951840 loss:        0.139824
Test - acc:         0.902200 loss:        0.327700
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.949300 loss:        0.143361
Test - acc:         0.890700 loss:        0.368254
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.950140 loss:        0.141386
Test - acc:         0.895500 loss:        0.337116
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.950540 loss:        0.144068
Test - acc:         0.897400 loss:        0.330051
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.869780 loss:        0.387668
Test - acc:         0.856000 loss:        0.437524
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.894140 loss:        0.306200
Test - acc:         0.865500 loss:        0.415046
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.898960 loss:        0.293099
Test - acc:         0.857500 loss:        0.458718
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.905020 loss:        0.276347
Test - acc:         0.868500 loss:        0.411122
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.906120 loss:        0.271363
Test - acc:         0.873800 loss:        0.395165
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.907800 loss:        0.268854
Test - acc:         0.861400 loss:        0.430425
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.910380 loss:        0.261148
Test - acc:         0.881000 loss:        0.371379
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.908320 loss:        0.266003
Test - acc:         0.862400 loss:        0.419334
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.910140 loss:        0.258266
Test - acc:         0.875800 loss:        0.372567
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.911560 loss:        0.252892
Test - acc:         0.876000 loss:        0.378156
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.912620 loss:        0.251067
Test - acc:         0.880200 loss:        0.374896
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.910940 loss:        0.255468
Test - acc:         0.876700 loss:        0.375146
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.913120 loss:        0.250729
Test - acc:         0.870900 loss:        0.397527
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.912560 loss:        0.249312
Test - acc:         0.867700 loss:        0.413830
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.913700 loss:        0.248286
Test - acc:         0.874100 loss:        0.393770
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.915600 loss:        0.244157
Test - acc:         0.873600 loss:        0.388916
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.912960 loss:        0.248132
Test - acc:         0.874400 loss:        0.392960
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.912020 loss:        0.250756
Test - acc:         0.877100 loss:        0.360549
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.914680 loss:        0.247000
Test - acc:         0.878800 loss:        0.376542
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.915100 loss:        0.241018
Test - acc:         0.873300 loss:        0.394910
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.915280 loss:        0.242106
Test - acc:         0.877300 loss:        0.389545
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.915520 loss:        0.239932
Test - acc:         0.875300 loss:        0.382976
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.916500 loss:        0.240459
Test - acc:         0.862700 loss:        0.431405
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.915780 loss:        0.240762
Test - acc:         0.883300 loss:        0.358504
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.914940 loss:        0.238439
Test - acc:         0.868300 loss:        0.413441
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.917000 loss:        0.238034
Test - acc:         0.870000 loss:        0.401446
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.915920 loss:        0.241394
Test - acc:         0.878600 loss:        0.385051
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.915920 loss:        0.240302
Test - acc:         0.880600 loss:        0.373666
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.915520 loss:        0.240337
Test - acc:         0.874100 loss:        0.390035
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.916540 loss:        0.237486
Test - acc:         0.865900 loss:        0.426388
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.913900 loss:        0.245498
Test - acc:         0.881200 loss:        0.361217
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.916700 loss:        0.235949
Test - acc:         0.864800 loss:        0.406498
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.789200 loss:        0.613542
Test - acc:         0.791900 loss:        0.637502
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.826920 loss:        0.502122
Test - acc:         0.813900 loss:        0.536856
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.834400 loss:        0.475821
Test - acc:         0.804400 loss:        0.571415
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.839840 loss:        0.464738
Test - acc:         0.812800 loss:        0.550190
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.843680 loss:        0.451235
Test - acc:         0.804800 loss:        0.569553
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.846240 loss:        0.443582
Test - acc:         0.817400 loss:        0.547988
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.847040 loss:        0.439202
Test - acc:         0.837200 loss:        0.485609
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.850340 loss:        0.432199
Test - acc:         0.833000 loss:        0.486190
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.850060 loss:        0.429457
Test - acc:         0.813800 loss:        0.549722
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.850760 loss:        0.427194
Test - acc:         0.826200 loss:        0.513978
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.851980 loss:        0.428455
Test - acc:         0.831900 loss:        0.505633
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.851400 loss:        0.427306
Test - acc:         0.827400 loss:        0.510898
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.855040 loss:        0.417674
Test - acc:         0.834200 loss:        0.489364
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.854260 loss:        0.417564
Test - acc:         0.826100 loss:        0.537939
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.853860 loss:        0.417872
Test - acc:         0.836500 loss:        0.487024
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.854520 loss:        0.419516
Test - acc:         0.838200 loss:        0.501831
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.854940 loss:        0.415597
Test - acc:         0.832200 loss:        0.494151
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.857160 loss:        0.413521
Test - acc:         0.807400 loss:        0.590865
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.856180 loss:        0.413424
Test - acc:         0.840600 loss:        0.482431
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.857960 loss:        0.411025
Test - acc:         0.833700 loss:        0.498471
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.860700 loss:        0.403191
Test - acc:         0.834000 loss:        0.502911
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.859260 loss:        0.409205
Test - acc:         0.831900 loss:        0.488713
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.859220 loss:        0.405210
Test - acc:         0.836000 loss:        0.491045
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.859240 loss:        0.405019
Test - acc:         0.837100 loss:        0.472537
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.859100 loss:        0.406423
Test - acc:         0.826700 loss:        0.503540
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.859400 loss:        0.406237
Test - acc:         0.807100 loss:        0.612252
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.877940 loss:        0.351955
Test - acc:         0.860800 loss:        0.408898
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.884720 loss:        0.332936
Test - acc:         0.865100 loss:        0.403790
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.885540 loss:        0.325757
Test - acc:         0.863800 loss:        0.403016
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.889780 loss:        0.319208
Test - acc:         0.862000 loss:        0.404395
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.889960 loss:        0.318659
Test - acc:         0.865900 loss:        0.399737
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.890600 loss:        0.316414
Test - acc:         0.863700 loss:        0.404325
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.594620 loss:        1.211086
Test - acc:         0.670600 loss:        0.951872
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.678540 loss:        0.931080
Test - acc:         0.702700 loss:        0.859002
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.696500 loss:        0.872206
Test - acc:         0.716000 loss:        0.824376
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.709760 loss:        0.834738
Test - acc:         0.719100 loss:        0.808729
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.722920 loss:        0.802687
Test - acc:         0.736700 loss:        0.767399
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.727440 loss:        0.785349
Test - acc:         0.732200 loss:        0.774905
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.731380 loss:        0.771619
Test - acc:         0.742100 loss:        0.742883
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.737220 loss:        0.756672
Test - acc:         0.740700 loss:        0.746563
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.741600 loss:        0.743941
Test - acc:         0.748600 loss:        0.731619
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.744040 loss:        0.731225
Test - acc:         0.751700 loss:        0.728031
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.748740 loss:        0.725185
Test - acc:         0.750200 loss:        0.720756
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.750120 loss:        0.718501
Test - acc:         0.749300 loss:        0.730817
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.750780 loss:        0.717274
Test - acc:         0.762000 loss:        0.691349
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.755440 loss:        0.706012
Test - acc:         0.757700 loss:        0.708841
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.755340 loss:        0.703241
Test - acc:         0.764000 loss:        0.688511
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.757420 loss:        0.698574
Test - acc:         0.763100 loss:        0.682994
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.761120 loss:        0.690439
Test - acc:         0.747200 loss:        0.731512
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.761440 loss:        0.689661
Test - acc:         0.752600 loss:        0.724929
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.761080 loss:        0.685542
Test - acc:         0.756300 loss:        0.718795
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.763000 loss:        0.681178
Test - acc:         0.767400 loss:        0.682399
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.765480 loss:        0.676543
Test - acc:         0.769500 loss:        0.671059
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.767780 loss:        0.668055
Test - acc:         0.748500 loss:        0.735552
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.767760 loss:        0.668776
Test - acc:         0.748600 loss:        0.730686
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.771000 loss:        0.661368
Test - acc:         0.500100 loss:        1.676853
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.769380 loss:        0.663454
Test - acc:         0.753900 loss:        0.723946
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.770900 loss:        0.660152
Test - acc:         0.764300 loss:        0.683175
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.771020 loss:        0.657465
Test - acc:         0.738500 loss:        0.772388
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.772000 loss:        0.656941
Test - acc:         0.774100 loss:        0.657142
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.773060 loss:        0.656498
Test - acc:         0.767500 loss:        0.670306
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.775320 loss:        0.646470
Test - acc:         0.754200 loss:        0.720115
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.774800 loss:        0.644555
Test - acc:         0.749900 loss:        0.732663
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.775180 loss:        0.644214
Test - acc:         0.778500 loss:        0.636944
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.407840 loss:        1.839764
Test - acc:         0.100000 loss:   140852.878125
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.487760 loss:        1.496672
Test - acc:         0.100000 loss:   240974.600000
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.513040 loss:        1.411703
Test - acc:         0.100000 loss:     7405.766309
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.530440 loss:        1.356216
Test - acc:         0.100000 loss:      577.312561
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.543180 loss:        1.306988
Test - acc:         0.100000 loss:   231405.300000
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.549220 loss:        1.278329
Test - acc:         0.100000 loss:     1218.286084
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.556140 loss:        1.254621
Test - acc:         0.100000 loss:      655.439746
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.561220 loss:        1.232220
Test - acc:         0.100000 loss:       77.621758
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.566080 loss:        1.216893
Test - acc:         0.100000 loss:   203497.825000
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.569820 loss:        1.199059
Test - acc:         0.100000 loss:   235518.996875
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.574100 loss:        1.186398
Test - acc:         0.100000 loss:     1995.440259
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.576540 loss:        1.176109
Test - acc:         0.100000 loss:      288.798279
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.584420 loss:        1.159626
Test - acc:         0.100000 loss:      294.830969
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.581640 loss:        1.153803
Test - acc:         0.100000 loss:     1400.275757
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.584400 loss:        1.143402
Test - acc:         0.100000 loss:    99051.353125
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.586940 loss:        1.133704
Test - acc:         0.100000 loss:     3193.256152
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.588640 loss:        1.123078
Test - acc:         0.100000 loss:   240779.571875
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.592200 loss:        1.114526
Test - acc:         0.100000 loss:      565.688257
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.597400 loss:        1.110226
Test - acc:         0.100000 loss:     1313.558032
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.606460 loss:        1.107494
Test - acc:         0.100000 loss:      614.426343
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.604080 loss:        1.095062
Test - acc:         0.100000 loss:   141918.396875
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.605300 loss:        1.092715
Test - acc:         0.100000 loss:      558.027539
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.611460 loss:        1.083443
Test - acc:         0.100000 loss:   150620.243750
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.611080 loss:        1.080259
Test - acc:         0.100000 loss:      654.249146
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.617640 loss:        1.070765
Test - acc:         0.100000 loss:      726.473083
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.607040 loss:        1.067654
Test - acc:         0.100000 loss:      838.698779
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.609240 loss:        1.062552
Test - acc:         0.100000 loss:    46669.289844
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.618880 loss:        1.053601
Test - acc:         0.100000 loss:     2392.039453
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.616760 loss:        1.053235
Test - acc:         0.100000 loss:       25.473324
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.613700 loss:        1.050590
Test - acc:         0.100000 loss:      906.179932
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.623900 loss:        1.045776
Test - acc:         0.100000 loss:   110170.262500
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.617800 loss:        1.043575
Test - acc:         0.100000 loss:      376.539600
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.270960 loss:        2.218442
Test - acc:         0.100000 loss:      693.841150
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.306680 loss:        1.953900
Test - acc:         0.100000 loss:     4666.916895
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.335920 loss:        1.845986
Test - acc:         0.100000 loss:     4612.406934
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.340720 loss:        1.786266
Test - acc:         0.100000 loss:   110521.100000
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.337440 loss:        1.752042
Test - acc:         0.100000 loss:     5808.637500
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.352420 loss:        1.728153
Test - acc:         0.100000 loss:    39442.693750
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.354340 loss:        1.715674
Test - acc:         0.100000 loss:      654.415442
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.363360 loss:        1.704657
Test - acc:         0.100000 loss:   150363.462500
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.356040 loss:        1.693043
Test - acc:         0.100000 loss:   109274.114062
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.362540 loss:        1.680889
Test - acc:         0.100000 loss:     1116.639087
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.360220 loss:        1.671531
Test - acc:         0.100000 loss:    42951.681250
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.371880 loss:        1.668625
Test - acc:         0.100000 loss:    52641.974219
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.364440 loss:        1.661138
Test - acc:         0.100000 loss:    67536.142187
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.364120 loss:        1.651892
Test - acc:         0.100000 loss:      749.477063
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.382420 loss:        1.652026
Test - acc:         0.100000 loss:      940.203918
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.366660 loss:        1.642239
Test - acc:         0.100000 loss:      967.669531
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.381020 loss:        1.641506
Test - acc:         0.100000 loss:    42783.397656
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.371320 loss:        1.634142
Test - acc:         0.100000 loss:     1729.409082
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.371280 loss:        1.630909
Test - acc:         0.100000 loss:     1622.690820
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.379300 loss:        1.626049
Test - acc:         0.100000 loss:     1559.010669
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.379920 loss:        1.621712
Test - acc:         0.100000 loss:    16529.674414
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.388660 loss:        1.616757
Test - acc:         0.100000 loss:    27787.059375
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.390160 loss:        1.614969
Test - acc:         0.100000 loss:    21863.415625
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.385120 loss:        1.614685
Test - acc:         0.100000 loss:     1360.632422
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.378360 loss:        1.608163
Test - acc:         0.100000 loss:     3072.105273
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.377900 loss:        1.610471
Test - acc:         0.100000 loss:    38359.795312
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.378120 loss:        1.606531
Test - acc:         0.100000 loss:    72357.028125
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.378940 loss:        1.602530
Test - acc:         0.100000 loss:     6319.398438
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.382660 loss:        1.600938
Test - acc:         0.100000 loss:    12813.406445
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.380260 loss:        1.597494
Test - acc:         0.100000 loss:     3205.812402
Sparsity :          0.9990
Wdecay :        0.000500
