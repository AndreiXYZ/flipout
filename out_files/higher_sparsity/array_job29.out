Running --model vgg19 --prune_criterion topflip --seed 44 --prune_freq 32 --prune_rate 0.5 --noise --comment=vgg19_crit=topflip_pf=32_seed=44 --save_model=pre-finetune/vgg19_topflip_pf32_s44 --logdir=criterion_experiment_no_bias/vgg19
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "topflip",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_topflip_pf32_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.142280 loss:        2.601396
Test - acc:         0.195100 loss:        2.206085
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.246620 loss:        1.929788
Test - acc:         0.236800 loss:        2.038829
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.317560 loss:        1.723914
Test - acc:         0.351700 loss:        1.641593
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.425080 loss:        1.509369
Test - acc:         0.464300 loss:        1.391149
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.541380 loss:        1.253156
Test - acc:         0.522800 loss:        1.493001
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.633180 loss:        1.041143
Test - acc:         0.596700 loss:        1.259192
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.678500 loss:        0.936769
Test - acc:         0.640400 loss:        1.105356
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.714680 loss:        0.849851
Test - acc:         0.644600 loss:        1.120985
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.737920 loss:        0.792265
Test - acc:         0.681900 loss:        0.964537
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.752860 loss:        0.748902
Test - acc:         0.563600 loss:        1.665525
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.767840 loss:        0.712795
Test - acc:         0.691600 loss:        0.983145
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.770600 loss:        0.698706
Test - acc:         0.638800 loss:        1.232823
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.781260 loss:        0.673360
Test - acc:         0.745400 loss:        0.827440
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.788280 loss:        0.654842
Test - acc:         0.754700 loss:        0.740684
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.795360 loss:        0.629470
Test - acc:         0.758100 loss:        0.771187
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.795060 loss:        0.629044
Test - acc:         0.725700 loss:        0.844025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.797820 loss:        0.619835
Test - acc:         0.711800 loss:        0.991450
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.802960 loss:        0.607546
Test - acc:         0.760700 loss:        0.740478
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.807800 loss:        0.589372
Test - acc:         0.766400 loss:        0.679525
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.808960 loss:        0.584498
Test - acc:         0.768200 loss:        0.704599
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.813640 loss:        0.575247
Test - acc:         0.781300 loss:        0.677502
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.815120 loss:        0.567870
Test - acc:         0.657500 loss:        1.206960
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.817880 loss:        0.565544
Test - acc:         0.748500 loss:        0.781382
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.816420 loss:        0.563773
Test - acc:         0.747700 loss:        0.820916
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.821880 loss:        0.550806
Test - acc:         0.784600 loss:        0.671629
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.822420 loss:        0.550200
Test - acc:         0.720700 loss:        0.864449
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.824940 loss:        0.540110
Test - acc:         0.777900 loss:        0.700156
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.825000 loss:        0.535996
Test - acc:         0.753200 loss:        0.851256
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.823200 loss:        0.541764
Test - acc:         0.794300 loss:        0.637654
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.825820 loss:        0.535093
Test - acc:         0.807500 loss:        0.606167
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.827760 loss:        0.530152
Test - acc:         0.766000 loss:        0.738118
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.828760 loss:        0.526739
Test - acc:         0.706400 loss:        0.947048
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.831720 loss:        0.512123
Test - acc:         0.796900 loss:        0.629559
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.833980 loss:        0.506449
Test - acc:         0.779400 loss:        0.642945
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.840520 loss:        0.479788
Test - acc:         0.778700 loss:        0.713930
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.842860 loss:        0.476983
Test - acc:         0.794700 loss:        0.641385
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.842040 loss:        0.478866
Test - acc:         0.818900 loss:        0.561352
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.842020 loss:        0.479728
Test - acc:         0.793600 loss:        0.650432
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.845920 loss:        0.465554
Test - acc:         0.824700 loss:        0.526580
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.841980 loss:        0.477758
Test - acc:         0.799400 loss:        0.615845
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.843720 loss:        0.472694
Test - acc:         0.821700 loss:        0.553543
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.845540 loss:        0.469520
Test - acc:         0.789500 loss:        0.652383
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.843540 loss:        0.466289
Test - acc:         0.783500 loss:        0.676712
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.846900 loss:        0.462623
Test - acc:         0.772800 loss:        0.719608
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.844760 loss:        0.464408
Test - acc:         0.810700 loss:        0.556028
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.846720 loss:        0.460142
Test - acc:         0.794300 loss:        0.691577
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.849180 loss:        0.456481
Test - acc:         0.819800 loss:        0.555155
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.847000 loss:        0.459120
Test - acc:         0.757900 loss:        0.763944
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.848080 loss:        0.457052
Test - acc:         0.822500 loss:        0.575115
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.844840 loss:        0.465780
Test - acc:         0.797500 loss:        0.647577
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.848380 loss:        0.455192
Test - acc:         0.804300 loss:        0.606265
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.849340 loss:        0.453719
Test - acc:         0.763500 loss:        0.717561
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.850040 loss:        0.451176
Test - acc:         0.820900 loss:        0.551133
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.851780 loss:        0.451988
Test - acc:         0.802000 loss:        0.611520
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.849100 loss:        0.452260
Test - acc:         0.706000 loss:        1.082579
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.851620 loss:        0.450310
Test - acc:         0.811400 loss:        0.575858
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.851220 loss:        0.449823
Test - acc:         0.809400 loss:        0.584099
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.850940 loss:        0.447297
Test - acc:         0.760200 loss:        0.777274
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.853180 loss:        0.443387
Test - acc:         0.806500 loss:        0.580667
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.849980 loss:        0.451919
Test - acc:         0.833300 loss:        0.501920
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.850340 loss:        0.451572
Test - acc:         0.783300 loss:        0.622379
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.850560 loss:        0.451003
Test - acc:         0.796000 loss:        0.631474
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.851560 loss:        0.449124
Test - acc:         0.797900 loss:        0.634771
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.848720 loss:        0.457048
Test - acc:         0.798900 loss:        0.618741
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.862760 loss:        0.406919
Test - acc:         0.840500 loss:        0.488813
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.860800 loss:        0.411235
Test - acc:         0.818600 loss:        0.561129
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.862280 loss:        0.409106
Test - acc:         0.838000 loss:        0.492514
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.861540 loss:        0.409831
Test - acc:         0.792300 loss:        0.682487
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.863500 loss:        0.405189
Test - acc:         0.829400 loss:        0.505388
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.866160 loss:        0.398502
Test - acc:         0.792700 loss:        0.634635
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.863720 loss:        0.404418
Test - acc:         0.768300 loss:        0.817756
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.867080 loss:        0.396809
Test - acc:         0.834900 loss:        0.507867
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.863500 loss:        0.405546
Test - acc:         0.827300 loss:        0.559315
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.867080 loss:        0.397061
Test - acc:         0.782400 loss:        0.700299
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.865700 loss:        0.396312
Test - acc:         0.804300 loss:        0.616766
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.867340 loss:        0.395459
Test - acc:         0.792800 loss:        0.658050
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.865500 loss:        0.401184
Test - acc:         0.846400 loss:        0.473006
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.865980 loss:        0.396441
Test - acc:         0.813400 loss:        0.571537
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.866200 loss:        0.400206
Test - acc:         0.821700 loss:        0.548799
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.864020 loss:        0.401722
Test - acc:         0.757600 loss:        0.752636
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.864120 loss:        0.402756
Test - acc:         0.843200 loss:        0.492304
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.865580 loss:        0.401176
Test - acc:         0.843000 loss:        0.485035
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.867680 loss:        0.392710
Test - acc:         0.851900 loss:        0.452813
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.866920 loss:        0.395186
Test - acc:         0.838500 loss:        0.495858
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.864400 loss:        0.405015
Test - acc:         0.821800 loss:        0.536555
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.866340 loss:        0.398332
Test - acc:         0.814800 loss:        0.574045
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.865520 loss:        0.397584
Test - acc:         0.812500 loss:        0.583921
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.868760 loss:        0.392228
Test - acc:         0.835300 loss:        0.510725
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.868020 loss:        0.396057
Test - acc:         0.787800 loss:        0.666964
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.867340 loss:        0.398739
Test - acc:         0.829300 loss:        0.527607
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.866100 loss:        0.395257
Test - acc:         0.828900 loss:        0.511568
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.869140 loss:        0.391351
Test - acc:         0.826500 loss:        0.532783
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.867620 loss:        0.392260
Test - acc:         0.816000 loss:        0.577825
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.865340 loss:        0.400023
Test - acc:         0.789800 loss:        0.610459
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.866700 loss:        0.396330
Test - acc:         0.835200 loss:        0.493819
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.866300 loss:        0.397267
Test - acc:         0.785100 loss:        0.691510
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.872920 loss:        0.373881
Test - acc:         0.829800 loss:        0.519629
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.872920 loss:        0.378008
Test - acc:         0.840300 loss:        0.477484
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.874080 loss:        0.374555
Test - acc:         0.851900 loss:        0.438504
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.873820 loss:        0.373169
Test - acc:         0.856900 loss:        0.427004
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.874060 loss:        0.373404
Test - acc:         0.841800 loss:        0.475159
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.873880 loss:        0.377302
Test - acc:         0.762000 loss:        0.822578
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.872920 loss:        0.372303
Test - acc:         0.845600 loss:        0.456004
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.873680 loss:        0.374314
Test - acc:         0.831500 loss:        0.502027
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.871640 loss:        0.378093
Test - acc:         0.810600 loss:        0.588595
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.874060 loss:        0.371890
Test - acc:         0.812400 loss:        0.565193
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.874200 loss:        0.373558
Test - acc:         0.792200 loss:        0.635739
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.871820 loss:        0.378713
Test - acc:         0.789000 loss:        0.672750
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.872740 loss:        0.374552
Test - acc:         0.843900 loss:        0.467081
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.874100 loss:        0.375092
Test - acc:         0.800000 loss:        0.628481
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.875780 loss:        0.370365
Test - acc:         0.806600 loss:        0.594249
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.871400 loss:        0.378415
Test - acc:         0.795100 loss:        0.630686
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.873720 loss:        0.372963
Test - acc:         0.810900 loss:        0.606116
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.871980 loss:        0.377839
Test - acc:         0.749300 loss:        0.880361
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.875120 loss:        0.369845
Test - acc:         0.818400 loss:        0.585908
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.874000 loss:        0.370477
Test - acc:         0.840000 loss:        0.487321
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.876040 loss:        0.367557
Test - acc:         0.819500 loss:        0.591487
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.875400 loss:        0.371891
Test - acc:         0.837700 loss:        0.500509
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.874880 loss:        0.371368
Test - acc:         0.836900 loss:        0.511787
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.874040 loss:        0.372469
Test - acc:         0.765800 loss:        0.729069
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.876300 loss:        0.369181
Test - acc:         0.805000 loss:        0.609378
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.873700 loss:        0.372290
Test - acc:         0.842800 loss:        0.477520
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.873680 loss:        0.373430
Test - acc:         0.825800 loss:        0.537244
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.871720 loss:        0.376091
Test - acc:         0.774000 loss:        0.702218
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.874660 loss:        0.368462
Test - acc:         0.723800 loss:        1.006994
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.872880 loss:        0.375133
Test - acc:         0.817100 loss:        0.553813
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.875620 loss:        0.367679
Test - acc:         0.835000 loss:        0.513923
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.875760 loss:        0.367070
Test - acc:         0.827900 loss:        0.538736
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.875860 loss:        0.368528
Test - acc:         0.804800 loss:        0.622553
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.876180 loss:        0.365994
Test - acc:         0.847600 loss:        0.466440
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.876200 loss:        0.364513
Test - acc:         0.857100 loss:        0.424431
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.877120 loss:        0.362620
Test - acc:         0.863300 loss:        0.407333
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.877120 loss:        0.364274
Test - acc:         0.819100 loss:        0.550184
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.877400 loss:        0.363943
Test - acc:         0.790500 loss:        0.743917
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.878340 loss:        0.360365
Test - acc:         0.842400 loss:        0.496927
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.874860 loss:        0.367352
Test - acc:         0.866700 loss:        0.407038
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.878140 loss:        0.360564
Test - acc:         0.845800 loss:        0.479021
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.877100 loss:        0.363999
Test - acc:         0.800000 loss:        0.637145
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.874760 loss:        0.368491
Test - acc:         0.842300 loss:        0.475950
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.875260 loss:        0.365405
Test - acc:         0.855500 loss:        0.440905
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.874820 loss:        0.367829
Test - acc:         0.844000 loss:        0.477550
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.878560 loss:        0.362315
Test - acc:         0.776700 loss:        0.706153
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.876360 loss:        0.363822
Test - acc:         0.816900 loss:        0.572327
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.875740 loss:        0.363964
Test - acc:         0.840700 loss:        0.476310
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.876060 loss:        0.364092
Test - acc:         0.816700 loss:        0.581204
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.877100 loss:        0.363377
Test - acc:         0.797400 loss:        0.666062
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.875580 loss:        0.362450
Test - acc:         0.796400 loss:        0.638242
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.876480 loss:        0.359098
Test - acc:         0.795600 loss:        0.680140
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.878660 loss:        0.362216
Test - acc:         0.789300 loss:        0.656614
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.877880 loss:        0.358830
Test - acc:         0.843800 loss:        0.471332
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.916560 loss:        0.244082
Test - acc:         0.904300 loss:        0.289903
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.929780 loss:        0.207167
Test - acc:         0.907200 loss:        0.278336
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.933900 loss:        0.192033
Test - acc:         0.910700 loss:        0.271928
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.939020 loss:        0.181297
Test - acc:         0.909100 loss:        0.279626
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.939720 loss:        0.176277
Test - acc:         0.913300 loss:        0.269183
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.943240 loss:        0.165268
Test - acc:         0.913000 loss:        0.280033
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.943940 loss:        0.164154
Test - acc:         0.911000 loss:        0.278754
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.944800 loss:        0.159358
Test - acc:         0.913600 loss:        0.270207
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.946500 loss:        0.155178
Test - acc:         0.914600 loss:        0.273589
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.947380 loss:        0.151529
Test - acc:         0.914100 loss:        0.276772
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.942180 loss:        0.169180
Test - acc:         0.909400 loss:        0.284085
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.942820 loss:        0.165796
Test - acc:         0.910600 loss:        0.281894
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.945140 loss:        0.158620
Test - acc:         0.912300 loss:        0.276844
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.946340 loss:        0.155740
Test - acc:         0.912300 loss:        0.279701
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.948480 loss:        0.149797
Test - acc:         0.907600 loss:        0.291712
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.947280 loss:        0.154900
Test - acc:         0.905100 loss:        0.294880
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.947560 loss:        0.150843
Test - acc:         0.910800 loss:        0.294038
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.948680 loss:        0.152071
Test - acc:         0.912100 loss:        0.282017
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.948100 loss:        0.149831
Test - acc:         0.907200 loss:        0.287383
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.948420 loss:        0.148069
Test - acc:         0.909800 loss:        0.291051
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.949600 loss:        0.144552
Test - acc:         0.908900 loss:        0.299819
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.948180 loss:        0.148407
Test - acc:         0.909000 loss:        0.295866
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.949440 loss:        0.143970
Test - acc:         0.907000 loss:        0.301998
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.949700 loss:        0.147112
Test - acc:         0.905400 loss:        0.311986
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.949420 loss:        0.144419
Test - acc:         0.907800 loss:        0.302445
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.948780 loss:        0.146368
Test - acc:         0.900300 loss:        0.328384
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.949080 loss:        0.147179
Test - acc:         0.903600 loss:        0.308715
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.947320 loss:        0.149900
Test - acc:         0.898400 loss:        0.339434
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.947280 loss:        0.150129
Test - acc:         0.907800 loss:        0.297651
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.948100 loss:        0.149069
Test - acc:         0.905600 loss:        0.308251
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.947900 loss:        0.150562
Test - acc:         0.899500 loss:        0.328689
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.947940 loss:        0.149582
Test - acc:         0.903900 loss:        0.316767
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.946720 loss:        0.152745
Test - acc:         0.908200 loss:        0.298139
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.948080 loss:        0.150228
Test - acc:         0.902800 loss:        0.314914
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.947540 loss:        0.149228
Test - acc:         0.905500 loss:        0.303473
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.947220 loss:        0.149572
Test - acc:         0.893800 loss:        0.341290
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.946600 loss:        0.152205
Test - acc:         0.896700 loss:        0.333543
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.947680 loss:        0.150380
Test - acc:         0.893800 loss:        0.338041
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.947000 loss:        0.151041
Test - acc:         0.901700 loss:        0.315506
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.947120 loss:        0.151055
Test - acc:         0.905700 loss:        0.306093
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.948440 loss:        0.148161
Test - acc:         0.906800 loss:        0.309914
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.946800 loss:        0.151997
Test - acc:         0.901400 loss:        0.325397
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.935820 loss:        0.182301
Test - acc:         0.902000 loss:        0.320219
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.941060 loss:        0.171040
Test - acc:         0.902600 loss:        0.312922
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.940940 loss:        0.168060
Test - acc:         0.896900 loss:        0.326320
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.942100 loss:        0.168741
Test - acc:         0.895400 loss:        0.321477
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.942280 loss:        0.166182
Test - acc:         0.893800 loss:        0.335858
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.940860 loss:        0.166270
Test - acc:         0.896000 loss:        0.341525
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.941620 loss:        0.166915
Test - acc:         0.899500 loss:        0.322220
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.942540 loss:        0.165623
Test - acc:         0.895800 loss:        0.331654
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.943300 loss:        0.162834
Test - acc:         0.899600 loss:        0.321696
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.940980 loss:        0.168580
Test - acc:         0.893300 loss:        0.330007
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.941640 loss:        0.166387
Test - acc:         0.894900 loss:        0.344577
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.941600 loss:        0.165008
Test - acc:         0.892400 loss:        0.337334
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.942540 loss:        0.166733
Test - acc:         0.900800 loss:        0.314202
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.940860 loss:        0.165890
Test - acc:         0.900100 loss:        0.312283
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.940720 loss:        0.168666
Test - acc:         0.899800 loss:        0.311057
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.940460 loss:        0.167055
Test - acc:         0.898000 loss:        0.324672
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.941860 loss:        0.164724
Test - acc:         0.895800 loss:        0.342481
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.941920 loss:        0.165606
Test - acc:         0.895800 loss:        0.333870
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.942940 loss:        0.162784
Test - acc:         0.895400 loss:        0.343180
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.943700 loss:        0.163120
Test - acc:         0.899800 loss:        0.325418
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.940540 loss:        0.168184
Test - acc:         0.891200 loss:        0.339173
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.943060 loss:        0.162612
Test - acc:         0.890500 loss:        0.362111
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.942480 loss:        0.163836
Test - acc:         0.893400 loss:        0.342567
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.942720 loss:        0.163240
Test - acc:         0.899800 loss:        0.323937
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.942400 loss:        0.163337
Test - acc:         0.898500 loss:        0.327222
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.941900 loss:        0.167211
Test - acc:         0.900200 loss:        0.311879
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.943500 loss:        0.160830
Test - acc:         0.893700 loss:        0.327126
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.941300 loss:        0.169305
Test - acc:         0.900200 loss:        0.322938
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.942320 loss:        0.164796
Test - acc:         0.899000 loss:        0.321155
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.943340 loss:        0.163204
Test - acc:         0.900300 loss:        0.323850
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.942040 loss:        0.162520
Test - acc:         0.900800 loss:        0.322280
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.941580 loss:        0.164174
Test - acc:         0.891100 loss:        0.342598
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.927620 loss:        0.207377
Test - acc:         0.884800 loss:        0.364430
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.931840 loss:        0.195088
Test - acc:         0.894500 loss:        0.341744
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.932460 loss:        0.192692
Test - acc:         0.887600 loss:        0.349451
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.932560 loss:        0.187482
Test - acc:         0.882300 loss:        0.383323
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.933940 loss:        0.186539
Test - acc:         0.889600 loss:        0.348550
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.936760 loss:        0.180842
Test - acc:         0.894400 loss:        0.334494
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.935060 loss:        0.183557
Test - acc:         0.885500 loss:        0.364900
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.935760 loss:        0.182750
Test - acc:         0.896500 loss:        0.329562
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.935140 loss:        0.184967
Test - acc:         0.890000 loss:        0.351872
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.938540 loss:        0.178211
Test - acc:         0.901800 loss:        0.319769
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.937640 loss:        0.179862
Test - acc:         0.894900 loss:        0.344292
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.937460 loss:        0.179691
Test - acc:         0.895300 loss:        0.336045
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.936480 loss:        0.180281
Test - acc:         0.893300 loss:        0.336324
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.938480 loss:        0.178402
Test - acc:         0.891700 loss:        0.347925
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.936980 loss:        0.180372
Test - acc:         0.898100 loss:        0.324622
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.937300 loss:        0.179679
Test - acc:         0.898900 loss:        0.338294
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.937500 loss:        0.179224
Test - acc:         0.896700 loss:        0.330486
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.936580 loss:        0.177007
Test - acc:         0.887900 loss:        0.372137
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.936560 loss:        0.180219
Test - acc:         0.884100 loss:        0.386977
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.938240 loss:        0.178579
Test - acc:         0.894200 loss:        0.325321
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.938660 loss:        0.177195
Test - acc:         0.889300 loss:        0.354769
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.936680 loss:        0.177283
Test - acc:         0.891800 loss:        0.341026
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.937280 loss:        0.179678
Test - acc:         0.891000 loss:        0.348801
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.939300 loss:        0.174784
Test - acc:         0.890900 loss:        0.349244
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.937700 loss:        0.180985
Test - acc:         0.898700 loss:        0.319793
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.937820 loss:        0.175635
Test - acc:         0.881600 loss:        0.384235
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.950860 loss:        0.140236
Test - acc:         0.909600 loss:        0.284084
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.956080 loss:        0.126657
Test - acc:         0.911600 loss:        0.282537
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.959600 loss:        0.120054
Test - acc:         0.912400 loss:        0.286640
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.961160 loss:        0.113715
Test - acc:         0.912800 loss:        0.282865
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.960940 loss:        0.112307
Test - acc:         0.911900 loss:        0.286774
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.961180 loss:        0.110400
Test - acc:         0.911300 loss:        0.287245
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.928000 loss:        0.204180
Test - acc:         0.895500 loss:        0.328349
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.936760 loss:        0.181921
Test - acc:         0.900600 loss:        0.318650
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.938740 loss:        0.175367
Test - acc:         0.899100 loss:        0.319596
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.940820 loss:        0.169731
Test - acc:         0.902500 loss:        0.316164
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.941700 loss:        0.165530
Test - acc:         0.903600 loss:        0.315413
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.944000 loss:        0.161921
Test - acc:         0.902300 loss:        0.308783
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.944680 loss:        0.158536
Test - acc:         0.903200 loss:        0.310111
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.945160 loss:        0.157858
Test - acc:         0.904100 loss:        0.310789
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.946540 loss:        0.152601
Test - acc:         0.906800 loss:        0.310579
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.946080 loss:        0.151234
Test - acc:         0.905400 loss:        0.312226
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.948520 loss:        0.149370
Test - acc:         0.903000 loss:        0.311385
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.946620 loss:        0.151079
Test - acc:         0.903900 loss:        0.311866
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.948240 loss:        0.148121
Test - acc:         0.902600 loss:        0.312168
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.948540 loss:        0.146504
Test - acc:         0.904100 loss:        0.308474
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.951020 loss:        0.142685
Test - acc:         0.902600 loss:        0.313701
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.949280 loss:        0.143228
Test - acc:         0.904700 loss:        0.311308
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.949180 loss:        0.144658
Test - acc:         0.905600 loss:        0.309447
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.950860 loss:        0.140135
Test - acc:         0.904900 loss:        0.307153
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.950860 loss:        0.140307
Test - acc:         0.904500 loss:        0.310981
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.950620 loss:        0.141255
Test - acc:         0.904700 loss:        0.311422
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.950160 loss:        0.140438
Test - acc:         0.904200 loss:        0.307807
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.951680 loss:        0.136720
Test - acc:         0.904400 loss:        0.307141
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.951400 loss:        0.136632
Test - acc:         0.905500 loss:        0.312568
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.951600 loss:        0.137930
Test - acc:         0.905600 loss:        0.307793
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.952280 loss:        0.136396
Test - acc:         0.905300 loss:        0.307698
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.952960 loss:        0.135036
Test - acc:         0.904500 loss:        0.308424
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.952580 loss:        0.135383
Test - acc:         0.905500 loss:        0.309540
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.953060 loss:        0.134399
Test - acc:         0.907300 loss:        0.308643
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.953660 loss:        0.132318
Test - acc:         0.906700 loss:        0.311047
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.955340 loss:        0.128967
Test - acc:         0.907400 loss:        0.312765
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.953760 loss:        0.131010
Test - acc:         0.904200 loss:        0.311847
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.953580 loss:        0.128792
Test - acc:         0.905500 loss:        0.313950
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.844000 loss:        0.460921
Test - acc:         0.855700 loss:        0.443692
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.873540 loss:        0.367863
Test - acc:         0.863200 loss:        0.411261
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.883040 loss:        0.340873
Test - acc:         0.867700 loss:        0.398811
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.886660 loss:        0.325770
Test - acc:         0.872600 loss:        0.387270
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.890460 loss:        0.312406
Test - acc:         0.874300 loss:        0.380210
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.893580 loss:        0.306424
Test - acc:         0.875600 loss:        0.376909
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.897040 loss:        0.298326
Test - acc:         0.874900 loss:        0.376180
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.896800 loss:        0.295356
Test - acc:         0.876100 loss:        0.371523
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.899480 loss:        0.289999
Test - acc:         0.879400 loss:        0.366524
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.900960 loss:        0.286618
Test - acc:         0.881000 loss:        0.366961
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.903440 loss:        0.280062
Test - acc:         0.882000 loss:        0.359382
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.903720 loss:        0.277420
Test - acc:         0.878600 loss:        0.367002
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.902900 loss:        0.277076
Test - acc:         0.879300 loss:        0.364042
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.905560 loss:        0.270768
Test - acc:         0.881200 loss:        0.363325
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.905940 loss:        0.270031
Test - acc:         0.882500 loss:        0.357977
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.906240 loss:        0.267435
Test - acc:         0.879200 loss:        0.361241
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.908120 loss:        0.263362
Test - acc:         0.882500 loss:        0.358982
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.907880 loss:        0.264183
Test - acc:         0.883000 loss:        0.356870
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.907540 loss:        0.263764
Test - acc:         0.882600 loss:        0.356614
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.909860 loss:        0.260855
Test - acc:         0.883400 loss:        0.351631
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.910740 loss:        0.255869
Test - acc:         0.884100 loss:        0.351701
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.909280 loss:        0.258918
Test - acc:         0.883300 loss:        0.357178
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.908540 loss:        0.259780
Test - acc:         0.884700 loss:        0.353378
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.911080 loss:        0.256398
Test - acc:         0.884400 loss:        0.355510
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.914000 loss:        0.249272
Test - acc:         0.885700 loss:        0.356811
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.911660 loss:        0.251209
Test - acc:         0.885300 loss:        0.355525
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.912520 loss:        0.250756
Test - acc:         0.884800 loss:        0.353142
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.912040 loss:        0.248710
Test - acc:         0.886300 loss:        0.353338
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.912960 loss:        0.249675
Test - acc:         0.885100 loss:        0.353453
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.913120 loss:        0.250615
Test - acc:         0.883800 loss:        0.352878
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.914580 loss:        0.246356
Test - acc:         0.883400 loss:        0.353123
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.913420 loss:        0.246495
Test - acc:         0.886200 loss:        0.353415
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.629120 loss:        1.087430
Test - acc:         0.722200 loss:        0.836727
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.738580 loss:        0.806444
Test - acc:         0.761700 loss:        0.722556
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.763440 loss:        0.718075
Test - acc:         0.779200 loss:        0.665559
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.775900 loss:        0.674767
Test - acc:         0.787900 loss:        0.621295
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.787660 loss:        0.638329
Test - acc:         0.792600 loss:        0.608370
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.792040 loss:        0.617248
Test - acc:         0.795700 loss:        0.591405
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.797060 loss:        0.597142
Test - acc:         0.799500 loss:        0.587124
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.803060 loss:        0.583449
Test - acc:         0.803600 loss:        0.571652
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.803820 loss:        0.576138
Test - acc:         0.810300 loss:        0.554253
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.806880 loss:        0.565790
Test - acc:         0.806700 loss:        0.557258
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.811600 loss:        0.553711
Test - acc:         0.810900 loss:        0.549130
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.813940 loss:        0.547776
Test - acc:         0.816000 loss:        0.538186
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.814680 loss:        0.543419
Test - acc:         0.810000 loss:        0.555286
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.817340 loss:        0.536892
Test - acc:         0.817900 loss:        0.541693
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.816660 loss:        0.532704
Test - acc:         0.816800 loss:        0.540759
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.821000 loss:        0.525006
Test - acc:         0.815000 loss:        0.537934
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.822000 loss:        0.523452
Test - acc:         0.825400 loss:        0.520161
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.822580 loss:        0.515202
Test - acc:         0.824700 loss:        0.515850
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.822780 loss:        0.513019
Test - acc:         0.819500 loss:        0.525752
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.826900 loss:        0.506506
Test - acc:         0.827300 loss:        0.512380
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.825420 loss:        0.508497
Test - acc:         0.826600 loss:        0.516990
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.826000 loss:        0.507950
Test - acc:         0.827300 loss:        0.507154
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.828920 loss:        0.500428
Test - acc:         0.827100 loss:        0.509024
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.828860 loss:        0.496034
Test - acc:         0.828600 loss:        0.507480
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.828160 loss:        0.497546
Test - acc:         0.821300 loss:        0.525283
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.829960 loss:        0.497625
Test - acc:         0.824400 loss:        0.513021
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.831160 loss:        0.493858
Test - acc:         0.832300 loss:        0.494737
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.829660 loss:        0.494120
Test - acc:         0.824600 loss:        0.510463
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.833360 loss:        0.487837
Test - acc:         0.825500 loss:        0.512746
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.831920 loss:        0.491666
Test - acc:         0.827100 loss:        0.502928
Sparsity :          0.9990
Wdecay :        0.000500
