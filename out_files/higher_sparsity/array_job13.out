Running --model resnet18 --prune_criterion random --seed 44 --prune_freq 32 --prune_rate 0.5 --comment=resnet18_crit=random_pf=32_seed=44 --save_model=pre-finetune/resnet18_random_pf32_s44 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "random",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_random_pf32_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.326500 loss:        1.844461
Test - acc:         0.423100 loss:        1.566802
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.523060 loss:        1.311480
Test - acc:         0.573700 loss:        1.162164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.631560 loss:        1.037143
Test - acc:         0.605500 loss:        1.154360
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.690180 loss:        0.879254
Test - acc:         0.698600 loss:        0.870134
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.743000 loss:        0.739316
Test - acc:         0.741600 loss:        0.760918
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.775920 loss:        0.646051
Test - acc:         0.761800 loss:        0.712189
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.798040 loss:        0.583349
Test - acc:         0.699400 loss:        0.960318
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.808880 loss:        0.550098
Test - acc:         0.719800 loss:        0.895329
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.822400 loss:        0.517782
Test - acc:         0.798300 loss:        0.592408
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.828280 loss:        0.498775
Test - acc:         0.803500 loss:        0.578936
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.838380 loss:        0.470293
Test - acc:         0.802200 loss:        0.613507
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.840980 loss:        0.464587
Test - acc:         0.794200 loss:        0.611622
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.847140 loss:        0.446681
Test - acc:         0.812200 loss:        0.584225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.851420 loss:        0.433831
Test - acc:         0.745900 loss:        0.817999
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.852860 loss:        0.428770
Test - acc:         0.797000 loss:        0.603769
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.856840 loss:        0.417291
Test - acc:         0.793900 loss:        0.644432
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.862140 loss:        0.409136
Test - acc:         0.824900 loss:        0.529044
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.861320 loss:        0.401354
Test - acc:         0.815400 loss:        0.555978
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.863660 loss:        0.396668
Test - acc:         0.817300 loss:        0.531897
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.866860 loss:        0.390623
Test - acc:         0.770700 loss:        0.716770
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.868780 loss:        0.385096
Test - acc:         0.832300 loss:        0.500326
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870100 loss:        0.382342
Test - acc:         0.813700 loss:        0.540815
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.869500 loss:        0.384136
Test - acc:         0.837000 loss:        0.476577
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.872000 loss:        0.375453
Test - acc:         0.842100 loss:        0.462574
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.874400 loss:        0.369435
Test - acc:         0.822800 loss:        0.543520
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.873880 loss:        0.369378
Test - acc:         0.853400 loss:        0.432042
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.876600 loss:        0.359764
Test - acc:         0.824500 loss:        0.514843
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.877660 loss:        0.357625
Test - acc:         0.769200 loss:        0.763895
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.878520 loss:        0.356873
Test - acc:         0.815700 loss:        0.576099
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.878700 loss:        0.357575
Test - acc:         0.829500 loss:        0.503405
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.877880 loss:        0.360216
Test - acc:         0.826900 loss:        0.531613
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.881500 loss:        0.347606
Test - acc:         0.833600 loss:        0.533857
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.820200 loss:        0.529785
Test - acc:         0.772300 loss:        0.733482
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.852440 loss:        0.432089
Test - acc:         0.831000 loss:        0.513110
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.863720 loss:        0.401092
Test - acc:         0.824000 loss:        0.508964
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.866420 loss:        0.392060
Test - acc:         0.832300 loss:        0.512655
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.868680 loss:        0.383378
Test - acc:         0.828100 loss:        0.529036
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.872840 loss:        0.374156
Test - acc:         0.851800 loss:        0.441207
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.873440 loss:        0.368543
Test - acc:         0.831600 loss:        0.520097
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.874280 loss:        0.368289
Test - acc:         0.848400 loss:        0.449238
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.875060 loss:        0.367127
Test - acc:         0.813800 loss:        0.579220
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.878520 loss:        0.357258
Test - acc:         0.825900 loss:        0.546501
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.876860 loss:        0.359916
Test - acc:         0.824100 loss:        0.525716
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.877980 loss:        0.357625
Test - acc:         0.827600 loss:        0.544874
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.877940 loss:        0.354910
Test - acc:         0.817500 loss:        0.566570
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.881280 loss:        0.351213
Test - acc:         0.859300 loss:        0.435727
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.878760 loss:        0.350661
Test - acc:         0.862600 loss:        0.403744
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.879640 loss:        0.350409
Test - acc:         0.823100 loss:        0.547223
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.881260 loss:        0.348837
Test - acc:         0.830700 loss:        0.527914
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.882880 loss:        0.346304
Test - acc:         0.824100 loss:        0.528781
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.882740 loss:        0.342750
Test - acc:         0.835400 loss:        0.504965
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.882500 loss:        0.347269
Test - acc:         0.842800 loss:        0.479915
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.883740 loss:        0.342466
Test - acc:         0.835400 loss:        0.508349
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.885580 loss:        0.341527
Test - acc:         0.824400 loss:        0.533775
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.881780 loss:        0.346688
Test - acc:         0.822300 loss:        0.542422
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.885340 loss:        0.338507
Test - acc:         0.780800 loss:        0.700306
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.882620 loss:        0.343848
Test - acc:         0.835000 loss:        0.513418
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.883580 loss:        0.335889
Test - acc:         0.819100 loss:        0.572119
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.884020 loss:        0.342765
Test - acc:         0.855300 loss:        0.426711
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.887580 loss:        0.331421
Test - acc:         0.825300 loss:        0.566401
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.886800 loss:        0.334264
Test - acc:         0.857400 loss:        0.442442
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.883680 loss:        0.340709
Test - acc:         0.806000 loss:        0.603358
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.885240 loss:        0.337869
Test - acc:         0.799700 loss:        0.605926
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.885580 loss:        0.335765
Test - acc:         0.851500 loss:        0.440705
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.799260 loss:        0.588885
Test - acc:         0.746200 loss:        0.806796
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.843820 loss:        0.458395
Test - acc:         0.832600 loss:        0.511691
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.854080 loss:        0.427852
Test - acc:         0.809300 loss:        0.566357
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.861540 loss:        0.409244
Test - acc:         0.840800 loss:        0.489064
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.860960 loss:        0.404279
Test - acc:         0.822700 loss:        0.540391
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.867280 loss:        0.388226
Test - acc:         0.801200 loss:        0.601852
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.867700 loss:        0.391333
Test - acc:         0.846500 loss:        0.456434
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.867920 loss:        0.385578
Test - acc:         0.842200 loss:        0.457865
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.870940 loss:        0.380046
Test - acc:         0.830900 loss:        0.513064
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.870480 loss:        0.379976
Test - acc:         0.813800 loss:        0.573482
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.868040 loss:        0.386130
Test - acc:         0.800200 loss:        0.594051
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.875000 loss:        0.368972
Test - acc:         0.814400 loss:        0.573845
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.873820 loss:        0.371407
Test - acc:         0.819300 loss:        0.547859
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.870920 loss:        0.374122
Test - acc:         0.780400 loss:        0.659804
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.874000 loss:        0.367725
Test - acc:         0.836600 loss:        0.490914
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.875620 loss:        0.363738
Test - acc:         0.835200 loss:        0.501010
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.872680 loss:        0.369072
Test - acc:         0.821700 loss:        0.536556
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.875920 loss:        0.364700
Test - acc:         0.844200 loss:        0.484879
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.876140 loss:        0.364690
Test - acc:         0.838900 loss:        0.463870
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.874800 loss:        0.366594
Test - acc:         0.798500 loss:        0.611152
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.875720 loss:        0.362134
Test - acc:         0.830600 loss:        0.486974
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.876500 loss:        0.360215
Test - acc:         0.844300 loss:        0.470998
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.875860 loss:        0.362665
Test - acc:         0.821900 loss:        0.532721
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.877200 loss:        0.360506
Test - acc:         0.844000 loss:        0.476381
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.877200 loss:        0.358035
Test - acc:         0.762100 loss:        0.795177
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.877720 loss:        0.357762
Test - acc:         0.822600 loss:        0.525779
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.877540 loss:        0.359902
Test - acc:         0.820100 loss:        0.528005
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.874720 loss:        0.364475
Test - acc:         0.840200 loss:        0.468811
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.877600 loss:        0.355518
Test - acc:         0.832700 loss:        0.499660
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.878140 loss:        0.355788
Test - acc:         0.847400 loss:        0.469799
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.878300 loss:        0.360112
Test - acc:         0.839200 loss:        0.486945
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.876680 loss:        0.361291
Test - acc:         0.852600 loss:        0.439980
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.752680 loss:        0.721352
Test - acc:         0.761700 loss:        0.730419
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.818200 loss:        0.534559
Test - acc:         0.734400 loss:        0.840818
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.830000 loss:        0.497625
Test - acc:         0.766700 loss:        0.707704
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.836700 loss:        0.475873
Test - acc:         0.742900 loss:        0.785733
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.842220 loss:        0.460245
Test - acc:         0.761700 loss:        0.744391
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.847000 loss:        0.445850
Test - acc:         0.821300 loss:        0.536048
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.849300 loss:        0.442082
Test - acc:         0.828500 loss:        0.527053
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.850080 loss:        0.439972
Test - acc:         0.775800 loss:        0.706328
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.851600 loss:        0.431804
Test - acc:         0.751300 loss:        0.795078
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.852200 loss:        0.430263
Test - acc:         0.767700 loss:        0.739354
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.852180 loss:        0.432483
Test - acc:         0.800200 loss:        0.579846
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.853520 loss:        0.423519
Test - acc:         0.778700 loss:        0.668187
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.854480 loss:        0.424607
Test - acc:         0.816400 loss:        0.560358
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.858040 loss:        0.416181
Test - acc:         0.718900 loss:        0.927856
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.857260 loss:        0.416239
Test - acc:         0.816900 loss:        0.547707
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.854960 loss:        0.421252
Test - acc:         0.801300 loss:        0.599339
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.857920 loss:        0.413821
Test - acc:         0.766700 loss:        0.739467
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.856740 loss:        0.413922
Test - acc:         0.735400 loss:        0.883469
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.858620 loss:        0.409966
Test - acc:         0.803500 loss:        0.574438
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.859960 loss:        0.411117
Test - acc:         0.814000 loss:        0.576038
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.861660 loss:        0.405288
Test - acc:         0.807300 loss:        0.592263
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.859160 loss:        0.411317
Test - acc:         0.822400 loss:        0.525435
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.859420 loss:        0.409517
Test - acc:         0.805800 loss:        0.557825
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.860220 loss:        0.405848
Test - acc:         0.828900 loss:        0.516474
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.860940 loss:        0.405631
Test - acc:         0.819100 loss:        0.524992
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.860500 loss:        0.404099
Test - acc:         0.822200 loss:        0.524263
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.860860 loss:        0.407859
Test - acc:         0.808600 loss:        0.578452
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.861480 loss:        0.406279
Test - acc:         0.819800 loss:        0.547320
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.860740 loss:        0.405109
Test - acc:         0.820100 loss:        0.539655
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.861060 loss:        0.402325
Test - acc:         0.824900 loss:        0.515975
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.863460 loss:        0.398648
Test - acc:         0.830100 loss:        0.496524
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.861080 loss:        0.405423
Test - acc:         0.800000 loss:        0.588187
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.688860 loss:        0.889546
Test - acc:         0.633700 loss:        1.122764
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.781440 loss:        0.633814
Test - acc:         0.699900 loss:        0.904850
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.798180 loss:        0.588537
Test - acc:         0.743700 loss:        0.796448
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.808620 loss:        0.558708
Test - acc:         0.788500 loss:        0.637497
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.810600 loss:        0.547777
Test - acc:         0.788500 loss:        0.627984
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.817080 loss:        0.529911
Test - acc:         0.784600 loss:        0.628610
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.819720 loss:        0.524063
Test - acc:         0.726500 loss:        0.851694
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.822800 loss:        0.517310
Test - acc:         0.764300 loss:        0.687684
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.824420 loss:        0.510171
Test - acc:         0.758600 loss:        0.752161
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.827580 loss:        0.501946
Test - acc:         0.776300 loss:        0.674962
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.829440 loss:        0.500893
Test - acc:         0.735100 loss:        0.802880
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.828840 loss:        0.499680
Test - acc:         0.783600 loss:        0.639118
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.830980 loss:        0.493276
Test - acc:         0.749500 loss:        0.774446
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.830300 loss:        0.491537
Test - acc:         0.778800 loss:        0.660988
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.832540 loss:        0.488433
Test - acc:         0.791700 loss:        0.622406
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.830640 loss:        0.490982
Test - acc:         0.799200 loss:        0.582768
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.832620 loss:        0.488111
Test - acc:         0.807600 loss:        0.573097
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.834120 loss:        0.482164
Test - acc:         0.778800 loss:        0.649534
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.833840 loss:        0.485460
Test - acc:         0.757800 loss:        0.719262
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.833360 loss:        0.481420
Test - acc:         0.797400 loss:        0.619826
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.835200 loss:        0.478800
Test - acc:         0.806000 loss:        0.580943
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.834820 loss:        0.479171
Test - acc:         0.762900 loss:        0.723522
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.886840 loss:        0.334741
Test - acc:         0.884300 loss:        0.332836
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.902520 loss:        0.285839
Test - acc:         0.888600 loss:        0.324542
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.909660 loss:        0.264835
Test - acc:         0.893000 loss:        0.309097
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.914880 loss:        0.248751
Test - acc:         0.894200 loss:        0.311857
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.919600 loss:        0.234225
Test - acc:         0.893300 loss:        0.314266
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.921240 loss:        0.227783
Test - acc:         0.895500 loss:        0.304481
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.924000 loss:        0.221115
Test - acc:         0.896800 loss:        0.313272
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.926700 loss:        0.213356
Test - acc:         0.899100 loss:        0.305446
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.928940 loss:        0.206393
Test - acc:         0.897800 loss:        0.311581
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.930540 loss:        0.200875
Test - acc:         0.895300 loss:        0.318688
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.545000 loss:        1.258662
Test - acc:         0.662400 loss:        0.944364
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.715440 loss:        0.815035
Test - acc:         0.727800 loss:        0.773801
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.772260 loss:        0.656189
Test - acc:         0.755800 loss:        0.703550
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.799700 loss:        0.582620
Test - acc:         0.804500 loss:        0.571349
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.813640 loss:        0.538499
Test - acc:         0.804300 loss:        0.563056
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.823320 loss:        0.510293
Test - acc:         0.811100 loss:        0.555789
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.831620 loss:        0.485445
Test - acc:         0.797300 loss:        0.602036
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.837740 loss:        0.472344
Test - acc:         0.825000 loss:        0.517317
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.845320 loss:        0.447551
Test - acc:         0.824400 loss:        0.513034
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.849840 loss:        0.433492
Test - acc:         0.835100 loss:        0.481208
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.853240 loss:        0.422945
Test - acc:         0.838100 loss:        0.469924
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.856520 loss:        0.416769
Test - acc:         0.822100 loss:        0.520853
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.857460 loss:        0.407535
Test - acc:         0.834400 loss:        0.495420
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.861540 loss:        0.399676
Test - acc:         0.828400 loss:        0.499506
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.861300 loss:        0.395064
Test - acc:         0.842100 loss:        0.461845
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.866640 loss:        0.386766
Test - acc:         0.841800 loss:        0.463178
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.869300 loss:        0.376633
Test - acc:         0.844600 loss:        0.467210
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.868040 loss:        0.375963
Test - acc:         0.842400 loss:        0.467455
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.871880 loss:        0.374027
Test - acc:         0.828200 loss:        0.506745
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.872860 loss:        0.364904
Test - acc:         0.837100 loss:        0.487577
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.873440 loss:        0.363010
Test - acc:         0.839800 loss:        0.483802
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.872660 loss:        0.361740
Test - acc:         0.852600 loss:        0.444136
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.876380 loss:        0.354042
Test - acc:         0.811600 loss:        0.584750
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.876940 loss:        0.350949
Test - acc:         0.852300 loss:        0.443094
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.881300 loss:        0.348854
Test - acc:         0.835600 loss:        0.506987
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.877360 loss:        0.346744
Test - acc:         0.856100 loss:        0.430678
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.880260 loss:        0.341507
Test - acc:         0.854700 loss:        0.432076
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.882980 loss:        0.340587
Test - acc:         0.854200 loss:        0.436539
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.882740 loss:        0.338524
Test - acc:         0.831100 loss:        0.507495
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.884620 loss:        0.334379
Test - acc:         0.849900 loss:        0.449716
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.882400 loss:        0.340982
Test - acc:         0.850600 loss:        0.447780
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.884580 loss:        0.329993
Test - acc:         0.862700 loss:        0.416079
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.409300 loss:        1.639469
Test - acc:         0.450500 loss:        1.626173
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.538920 loss:        1.284619
Test - acc:         0.531000 loss:        1.312903
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.601100 loss:        1.119012
Test - acc:         0.616800 loss:        1.074117
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.640880 loss:        1.020040
Test - acc:         0.641500 loss:        0.995931
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.662540 loss:        0.953519
Test - acc:         0.631600 loss:        1.072282
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.681240 loss:        0.901498
Test - acc:         0.701000 loss:        0.849894
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.698120 loss:        0.857680
Test - acc:         0.692300 loss:        0.876439
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.710320 loss:        0.820696
Test - acc:         0.663900 loss:        0.987509
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.724860 loss:        0.784999
Test - acc:         0.721100 loss:        0.796850
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.735920 loss:        0.759722
Test - acc:         0.720400 loss:        0.803134
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.740640 loss:        0.738141
Test - acc:         0.705900 loss:        0.850563
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.748800 loss:        0.713383
Test - acc:         0.717600 loss:        0.819747
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.754960 loss:        0.699295
Test - acc:         0.749200 loss:        0.719647
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.759540 loss:        0.686170
Test - acc:         0.760000 loss:        0.688844
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.767480 loss:        0.665061
Test - acc:         0.746500 loss:        0.722085
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.769460 loss:        0.658223
Test - acc:         0.742900 loss:        0.761017
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.775960 loss:        0.644996
Test - acc:         0.746800 loss:        0.726490
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.780220 loss:        0.630720
Test - acc:         0.735700 loss:        0.775496
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.781840 loss:        0.622931
Test - acc:         0.762300 loss:        0.676669
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.784020 loss:        0.614078
Test - acc:         0.736500 loss:        0.742779
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.787120 loss:        0.605167
Test - acc:         0.769200 loss:        0.680656
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.791500 loss:        0.597423
Test - acc:         0.783500 loss:        0.619107
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.793700 loss:        0.590525
Test - acc:         0.784100 loss:        0.620414
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.795660 loss:        0.586095
Test - acc:         0.785800 loss:        0.609764
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.798540 loss:        0.571645
Test - acc:         0.777800 loss:        0.653351
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.797400 loss:        0.574674
Test - acc:         0.785500 loss:        0.631495
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.800740 loss:        0.571779
Test - acc:         0.769500 loss:        0.665497
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.804120 loss:        0.563844
Test - acc:         0.780900 loss:        0.664435
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.805540 loss:        0.554672
Test - acc:         0.789500 loss:        0.602220
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.807460 loss:        0.554548
Test - acc:         0.785500 loss:        0.623286
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.807340 loss:        0.551453
Test - acc:         0.764300 loss:        0.684315
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.811120 loss:        0.545928
Test - acc:         0.782200 loss:        0.640862
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.371800 loss:        1.753974
Test - acc:         0.364600 loss:        1.786548
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.464540 loss:        1.485363
Test - acc:         0.478000 loss:        1.449265
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.508060 loss:        1.370154
Test - acc:         0.504300 loss:        1.364636
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.538640 loss:        1.288884
Test - acc:         0.547400 loss:        1.278367
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.564380 loss:        1.222849
Test - acc:         0.537500 loss:        1.372440
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.580700 loss:        1.174996
Test - acc:         0.538300 loss:        1.284101
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.596560 loss:        1.139235
Test - acc:         0.585400 loss:        1.166949
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.607540 loss:        1.103730
Test - acc:         0.590800 loss:        1.158685
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.618980 loss:        1.077032
Test - acc:         0.577000 loss:        1.190352
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.625580 loss:        1.053528
Test - acc:         0.649500 loss:        1.004522
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.637440 loss:        1.026924
Test - acc:         0.616600 loss:        1.092057
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.643040 loss:        1.006087
Test - acc:         0.613400 loss:        1.093946
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.650520 loss:        0.986471
Test - acc:         0.606200 loss:        1.121913
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.658060 loss:        0.973040
Test - acc:         0.601400 loss:        1.179485
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.661100 loss:        0.962383
Test - acc:         0.610200 loss:        1.170188
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.668880 loss:        0.945513
Test - acc:         0.666000 loss:        0.954468
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.674280 loss:        0.932212
Test - acc:         0.614500 loss:        1.159937
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.673920 loss:        0.924228
Test - acc:         0.624100 loss:        1.093739
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.681280 loss:        0.909176
Test - acc:         0.683100 loss:        0.898309
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.682460 loss:        0.900869
Test - acc:         0.650100 loss:        0.990660
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.683940 loss:        0.897994
Test - acc:         0.665800 loss:        0.935651
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.690680 loss:        0.882804
Test - acc:         0.680900 loss:        0.899804
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.693140 loss:        0.877008
Test - acc:         0.574600 loss:        1.282179
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.698680 loss:        0.862429
Test - acc:         0.689400 loss:        0.886178
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.699380 loss:        0.860298
Test - acc:         0.681700 loss:        0.918190
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.699700 loss:        0.849936
Test - acc:         0.673700 loss:        0.941307
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.728280 loss:        0.781305
Test - acc:         0.739000 loss:        0.749718
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.737000 loss:        0.757887
Test - acc:         0.740300 loss:        0.742337
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.740760 loss:        0.747169
Test - acc:         0.742400 loss:        0.737397
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.742180 loss:        0.741619
Test - acc:         0.746800 loss:        0.732124
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.745420 loss:        0.735396
Test - acc:         0.748000 loss:        0.730103
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.745820 loss:        0.733851
Test - acc:         0.745500 loss:        0.727301
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.222460 loss:        2.263121
Test - acc:         0.300400 loss:        1.948716
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.311700 loss:        1.903430
Test - acc:         0.347000 loss:        1.811301
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.339500 loss:        1.826645
Test - acc:         0.371700 loss:        1.744653
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.357240 loss:        1.775694
Test - acc:         0.377500 loss:        1.712234
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.375080 loss:        1.733660
Test - acc:         0.390500 loss:        1.681512
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.382920 loss:        1.700103
Test - acc:         0.401800 loss:        1.635502
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.397660 loss:        1.668335
Test - acc:         0.416300 loss:        1.610588
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.409160 loss:        1.636321
Test - acc:         0.427700 loss:        1.590705
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.421240 loss:        1.608960
Test - acc:         0.440400 loss:        1.560434
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.432100 loss:        1.584586
Test - acc:         0.435300 loss:        1.560227
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.437940 loss:        1.566059
Test - acc:         0.458800 loss:        1.506659
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.444560 loss:        1.549222
Test - acc:         0.454700 loss:        1.509158
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.450160 loss:        1.534347
Test - acc:         0.465100 loss:        1.503171
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.456280 loss:        1.518634
Test - acc:         0.446300 loss:        1.522915
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.463600 loss:        1.500910
Test - acc:         0.474500 loss:        1.473255
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.467980 loss:        1.491359
Test - acc:         0.491300 loss:        1.444651
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.473440 loss:        1.477226
Test - acc:         0.481400 loss:        1.463515
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.478980 loss:        1.464814
Test - acc:         0.485700 loss:        1.448722
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.479960 loss:        1.454663
Test - acc:         0.494200 loss:        1.414467
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.487520 loss:        1.447871
Test - acc:         0.508000 loss:        1.387204
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.488600 loss:        1.436351
Test - acc:         0.503800 loss:        1.392354
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.493740 loss:        1.429911
Test - acc:         0.509200 loss:        1.385411
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.496440 loss:        1.418430
Test - acc:         0.509300 loss:        1.399763
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.498780 loss:        1.411982
Test - acc:         0.506900 loss:        1.402500
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.504340 loss:        1.397719
Test - acc:         0.374900 loss:        1.781110
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.508220 loss:        1.389746
Test - acc:         0.498700 loss:        1.415054
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.509140 loss:        1.387130
Test - acc:         0.517900 loss:        1.355057
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.512020 loss:        1.379816
Test - acc:         0.514800 loss:        1.382232
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.513040 loss:        1.375174
Test - acc:         0.437800 loss:        1.543479
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.517640 loss:        1.363100
Test - acc:         0.509100 loss:        1.389349
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.519020 loss:        1.358702
Test - acc:         0.537900 loss:        1.320439
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.524220 loss:        1.349742
Test - acc:         0.434900 loss:        1.636122
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.133840 loss:        2.576480
Test - acc:         0.180600 loss:        2.378755
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.190500 loss:        2.290948
Test - acc:         0.221000 loss:        2.237397
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.221160 loss:        2.222272
Test - acc:         0.226800 loss:        2.185273
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.233940 loss:        2.176218
Test - acc:         0.235600 loss:        2.160243
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.241720 loss:        2.144589
Test - acc:         0.253700 loss:        2.115875
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.246080 loss:        2.119073
Test - acc:         0.261600 loss:        2.085094
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.251700 loss:        2.101294
Test - acc:         0.240700 loss:        2.133382
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.256080 loss:        2.086904
Test - acc:         0.238700 loss:        2.157485
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.260820 loss:        2.073165
Test - acc:         0.268000 loss:        2.051715
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.262700 loss:        2.065528
Test - acc:         0.232300 loss:        2.093483
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.267820 loss:        2.057915
Test - acc:         0.271500 loss:        2.039850
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.271660 loss:        2.046577
Test - acc:         0.225600 loss:        2.143077
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.269120 loss:        2.042562
Test - acc:         0.283300 loss:        2.035243
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.276140 loss:        2.035585
Test - acc:         0.278000 loss:        2.035977
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.269300 loss:        2.031725
Test - acc:         0.276200 loss:        2.035055
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.268320 loss:        2.025039
Test - acc:         0.251000 loss:        2.108966
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.265960 loss:        2.024169
Test - acc:         0.261800 loss:        2.042913
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.279280 loss:        2.017515
Test - acc:         0.293000 loss:        1.993911
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.276480 loss:        2.017795
Test - acc:         0.286500 loss:        2.009146
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.283100 loss:        2.009175
Test - acc:         0.281400 loss:        1.996462
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.279860 loss:        2.006359
Test - acc:         0.268000 loss:        2.009424
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.281380 loss:        2.001492
Test - acc:         0.272400 loss:        2.019764
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.285740 loss:        1.996018
Test - acc:         0.267900 loss:        2.007270
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.282980 loss:        1.996128
Test - acc:         0.283600 loss:        1.974556
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.290380 loss:        1.988796
Test - acc:         0.245300 loss:        2.143420
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.281480 loss:        1.987903
Test - acc:         0.277900 loss:        1.969805
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.288880 loss:        1.983005
Test - acc:         0.276800 loss:        1.978065
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.280800 loss:        1.980105
Test - acc:         0.279500 loss:        1.999183
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.289060 loss:        1.976679
Test - acc:         0.266000 loss:        2.089372
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.292780 loss:        1.974902
Test - acc:         0.306800 loss:        1.954399
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.287600 loss:        1.974639
Test - acc:         0.263400 loss:        2.045873
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.294460 loss:        1.967595
Test - acc:         0.298300 loss:        1.941290
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.662067
Test - acc:         0.100000 loss:        2.589288
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.545608
Test - acc:         0.100000 loss:        2.507842
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.478576
Test - acc:         0.100000 loss:        2.451717
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.429349
Test - acc:         0.100000 loss:        2.409027
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.391579
Test - acc:         0.100000 loss:        2.375399
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.362361
Test - acc:         0.100000 loss:        2.350309
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.340829
Test - acc:         0.100000 loss:        2.332061
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.325714
Test - acc:         0.100000 loss:        2.319800
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.315784
Test - acc:         0.100000 loss:        2.312189
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.100080 loss:        2.309766
Test - acc:         0.100000 loss:        2.307674
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.100120 loss:        2.306393
Test - acc:         0.100000 loss:        2.305245
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.098240 loss:        2.304529
Test - acc:         0.100000 loss:        2.303962
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.098460 loss:        2.303638
Test - acc:         0.100000 loss:        2.303178
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.099220 loss:        2.303151
Test - acc:         0.100000 loss:        2.302902
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.097040 loss:        2.303028
Test - acc:         0.100000 loss:        2.302864
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.096320 loss:        2.302845
Test - acc:         0.100000 loss:        2.302740
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.099120 loss:        2.302718
Test - acc:         0.100000 loss:        2.302721
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.100580 loss:        2.302736
Test - acc:         0.100000 loss:        2.302617
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.099140 loss:        2.302758
Test - acc:         0.100000 loss:        2.302669
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.100380 loss:        2.302652
Test - acc:         0.100000 loss:        2.302753
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.101260 loss:        2.302664
Test - acc:         0.100000 loss:        2.302692
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.099020 loss:        2.302691
Test - acc:         0.100000 loss:        2.302773
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.098160 loss:        2.302786
Test - acc:         0.100000 loss:        2.302826
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.097740 loss:        2.302751
Test - acc:         0.100000 loss:        2.302630
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.098180 loss:        2.302766
Test - acc:         0.100000 loss:        2.302598
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.097960 loss:        2.302750
Test - acc:         0.100000 loss:        2.302596
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.100020 loss:        2.302748
Test - acc:         0.100000 loss:        2.302780
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.098660 loss:        2.302718
Test - acc:         0.100000 loss:        2.302616
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.098580 loss:        2.302700
Test - acc:         0.100000 loss:        2.302734
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.098240 loss:        2.302738
Test - acc:         0.100000 loss:        2.302586
Sparsity :          0.9990
Wdecay :        0.000500
