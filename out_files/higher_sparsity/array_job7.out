Running --model resnet18 --prune_criterion magnitude --seed 43 --prune_freq 32 --prune_rate 0.5 --comment=resnet18_crit=magnitude_pf=32_seed=43 --save_model=pre-finetune/resnet18_magnitude_pf32_s43 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "magnitude",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf32_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.348320 loss:        1.810961
Test - acc:         0.439400 loss:        1.484592
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.526380 loss:        1.296187
Test - acc:         0.587300 loss:        1.162469
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.635500 loss:        1.021202
Test - acc:         0.626700 loss:        1.042242
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.702640 loss:        0.843171
Test - acc:         0.711600 loss:        0.860480
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.752060 loss:        0.713263
Test - acc:         0.752700 loss:        0.721025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.783660 loss:        0.623389
Test - acc:         0.733700 loss:        0.786503
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.801860 loss:        0.570900
Test - acc:         0.757000 loss:        0.762860
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.814640 loss:        0.539573
Test - acc:         0.704900 loss:        0.941443
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.825480 loss:        0.505140
Test - acc:         0.784800 loss:        0.633475
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.830340 loss:        0.493756
Test - acc:         0.779500 loss:        0.657745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.838340 loss:        0.474250
Test - acc:         0.804200 loss:        0.593576
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.844280 loss:        0.458721
Test - acc:         0.834400 loss:        0.502315
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.846500 loss:        0.449426
Test - acc:         0.817100 loss:        0.554424
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.847180 loss:        0.443512
Test - acc:         0.812100 loss:        0.573417
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.853420 loss:        0.425940
Test - acc:         0.804500 loss:        0.581330
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.857080 loss:        0.415865
Test - acc:         0.827800 loss:        0.517806
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.861940 loss:        0.404304
Test - acc:         0.815300 loss:        0.557021
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.863300 loss:        0.400604
Test - acc:         0.797600 loss:        0.647381
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.864700 loss:        0.398190
Test - acc:         0.820300 loss:        0.561645
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.869280 loss:        0.387392
Test - acc:         0.815000 loss:        0.584442
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.870620 loss:        0.382326
Test - acc:         0.818900 loss:        0.536924
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870120 loss:        0.380016
Test - acc:         0.783000 loss:        0.719661
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.869920 loss:        0.379140
Test - acc:         0.822700 loss:        0.551593
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.874520 loss:        0.374324
Test - acc:         0.836900 loss:        0.490705
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.874720 loss:        0.370254
Test - acc:         0.827900 loss:        0.513918
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.876980 loss:        0.362354
Test - acc:         0.850800 loss:        0.456385
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.873500 loss:        0.368452
Test - acc:         0.823200 loss:        0.540426
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.876640 loss:        0.360789
Test - acc:         0.848400 loss:        0.446691
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.876560 loss:        0.360781
Test - acc:         0.803700 loss:        0.617252
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.879380 loss:        0.354437
Test - acc:         0.703300 loss:        1.070219
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.879920 loss:        0.350535
Test - acc:         0.835700 loss:        0.514332
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.877980 loss:        0.357668
Test - acc:         0.852200 loss:        0.417868
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.897480 loss:        0.297851
Test - acc:         0.847700 loss:        0.470406
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.896200 loss:        0.302623
Test - acc:         0.834200 loss:        0.500616
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.896160 loss:        0.302616
Test - acc:         0.850100 loss:        0.479304
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.897840 loss:        0.299309
Test - acc:         0.842800 loss:        0.482788
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.893660 loss:        0.307857
Test - acc:         0.734700 loss:        0.877614
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.896160 loss:        0.303624
Test - acc:         0.858300 loss:        0.429326
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.898280 loss:        0.300457
Test - acc:         0.853000 loss:        0.456346
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.895780 loss:        0.303904
Test - acc:         0.855700 loss:        0.435062
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.900120 loss:        0.291968
Test - acc:         0.820900 loss:        0.553856
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.896740 loss:        0.299103
Test - acc:         0.824900 loss:        0.556509
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.895500 loss:        0.304093
Test - acc:         0.861700 loss:        0.423909
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.897460 loss:        0.299994
Test - acc:         0.869300 loss:        0.394959
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.899600 loss:        0.295267
Test - acc:         0.834100 loss:        0.521264
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.899260 loss:        0.294269
Test - acc:         0.753500 loss:        0.795241
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.896700 loss:        0.301236
Test - acc:         0.843300 loss:        0.484851
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.899380 loss:        0.295414
Test - acc:         0.856500 loss:        0.439194
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.899200 loss:        0.291042
Test - acc:         0.850500 loss:        0.464749
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.900220 loss:        0.293386
Test - acc:         0.865300 loss:        0.418508
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.898480 loss:        0.294454
Test - acc:         0.870600 loss:        0.386270
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.900120 loss:        0.292307
Test - acc:         0.848100 loss:        0.457314
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.900780 loss:        0.290095
Test - acc:         0.852100 loss:        0.468728
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.901300 loss:        0.290540
Test - acc:         0.849600 loss:        0.451538
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.899460 loss:        0.294432
Test - acc:         0.840600 loss:        0.496996
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.900580 loss:        0.289216
Test - acc:         0.831600 loss:        0.535411
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.897620 loss:        0.295781
Test - acc:         0.851700 loss:        0.434105
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.900080 loss:        0.290671
Test - acc:         0.823800 loss:        0.571757
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.902800 loss:        0.284764
Test - acc:         0.858600 loss:        0.445145
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.899320 loss:        0.290949
Test - acc:         0.858400 loss:        0.408714
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.900040 loss:        0.290154
Test - acc:         0.843800 loss:        0.471878
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.900140 loss:        0.293938
Test - acc:         0.855700 loss:        0.442500
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.901720 loss:        0.285677
Test - acc:         0.859200 loss:        0.428849
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.897900 loss:        0.295898
Test - acc:         0.849300 loss:        0.459450
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.915880 loss:        0.247467
Test - acc:         0.879000 loss:        0.358827
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.911760 loss:        0.254623
Test - acc:         0.857500 loss:        0.432953
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.912420 loss:        0.258082
Test - acc:         0.875300 loss:        0.384493
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.911580 loss:        0.257945
Test - acc:         0.806700 loss:        0.622269
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.909840 loss:        0.262537
Test - acc:         0.873800 loss:        0.378524
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.909820 loss:        0.261431
Test - acc:         0.848300 loss:        0.492143
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.911040 loss:        0.260512
Test - acc:         0.827000 loss:        0.586014
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.910420 loss:        0.260309
Test - acc:         0.873700 loss:        0.387426
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.907420 loss:        0.264553
Test - acc:         0.860700 loss:        0.411285
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.912740 loss:        0.256148
Test - acc:         0.862600 loss:        0.426557
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.912360 loss:        0.257787
Test - acc:         0.855800 loss:        0.447303
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.913040 loss:        0.253007
Test - acc:         0.860700 loss:        0.435123
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.909640 loss:        0.260947
Test - acc:         0.869800 loss:        0.402594
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.913120 loss:        0.254137
Test - acc:         0.862900 loss:        0.409663
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.911800 loss:        0.259007
Test - acc:         0.864200 loss:        0.398103
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.908740 loss:        0.263088
Test - acc:         0.868100 loss:        0.403506
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.911780 loss:        0.258204
Test - acc:         0.833000 loss:        0.516331
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.911500 loss:        0.257770
Test - acc:         0.833700 loss:        0.522169
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.912360 loss:        0.256704
Test - acc:         0.860400 loss:        0.435961
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.912440 loss:        0.258217
Test - acc:         0.868100 loss:        0.404093
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.912900 loss:        0.254450
Test - acc:         0.825100 loss:        0.556893
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.912740 loss:        0.256970
Test - acc:         0.861300 loss:        0.421309
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.912080 loss:        0.255248
Test - acc:         0.844000 loss:        0.501551
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.911260 loss:        0.258580
Test - acc:         0.860800 loss:        0.428839
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.909800 loss:        0.264189
Test - acc:         0.800300 loss:        0.693394
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.911940 loss:        0.257419
Test - acc:         0.842900 loss:        0.476240
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.912840 loss:        0.255382
Test - acc:         0.843700 loss:        0.502762
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.911920 loss:        0.253988
Test - acc:         0.859600 loss:        0.428560
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.909460 loss:        0.258176
Test - acc:         0.862200 loss:        0.441136
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.913660 loss:        0.252991
Test - acc:         0.849200 loss:        0.471935
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.909600 loss:        0.262821
Test - acc:         0.865000 loss:        0.410609
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.912900 loss:        0.255262
Test - acc:         0.834700 loss:        0.498871
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.919740 loss:        0.233256
Test - acc:         0.858500 loss:        0.454769
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.918500 loss:        0.238745
Test - acc:         0.871100 loss:        0.397031
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.917440 loss:        0.241829
Test - acc:         0.885000 loss:        0.358865
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.917220 loss:        0.239308
Test - acc:         0.880500 loss:        0.354499
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.917440 loss:        0.241007
Test - acc:         0.879700 loss:        0.360894
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.918680 loss:        0.238199
Test - acc:         0.827500 loss:        0.515513
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.917980 loss:        0.242140
Test - acc:         0.871300 loss:        0.390821
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.916520 loss:        0.244276
Test - acc:         0.881700 loss:        0.371334
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.916400 loss:        0.243254
Test - acc:         0.825800 loss:        0.572064
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.917680 loss:        0.239230
Test - acc:         0.842500 loss:        0.512755
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.916660 loss:        0.242285
Test - acc:         0.880700 loss:        0.371169
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.917700 loss:        0.238373
Test - acc:         0.878800 loss:        0.381594
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.917700 loss:        0.241829
Test - acc:         0.874100 loss:        0.381656
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.915860 loss:        0.241136
Test - acc:         0.848900 loss:        0.465270
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.916660 loss:        0.239528
Test - acc:         0.820300 loss:        0.585861
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.917140 loss:        0.242424
Test - acc:         0.863000 loss:        0.435766
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.917480 loss:        0.239922
Test - acc:         0.872400 loss:        0.400571
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.919440 loss:        0.235913
Test - acc:         0.868600 loss:        0.392748
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.915820 loss:        0.242437
Test - acc:         0.884200 loss:        0.353750
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.917240 loss:        0.239487
Test - acc:         0.885900 loss:        0.344822
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.918000 loss:        0.240633
Test - acc:         0.842400 loss:        0.488407
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.916540 loss:        0.243528
Test - acc:         0.868100 loss:        0.405115
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.918000 loss:        0.238370
Test - acc:         0.869500 loss:        0.394615
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.915680 loss:        0.245158
Test - acc:         0.864400 loss:        0.409668
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.915480 loss:        0.244380
Test - acc:         0.874700 loss:        0.386355
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.917140 loss:        0.241043
Test - acc:         0.872000 loss:        0.384238
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.917420 loss:        0.237976
Test - acc:         0.826300 loss:        0.561089
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.916440 loss:        0.241826
Test - acc:         0.874800 loss:        0.377725
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.918180 loss:        0.240302
Test - acc:         0.871000 loss:        0.392787
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.915360 loss:        0.249391
Test - acc:         0.846500 loss:        0.497265
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.917320 loss:        0.236627
Test - acc:         0.814300 loss:        0.597477
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.915420 loss:        0.244616
Test - acc:         0.858100 loss:        0.448669
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.914300 loss:        0.248595
Test - acc:         0.876300 loss:        0.384485
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.916820 loss:        0.243938
Test - acc:         0.852800 loss:        0.461947
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.914680 loss:        0.246029
Test - acc:         0.862800 loss:        0.390296
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.916200 loss:        0.243587
Test - acc:         0.856400 loss:        0.454177
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.917220 loss:        0.242830
Test - acc:         0.856700 loss:        0.470598
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.914780 loss:        0.245717
Test - acc:         0.877100 loss:        0.377914
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.914780 loss:        0.246663
Test - acc:         0.851200 loss:        0.492571
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.917940 loss:        0.241472
Test - acc:         0.873400 loss:        0.381298
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.916400 loss:        0.244194
Test - acc:         0.863300 loss:        0.407290
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.915280 loss:        0.244139
Test - acc:         0.857300 loss:        0.450746
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.915980 loss:        0.244081
Test - acc:         0.856000 loss:        0.439944
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.914200 loss:        0.248799
Test - acc:         0.868000 loss:        0.431042
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.915820 loss:        0.244900
Test - acc:         0.874800 loss:        0.378709
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.915540 loss:        0.244260
Test - acc:         0.860300 loss:        0.438056
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.914780 loss:        0.246485
Test - acc:         0.876300 loss:        0.381691
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.915840 loss:        0.243034
Test - acc:         0.855800 loss:        0.467869
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.915880 loss:        0.243157
Test - acc:         0.857200 loss:        0.446045
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.914600 loss:        0.245057
Test - acc:         0.855400 loss:        0.452889
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.916920 loss:        0.244452
Test - acc:         0.879400 loss:        0.376249
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.916120 loss:        0.243926
Test - acc:         0.872600 loss:        0.392653
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.913760 loss:        0.247059
Test - acc:         0.861900 loss:        0.424175
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.916900 loss:        0.243794
Test - acc:         0.860800 loss:        0.457746
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.953740 loss:        0.137291
Test - acc:         0.933100 loss:        0.201068
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.965320 loss:        0.104416
Test - acc:         0.934800 loss:        0.197573
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.970380 loss:        0.089591
Test - acc:         0.937400 loss:        0.194031
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.973380 loss:        0.082540
Test - acc:         0.933100 loss:        0.200115
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.975520 loss:        0.074079
Test - acc:         0.939000 loss:        0.196797
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.977980 loss:        0.068724
Test - acc:         0.937800 loss:        0.198278
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.979340 loss:        0.062438
Test - acc:         0.937300 loss:        0.204423
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.981000 loss:        0.058107
Test - acc:         0.938400 loss:        0.205672
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.981480 loss:        0.056497
Test - acc:         0.939000 loss:        0.202104
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.982080 loss:        0.054633
Test - acc:         0.939200 loss:        0.204779
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.964780 loss:        0.111478
Test - acc:         0.930100 loss:        0.213391
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.974140 loss:        0.081013
Test - acc:         0.932300 loss:        0.214632
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.974260 loss:        0.078608
Test - acc:         0.930000 loss:        0.217466
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.977580 loss:        0.069222
Test - acc:         0.931400 loss:        0.225912
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.978120 loss:        0.066897
Test - acc:         0.931000 loss:        0.231976
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.977260 loss:        0.066027
Test - acc:         0.927700 loss:        0.237712
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.978520 loss:        0.065316
Test - acc:         0.932200 loss:        0.229163
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.978760 loss:        0.062910
Test - acc:         0.930200 loss:        0.240372
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.978800 loss:        0.062389
Test - acc:         0.931600 loss:        0.228811
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.981100 loss:        0.057968
Test - acc:         0.930200 loss:        0.239268
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.980560 loss:        0.059974
Test - acc:         0.931700 loss:        0.238950
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.979020 loss:        0.062356
Test - acc:         0.930100 loss:        0.246796
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.979300 loss:        0.060840
Test - acc:         0.928200 loss:        0.247204
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.980640 loss:        0.059341
Test - acc:         0.926400 loss:        0.267163
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.979680 loss:        0.058949
Test - acc:         0.926400 loss:        0.245591
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.979760 loss:        0.059187
Test - acc:         0.927100 loss:        0.244452
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.979820 loss:        0.060558
Test - acc:         0.926400 loss:        0.242206
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.979080 loss:        0.061116
Test - acc:         0.922500 loss:        0.265209
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.979440 loss:        0.060402
Test - acc:         0.927900 loss:        0.245096
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.979600 loss:        0.061857
Test - acc:         0.924900 loss:        0.250675
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.980140 loss:        0.059088
Test - acc:         0.926700 loss:        0.250023
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.978820 loss:        0.062995
Test - acc:         0.926000 loss:        0.253306
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.979060 loss:        0.061864
Test - acc:         0.929100 loss:        0.245386
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.977880 loss:        0.065047
Test - acc:         0.927100 loss:        0.253410
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.978300 loss:        0.064505
Test - acc:         0.926100 loss:        0.264990
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.977840 loss:        0.065073
Test - acc:         0.920800 loss:        0.264918
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.978920 loss:        0.062650
Test - acc:         0.925600 loss:        0.259182
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.977500 loss:        0.067433
Test - acc:         0.921000 loss:        0.275191
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.977740 loss:        0.064570
Test - acc:         0.921800 loss:        0.262029
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.976540 loss:        0.068142
Test - acc:         0.926700 loss:        0.252782
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.975880 loss:        0.070505
Test - acc:         0.922500 loss:        0.261979
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.975360 loss:        0.070778
Test - acc:         0.925700 loss:        0.253357
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.924020 loss:        0.227383
Test - acc:         0.902200 loss:        0.302615
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.941080 loss:        0.173319
Test - acc:         0.889200 loss:        0.359635
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.946100 loss:        0.156313
Test - acc:         0.895800 loss:        0.339795
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.948360 loss:        0.148497
Test - acc:         0.907000 loss:        0.301048
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.949480 loss:        0.147296
Test - acc:         0.902700 loss:        0.311827
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.950780 loss:        0.142261
Test - acc:         0.910900 loss:        0.285672
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.950740 loss:        0.140231
Test - acc:         0.907900 loss:        0.295742
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.951800 loss:        0.136487
Test - acc:         0.898300 loss:        0.337701
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.954900 loss:        0.130122
Test - acc:         0.911900 loss:        0.295442
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.954320 loss:        0.130078
Test - acc:         0.909900 loss:        0.289030
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.955780 loss:        0.127527
Test - acc:         0.904900 loss:        0.322200
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.955300 loss:        0.129083
Test - acc:         0.912300 loss:        0.288911
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.956800 loss:        0.124137
Test - acc:         0.908200 loss:        0.301810
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.955880 loss:        0.126978
Test - acc:         0.903900 loss:        0.323132
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.956140 loss:        0.125637
Test - acc:         0.910100 loss:        0.301863
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.956920 loss:        0.122693
Test - acc:         0.897300 loss:        0.357143
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.955640 loss:        0.123310
Test - acc:         0.906900 loss:        0.312543
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.957100 loss:        0.123480
Test - acc:         0.913600 loss:        0.291047
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.957360 loss:        0.121545
Test - acc:         0.910500 loss:        0.299671
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.957720 loss:        0.119434
Test - acc:         0.911900 loss:        0.294992
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.958220 loss:        0.119601
Test - acc:         0.908600 loss:        0.311888
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.959140 loss:        0.116226
Test - acc:         0.906000 loss:        0.312932
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.958000 loss:        0.120632
Test - acc:         0.912700 loss:        0.293942
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.958620 loss:        0.117143
Test - acc:         0.909900 loss:        0.309054
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.958800 loss:        0.117363
Test - acc:         0.909300 loss:        0.314844
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.959000 loss:        0.115624
Test - acc:         0.910100 loss:        0.295050
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.959340 loss:        0.116640
Test - acc:         0.910100 loss:        0.290224
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.959840 loss:        0.113722
Test - acc:         0.912100 loss:        0.297152
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.959680 loss:        0.117051
Test - acc:         0.910000 loss:        0.312130
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.960900 loss:        0.113092
Test - acc:         0.905500 loss:        0.307882
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.959780 loss:        0.115813
Test - acc:         0.912000 loss:        0.300125
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.960820 loss:        0.115090
Test - acc:         0.908900 loss:        0.300190
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.871960 loss:        0.381545
Test - acc:         0.847600 loss:        0.460265
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.897800 loss:        0.297880
Test - acc:         0.867900 loss:        0.404790
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.903860 loss:        0.278734
Test - acc:         0.870300 loss:        0.401302
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.908060 loss:        0.267481
Test - acc:         0.865700 loss:        0.408363
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.910020 loss:        0.260240
Test - acc:         0.874500 loss:        0.377943
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.912080 loss:        0.255889
Test - acc:         0.857000 loss:        0.435683
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.910840 loss:        0.255967
Test - acc:         0.876700 loss:        0.373298
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.913560 loss:        0.248530
Test - acc:         0.870000 loss:        0.407283
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.913280 loss:        0.248330
Test - acc:         0.877000 loss:        0.372479
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.916680 loss:        0.241537
Test - acc:         0.884700 loss:        0.354145
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.916320 loss:        0.239352
Test - acc:         0.872000 loss:        0.394501
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.918300 loss:        0.235359
Test - acc:         0.884500 loss:        0.363096
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.915740 loss:        0.237218
Test - acc:         0.877500 loss:        0.379844
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.916340 loss:        0.235376
Test - acc:         0.881000 loss:        0.371891
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.918600 loss:        0.233906
Test - acc:         0.882100 loss:        0.365031
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.919000 loss:        0.231854
Test - acc:         0.885900 loss:        0.351345
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.917740 loss:        0.231319
Test - acc:         0.880100 loss:        0.368671
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.920020 loss:        0.228445
Test - acc:         0.878800 loss:        0.384107
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.922120 loss:        0.226993
Test - acc:         0.874400 loss:        0.397086
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.918420 loss:        0.229444
Test - acc:         0.883500 loss:        0.357285
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.920120 loss:        0.230460
Test - acc:         0.881300 loss:        0.361737
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.920140 loss:        0.228903
Test - acc:         0.886500 loss:        0.357951
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.922720 loss:        0.222901
Test - acc:         0.872000 loss:        0.399368
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.918500 loss:        0.229454
Test - acc:         0.887300 loss:        0.349130
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.920080 loss:        0.227377
Test - acc:         0.882600 loss:        0.376868
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.922620 loss:        0.221485
Test - acc:         0.886000 loss:        0.358258
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.939780 loss:        0.176648
Test - acc:         0.904700 loss:        0.291251
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.946180 loss:        0.158958
Test - acc:         0.906900 loss:        0.285575
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.948200 loss:        0.151872
Test - acc:         0.909900 loss:        0.285731
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.949760 loss:        0.147471
Test - acc:         0.909100 loss:        0.284127
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.950740 loss:        0.145907
Test - acc:         0.909600 loss:        0.286234
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.949320 loss:        0.144459
Test - acc:         0.911800 loss:        0.283917
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.602780 loss:        1.387215
Test - acc:         0.687900 loss:        0.984194
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.701920 loss:        0.913016
Test - acc:         0.731900 loss:        0.837956
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.752620 loss:        0.789025
Test - acc:         0.779200 loss:        0.726301
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.780560 loss:        0.719068
Test - acc:         0.788700 loss:        0.704622
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.797520 loss:        0.668071
Test - acc:         0.801300 loss:        0.665532
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.806340 loss:        0.632005
Test - acc:         0.804300 loss:        0.643026
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.816040 loss:        0.599521
Test - acc:         0.809800 loss:        0.606646
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.823100 loss:        0.569548
Test - acc:         0.819800 loss:        0.575913
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.828120 loss:        0.550908
Test - acc:         0.822500 loss:        0.556273
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.830560 loss:        0.536231
Test - acc:         0.818500 loss:        0.562500
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.833720 loss:        0.524383
Test - acc:         0.675300 loss:        1.032550
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.834720 loss:        0.514774
Test - acc:         0.819600 loss:        0.569380
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.836880 loss:        0.508412
Test - acc:         0.833700 loss:        0.513227
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.842760 loss:        0.492765
Test - acc:         0.794000 loss:        0.636222
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.845640 loss:        0.479019
Test - acc:         0.817200 loss:        0.549436
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.845920 loss:        0.475928
Test - acc:         0.830200 loss:        0.534413
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.846420 loss:        0.470831
Test - acc:         0.836400 loss:        0.497190
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.848780 loss:        0.463372
Test - acc:         0.822000 loss:        0.529631
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.848920 loss:        0.458306
Test - acc:         0.822500 loss:        0.531957
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.852560 loss:        0.452839
Test - acc:         0.769200 loss:        0.702155
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.853320 loss:        0.446373
Test - acc:         0.792400 loss:        0.618017
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.853600 loss:        0.444964
Test - acc:         0.838800 loss:        0.492488
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.856180 loss:        0.435125
Test - acc:         0.837500 loss:        0.476156
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.855400 loss:        0.436496
Test - acc:         0.843800 loss:        0.473372
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.859040 loss:        0.428030
Test - acc:         0.834100 loss:        0.509257
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.855940 loss:        0.432635
Test - acc:         0.846100 loss:        0.472013
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.857700 loss:        0.425226
Test - acc:         0.826700 loss:        0.509856
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.860580 loss:        0.419875
Test - acc:         0.832400 loss:        0.502075
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.861620 loss:        0.416249
Test - acc:         0.843000 loss:        0.472543
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.860240 loss:        0.418715
Test - acc:         0.842600 loss:        0.465774
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.860540 loss:        0.416946
Test - acc:         0.840800 loss:        0.464132
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.861620 loss:        0.413385
Test - acc:         0.796900 loss:        0.611078
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.481480 loss:        1.740904
Test - acc:         0.508800 loss:        1.642427
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.586600 loss:        1.357363
Test - acc:         0.591900 loss:        1.288581
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.612940 loss:        1.233708
Test - acc:         0.617800 loss:        1.176475
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.627720 loss:        1.156071
Test - acc:         0.637200 loss:        1.095385
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.634060 loss:        1.102943
Test - acc:         0.637000 loss:        1.059374
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.642920 loss:        1.060174
Test - acc:         0.622500 loss:        1.127182
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.651220 loss:        1.022867
Test - acc:         0.660000 loss:        0.976616
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.655420 loss:        0.997582
Test - acc:         0.623800 loss:        1.094030
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.660120 loss:        0.978831
Test - acc:         0.674300 loss:        0.932424
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.659920 loss:        0.971654
Test - acc:         0.656900 loss:        0.968132
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.664740 loss:        0.956248
Test - acc:         0.674900 loss:        0.918173
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.665840 loss:        0.948656
Test - acc:         0.674600 loss:        0.913723
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.670020 loss:        0.936592
Test - acc:         0.644500 loss:        1.015798
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.673320 loss:        0.929423
Test - acc:         0.689300 loss:        0.887747
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.670600 loss:        0.924581
Test - acc:         0.683300 loss:        0.911790
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.671700 loss:        0.914643
Test - acc:         0.680200 loss:        0.895617
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.676980 loss:        0.911111
Test - acc:         0.692400 loss:        0.873297
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.677620 loss:        0.904074
Test - acc:         0.686600 loss:        0.869693
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.677320 loss:        0.899171
Test - acc:         0.671300 loss:        0.932735
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.680360 loss:        0.895035
Test - acc:         0.477700 loss:        2.039912
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.682160 loss:        0.887678
Test - acc:         0.688600 loss:        0.892515
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.678880 loss:        0.889065
Test - acc:         0.687600 loss:        0.894717
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.672140 loss:        0.886538
Test - acc:         0.697000 loss:        0.860061
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.677820 loss:        0.881318
Test - acc:         0.695300 loss:        0.847759
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.683880 loss:        0.874194
Test - acc:         0.660700 loss:        0.989729
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.679120 loss:        0.872289
Test - acc:         0.660700 loss:        0.877143
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.684720 loss:        0.866948
Test - acc:         0.693900 loss:        0.845660
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.676120 loss:        0.869299
Test - acc:         0.608500 loss:        1.217260
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.679540 loss:        0.862789
Test - acc:         0.677700 loss:        0.838591
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.676420 loss:        0.861723
Test - acc:         0.698500 loss:        0.845586
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.686660 loss:        0.857896
Test - acc:         0.628600 loss:        1.031313
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.687140 loss:        0.851440
Test - acc:         0.686700 loss:        0.897149
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.256360 loss:        2.603233
Test - acc:         0.135000 loss:        2.849078
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.293140 loss:        2.291824
Test - acc:         0.188400 loss:        2.510002
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.312020 loss:        2.105638
Test - acc:         0.186000 loss:        2.746342
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.322880 loss:        1.985425
Test - acc:         0.156000 loss:        2.429928
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.323480 loss:        1.907990
Test - acc:         0.100000 loss:     1534.135254
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.334360 loss:        1.859552
Test - acc:         0.316400 loss:        1.888731
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.344620 loss:        1.829130
Test - acc:         0.333400 loss:        1.841079
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.343000 loss:        1.812603
Test - acc:         0.100000 loss:      104.524966
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.331600 loss:        1.800194
Test - acc:         0.100000 loss:     1121.858496
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.342460 loss:        1.794297
Test - acc:         0.206600 loss:        2.495260
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.337300 loss:        1.784471
Test - acc:         0.304000 loss:        1.929201
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.344100 loss:        1.779031
Test - acc:         0.327300 loss:        1.857527
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.350540 loss:        1.771776
Test - acc:         0.317400 loss:        1.865551
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.351900 loss:        1.763821
Test - acc:         0.100000 loss:      331.749854
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.343240 loss:        1.761878
Test - acc:         0.162200 loss:        2.741991
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.361580 loss:        1.753528
Test - acc:         0.348300 loss:        1.809493
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.358160 loss:        1.750545
Test - acc:         0.100000 loss:      137.658414
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.359640 loss:        1.746756
Test - acc:         0.100000 loss:      553.431091
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.346600 loss:        1.742514
Test - acc:         0.340900 loss:        1.794638
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.347700 loss:        1.737711
Test - acc:         0.177900 loss:        4.585647
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.363040 loss:        1.735795
Test - acc:         0.313400 loss:        1.930214
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.358740 loss:        1.734016
Test - acc:         0.100000 loss:      324.839441
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.350500 loss:        1.729810
Test - acc:         0.307700 loss:        1.938122
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.364460 loss:        1.727182
Test - acc:         0.369000 loss:        1.768131
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.363280 loss:        1.724533
Test - acc:         0.100000 loss:      163.634937
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.358740 loss:        1.719591
Test - acc:         0.100000 loss:      110.586374
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.375880 loss:        1.721295
Test - acc:         0.325300 loss:        1.880490
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.362920 loss:        1.716418
Test - acc:         0.337800 loss:        1.842197
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.375440 loss:        1.710733
Test - acc:         0.122900 loss:       76.597095
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.372120 loss:        1.712782
Test - acc:         0.120700 loss:        7.792057
Sparsity :          0.9990
Wdecay :        0.000500
