Running --model vgg19 --prune_criterion random --seed 44 --prune_freq 32 --prune_rate 0.5 --comment=vgg19_crit=random_pf=32_seed=44 --save_model=pre-finetune/vgg19_random_pf32_s44 --logdir=criterion_experiment_no_bias/vgg19
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "random",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_random_pf32_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.121220 loss:        2.493571
Test - acc:         0.162400 loss:        2.247057
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.238540 loss:        1.988574
Test - acc:         0.291200 loss:        1.816521
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.309940 loss:        1.770952
Test - acc:         0.338500 loss:        1.698639
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.379240 loss:        1.597609
Test - acc:         0.373000 loss:        1.624368
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.485060 loss:        1.383755
Test - acc:         0.492900 loss:        1.383657
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.595460 loss:        1.144366
Test - acc:         0.548400 loss:        1.317607
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.663920 loss:        0.966378
Test - acc:         0.628100 loss:        1.085081
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.708800 loss:        0.854189
Test - acc:         0.628000 loss:        1.141725
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.733740 loss:        0.788849
Test - acc:         0.738400 loss:        0.793110
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.751300 loss:        0.746240
Test - acc:         0.596200 loss:        1.291147
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.771620 loss:        0.698481
Test - acc:         0.701500 loss:        0.996400
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.777780 loss:        0.677521
Test - acc:         0.672600 loss:        1.052496
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.785620 loss:        0.657763
Test - acc:         0.678700 loss:        1.063080
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.790880 loss:        0.638954
Test - acc:         0.713700 loss:        0.933064
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.796840 loss:        0.622712
Test - acc:         0.778500 loss:        0.679065
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.803920 loss:        0.610713
Test - acc:         0.757800 loss:        0.748650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.807580 loss:        0.596009
Test - acc:         0.752600 loss:        0.749324
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.809320 loss:        0.586204
Test - acc:         0.688000 loss:        0.984244
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.813800 loss:        0.574660
Test - acc:         0.712500 loss:        0.908072
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.822480 loss:        0.554705
Test - acc:         0.751600 loss:        0.800250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.822680 loss:        0.542792
Test - acc:         0.747900 loss:        0.776715
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.825960 loss:        0.537973
Test - acc:         0.783600 loss:        0.670969
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.827120 loss:        0.530511
Test - acc:         0.725800 loss:        0.980620
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.829620 loss:        0.524179
Test - acc:         0.759400 loss:        0.772408
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.834520 loss:        0.512033
Test - acc:         0.744100 loss:        0.875782
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.835800 loss:        0.505285
Test - acc:         0.759700 loss:        0.725445
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.839820 loss:        0.494703
Test - acc:         0.784400 loss:        0.703626
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.840580 loss:        0.488501
Test - acc:         0.780800 loss:        0.733794
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.842180 loss:        0.488092
Test - acc:         0.826500 loss:        0.554136
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.843940 loss:        0.479515
Test - acc:         0.802400 loss:        0.607688
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.845500 loss:        0.478120
Test - acc:         0.758200 loss:        0.820148
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.844480 loss:        0.475611
Test - acc:         0.762400 loss:        0.770402
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.772240 loss:        0.699368
Test - acc:         0.733900 loss:        0.846052
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.815060 loss:        0.571398
Test - acc:         0.718000 loss:        0.877663
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.825880 loss:        0.533870
Test - acc:         0.712900 loss:        0.961549
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.832440 loss:        0.513603
Test - acc:         0.782200 loss:        0.690048
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.834340 loss:        0.501148
Test - acc:         0.792100 loss:        0.648221
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.837360 loss:        0.495900
Test - acc:         0.813200 loss:        0.577152
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.839500 loss:        0.488929
Test - acc:         0.748400 loss:        0.774266
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.841980 loss:        0.482517
Test - acc:         0.770800 loss:        0.730048
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.846560 loss:        0.468805
Test - acc:         0.778100 loss:        0.730361
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.844520 loss:        0.475313
Test - acc:         0.780900 loss:        0.687173
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.845180 loss:        0.469851
Test - acc:         0.814300 loss:        0.586336
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.844080 loss:        0.471646
Test - acc:         0.826200 loss:        0.542150
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.850460 loss:        0.460206
Test - acc:         0.772800 loss:        0.744789
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.847340 loss:        0.459041
Test - acc:         0.799500 loss:        0.616100
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.849240 loss:        0.457941
Test - acc:         0.742800 loss:        0.885600
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.850200 loss:        0.451473
Test - acc:         0.761500 loss:        0.756686
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.852720 loss:        0.448620
Test - acc:         0.755000 loss:        0.808331
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.851280 loss:        0.452119
Test - acc:         0.806600 loss:        0.601628
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.849300 loss:        0.453610
Test - acc:         0.754600 loss:        0.830150
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.854840 loss:        0.445039
Test - acc:         0.819700 loss:        0.548108
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.854680 loss:        0.441060
Test - acc:         0.665400 loss:        1.246704
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.855100 loss:        0.441288
Test - acc:         0.791800 loss:        0.659672
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.858080 loss:        0.436178
Test - acc:         0.722400 loss:        0.967058
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.858080 loss:        0.434347
Test - acc:         0.810900 loss:        0.588782
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.857320 loss:        0.435980
Test - acc:         0.802100 loss:        0.595255
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.857240 loss:        0.436048
Test - acc:         0.790800 loss:        0.657471
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.854040 loss:        0.440321
Test - acc:         0.803400 loss:        0.611150
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.856320 loss:        0.438375
Test - acc:         0.754800 loss:        0.763295
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.855680 loss:        0.437548
Test - acc:         0.808500 loss:        0.603207
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.857740 loss:        0.434955
Test - acc:         0.802100 loss:        0.622616
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.861920 loss:        0.424194
Test - acc:         0.820400 loss:        0.549890
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.856580 loss:        0.435420
Test - acc:         0.794700 loss:        0.630388
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.717880 loss:        0.839547
Test - acc:         0.714200 loss:        0.863747
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.800940 loss:        0.601537
Test - acc:         0.762300 loss:        0.736774
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.816260 loss:        0.552020
Test - acc:         0.797100 loss:        0.637374
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.825840 loss:        0.521875
Test - acc:         0.797500 loss:        0.621832
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.831580 loss:        0.503879
Test - acc:         0.793000 loss:        0.626945
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.833940 loss:        0.494092
Test - acc:         0.751400 loss:        0.789171
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.833780 loss:        0.497176
Test - acc:         0.774000 loss:        0.685755
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.841600 loss:        0.476799
Test - acc:         0.780500 loss:        0.673732
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.840220 loss:        0.480994
Test - acc:         0.824800 loss:        0.543141
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.842320 loss:        0.471345
Test - acc:         0.813900 loss:        0.574209
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.844080 loss:        0.466229
Test - acc:         0.797700 loss:        0.616674
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.845900 loss:        0.462368
Test - acc:         0.811800 loss:        0.563864
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.844780 loss:        0.464569
Test - acc:         0.743800 loss:        0.822037
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.847500 loss:        0.459594
Test - acc:         0.752300 loss:        0.781584
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.847280 loss:        0.457597
Test - acc:         0.769500 loss:        0.704356
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.846920 loss:        0.461440
Test - acc:         0.755800 loss:        0.798417
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.844220 loss:        0.458491
Test - acc:         0.807400 loss:        0.619768
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.844320 loss:        0.457982
Test - acc:         0.794200 loss:        0.629676
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.851300 loss:        0.443488
Test - acc:         0.800500 loss:        0.621659
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.849320 loss:        0.452538
Test - acc:         0.789700 loss:        0.613760
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.849680 loss:        0.449643
Test - acc:         0.807800 loss:        0.594346
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.849220 loss:        0.451466
Test - acc:         0.787600 loss:        0.671083
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.849140 loss:        0.449404
Test - acc:         0.805800 loss:        0.578562
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.853720 loss:        0.440782
Test - acc:         0.801800 loss:        0.611854
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.851160 loss:        0.442562
Test - acc:         0.747200 loss:        0.795590
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.851320 loss:        0.446963
Test - acc:         0.796300 loss:        0.666741
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.848800 loss:        0.446776
Test - acc:         0.812500 loss:        0.550962
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.851920 loss:        0.442239
Test - acc:         0.804600 loss:        0.587733
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.851540 loss:        0.438003
Test - acc:         0.797800 loss:        0.648672
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.850400 loss:        0.442105
Test - acc:         0.829300 loss:        0.504655
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.853820 loss:        0.437052
Test - acc:         0.819400 loss:        0.537753
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.852320 loss:        0.442963
Test - acc:         0.812600 loss:        0.558468
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.684060 loss:        0.921565
Test - acc:         0.733900 loss:        0.774893
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.776240 loss:        0.656220
Test - acc:         0.749600 loss:        0.753546
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.794600 loss:        0.605359
Test - acc:         0.727400 loss:        0.830545
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.802580 loss:        0.578232
Test - acc:         0.757700 loss:        0.738814
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.808800 loss:        0.563753
Test - acc:         0.789800 loss:        0.636931
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.812520 loss:        0.553904
Test - acc:         0.776100 loss:        0.660115
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.817740 loss:        0.539280
Test - acc:         0.771200 loss:        0.706302
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.818000 loss:        0.535120
Test - acc:         0.733900 loss:        0.816349
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.822360 loss:        0.533164
Test - acc:         0.759300 loss:        0.752055
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.823160 loss:        0.524015
Test - acc:         0.778800 loss:        0.676038
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.824140 loss:        0.518019
Test - acc:         0.780700 loss:        0.668663
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.823440 loss:        0.523382
Test - acc:         0.726000 loss:        0.905363
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.825520 loss:        0.519420
Test - acc:         0.765900 loss:        0.714214
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.827240 loss:        0.513781
Test - acc:         0.680000 loss:        1.025551
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.828780 loss:        0.510983
Test - acc:         0.798300 loss:        0.592168
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.827500 loss:        0.508719
Test - acc:         0.711200 loss:        0.904349
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.826740 loss:        0.509194
Test - acc:         0.735900 loss:        0.856735
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.829460 loss:        0.507934
Test - acc:         0.795200 loss:        0.613786
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.830060 loss:        0.503749
Test - acc:         0.788700 loss:        0.648040
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.829120 loss:        0.507994
Test - acc:         0.769200 loss:        0.689539
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.831140 loss:        0.498555
Test - acc:         0.770000 loss:        0.728096
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.831020 loss:        0.501276
Test - acc:         0.800500 loss:        0.592597
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.830480 loss:        0.499725
Test - acc:         0.810900 loss:        0.564714
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.831000 loss:        0.498786
Test - acc:         0.800900 loss:        0.594627
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.835040 loss:        0.491338
Test - acc:         0.790000 loss:        0.624249
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.834020 loss:        0.494032
Test - acc:         0.764500 loss:        0.741529
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.832180 loss:        0.492567
Test - acc:         0.799200 loss:        0.610016
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.832000 loss:        0.500087
Test - acc:         0.786300 loss:        0.644718
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.833940 loss:        0.488031
Test - acc:         0.693900 loss:        1.036363
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.834080 loss:        0.491751
Test - acc:         0.793300 loss:        0.639231
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.833800 loss:        0.487997
Test - acc:         0.771800 loss:        0.713264
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.835280 loss:        0.484366
Test - acc:         0.793000 loss:        0.627782
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.534460 loss:        1.289952
Test - acc:         0.595100 loss:        1.199797
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.689220 loss:        0.896371
Test - acc:         0.692100 loss:        0.924211
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.725400 loss:        0.798591
Test - acc:         0.695700 loss:        0.894925
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.743620 loss:        0.746282
Test - acc:         0.742500 loss:        0.781084
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.754400 loss:        0.713130
Test - acc:         0.688200 loss:        0.907438
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.762620 loss:        0.694324
Test - acc:         0.652000 loss:        1.160604
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.767800 loss:        0.674872
Test - acc:         0.749300 loss:        0.742936
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.771700 loss:        0.666174
Test - acc:         0.757300 loss:        0.710035
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.779800 loss:        0.648978
Test - acc:         0.706200 loss:        0.882668
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.780060 loss:        0.648647
Test - acc:         0.742600 loss:        0.758973
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.778200 loss:        0.645936
Test - acc:         0.741600 loss:        0.777098
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.780720 loss:        0.641127
Test - acc:         0.696100 loss:        0.887598
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.785020 loss:        0.629028
Test - acc:         0.741600 loss:        0.788422
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.786360 loss:        0.628201
Test - acc:         0.715600 loss:        0.844287
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.789180 loss:        0.618327
Test - acc:         0.768200 loss:        0.722914
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.787360 loss:        0.620857
Test - acc:         0.771500 loss:        0.668780
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.791700 loss:        0.610090
Test - acc:         0.694800 loss:        0.941849
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.791020 loss:        0.605111
Test - acc:         0.702800 loss:        0.963103
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.792720 loss:        0.607013
Test - acc:         0.780800 loss:        0.640943
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.793840 loss:        0.602842
Test - acc:         0.719200 loss:        0.832537
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.790600 loss:        0.612633
Test - acc:         0.750800 loss:        0.736978
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.796140 loss:        0.593437
Test - acc:         0.764600 loss:        0.680355
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.851660 loss:        0.434388
Test - acc:         0.855300 loss:        0.438733
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.868880 loss:        0.382417
Test - acc:         0.859300 loss:        0.417840
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.875360 loss:        0.363091
Test - acc:         0.861400 loss:        0.408415
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.879580 loss:        0.349306
Test - acc:         0.865100 loss:        0.399017
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.884400 loss:        0.337120
Test - acc:         0.863600 loss:        0.406261
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.887180 loss:        0.327268
Test - acc:         0.856800 loss:        0.431093
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.891080 loss:        0.316213
Test - acc:         0.866400 loss:        0.395514
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.894060 loss:        0.309677
Test - acc:         0.854800 loss:        0.429593
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.894580 loss:        0.301634
Test - acc:         0.866500 loss:        0.396403
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.897380 loss:        0.296443
Test - acc:         0.868800 loss:        0.393092
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.475000 loss:        1.444497
Test - acc:         0.622700 loss:        1.050463
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.635820 loss:        1.018997
Test - acc:         0.657000 loss:        0.994465
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.694640 loss:        0.872228
Test - acc:         0.706200 loss:        0.839953
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.726540 loss:        0.779051
Test - acc:         0.719300 loss:        0.809361
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.745760 loss:        0.725159
Test - acc:         0.743000 loss:        0.745960
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.758260 loss:        0.691624
Test - acc:         0.767700 loss:        0.671365
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.771400 loss:        0.658510
Test - acc:         0.758200 loss:        0.708996
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.781980 loss:        0.628374
Test - acc:         0.761700 loss:        0.708416
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.786340 loss:        0.615278
Test - acc:         0.757800 loss:        0.691678
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.793440 loss:        0.598096
Test - acc:         0.780700 loss:        0.648029
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.800560 loss:        0.579379
Test - acc:         0.782100 loss:        0.625993
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.801520 loss:        0.574229
Test - acc:         0.789200 loss:        0.613563
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.803860 loss:        0.562447
Test - acc:         0.792800 loss:        0.602111
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.811280 loss:        0.548088
Test - acc:         0.778700 loss:        0.659698
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.810760 loss:        0.544321
Test - acc:         0.788600 loss:        0.620351
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.814520 loss:        0.535608
Test - acc:         0.795600 loss:        0.609068
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.818300 loss:        0.524696
Test - acc:         0.780900 loss:        0.640092
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.820880 loss:        0.520914
Test - acc:         0.774300 loss:        0.678834
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.821580 loss:        0.514813
Test - acc:         0.785500 loss:        0.631356
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.825160 loss:        0.507314
Test - acc:         0.801600 loss:        0.587314
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.824760 loss:        0.504802
Test - acc:         0.769900 loss:        0.706720
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.827460 loss:        0.501258
Test - acc:         0.789800 loss:        0.615594
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.827920 loss:        0.495426
Test - acc:         0.799500 loss:        0.595438
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.830560 loss:        0.491915
Test - acc:         0.805900 loss:        0.573343
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.829540 loss:        0.487280
Test - acc:         0.804100 loss:        0.574135
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.831380 loss:        0.482112
Test - acc:         0.814900 loss:        0.544559
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.832020 loss:        0.481576
Test - acc:         0.798000 loss:        0.596934
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.835660 loss:        0.473453
Test - acc:         0.804900 loss:        0.571281
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.835220 loss:        0.475128
Test - acc:         0.789000 loss:        0.650704
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.840040 loss:        0.466703
Test - acc:         0.809800 loss:        0.554195
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.840120 loss:        0.463311
Test - acc:         0.774900 loss:        0.691915
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.839300 loss:        0.464277
Test - acc:         0.801400 loss:        0.581025
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.388480 loss:        1.654041
Test - acc:         0.448900 loss:        1.506552
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.523000 loss:        1.315619
Test - acc:         0.557300 loss:        1.220442
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.571560 loss:        1.191695
Test - acc:         0.598200 loss:        1.126366
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.603580 loss:        1.111289
Test - acc:         0.620100 loss:        1.064689
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.621880 loss:        1.058732
Test - acc:         0.622800 loss:        1.068912
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.634480 loss:        1.020661
Test - acc:         0.650500 loss:        0.989994
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.651340 loss:        0.979311
Test - acc:         0.645200 loss:        0.987664
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.663820 loss:        0.950147
Test - acc:         0.640700 loss:        1.016400
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.672020 loss:        0.926900
Test - acc:         0.673400 loss:        0.918516
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.684100 loss:        0.890336
Test - acc:         0.682400 loss:        0.898562
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.688460 loss:        0.880828
Test - acc:         0.675900 loss:        0.897078
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.695300 loss:        0.858531
Test - acc:         0.693800 loss:        0.875516
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.705560 loss:        0.838895
Test - acc:         0.692800 loss:        0.875925
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.708880 loss:        0.824368
Test - acc:         0.701100 loss:        0.862100
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.714520 loss:        0.812183
Test - acc:         0.694800 loss:        0.873119
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.717860 loss:        0.800863
Test - acc:         0.714800 loss:        0.822129
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.724600 loss:        0.784331
Test - acc:         0.704500 loss:        0.841652
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.726540 loss:        0.779751
Test - acc:         0.712700 loss:        0.826589
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.730820 loss:        0.769158
Test - acc:         0.714500 loss:        0.831498
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.732400 loss:        0.760211
Test - acc:         0.724000 loss:        0.793145
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.738100 loss:        0.749315
Test - acc:         0.727400 loss:        0.767015
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.739140 loss:        0.741754
Test - acc:         0.727000 loss:        0.786230
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.742880 loss:        0.733354
Test - acc:         0.706300 loss:        0.868262
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.744580 loss:        0.724153
Test - acc:         0.719900 loss:        0.801027
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.749080 loss:        0.716884
Test - acc:         0.747100 loss:        0.730787
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.749900 loss:        0.714694
Test - acc:         0.738200 loss:        0.761872
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.752360 loss:        0.707226
Test - acc:         0.728500 loss:        0.767360
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.753000 loss:        0.702424
Test - acc:         0.734300 loss:        0.789358
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.753640 loss:        0.704325
Test - acc:         0.730600 loss:        0.805463
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.756560 loss:        0.696137
Test - acc:         0.753300 loss:        0.708654
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.753720 loss:        0.699315
Test - acc:         0.743500 loss:        0.728306
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.757580 loss:        0.689297
Test - acc:         0.743500 loss:        0.746299
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.255980 loss:        2.060909
Test - acc:         0.100000 loss:        3.905742
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.359400 loss:        1.771337
Test - acc:         0.371300 loss:        1.706969
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.408680 loss:        1.653257
Test - acc:         0.376900 loss:        1.715378
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.443580 loss:        1.546927
Test - acc:         0.453700 loss:        1.531102
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.467880 loss:        1.482174
Test - acc:         0.480300 loss:        1.424065
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.480700 loss:        1.438532
Test - acc:         0.479900 loss:        1.409406
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.485920 loss:        1.410102
Test - acc:         0.493400 loss:        1.382253
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.503660 loss:        1.376026
Test - acc:         0.514700 loss:        1.323282
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.507340 loss:        1.356340
Test - acc:         0.503600 loss:        1.326358
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.519920 loss:        1.329557
Test - acc:         0.548900 loss:        1.322848
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.527900 loss:        1.309733
Test - acc:         0.514300 loss:        1.398993
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.533020 loss:        1.294842
Test - acc:         0.538000 loss:        1.277851
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.539600 loss:        1.274852
Test - acc:         0.533900 loss:        1.277812
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.545120 loss:        1.261889
Test - acc:         0.572100 loss:        1.279417
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.549100 loss:        1.256394
Test - acc:         0.540100 loss:        1.337794
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.549860 loss:        1.241884
Test - acc:         0.578600 loss:        1.204187
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.561520 loss:        1.230912
Test - acc:         0.536700 loss:        1.237187
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.578840 loss:        1.205382
Test - acc:         0.594300 loss:        1.172841
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.598360 loss:        1.177372
Test - acc:         0.595900 loss:        1.203881
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.602240 loss:        1.164107
Test - acc:         0.534800 loss:        1.324663
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.608620 loss:        1.145045
Test - acc:         0.611700 loss:        1.119425
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.610780 loss:        1.139104
Test - acc:         0.606500 loss:        1.141463
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.611000 loss:        1.138689
Test - acc:         0.611200 loss:        1.144203
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.617620 loss:        1.120729
Test - acc:         0.610900 loss:        1.141494
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.621280 loss:        1.111043
Test - acc:         0.619200 loss:        1.105737
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.623840 loss:        1.099627
Test - acc:         0.629100 loss:        1.073786
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.649980 loss:        1.028943
Test - acc:         0.670700 loss:        0.983019
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.661360 loss:        1.002855
Test - acc:         0.670900 loss:        0.974211
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.663720 loss:        0.995449
Test - acc:         0.672000 loss:        0.969142
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.665800 loss:        0.984836
Test - acc:         0.675600 loss:        0.969107
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.671900 loss:        0.977108
Test - acc:         0.678600 loss:        0.956602
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.670060 loss:        0.976169
Test - acc:         0.678500 loss:        0.954540
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.107560 loss:        2.905116
Test - acc:         0.129400 loss:        2.614637
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.132200 loss:        2.541594
Test - acc:         0.124400 loss:        2.512769
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.137000 loss:        2.458705
Test - acc:         0.130000 loss:        2.445319
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.138100 loss:        2.408017
Test - acc:         0.142400 loss:        2.377065
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.138260 loss:        2.366303
Test - acc:         0.143200 loss:        2.337775
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.140120 loss:        2.328790
Test - acc:         0.148500 loss:        2.298765
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.153420 loss:        2.282341
Test - acc:         0.163400 loss:        2.248175
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.162460 loss:        2.244173
Test - acc:         0.168200 loss:        2.216374
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.164680 loss:        2.220381
Test - acc:         0.165500 loss:        2.198710
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.167300 loss:        2.203742
Test - acc:         0.179000 loss:        2.177811
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.168960 loss:        2.193232
Test - acc:         0.176400 loss:        2.172838
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.174940 loss:        2.185124
Test - acc:         0.179200 loss:        2.169304
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.178880 loss:        2.177128
Test - acc:         0.179800 loss:        2.157887
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.173580 loss:        2.172401
Test - acc:         0.185000 loss:        2.149180
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.177340 loss:        2.168114
Test - acc:         0.182200 loss:        2.145324
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.194460 loss:        2.163179
Test - acc:         0.216200 loss:        2.143445
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.204720 loss:        2.158020
Test - acc:         0.221700 loss:        2.138489
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.208940 loss:        2.154292
Test - acc:         0.222600 loss:        2.130646
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.191540 loss:        2.147908
Test - acc:         0.217600 loss:        2.130901
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.190660 loss:        2.149758
Test - acc:         0.175200 loss:        2.125125
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.187920 loss:        2.145072
Test - acc:         0.225800 loss:        2.124704
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.199860 loss:        2.143790
Test - acc:         0.224100 loss:        2.121581
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.217560 loss:        2.136576
Test - acc:         0.223900 loss:        2.116457
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.193180 loss:        2.135498
Test - acc:         0.225300 loss:        2.117821
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.185700 loss:        2.132385
Test - acc:         0.226100 loss:        2.117473
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.207280 loss:        2.130451
Test - acc:         0.224000 loss:        2.109534
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.202800 loss:        2.127360
Test - acc:         0.220700 loss:        2.117691
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.209680 loss:        2.126347
Test - acc:         0.199400 loss:        2.101903
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.205680 loss:        2.122500
Test - acc:         0.187900 loss:        2.109508
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.203720 loss:        2.120632
Test - acc:         0.232200 loss:        2.103066
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.210380 loss:        2.118033
Test - acc:         0.198500 loss:        2.096995
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.218460 loss:        2.115320
Test - acc:         0.232900 loss:        2.092194
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.744820
Test - acc:         0.100000 loss:        2.452494
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.386016
Test - acc:         0.100000 loss:        2.345846
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.329242
Test - acc:         0.100000 loss:        2.317789
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.312444
Test - acc:         0.100000 loss:        2.308513
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.306570
Test - acc:         0.100000 loss:        2.304999
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.304340
Test - acc:         0.100000 loss:        2.303587
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.303408
Test - acc:         0.100000 loss:        2.303073
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.302997
Test - acc:         0.100000 loss:        2.302888
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.302858
Test - acc:         0.100000 loss:        2.302680
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.100860 loss:        2.302772
Test - acc:         0.100000 loss:        2.302632
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.099480 loss:        2.302728
Test - acc:         0.100000 loss:        2.302645
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.097960 loss:        2.302738
Test - acc:         0.100000 loss:        2.302663
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.097060 loss:        2.302715
Test - acc:         0.100000 loss:        2.302804
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.099600 loss:        2.302742
Test - acc:         0.100000 loss:        2.302605
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.098660 loss:        2.302708
Test - acc:         0.100000 loss:        2.302605
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.098180 loss:        2.302692
Test - acc:         0.100000 loss:        2.302603
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.097120 loss:        2.302700
Test - acc:         0.100000 loss:        2.302594
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.097780 loss:        2.302690
Test - acc:         0.100000 loss:        2.302593
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.098060 loss:        2.302698
Test - acc:         0.100000 loss:        2.302600
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.099120 loss:        2.302711
Test - acc:         0.100000 loss:        2.302602
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.098380 loss:        2.302719
Test - acc:         0.100000 loss:        2.302621
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.098260 loss:        2.302699
Test - acc:         0.100000 loss:        2.302598
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.098660 loss:        2.302708
Test - acc:         0.100000 loss:        2.302589
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.099080 loss:        2.302649
Test - acc:         0.100000 loss:        2.302658
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.099800 loss:        2.302681
Test - acc:         0.100000 loss:        2.302630
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.098640 loss:        2.302698
Test - acc:         0.100000 loss:        2.302593
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.099740 loss:        2.302674
Test - acc:         0.100000 loss:        2.302606
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.098840 loss:        2.302658
Test - acc:         0.100000 loss:        2.302634
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.098880 loss:        2.302704
Test - acc:         0.100000 loss:        2.302599
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.099080 loss:        2.302692
Test - acc:         0.100000 loss:        2.302616
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.098360 loss:        2.302684
Test - acc:         0.100000 loss:        2.302598
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.097240 loss:        2.302686
Test - acc:         0.100000 loss:        2.302598
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.100240 loss:        2.446041
Test - acc:         0.100000 loss:        2.375022
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.349126
Test - acc:         0.100000 loss:        2.330658
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.321158
Test - acc:         0.100000 loss:        2.314092
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.310418
Test - acc:         0.100000 loss:        2.307459
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.305979
Test - acc:         0.100000 loss:        2.304717
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.100000 loss:        2.304132
Test - acc:         0.100000 loss:        2.303548
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.099900 loss:        2.303273
Test - acc:         0.100000 loss:        2.303074
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.097880 loss:        2.302956
Test - acc:         0.100000 loss:        2.302768
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.099500 loss:        2.302766
Test - acc:         0.100000 loss:        2.302697
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.099140 loss:        2.302727
Test - acc:         0.100000 loss:        2.302640
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.099020 loss:        2.302710
Test - acc:         0.100000 loss:        2.302620
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.099540 loss:        2.302665
Test - acc:         0.100000 loss:        2.302601
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.099900 loss:        2.302671
Test - acc:         0.100000 loss:        2.302615
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.099260 loss:        2.302638
Test - acc:         0.100000 loss:        2.302591
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.098040 loss:        2.302662
Test - acc:         0.100000 loss:        2.302595
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.096100 loss:        2.302677
Test - acc:         0.100000 loss:        2.302586
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.098020 loss:        2.302665
Test - acc:         0.100000 loss:        2.302586
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.096840 loss:        2.302647
Test - acc:         0.100000 loss:        2.302593
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.099440 loss:        2.302629
Test - acc:         0.100000 loss:        2.302600
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.099900 loss:        2.302645
Test - acc:         0.100000 loss:        2.302606
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.098000 loss:        2.302644
Test - acc:         0.100000 loss:        2.302586
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.098760 loss:        2.302656
Test - acc:         0.100000 loss:        2.302593
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.099680 loss:        2.302632
Test - acc:         0.100000 loss:        2.302597
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.096780 loss:        2.302672
Test - acc:         0.100000 loss:        2.302598
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.098900 loss:        2.302642
Test - acc:         0.100000 loss:        2.302594
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.097340 loss:        2.302622
Test - acc:         0.100000 loss:        2.302612
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.096900 loss:        2.302661
Test - acc:         0.100000 loss:        2.302588
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.097000 loss:        2.302636
Test - acc:         0.100000 loss:        2.302608
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.099720 loss:        2.302624
Test - acc:         0.100000 loss:        2.302596
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.096580 loss:        2.302639
Test - acc:         0.100000 loss:        2.302585
Sparsity :          0.9990
Wdecay :        0.000500
