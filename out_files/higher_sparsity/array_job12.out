Running --model resnet18 --prune_criterion magnitude --seed 44 --prune_freq 32 --prune_rate 0.5 --comment=resnet18_crit=magnitude_pf=32_seed=44 --save_model=pre-finetune/resnet18_magnitude_pf32_s44 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "magnitude",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_magnitude_pf32_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.326500 loss:        1.844461
Test - acc:         0.423100 loss:        1.566802
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.523060 loss:        1.311480
Test - acc:         0.573700 loss:        1.162164
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.631560 loss:        1.037143
Test - acc:         0.605500 loss:        1.154360
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.690180 loss:        0.879254
Test - acc:         0.698600 loss:        0.870134
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.743000 loss:        0.739316
Test - acc:         0.741600 loss:        0.760918
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.775920 loss:        0.646051
Test - acc:         0.761800 loss:        0.712189
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.798040 loss:        0.583349
Test - acc:         0.699400 loss:        0.960318
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.808880 loss:        0.550098
Test - acc:         0.719800 loss:        0.895329
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.822400 loss:        0.517782
Test - acc:         0.798300 loss:        0.592408
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.828280 loss:        0.498775
Test - acc:         0.803500 loss:        0.578936
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.838380 loss:        0.470293
Test - acc:         0.802200 loss:        0.613507
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.840980 loss:        0.464587
Test - acc:         0.794200 loss:        0.611622
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.847140 loss:        0.446681
Test - acc:         0.812200 loss:        0.584225
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.851420 loss:        0.433831
Test - acc:         0.745900 loss:        0.817999
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.852860 loss:        0.428770
Test - acc:         0.797000 loss:        0.603769
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.856840 loss:        0.417291
Test - acc:         0.793900 loss:        0.644432
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.862140 loss:        0.409136
Test - acc:         0.824900 loss:        0.529044
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.861320 loss:        0.401354
Test - acc:         0.815400 loss:        0.555978
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.863660 loss:        0.396668
Test - acc:         0.817300 loss:        0.531897
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.866860 loss:        0.390623
Test - acc:         0.770700 loss:        0.716770
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.868780 loss:        0.385096
Test - acc:         0.832300 loss:        0.500326
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870100 loss:        0.382342
Test - acc:         0.813700 loss:        0.540815
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.869500 loss:        0.384136
Test - acc:         0.837000 loss:        0.476577
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.872000 loss:        0.375453
Test - acc:         0.842100 loss:        0.462574
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.874400 loss:        0.369435
Test - acc:         0.822800 loss:        0.543520
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.873880 loss:        0.369378
Test - acc:         0.853400 loss:        0.432042
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.876600 loss:        0.359764
Test - acc:         0.824500 loss:        0.514843
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.877660 loss:        0.357625
Test - acc:         0.769200 loss:        0.763895
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.878520 loss:        0.356873
Test - acc:         0.815700 loss:        0.576099
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.878700 loss:        0.357575
Test - acc:         0.829500 loss:        0.503405
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.877880 loss:        0.360216
Test - acc:         0.826900 loss:        0.531613
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.881500 loss:        0.347606
Test - acc:         0.833600 loss:        0.533857
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.899880 loss:        0.295623
Test - acc:         0.842200 loss:        0.481202
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.897120 loss:        0.299820
Test - acc:         0.857300 loss:        0.434882
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.896740 loss:        0.304061
Test - acc:         0.831700 loss:        0.507554
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.896320 loss:        0.303495
Test - acc:         0.842700 loss:        0.485474
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.894560 loss:        0.306128
Test - acc:         0.824300 loss:        0.545985
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.896980 loss:        0.300994
Test - acc:         0.815000 loss:        0.577626
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.895600 loss:        0.303990
Test - acc:         0.860800 loss:        0.421196
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.896580 loss:        0.302300
Test - acc:         0.832200 loss:        0.514610
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.896560 loss:        0.304249
Test - acc:         0.846800 loss:        0.437125
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.900480 loss:        0.293673
Test - acc:         0.858500 loss:        0.441684
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.897820 loss:        0.298831
Test - acc:         0.822600 loss:        0.571491
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.896840 loss:        0.296283
Test - acc:         0.867000 loss:        0.402610
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.898940 loss:        0.294304
Test - acc:         0.861000 loss:        0.432623
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.899540 loss:        0.294364
Test - acc:         0.860500 loss:        0.423256
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.898780 loss:        0.295835
Test - acc:         0.865100 loss:        0.412036
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.897000 loss:        0.296685
Test - acc:         0.852100 loss:        0.451845
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.899740 loss:        0.292026
Test - acc:         0.851400 loss:        0.450645
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.897300 loss:        0.299548
Test - acc:         0.843000 loss:        0.494542
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.901780 loss:        0.288790
Test - acc:         0.851200 loss:        0.451858
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.899500 loss:        0.293369
Test - acc:         0.876900 loss:        0.346995
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.899980 loss:        0.292977
Test - acc:         0.807300 loss:        0.574623
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.902180 loss:        0.289880
Test - acc:         0.818800 loss:        0.592476
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.899980 loss:        0.292370
Test - acc:         0.822800 loss:        0.557468
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.900120 loss:        0.290811
Test - acc:         0.844400 loss:        0.464990
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.899800 loss:        0.291097
Test - acc:         0.820200 loss:        0.559075
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.902780 loss:        0.288948
Test - acc:         0.836500 loss:        0.515872
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.899900 loss:        0.291802
Test - acc:         0.849100 loss:        0.470900
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.901960 loss:        0.288039
Test - acc:         0.833200 loss:        0.529930
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.898940 loss:        0.293076
Test - acc:         0.866100 loss:        0.407500
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.900680 loss:        0.293415
Test - acc:         0.867900 loss:        0.393312
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.901860 loss:        0.287973
Test - acc:         0.826900 loss:        0.540324
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.898460 loss:        0.293327
Test - acc:         0.864000 loss:        0.407166
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.914840 loss:        0.249792
Test - acc:         0.857200 loss:        0.438579
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.913540 loss:        0.252373
Test - acc:         0.874200 loss:        0.388084
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.913340 loss:        0.252570
Test - acc:         0.822500 loss:        0.556304
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.910580 loss:        0.262960
Test - acc:         0.865600 loss:        0.419370
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.910360 loss:        0.262404
Test - acc:         0.832200 loss:        0.546586
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.913400 loss:        0.252260
Test - acc:         0.881700 loss:        0.353833
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.912080 loss:        0.254458
Test - acc:         0.853600 loss:        0.459243
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.909840 loss:        0.262346
Test - acc:         0.865800 loss:        0.408699
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.910000 loss:        0.261452
Test - acc:         0.871500 loss:        0.387006
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.911460 loss:        0.258246
Test - acc:         0.867200 loss:        0.398118
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.910240 loss:        0.263675
Test - acc:         0.833200 loss:        0.513127
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.912640 loss:        0.254060
Test - acc:         0.853900 loss:        0.465322
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.911920 loss:        0.256691
Test - acc:         0.785200 loss:        0.738777
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.908240 loss:        0.263987
Test - acc:         0.871300 loss:        0.378402
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.911840 loss:        0.256531
Test - acc:         0.858600 loss:        0.438950
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.909780 loss:        0.259592
Test - acc:         0.857700 loss:        0.462846
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.910440 loss:        0.259345
Test - acc:         0.876600 loss:        0.367419
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.911220 loss:        0.257227
Test - acc:         0.877000 loss:        0.375463
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.910220 loss:        0.258824
Test - acc:         0.835100 loss:        0.536628
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.910600 loss:        0.260357
Test - acc:         0.869400 loss:        0.386850
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.912680 loss:        0.256733
Test - acc:         0.826000 loss:        0.548577
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.912440 loss:        0.256611
Test - acc:         0.864000 loss:        0.406136
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.912280 loss:        0.256746
Test - acc:         0.842700 loss:        0.479878
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.911000 loss:        0.258298
Test - acc:         0.859900 loss:        0.416461
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.912820 loss:        0.256068
Test - acc:         0.832100 loss:        0.546618
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.911580 loss:        0.256573
Test - acc:         0.835500 loss:        0.525484
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.910560 loss:        0.256091
Test - acc:         0.878300 loss:        0.362837
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.910500 loss:        0.258265
Test - acc:         0.878000 loss:        0.365272
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.912020 loss:        0.253729
Test - acc:         0.888200 loss:        0.347883
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.914480 loss:        0.256356
Test - acc:         0.869100 loss:        0.384888
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.911960 loss:        0.256383
Test - acc:         0.853600 loss:        0.442903
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.912020 loss:        0.255921
Test - acc:         0.867500 loss:        0.409117
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.918580 loss:        0.238290
Test - acc:         0.874600 loss:        0.378768
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.918260 loss:        0.238340
Test - acc:         0.851400 loss:        0.471128
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.918000 loss:        0.241631
Test - acc:         0.866300 loss:        0.400303
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.917960 loss:        0.241896
Test - acc:         0.830600 loss:        0.551172
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.916460 loss:        0.244285
Test - acc:         0.854300 loss:        0.464252
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.917540 loss:        0.239809
Test - acc:         0.841200 loss:        0.473529
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.916500 loss:        0.240808
Test - acc:         0.865200 loss:        0.406210
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.916140 loss:        0.244885
Test - acc:         0.866600 loss:        0.414790
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.917760 loss:        0.236859
Test - acc:         0.868000 loss:        0.402921
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.916140 loss:        0.245869
Test - acc:         0.880200 loss:        0.375300
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.914380 loss:        0.246324
Test - acc:         0.870100 loss:        0.396127
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.916720 loss:        0.240663
Test - acc:         0.819400 loss:        0.580069
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.917020 loss:        0.239903
Test - acc:         0.861500 loss:        0.420321
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.917420 loss:        0.241560
Test - acc:         0.869000 loss:        0.404804
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.917840 loss:        0.237993
Test - acc:         0.872300 loss:        0.391446
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.916500 loss:        0.242685
Test - acc:         0.861600 loss:        0.440941
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.918680 loss:        0.238131
Test - acc:         0.863900 loss:        0.414851
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.917520 loss:        0.240828
Test - acc:         0.866100 loss:        0.406423
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.916400 loss:        0.241668
Test - acc:         0.861600 loss:        0.425576
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.919340 loss:        0.234183
Test - acc:         0.871000 loss:        0.414498
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.918040 loss:        0.239870
Test - acc:         0.862500 loss:        0.428895
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.916720 loss:        0.244054
Test - acc:         0.883000 loss:        0.345477
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.918040 loss:        0.238423
Test - acc:         0.872600 loss:        0.398297
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.917240 loss:        0.240617
Test - acc:         0.845100 loss:        0.491789
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.917440 loss:        0.240914
Test - acc:         0.880700 loss:        0.379649
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.918360 loss:        0.240408
Test - acc:         0.875200 loss:        0.387132
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.914220 loss:        0.247236
Test - acc:         0.840800 loss:        0.493999
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.916800 loss:        0.241715
Test - acc:         0.876500 loss:        0.378345
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.917000 loss:        0.240767
Test - acc:         0.847500 loss:        0.509191
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.916280 loss:        0.241766
Test - acc:         0.861100 loss:        0.425441
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.920180 loss:        0.233484
Test - acc:         0.864400 loss:        0.436105
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.916080 loss:        0.242719
Test - acc:         0.858000 loss:        0.436029
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.914700 loss:        0.250207
Test - acc:         0.874000 loss:        0.387709
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.916120 loss:        0.244983
Test - acc:         0.850200 loss:        0.480822
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.914260 loss:        0.248002
Test - acc:         0.865200 loss:        0.428027
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.916980 loss:        0.242569
Test - acc:         0.877900 loss:        0.366326
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.914780 loss:        0.247791
Test - acc:         0.816200 loss:        0.585890
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.915760 loss:        0.244450
Test - acc:         0.826300 loss:        0.533209
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.916660 loss:        0.240734
Test - acc:         0.868900 loss:        0.405212
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.914120 loss:        0.247733
Test - acc:         0.842500 loss:        0.495062
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.914080 loss:        0.248400
Test - acc:         0.891600 loss:        0.325138
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.915180 loss:        0.244884
Test - acc:         0.874900 loss:        0.386897
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.915120 loss:        0.244354
Test - acc:         0.888300 loss:        0.349547
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.916620 loss:        0.242948
Test - acc:         0.866800 loss:        0.411060
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.916720 loss:        0.243745
Test - acc:         0.801500 loss:        0.696912
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.914440 loss:        0.249012
Test - acc:         0.830900 loss:        0.542612
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.916940 loss:        0.241972
Test - acc:         0.827900 loss:        0.538844
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.916080 loss:        0.246692
Test - acc:         0.864100 loss:        0.424127
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.916300 loss:        0.245065
Test - acc:         0.856400 loss:        0.440192
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.916340 loss:        0.241388
Test - acc:         0.857700 loss:        0.431811
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.912320 loss:        0.251351
Test - acc:         0.867700 loss:        0.398244
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.914220 loss:        0.246669
Test - acc:         0.845500 loss:        0.498057
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.916660 loss:        0.244470
Test - acc:         0.867000 loss:        0.406004
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.916540 loss:        0.247850
Test - acc:         0.856500 loss:        0.439869
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.953860 loss:        0.139980
Test - acc:         0.932600 loss:        0.199079
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.967740 loss:        0.101618
Test - acc:         0.935300 loss:        0.196980
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.971560 loss:        0.088208
Test - acc:         0.938200 loss:        0.190944
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.972880 loss:        0.081425
Test - acc:         0.939900 loss:        0.190440
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.976360 loss:        0.073126
Test - acc:         0.937000 loss:        0.194277
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.976420 loss:        0.068449
Test - acc:         0.936500 loss:        0.194983
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.978980 loss:        0.063392
Test - acc:         0.936900 loss:        0.197420
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.980400 loss:        0.060719
Test - acc:         0.937800 loss:        0.194699
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.982180 loss:        0.055960
Test - acc:         0.937700 loss:        0.201761
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.983020 loss:        0.052146
Test - acc:         0.938600 loss:        0.199560
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.963180 loss:        0.114837
Test - acc:         0.926100 loss:        0.221625
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.973820 loss:        0.083105
Test - acc:         0.931800 loss:        0.216179
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.975040 loss:        0.075939
Test - acc:         0.930500 loss:        0.215417
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.975160 loss:        0.073422
Test - acc:         0.930000 loss:        0.224482
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.976880 loss:        0.071084
Test - acc:         0.930000 loss:        0.216792
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.977320 loss:        0.068538
Test - acc:         0.931700 loss:        0.226256
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.979560 loss:        0.062280
Test - acc:         0.931900 loss:        0.224238
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.977880 loss:        0.064018
Test - acc:         0.929800 loss:        0.228116
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.979760 loss:        0.060826
Test - acc:         0.929200 loss:        0.234420
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.978300 loss:        0.063542
Test - acc:         0.925100 loss:        0.241770
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.979400 loss:        0.061039
Test - acc:         0.930200 loss:        0.227834
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.979560 loss:        0.060708
Test - acc:         0.927000 loss:        0.232050
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.980160 loss:        0.060829
Test - acc:         0.929200 loss:        0.227251
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.981020 loss:        0.056733
Test - acc:         0.928800 loss:        0.236065
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.979600 loss:        0.060137
Test - acc:         0.930100 loss:        0.230985
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.979620 loss:        0.061250
Test - acc:         0.928200 loss:        0.246748
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.980320 loss:        0.058721
Test - acc:         0.924900 loss:        0.250319
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.979300 loss:        0.060338
Test - acc:         0.925200 loss:        0.256233
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.980540 loss:        0.060250
Test - acc:         0.931700 loss:        0.231347
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.979060 loss:        0.062110
Test - acc:         0.926500 loss:        0.241172
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.978800 loss:        0.062214
Test - acc:         0.923700 loss:        0.251995
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.977420 loss:        0.065105
Test - acc:         0.923900 loss:        0.255315
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.978700 loss:        0.063193
Test - acc:         0.920400 loss:        0.266711
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.979800 loss:        0.060207
Test - acc:         0.926900 loss:        0.247276
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.977740 loss:        0.065205
Test - acc:         0.922400 loss:        0.263863
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.977080 loss:        0.066120
Test - acc:         0.926800 loss:        0.247275
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.977560 loss:        0.065887
Test - acc:         0.923800 loss:        0.254440
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.978960 loss:        0.063802
Test - acc:         0.923700 loss:        0.264626
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.978200 loss:        0.065234
Test - acc:         0.922200 loss:        0.261031
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.977420 loss:        0.069040
Test - acc:         0.921600 loss:        0.256279
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.978480 loss:        0.064766
Test - acc:         0.926000 loss:        0.259009
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.976680 loss:        0.066923
Test - acc:         0.920000 loss:        0.269641
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.921560 loss:        0.236002
Test - acc:         0.896000 loss:        0.313846
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.942940 loss:        0.166110
Test - acc:         0.904800 loss:        0.298306
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.947680 loss:        0.153847
Test - acc:         0.905500 loss:        0.299130
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.948160 loss:        0.150454
Test - acc:         0.892900 loss:        0.329101
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.950280 loss:        0.143823
Test - acc:         0.906000 loss:        0.292406
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.952980 loss:        0.136172
Test - acc:         0.898900 loss:        0.326307
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.951440 loss:        0.138155
Test - acc:         0.908800 loss:        0.290625
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.954420 loss:        0.131262
Test - acc:         0.908900 loss:        0.292121
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.956560 loss:        0.124491
Test - acc:         0.907800 loss:        0.290927
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.954720 loss:        0.128699
Test - acc:         0.904300 loss:        0.312869
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.955440 loss:        0.129297
Test - acc:         0.906400 loss:        0.301208
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.956740 loss:        0.125823
Test - acc:         0.912100 loss:        0.284231
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.955940 loss:        0.126991
Test - acc:         0.907300 loss:        0.300124
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.958620 loss:        0.119011
Test - acc:         0.910700 loss:        0.290488
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.955200 loss:        0.123856
Test - acc:         0.906100 loss:        0.305531
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.957080 loss:        0.122370
Test - acc:         0.903700 loss:        0.314725
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.955920 loss:        0.125270
Test - acc:         0.907800 loss:        0.282470
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.958480 loss:        0.120247
Test - acc:         0.906900 loss:        0.307628
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.959600 loss:        0.116086
Test - acc:         0.905800 loss:        0.298990
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.958000 loss:        0.120311
Test - acc:         0.905000 loss:        0.297859
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.958820 loss:        0.119016
Test - acc:         0.906700 loss:        0.304120
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.958780 loss:        0.116258
Test - acc:         0.907600 loss:        0.296636
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.957720 loss:        0.119974
Test - acc:         0.911300 loss:        0.285613
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.959540 loss:        0.116081
Test - acc:         0.904300 loss:        0.306129
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.958580 loss:        0.119631
Test - acc:         0.908000 loss:        0.302311
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.960040 loss:        0.117338
Test - acc:         0.903600 loss:        0.309977
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.958880 loss:        0.116470
Test - acc:         0.913700 loss:        0.281579
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.960660 loss:        0.111865
Test - acc:         0.909400 loss:        0.295296
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.961480 loss:        0.112126
Test - acc:         0.909900 loss:        0.292076
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.959280 loss:        0.116002
Test - acc:         0.909800 loss:        0.284208
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.959760 loss:        0.114355
Test - acc:         0.911100 loss:        0.288465
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.959960 loss:        0.114666
Test - acc:         0.912200 loss:        0.281732
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.845360 loss:        0.465054
Test - acc:         0.861200 loss:        0.410795
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.884400 loss:        0.335600
Test - acc:         0.864100 loss:        0.409688
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.892640 loss:        0.311323
Test - acc:         0.865800 loss:        0.407641
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.896980 loss:        0.299368
Test - acc:         0.860100 loss:        0.425960
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.899420 loss:        0.285921
Test - acc:         0.864600 loss:        0.428117
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.901800 loss:        0.284202
Test - acc:         0.871600 loss:        0.396516
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.904520 loss:        0.273197
Test - acc:         0.877800 loss:        0.368533
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.906580 loss:        0.267262
Test - acc:         0.859100 loss:        0.440599
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.905120 loss:        0.270952
Test - acc:         0.875300 loss:        0.380735
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.907100 loss:        0.265455
Test - acc:         0.874800 loss:        0.378332
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.909740 loss:        0.257798
Test - acc:         0.876700 loss:        0.371220
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.908620 loss:        0.261286
Test - acc:         0.870900 loss:        0.402592
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.908940 loss:        0.259617
Test - acc:         0.867100 loss:        0.413999
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.910820 loss:        0.257086
Test - acc:         0.883500 loss:        0.357054
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.912700 loss:        0.251625
Test - acc:         0.875400 loss:        0.381252
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.911300 loss:        0.255685
Test - acc:         0.877500 loss:        0.367401
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.911240 loss:        0.253391
Test - acc:         0.874300 loss:        0.388319
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.912380 loss:        0.251698
Test - acc:         0.876600 loss:        0.376740
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.911440 loss:        0.253722
Test - acc:         0.867400 loss:        0.398949
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.912000 loss:        0.251618
Test - acc:         0.877300 loss:        0.378719
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.914980 loss:        0.243426
Test - acc:         0.879000 loss:        0.375324
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.911280 loss:        0.249929
Test - acc:         0.878500 loss:        0.369886
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.914100 loss:        0.245142
Test - acc:         0.880800 loss:        0.360449
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.913560 loss:        0.245370
Test - acc:         0.877400 loss:        0.385041
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.911800 loss:        0.249265
Test - acc:         0.880100 loss:        0.369764
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.914760 loss:        0.246010
Test - acc:         0.881500 loss:        0.365147
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.932880 loss:        0.197593
Test - acc:         0.902500 loss:        0.295297
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.939120 loss:        0.178942
Test - acc:         0.902800 loss:        0.291874
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.939100 loss:        0.174686
Test - acc:         0.903900 loss:        0.290546
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.943080 loss:        0.168692
Test - acc:         0.903300 loss:        0.291714
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.944760 loss:        0.162659
Test - acc:         0.905200 loss:        0.290994
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.943820 loss:        0.163052
Test - acc:         0.907100 loss:        0.285284
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.669540 loss:        1.106194
Test - acc:         0.726700 loss:        0.833217
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.753060 loss:        0.758951
Test - acc:         0.775000 loss:        0.716650
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.799520 loss:        0.661016
Test - acc:         0.803500 loss:        0.658026
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.823620 loss:        0.605674
Test - acc:         0.822000 loss:        0.601149
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.836800 loss:        0.559576
Test - acc:         0.832900 loss:        0.567211
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.845720 loss:        0.529207
Test - acc:         0.837300 loss:        0.539514
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.854000 loss:        0.499943
Test - acc:         0.843100 loss:        0.515147
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.857620 loss:        0.479689
Test - acc:         0.845100 loss:        0.503367
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.863900 loss:        0.460582
Test - acc:         0.850700 loss:        0.487424
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.865260 loss:        0.447135
Test - acc:         0.851800 loss:        0.476289
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.868000 loss:        0.432978
Test - acc:         0.854400 loss:        0.470038
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.870020 loss:        0.423757
Test - acc:         0.853000 loss:        0.463607
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.871340 loss:        0.416131
Test - acc:         0.858400 loss:        0.451842
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.870860 loss:        0.409036
Test - acc:         0.853600 loss:        0.452284
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.874580 loss:        0.397962
Test - acc:         0.856900 loss:        0.448211
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.875540 loss:        0.391892
Test - acc:         0.856900 loss:        0.443902
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.876700 loss:        0.388332
Test - acc:         0.860500 loss:        0.435042
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.877480 loss:        0.382045
Test - acc:         0.859800 loss:        0.435627
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.879840 loss:        0.375043
Test - acc:         0.856700 loss:        0.447860
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.878420 loss:        0.375442
Test - acc:         0.862700 loss:        0.429577
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.880460 loss:        0.370661
Test - acc:         0.861000 loss:        0.423427
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.881200 loss:        0.366141
Test - acc:         0.862300 loss:        0.422517
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.882740 loss:        0.363099
Test - acc:         0.860700 loss:        0.428091
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.880300 loss:        0.362105
Test - acc:         0.861900 loss:        0.425335
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.882800 loss:        0.359456
Test - acc:         0.863400 loss:        0.421770
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.884000 loss:        0.358071
Test - acc:         0.862200 loss:        0.419339
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.884320 loss:        0.353023
Test - acc:         0.863100 loss:        0.420201
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.881520 loss:        0.355176
Test - acc:         0.861200 loss:        0.425530
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.884260 loss:        0.348875
Test - acc:         0.861900 loss:        0.414790
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.885880 loss:        0.344769
Test - acc:         0.864700 loss:        0.414638
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.887400 loss:        0.341517
Test - acc:         0.864800 loss:        0.409140
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.886920 loss:        0.343027
Test - acc:         0.867000 loss:        0.408395
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.400380 loss:        2.150343
Test - acc:         0.119600 loss:       19.732580
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.485660 loss:        1.714086
Test - acc:         0.337200 loss:        2.373268
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.508240 loss:        1.542225
Test - acc:         0.481900 loss:        1.662405
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.525120 loss:        1.425605
Test - acc:         0.509000 loss:        1.494642
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.531620 loss:        1.357695
Test - acc:         0.553800 loss:        1.291062
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.541100 loss:        1.307465
Test - acc:         0.558600 loss:        1.254040
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.546060 loss:        1.278130
Test - acc:         0.561900 loss:        1.230602
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.552540 loss:        1.250973
Test - acc:         0.474300 loss:        1.514654
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.553180 loss:        1.236319
Test - acc:         0.320800 loss:        2.564842
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.558680 loss:        1.222876
Test - acc:         0.351000 loss:        2.316003
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.558800 loss:        1.212803
Test - acc:         0.569100 loss:        1.177853
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.564340 loss:        1.199151
Test - acc:         0.570200 loss:        1.173450
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.566280 loss:        1.191951
Test - acc:         0.235200 loss:        4.950507
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.567920 loss:        1.187235
Test - acc:         0.565800 loss:        1.177608
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.570580 loss:        1.176940
Test - acc:         0.330600 loss:        2.575865
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.570360 loss:        1.172709
Test - acc:         0.582800 loss:        1.136583
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.573460 loss:        1.164586
Test - acc:         0.586300 loss:        1.144846
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.574420 loss:        1.157946
Test - acc:         0.392000 loss:        2.042113
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.568540 loss:        1.158979
Test - acc:         0.438400 loss:        1.744046
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.576300 loss:        1.154014
Test - acc:         0.579100 loss:        1.137033
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.579560 loss:        1.148050
Test - acc:         0.537000 loss:        1.269458
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.580380 loss:        1.141773
Test - acc:         0.586200 loss:        1.112647
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.579140 loss:        1.136610
Test - acc:         0.559900 loss:        1.193623
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.563640 loss:        1.136793
Test - acc:         0.499600 loss:        1.482598
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.564040 loss:        1.129298
Test - acc:         0.569400 loss:        1.095937
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.580880 loss:        1.130059
Test - acc:         0.515300 loss:        1.370133
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.583600 loss:        1.121136
Test - acc:         0.594800 loss:        1.094143
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.566620 loss:        1.120639
Test - acc:         0.601100 loss:        1.090633
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.581580 loss:        1.120053
Test - acc:         0.593200 loss:        1.097791
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.585760 loss:        1.114895
Test - acc:         0.565200 loss:        1.110948
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.587260 loss:        1.110475
Test - acc:         0.594100 loss:        1.088867
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.579220 loss:        1.108217
Test - acc:         0.483700 loss:        1.551039
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.242400 loss:        2.394031
Test - acc:         0.268500 loss:        2.243234
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.287040 loss:        2.133227
Test - acc:         0.195700 loss:        2.904198
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.304220 loss:        2.022807
Test - acc:         0.324100 loss:        1.985866
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.333940 loss:        1.939275
Test - acc:         0.338400 loss:        1.917862
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.336680 loss:        1.885233
Test - acc:         0.287800 loss:        1.889508
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.330620 loss:        1.851208
Test - acc:         0.268200 loss:        1.946591
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.339780 loss:        1.825858
Test - acc:         0.278600 loss:        1.901604
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.330780 loss:        1.806811
Test - acc:         0.371100 loss:        1.792662
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.372300 loss:        1.791396
Test - acc:         0.305400 loss:        1.922176
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.349400 loss:        1.782866
Test - acc:         0.157800 loss:        3.732431
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.346480 loss:        1.777408
Test - acc:         0.317600 loss:        1.786319
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.347040 loss:        1.764426
Test - acc:         0.380500 loss:        1.749497
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.355940 loss:        1.757811
Test - acc:         0.340300 loss:        1.781092
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.359760 loss:        1.749402
Test - acc:         0.332300 loss:        1.759262
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.380360 loss:        1.746217
Test - acc:         0.351200 loss:        1.757246
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.356200 loss:        1.740897
Test - acc:         0.347500 loss:        1.786431
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.365440 loss:        1.733029
Test - acc:         0.346300 loss:        1.783750
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.359820 loss:        1.730659
Test - acc:         0.358400 loss:        1.704476
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.377920 loss:        1.727268
Test - acc:         0.325600 loss:        1.937875
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.366100 loss:        1.724238
Test - acc:         0.289400 loss:        2.304061
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.383360 loss:        1.715063
Test - acc:         0.302600 loss:        1.904577
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.372600 loss:        1.714297
Test - acc:         0.344400 loss:        1.726821
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.355780 loss:        1.707834
Test - acc:         0.368600 loss:        1.718119
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.367240 loss:        1.701578
Test - acc:         0.350900 loss:        1.765085
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.376880 loss:        1.703567
Test - acc:         0.368700 loss:        1.684219
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.380720 loss:        1.697762
Test - acc:         0.301300 loss:        1.871267
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.366980 loss:        1.693648
Test - acc:         0.323800 loss:        1.808469
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.367220 loss:        1.695002
Test - acc:         0.280000 loss:        2.012769
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.383420 loss:        1.690803
Test - acc:         0.361500 loss:        1.715049
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.382160 loss:        1.686398
Test - acc:         0.360200 loss:        1.709192
Sparsity :          0.9990
Wdecay :        0.000500
