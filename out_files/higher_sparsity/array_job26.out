Running --model vgg19 --prune_criterion global_magnitude --seed 44 --prune_freq 32 --prune_rate 0.5 --comment=vgg19_crit=global_magnitude_pf=32_seed=44 --save_model=pre-finetune/vgg19_global_magnitude_pf32_s44 --logdir=criterion_experiment_no_bias/vgg19
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 44,
    "prune_criterion": "global_magnitude",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_global_magnitude_pf32_s44",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.121220 loss:        2.493571
Test - acc:         0.162400 loss:        2.247057
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.238540 loss:        1.988574
Test - acc:         0.291200 loss:        1.816521
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.309940 loss:        1.770952
Test - acc:         0.338500 loss:        1.698639
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.379240 loss:        1.597609
Test - acc:         0.373000 loss:        1.624368
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.485060 loss:        1.383755
Test - acc:         0.492900 loss:        1.383657
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.595460 loss:        1.144366
Test - acc:         0.548400 loss:        1.317607
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.663920 loss:        0.966378
Test - acc:         0.628100 loss:        1.085081
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.708800 loss:        0.854189
Test - acc:         0.628000 loss:        1.141725
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.733740 loss:        0.788849
Test - acc:         0.738400 loss:        0.793110
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.751300 loss:        0.746240
Test - acc:         0.596200 loss:        1.291147
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.771620 loss:        0.698481
Test - acc:         0.701500 loss:        0.996400
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.777780 loss:        0.677521
Test - acc:         0.672600 loss:        1.052496
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.785620 loss:        0.657763
Test - acc:         0.678700 loss:        1.063080
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.790880 loss:        0.638954
Test - acc:         0.713700 loss:        0.933064
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.796840 loss:        0.622712
Test - acc:         0.778500 loss:        0.679065
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.803920 loss:        0.610713
Test - acc:         0.757800 loss:        0.748650
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.807580 loss:        0.596009
Test - acc:         0.752600 loss:        0.749324
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.809320 loss:        0.586204
Test - acc:         0.688000 loss:        0.984244
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.813800 loss:        0.574660
Test - acc:         0.712500 loss:        0.908072
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.822480 loss:        0.554705
Test - acc:         0.751600 loss:        0.800250
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.822680 loss:        0.542792
Test - acc:         0.747900 loss:        0.776715
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.825960 loss:        0.537973
Test - acc:         0.783600 loss:        0.670969
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.827120 loss:        0.530511
Test - acc:         0.725800 loss:        0.980620
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.829620 loss:        0.524179
Test - acc:         0.759400 loss:        0.772408
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.834520 loss:        0.512033
Test - acc:         0.744100 loss:        0.875782
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.835800 loss:        0.505285
Test - acc:         0.759700 loss:        0.725445
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.839820 loss:        0.494703
Test - acc:         0.784400 loss:        0.703626
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.840580 loss:        0.488501
Test - acc:         0.780800 loss:        0.733794
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.842180 loss:        0.488092
Test - acc:         0.826500 loss:        0.554136
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.843940 loss:        0.479515
Test - acc:         0.802400 loss:        0.607688
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.845500 loss:        0.478120
Test - acc:         0.758200 loss:        0.820148
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.844480 loss:        0.475611
Test - acc:         0.762400 loss:        0.770402
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.849720 loss:        0.462625
Test - acc:         0.805700 loss:        0.590715
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.849260 loss:        0.458467
Test - acc:         0.757800 loss:        0.773346
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.849460 loss:        0.460357
Test - acc:         0.781600 loss:        0.681658
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.854800 loss:        0.447055
Test - acc:         0.812300 loss:        0.594666
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.854440 loss:        0.445013
Test - acc:         0.732800 loss:        0.885280
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.853840 loss:        0.451381
Test - acc:         0.811900 loss:        0.627812
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.853580 loss:        0.447955
Test - acc:         0.802400 loss:        0.604698
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.855660 loss:        0.439709
Test - acc:         0.769000 loss:        0.747659
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.855840 loss:        0.441601
Test - acc:         0.774700 loss:        0.716731
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.856840 loss:        0.440475
Test - acc:         0.768000 loss:        0.703291
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.857280 loss:        0.436375
Test - acc:         0.820200 loss:        0.567745
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.856400 loss:        0.435521
Test - acc:         0.821500 loss:        0.551144
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.859700 loss:        0.431056
Test - acc:         0.743700 loss:        0.881800
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.862060 loss:        0.422353
Test - acc:         0.835300 loss:        0.519594
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.860940 loss:        0.424205
Test - acc:         0.786200 loss:        0.724020
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.860000 loss:        0.423475
Test - acc:         0.769500 loss:        0.769878
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.866860 loss:        0.411751
Test - acc:         0.825900 loss:        0.543532
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.858540 loss:        0.427443
Test - acc:         0.758700 loss:        0.783470
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.865180 loss:        0.409563
Test - acc:         0.755400 loss:        0.822207
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.863440 loss:        0.413946
Test - acc:         0.778300 loss:        0.692301
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.865760 loss:        0.411554
Test - acc:         0.824800 loss:        0.539416
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.864500 loss:        0.409767
Test - acc:         0.808400 loss:        0.614466
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.864700 loss:        0.406167
Test - acc:         0.743100 loss:        0.903095
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.866640 loss:        0.402830
Test - acc:         0.755700 loss:        0.794904
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.864680 loss:        0.408164
Test - acc:         0.789400 loss:        0.683354
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.867240 loss:        0.400495
Test - acc:         0.805700 loss:        0.603565
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.866180 loss:        0.406071
Test - acc:         0.779600 loss:        0.674993
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.867420 loss:        0.400557
Test - acc:         0.808400 loss:        0.603034
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.867260 loss:        0.399957
Test - acc:         0.708800 loss:        0.959604
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.870080 loss:        0.397111
Test - acc:         0.782900 loss:        0.703916
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.866860 loss:        0.396891
Test - acc:         0.806400 loss:        0.621631
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.865920 loss:        0.406540
Test - acc:         0.795000 loss:        0.681904
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.870820 loss:        0.388317
Test - acc:         0.776800 loss:        0.693981
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.871560 loss:        0.387915
Test - acc:         0.752100 loss:        0.875842
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.866460 loss:        0.403438
Test - acc:         0.829600 loss:        0.509476
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.872740 loss:        0.385091
Test - acc:         0.812500 loss:        0.588207
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.872100 loss:        0.389258
Test - acc:         0.769400 loss:        0.734733
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.871340 loss:        0.386597
Test - acc:         0.824300 loss:        0.543990
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.870920 loss:        0.389299
Test - acc:         0.782000 loss:        0.732965
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.874240 loss:        0.382606
Test - acc:         0.819000 loss:        0.576383
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.871220 loss:        0.388693
Test - acc:         0.817200 loss:        0.577393
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.872840 loss:        0.380850
Test - acc:         0.772600 loss:        0.748985
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.876080 loss:        0.376298
Test - acc:         0.822200 loss:        0.570809
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.874480 loss:        0.377530
Test - acc:         0.796000 loss:        0.640870
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.872140 loss:        0.380049
Test - acc:         0.827400 loss:        0.539380
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.876260 loss:        0.375473
Test - acc:         0.802300 loss:        0.608608
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.874400 loss:        0.379694
Test - acc:         0.830800 loss:        0.521296
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.870960 loss:        0.383013
Test - acc:         0.797400 loss:        0.645853
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.874200 loss:        0.377102
Test - acc:         0.756300 loss:        0.874958
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.874200 loss:        0.377633
Test - acc:         0.820300 loss:        0.561845
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.875600 loss:        0.375352
Test - acc:         0.837800 loss:        0.516435
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.875120 loss:        0.374419
Test - acc:         0.802300 loss:        0.589442
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.873300 loss:        0.380362
Test - acc:         0.817500 loss:        0.549095
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.876820 loss:        0.373019
Test - acc:         0.844400 loss:        0.486447
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.876140 loss:        0.374680
Test - acc:         0.831700 loss:        0.511029
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.875560 loss:        0.373550
Test - acc:         0.826000 loss:        0.529175
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.876540 loss:        0.370908
Test - acc:         0.733600 loss:        0.969119
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.874700 loss:        0.376417
Test - acc:         0.827400 loss:        0.533360
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.875540 loss:        0.373096
Test - acc:         0.786100 loss:        0.671142
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.875840 loss:        0.375901
Test - acc:         0.822500 loss:        0.581623
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.874400 loss:        0.374534
Test - acc:         0.817500 loss:        0.577772
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.874960 loss:        0.372345
Test - acc:         0.788800 loss:        0.655320
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.875820 loss:        0.370200
Test - acc:         0.840800 loss:        0.486391
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.873040 loss:        0.378067
Test - acc:         0.795300 loss:        0.676027
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.880660 loss:        0.354383
Test - acc:         0.834100 loss:        0.529422
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.882300 loss:        0.357667
Test - acc:         0.852600 loss:        0.445912
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.880740 loss:        0.356042
Test - acc:         0.852100 loss:        0.444389
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.880380 loss:        0.358597
Test - acc:         0.815800 loss:        0.564993
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.877740 loss:        0.361334
Test - acc:         0.795400 loss:        0.700453
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.879880 loss:        0.356224
Test - acc:         0.807100 loss:        0.578241
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.882780 loss:        0.349865
Test - acc:         0.815500 loss:        0.562276
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.879880 loss:        0.359776
Test - acc:         0.830000 loss:        0.494900
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.879060 loss:        0.363330
Test - acc:         0.824800 loss:        0.523524
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.883660 loss:        0.353611
Test - acc:         0.794800 loss:        0.636011
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.880960 loss:        0.357255
Test - acc:         0.851100 loss:        0.479729
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.870260 loss:        0.394649
Test - acc:         0.844600 loss:        0.492942
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.878000 loss:        0.368977
Test - acc:         0.800300 loss:        0.632517
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.879060 loss:        0.367400
Test - acc:         0.819500 loss:        0.572403
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.881660 loss:        0.356395
Test - acc:         0.811100 loss:        0.582314
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.878020 loss:        0.365049
Test - acc:         0.777300 loss:        0.715410
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.876920 loss:        0.364157
Test - acc:         0.720400 loss:        1.082036
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.877120 loss:        0.366556
Test - acc:         0.759000 loss:        0.765997
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.881540 loss:        0.354379
Test - acc:         0.785100 loss:        0.679311
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.879900 loss:        0.358493
Test - acc:         0.779400 loss:        0.715514
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.880260 loss:        0.356106
Test - acc:         0.818300 loss:        0.587456
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.880920 loss:        0.358617
Test - acc:         0.827700 loss:        0.542734
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.880140 loss:        0.355447
Test - acc:         0.844400 loss:        0.486034
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.882460 loss:        0.352276
Test - acc:         0.799000 loss:        0.692592
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.880640 loss:        0.359748
Test - acc:         0.834400 loss:        0.504195
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.880020 loss:        0.355965
Test - acc:         0.853300 loss:        0.445632
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.878120 loss:        0.360834
Test - acc:         0.810200 loss:        0.578885
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.880380 loss:        0.356848
Test - acc:         0.804300 loss:        0.634517
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.881440 loss:        0.354676
Test - acc:         0.767800 loss:        0.740699
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.880840 loss:        0.359400
Test - acc:         0.821200 loss:        0.538713
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.883100 loss:        0.346909
Test - acc:         0.811900 loss:        0.574758
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.878520 loss:        0.360846
Test - acc:         0.827900 loss:        0.559858
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.893140 loss:        0.319834
Test - acc:         0.820800 loss:        0.551257
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.888200 loss:        0.325496
Test - acc:         0.796100 loss:        0.688475
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.890700 loss:        0.326049
Test - acc:         0.810700 loss:        0.606279
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.887300 loss:        0.338440
Test - acc:         0.809200 loss:        0.604773
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.888140 loss:        0.328114
Test - acc:         0.819500 loss:        0.544623
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.892800 loss:        0.323598
Test - acc:         0.818800 loss:        0.608600
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.889540 loss:        0.328137
Test - acc:         0.801000 loss:        0.633807
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.889400 loss:        0.329270
Test - acc:         0.845900 loss:        0.471693
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.890200 loss:        0.327221
Test - acc:         0.800800 loss:        0.649840
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.888340 loss:        0.333264
Test - acc:         0.819900 loss:        0.535220
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.889300 loss:        0.331696
Test - acc:         0.817900 loss:        0.577553
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.890780 loss:        0.326987
Test - acc:         0.792700 loss:        0.626152
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.888680 loss:        0.328533
Test - acc:         0.839600 loss:        0.488755
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.889920 loss:        0.324866
Test - acc:         0.786000 loss:        0.674628
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.890060 loss:        0.326540
Test - acc:         0.812100 loss:        0.570230
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.887400 loss:        0.331231
Test - acc:         0.816200 loss:        0.569227
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.888640 loss:        0.327638
Test - acc:         0.856900 loss:        0.445255
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.888060 loss:        0.331121
Test - acc:         0.786400 loss:        0.756711
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.888500 loss:        0.332902
Test - acc:         0.823200 loss:        0.539170
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.887980 loss:        0.333835
Test - acc:         0.816800 loss:        0.559298
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.889560 loss:        0.326366
Test - acc:         0.805900 loss:        0.616587
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.890740 loss:        0.325312
Test - acc:         0.857600 loss:        0.429194
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.941320 loss:        0.172831
Test - acc:         0.917700 loss:        0.253324
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.956260 loss:        0.127174
Test - acc:         0.922700 loss:        0.239802
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.963260 loss:        0.108917
Test - acc:         0.923600 loss:        0.237782
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.968420 loss:        0.095922
Test - acc:         0.927900 loss:        0.237504
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.971300 loss:        0.083231
Test - acc:         0.925500 loss:        0.240620
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.975380 loss:        0.074489
Test - acc:         0.925700 loss:        0.250597
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.977180 loss:        0.067289
Test - acc:         0.930300 loss:        0.244614
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.977880 loss:        0.063770
Test - acc:         0.926400 loss:        0.263657
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.980740 loss:        0.057870
Test - acc:         0.925200 loss:        0.266383
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.980420 loss:        0.057334
Test - acc:         0.924600 loss:        0.277320
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.982180 loss:        0.052711
Test - acc:         0.931600 loss:        0.254696
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.984500 loss:        0.045843
Test - acc:         0.925900 loss:        0.281547
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.984800 loss:        0.045209
Test - acc:         0.925000 loss:        0.274659
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.985360 loss:        0.044300
Test - acc:         0.924300 loss:        0.284807
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.986880 loss:        0.039451
Test - acc:         0.924300 loss:        0.281133
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.987340 loss:        0.038357
Test - acc:         0.926400 loss:        0.282929
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.986780 loss:        0.039165
Test - acc:         0.927200 loss:        0.283041
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.987260 loss:        0.037851
Test - acc:         0.927700 loss:        0.276000
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.987840 loss:        0.036745
Test - acc:         0.922700 loss:        0.295122
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.987320 loss:        0.036344
Test - acc:         0.929900 loss:        0.276387
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.988360 loss:        0.034067
Test - acc:         0.926500 loss:        0.292740
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.987800 loss:        0.035230
Test - acc:         0.923800 loss:        0.300519
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.987760 loss:        0.037330
Test - acc:         0.923600 loss:        0.301131
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.987720 loss:        0.035998
Test - acc:         0.917700 loss:        0.318358
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.987080 loss:        0.038618
Test - acc:         0.928200 loss:        0.282254
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.987140 loss:        0.037679
Test - acc:         0.925500 loss:        0.297256
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.985900 loss:        0.040810
Test - acc:         0.920500 loss:        0.319628
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.985820 loss:        0.041380
Test - acc:         0.925200 loss:        0.295329
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.986800 loss:        0.039311
Test - acc:         0.908800 loss:        0.364043
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.986000 loss:        0.040358
Test - acc:         0.924000 loss:        0.290248
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.984780 loss:        0.044388
Test - acc:         0.918300 loss:        0.315883
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.985320 loss:        0.042325
Test - acc:         0.921100 loss:        0.295241
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.984940 loss:        0.045185
Test - acc:         0.923500 loss:        0.291538
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.985320 loss:        0.043328
Test - acc:         0.918600 loss:        0.304155
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.985080 loss:        0.044578
Test - acc:         0.922400 loss:        0.300258
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.984360 loss:        0.047060
Test - acc:         0.916400 loss:        0.326187
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.982660 loss:        0.050986
Test - acc:         0.909400 loss:        0.356019
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.984960 loss:        0.045003
Test - acc:         0.916000 loss:        0.312711
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.982300 loss:        0.051301
Test - acc:         0.914800 loss:        0.324016
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.983700 loss:        0.047893
Test - acc:         0.912600 loss:        0.323475
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.982640 loss:        0.048717
Test - acc:         0.916400 loss:        0.318990
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.982360 loss:        0.051884
Test - acc:         0.918000 loss:        0.305368
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.973740 loss:        0.077871
Test - acc:         0.909800 loss:        0.339606
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.979240 loss:        0.061376
Test - acc:         0.913900 loss:        0.316114
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.981860 loss:        0.053452
Test - acc:         0.916800 loss:        0.311911
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.980620 loss:        0.057553
Test - acc:         0.918500 loss:        0.311723
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.983420 loss:        0.050506
Test - acc:         0.912500 loss:        0.334701
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.982480 loss:        0.052019
Test - acc:         0.917700 loss:        0.299069
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.983000 loss:        0.050673
Test - acc:         0.917200 loss:        0.322018
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.982780 loss:        0.048992
Test - acc:         0.915800 loss:        0.312868
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.983140 loss:        0.050754
Test - acc:         0.920600 loss:        0.306563
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.982760 loss:        0.050952
Test - acc:         0.914100 loss:        0.316176
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.982280 loss:        0.051182
Test - acc:         0.911200 loss:        0.330525
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.981940 loss:        0.052644
Test - acc:         0.912100 loss:        0.335408
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.981660 loss:        0.052617
Test - acc:         0.918500 loss:        0.302310
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.981120 loss:        0.055571
Test - acc:         0.919400 loss:        0.321341
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.981280 loss:        0.055119
Test - acc:         0.901700 loss:        0.370874
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.982740 loss:        0.050885
Test - acc:         0.916200 loss:        0.320042
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.981400 loss:        0.053394
Test - acc:         0.919200 loss:        0.302534
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.982500 loss:        0.050458
Test - acc:         0.911800 loss:        0.330320
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.981500 loss:        0.054765
Test - acc:         0.908000 loss:        0.349265
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.979980 loss:        0.056974
Test - acc:         0.916600 loss:        0.318018
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.981320 loss:        0.054610
Test - acc:         0.916200 loss:        0.314856
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.980340 loss:        0.056716
Test - acc:         0.910200 loss:        0.337336
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.983520 loss:        0.050814
Test - acc:         0.910500 loss:        0.337957
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.982120 loss:        0.052555
Test - acc:         0.909900 loss:        0.348963
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.982560 loss:        0.051722
Test - acc:         0.909900 loss:        0.340800
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.980920 loss:        0.054946
Test - acc:         0.913100 loss:        0.333293
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.981120 loss:        0.054194
Test - acc:         0.912500 loss:        0.329029
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.980180 loss:        0.056660
Test - acc:         0.912300 loss:        0.332469
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.981780 loss:        0.053238
Test - acc:         0.904900 loss:        0.375732
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.980820 loss:        0.056243
Test - acc:         0.915900 loss:        0.319171
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.981140 loss:        0.054382
Test - acc:         0.897800 loss:        0.390403
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.981680 loss:        0.053201
Test - acc:         0.902900 loss:        0.392730
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.957800 loss:        0.121960
Test - acc:         0.896300 loss:        0.366787
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.969520 loss:        0.088599
Test - acc:         0.897000 loss:        0.379123
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.971620 loss:        0.082693
Test - acc:         0.905500 loss:        0.326382
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.972840 loss:        0.081136
Test - acc:         0.904500 loss:        0.347549
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.973900 loss:        0.075984
Test - acc:         0.907300 loss:        0.347686
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.973440 loss:        0.075907
Test - acc:         0.906800 loss:        0.331935
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.974900 loss:        0.074850
Test - acc:         0.905900 loss:        0.362086
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.974200 loss:        0.074447
Test - acc:         0.909300 loss:        0.342615
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.974640 loss:        0.072857
Test - acc:         0.908900 loss:        0.325376
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.975760 loss:        0.071910
Test - acc:         0.905900 loss:        0.348815
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.975600 loss:        0.071664
Test - acc:         0.907100 loss:        0.348363
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.976280 loss:        0.067610
Test - acc:         0.908400 loss:        0.348367
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.975740 loss:        0.071860
Test - acc:         0.908600 loss:        0.329294
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.976720 loss:        0.066488
Test - acc:         0.899900 loss:        0.367936
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.977160 loss:        0.067406
Test - acc:         0.911200 loss:        0.328190
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.976440 loss:        0.068824
Test - acc:         0.911700 loss:        0.322103
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.976700 loss:        0.066781
Test - acc:         0.908900 loss:        0.340860
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.975540 loss:        0.070366
Test - acc:         0.902200 loss:        0.357652
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.977140 loss:        0.066914
Test - acc:         0.905000 loss:        0.374033
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.976260 loss:        0.067749
Test - acc:         0.901700 loss:        0.385739
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.975660 loss:        0.067987
Test - acc:         0.903400 loss:        0.357513
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.977480 loss:        0.064652
Test - acc:         0.905100 loss:        0.348545
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.975660 loss:        0.070295
Test - acc:         0.903500 loss:        0.352232
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.976420 loss:        0.067221
Test - acc:         0.900400 loss:        0.368722
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.976960 loss:        0.067884
Test - acc:         0.904400 loss:        0.357696
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.977660 loss:        0.064500
Test - acc:         0.907300 loss:        0.340856
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.986760 loss:        0.040638
Test - acc:         0.923100 loss:        0.283889
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.990980 loss:        0.029470
Test - acc:         0.924200 loss:        0.275590
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.993480 loss:        0.023763
Test - acc:         0.926200 loss:        0.274658
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.994200 loss:        0.020460
Test - acc:         0.927000 loss:        0.272994
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.994720 loss:        0.019246
Test - acc:         0.927600 loss:        0.275777
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.994540 loss:        0.019361
Test - acc:         0.926900 loss:        0.274942
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.956440 loss:        0.126775
Test - acc:         0.906800 loss:        0.330821
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.965980 loss:        0.096450
Test - acc:         0.911800 loss:        0.317213
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.970540 loss:        0.086542
Test - acc:         0.914900 loss:        0.313542
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.973560 loss:        0.078401
Test - acc:         0.913200 loss:        0.307843
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.974540 loss:        0.074321
Test - acc:         0.915900 loss:        0.305990
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.976000 loss:        0.070308
Test - acc:         0.918300 loss:        0.302706
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.977940 loss:        0.064923
Test - acc:         0.918400 loss:        0.302064
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.979580 loss:        0.061490
Test - acc:         0.917300 loss:        0.305967
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.979740 loss:        0.059025
Test - acc:         0.919300 loss:        0.305824
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.979980 loss:        0.059280
Test - acc:         0.917900 loss:        0.303749
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.981700 loss:        0.054146
Test - acc:         0.918600 loss:        0.304403
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.982920 loss:        0.052445
Test - acc:         0.918200 loss:        0.302993
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.983660 loss:        0.050011
Test - acc:         0.917200 loss:        0.306537
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.982300 loss:        0.050582
Test - acc:         0.920300 loss:        0.302275
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.984000 loss:        0.047789
Test - acc:         0.920400 loss:        0.310487
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.984380 loss:        0.048592
Test - acc:         0.921000 loss:        0.306828
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.984960 loss:        0.045691
Test - acc:         0.921500 loss:        0.305535
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.985300 loss:        0.044748
Test - acc:         0.920800 loss:        0.305570
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.986380 loss:        0.042309
Test - acc:         0.922000 loss:        0.306307
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.986540 loss:        0.042263
Test - acc:         0.921900 loss:        0.313470
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.985900 loss:        0.041560
Test - acc:         0.921700 loss:        0.312897
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.987520 loss:        0.039135
Test - acc:         0.921800 loss:        0.314908
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.987000 loss:        0.039445
Test - acc:         0.919300 loss:        0.315063
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.986980 loss:        0.039191
Test - acc:         0.922700 loss:        0.314961
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.986800 loss:        0.038927
Test - acc:         0.921900 loss:        0.310887
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.987260 loss:        0.038810
Test - acc:         0.922900 loss:        0.311046
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.987880 loss:        0.036867
Test - acc:         0.921500 loss:        0.316312
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.988420 loss:        0.036310
Test - acc:         0.921100 loss:        0.313476
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.988540 loss:        0.034765
Test - acc:         0.922600 loss:        0.313633
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.989020 loss:        0.034143
Test - acc:         0.922200 loss:        0.314363
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.988860 loss:        0.034376
Test - acc:         0.919900 loss:        0.317456
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.988960 loss:        0.034264
Test - acc:         0.922300 loss:        0.317068
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.856440 loss:        0.450322
Test - acc:         0.860900 loss:        0.451733
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.898620 loss:        0.303998
Test - acc:         0.873200 loss:        0.405901
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.908180 loss:        0.266979
Test - acc:         0.880600 loss:        0.385714
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.916000 loss:        0.245888
Test - acc:         0.883300 loss:        0.379180
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.919300 loss:        0.233932
Test - acc:         0.887800 loss:        0.365153
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.922640 loss:        0.222838
Test - acc:         0.891700 loss:        0.352255
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.927520 loss:        0.209509
Test - acc:         0.887300 loss:        0.368111
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.928380 loss:        0.206950
Test - acc:         0.891300 loss:        0.356794
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.930820 loss:        0.200523
Test - acc:         0.890400 loss:        0.352677
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.932580 loss:        0.194708
Test - acc:         0.891000 loss:        0.350891
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.934720 loss:        0.189008
Test - acc:         0.893600 loss:        0.349187
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.934640 loss:        0.186559
Test - acc:         0.894200 loss:        0.352708
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.935340 loss:        0.184651
Test - acc:         0.890000 loss:        0.347480
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.938540 loss:        0.175805
Test - acc:         0.893900 loss:        0.356298
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.938800 loss:        0.174829
Test - acc:         0.894500 loss:        0.355899
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.940200 loss:        0.172114
Test - acc:         0.894200 loss:        0.354395
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.939860 loss:        0.170984
Test - acc:         0.895100 loss:        0.349344
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.940480 loss:        0.168814
Test - acc:         0.895100 loss:        0.348827
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.940980 loss:        0.165412
Test - acc:         0.894900 loss:        0.342764
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.943640 loss:        0.162129
Test - acc:         0.895100 loss:        0.350224
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.944000 loss:        0.162520
Test - acc:         0.895200 loss:        0.351730
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.944000 loss:        0.159247
Test - acc:         0.898100 loss:        0.353509
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.943820 loss:        0.161694
Test - acc:         0.895700 loss:        0.356144
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.945080 loss:        0.157107
Test - acc:         0.899900 loss:        0.342955
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.944980 loss:        0.157165
Test - acc:         0.899100 loss:        0.348341
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.945420 loss:        0.154484
Test - acc:         0.900700 loss:        0.345782
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.946780 loss:        0.154414
Test - acc:         0.900300 loss:        0.343948
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.946060 loss:        0.152778
Test - acc:         0.899800 loss:        0.335844
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.947320 loss:        0.150478
Test - acc:         0.900200 loss:        0.339569
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.946860 loss:        0.151894
Test - acc:         0.896800 loss:        0.348833
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.946900 loss:        0.151307
Test - acc:         0.898300 loss:        0.349893
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.947120 loss:        0.151664
Test - acc:         0.896800 loss:        0.351424
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.666600 loss:        0.992850
Test - acc:         0.734000 loss:        0.819056
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.747120 loss:        0.767901
Test - acc:         0.749200 loss:        0.778529
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.773180 loss:        0.689015
Test - acc:         0.772800 loss:        0.712331
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.785000 loss:        0.650861
Test - acc:         0.779800 loss:        0.668503
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.794940 loss:        0.622045
Test - acc:         0.790600 loss:        0.645135
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.800920 loss:        0.600941
Test - acc:         0.791200 loss:        0.638376
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.807300 loss:        0.582068
Test - acc:         0.797900 loss:        0.628125
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.811340 loss:        0.571022
Test - acc:         0.802300 loss:        0.602912
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.815360 loss:        0.560866
Test - acc:         0.809000 loss:        0.594537
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.817400 loss:        0.550463
Test - acc:         0.808000 loss:        0.599468
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.820880 loss:        0.540142
Test - acc:         0.813800 loss:        0.578230
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.823180 loss:        0.531855
Test - acc:         0.806100 loss:        0.608634
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.827880 loss:        0.519482
Test - acc:         0.816700 loss:        0.570754
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.828460 loss:        0.518043
Test - acc:         0.811900 loss:        0.592986
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.830700 loss:        0.512164
Test - acc:         0.817400 loss:        0.567926
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.832840 loss:        0.507022
Test - acc:         0.818500 loss:        0.567280
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.833040 loss:        0.505348
Test - acc:         0.819100 loss:        0.553568
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.832880 loss:        0.500643
Test - acc:         0.821300 loss:        0.549468
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.836660 loss:        0.494380
Test - acc:         0.822300 loss:        0.556799
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.840780 loss:        0.485208
Test - acc:         0.828100 loss:        0.537813
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.839260 loss:        0.480943
Test - acc:         0.826100 loss:        0.543077
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.840140 loss:        0.479729
Test - acc:         0.822500 loss:        0.562374
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.842360 loss:        0.475059
Test - acc:         0.824900 loss:        0.543697
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.843780 loss:        0.467988
Test - acc:         0.830000 loss:        0.531154
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.843320 loss:        0.472356
Test - acc:         0.828900 loss:        0.534931
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.843100 loss:        0.468585
Test - acc:         0.822100 loss:        0.564063
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.846460 loss:        0.464313
Test - acc:         0.832900 loss:        0.521383
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.847040 loss:        0.458221
Test - acc:         0.836700 loss:        0.524057
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.846380 loss:        0.457651
Test - acc:         0.837200 loss:        0.524945
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.846620 loss:        0.459228
Test - acc:         0.833300 loss:        0.522080
Sparsity :          0.9990
Wdecay :        0.000500
