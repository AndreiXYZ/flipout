Running --model resnet18 --prune_criterion random --seed 43 --prune_freq 32 --prune_rate 0.5 --comment=resnet18_crit=random_pf=32_seed=43 --save_model=pre-finetune/resnet18_random_pf32_s43 --logdir=criterion_experiment_no_bias/resnet18
******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 43,
    "prune_criterion": "random",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/resnet18",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/resnet18_random_pf32_s43",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.348320 loss:        1.810961
Test - acc:         0.439400 loss:        1.484592
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.526380 loss:        1.296187
Test - acc:         0.587300 loss:        1.162469
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.635500 loss:        1.021202
Test - acc:         0.626700 loss:        1.042242
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.702640 loss:        0.843171
Test - acc:         0.711600 loss:        0.860480
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.752060 loss:        0.713263
Test - acc:         0.752700 loss:        0.721025
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.783660 loss:        0.623389
Test - acc:         0.733700 loss:        0.786503
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.801860 loss:        0.570900
Test - acc:         0.757000 loss:        0.762860
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.814640 loss:        0.539573
Test - acc:         0.704900 loss:        0.941443
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.825480 loss:        0.505140
Test - acc:         0.784800 loss:        0.633475
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.830340 loss:        0.493756
Test - acc:         0.779500 loss:        0.657745
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.838340 loss:        0.474250
Test - acc:         0.804200 loss:        0.593576
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.844280 loss:        0.458721
Test - acc:         0.834400 loss:        0.502315
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.846500 loss:        0.449426
Test - acc:         0.817100 loss:        0.554424
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.847180 loss:        0.443512
Test - acc:         0.812100 loss:        0.573417
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.853420 loss:        0.425940
Test - acc:         0.804500 loss:        0.581330
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.857080 loss:        0.415865
Test - acc:         0.827800 loss:        0.517806
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.861940 loss:        0.404304
Test - acc:         0.815300 loss:        0.557021
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.863300 loss:        0.400604
Test - acc:         0.797600 loss:        0.647381
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.864700 loss:        0.398190
Test - acc:         0.820300 loss:        0.561645
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.869280 loss:        0.387392
Test - acc:         0.815000 loss:        0.584442
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.870620 loss:        0.382326
Test - acc:         0.818900 loss:        0.536924
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.870120 loss:        0.380016
Test - acc:         0.783000 loss:        0.719661
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.869920 loss:        0.379140
Test - acc:         0.822700 loss:        0.551593
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.874520 loss:        0.374324
Test - acc:         0.836900 loss:        0.490705
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.874720 loss:        0.370254
Test - acc:         0.827900 loss:        0.513918
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.876980 loss:        0.362354
Test - acc:         0.850800 loss:        0.456385
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.873500 loss:        0.368452
Test - acc:         0.823200 loss:        0.540426
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.876640 loss:        0.360789
Test - acc:         0.848400 loss:        0.446691
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.876560 loss:        0.360781
Test - acc:         0.803700 loss:        0.617252
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.879380 loss:        0.354437
Test - acc:         0.703300 loss:        1.070219
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.879920 loss:        0.350535
Test - acc:         0.835700 loss:        0.514332
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.877980 loss:        0.357668
Test - acc:         0.852200 loss:        0.417868
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.819980 loss:        0.529026
Test - acc:         0.753100 loss:        0.768355
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.853020 loss:        0.433851
Test - acc:         0.821900 loss:        0.546068
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.861500 loss:        0.402356
Test - acc:         0.836000 loss:        0.489011
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.869820 loss:        0.386376
Test - acc:         0.766700 loss:        0.749319
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.868880 loss:        0.383776
Test - acc:         0.783600 loss:        0.654340
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.870760 loss:        0.375141
Test - acc:         0.838900 loss:        0.507214
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.875500 loss:        0.367260
Test - acc:         0.803600 loss:        0.619695
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.874220 loss:        0.371998
Test - acc:         0.828500 loss:        0.524782
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.874660 loss:        0.365322
Test - acc:         0.812700 loss:        0.587240
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.877800 loss:        0.359616
Test - acc:         0.828300 loss:        0.528097
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.878440 loss:        0.359330
Test - acc:         0.840000 loss:        0.491103
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.877620 loss:        0.358937
Test - acc:         0.827300 loss:        0.536216
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.875800 loss:        0.360403
Test - acc:         0.833200 loss:        0.505360
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.880140 loss:        0.353718
Test - acc:         0.801500 loss:        0.585048
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.881080 loss:        0.348608
Test - acc:         0.858800 loss:        0.403612
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.881840 loss:        0.346674
Test - acc:         0.854600 loss:        0.427639
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.879900 loss:        0.352374
Test - acc:         0.817200 loss:        0.608109
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.883840 loss:        0.345042
Test - acc:         0.844400 loss:        0.471641
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.882960 loss:        0.347150
Test - acc:         0.849800 loss:        0.432557
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.884060 loss:        0.343820
Test - acc:         0.810500 loss:        0.558734
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.880980 loss:        0.346227
Test - acc:         0.811700 loss:        0.628245
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.884760 loss:        0.340052
Test - acc:         0.820500 loss:        0.549492
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.882180 loss:        0.347473
Test - acc:         0.856500 loss:        0.425700
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.886280 loss:        0.338298
Test - acc:         0.837500 loss:        0.486915
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.883700 loss:        0.343945
Test - acc:         0.853700 loss:        0.445941
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.882040 loss:        0.344509
Test - acc:         0.856200 loss:        0.438689
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.886560 loss:        0.334657
Test - acc:         0.833500 loss:        0.520807
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.882980 loss:        0.339479
Test - acc:         0.811900 loss:        0.568355
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.883180 loss:        0.341484
Test - acc:         0.831800 loss:        0.510104
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.886080 loss:        0.335368
Test - acc:         0.825300 loss:        0.536314
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.885300 loss:        0.335489
Test - acc:         0.786400 loss:        0.731371
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.883840 loss:        0.338034
Test - acc:         0.862300 loss:        0.424257
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.809920 loss:        0.565710
Test - acc:         0.791900 loss:        0.649434
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.846500 loss:        0.452712
Test - acc:         0.795900 loss:        0.633989
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.855420 loss:        0.421121
Test - acc:         0.826900 loss:        0.514431
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.865460 loss:        0.396596
Test - acc:         0.822500 loss:        0.542356
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.864760 loss:        0.395021
Test - acc:         0.809800 loss:        0.555007
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.865280 loss:        0.391159
Test - acc:         0.783300 loss:        0.707579
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.867980 loss:        0.389527
Test - acc:         0.840300 loss:        0.475170
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.869820 loss:        0.378624
Test - acc:         0.837300 loss:        0.476835
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.868080 loss:        0.381697
Test - acc:         0.828500 loss:        0.526594
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.871620 loss:        0.375906
Test - acc:         0.771700 loss:        0.755350
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.872040 loss:        0.376256
Test - acc:         0.819200 loss:        0.558563
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.872960 loss:        0.370901
Test - acc:         0.810900 loss:        0.589452
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.874780 loss:        0.367798
Test - acc:         0.820800 loss:        0.562160
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.875360 loss:        0.364766
Test - acc:         0.846000 loss:        0.470459
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.873600 loss:        0.366651
Test - acc:         0.829600 loss:        0.529389
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.870560 loss:        0.375626
Test - acc:         0.808900 loss:        0.593286
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.873360 loss:        0.365535
Test - acc:         0.788100 loss:        0.684132
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.874320 loss:        0.369545
Test - acc:         0.830800 loss:        0.517275
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.874160 loss:        0.368930
Test - acc:         0.832900 loss:        0.498842
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.878320 loss:        0.359078
Test - acc:         0.826100 loss:        0.542135
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.875620 loss:        0.367612
Test - acc:         0.828400 loss:        0.522090
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.878120 loss:        0.357336
Test - acc:         0.842400 loss:        0.466573
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.877340 loss:        0.357600
Test - acc:         0.789400 loss:        0.653066
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.875900 loss:        0.363636
Test - acc:         0.855200 loss:        0.433774
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.875920 loss:        0.360972
Test - acc:         0.810600 loss:        0.581352
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.878620 loss:        0.358007
Test - acc:         0.834900 loss:        0.485202
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.874560 loss:        0.362501
Test - acc:         0.780200 loss:        0.665429
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.879420 loss:        0.355380
Test - acc:         0.837500 loss:        0.488968
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.878020 loss:        0.359195
Test - acc:         0.834300 loss:        0.500036
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.881060 loss:        0.352236
Test - acc:         0.728100 loss:        0.918128
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.874820 loss:        0.356995
Test - acc:         0.799300 loss:        0.603439
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.878860 loss:        0.355404
Test - acc:         0.820900 loss:        0.530681
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.759380 loss:        0.700788
Test - acc:         0.703900 loss:        0.933359
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.819560 loss:        0.525387
Test - acc:         0.798900 loss:        0.601934
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.832600 loss:        0.486872
Test - acc:         0.772000 loss:        0.705872
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.842340 loss:        0.458942
Test - acc:         0.770200 loss:        0.665636
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.844160 loss:        0.453118
Test - acc:         0.782300 loss:        0.664916
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.848500 loss:        0.441326
Test - acc:         0.807800 loss:        0.572270
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.849220 loss:        0.437963
Test - acc:         0.801500 loss:        0.610805
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.848620 loss:        0.440076
Test - acc:         0.790400 loss:        0.623199
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.851140 loss:        0.434800
Test - acc:         0.806500 loss:        0.580913
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.851700 loss:        0.429494
Test - acc:         0.823000 loss:        0.539588
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.853880 loss:        0.428084
Test - acc:         0.798500 loss:        0.593745
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.852880 loss:        0.425369
Test - acc:         0.744500 loss:        0.808600
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.857020 loss:        0.418927
Test - acc:         0.758500 loss:        0.722810
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.855820 loss:        0.421856
Test - acc:         0.798700 loss:        0.611926
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.858000 loss:        0.417735
Test - acc:         0.827700 loss:        0.516748
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.857000 loss:        0.416781
Test - acc:         0.799000 loss:        0.609195
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.856880 loss:        0.416612
Test - acc:         0.805700 loss:        0.606933
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.858400 loss:        0.416493
Test - acc:         0.829300 loss:        0.504332
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.857100 loss:        0.415684
Test - acc:         0.770400 loss:        0.704892
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.857500 loss:        0.417615
Test - acc:         0.815500 loss:        0.564171
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.859320 loss:        0.407845
Test - acc:         0.835200 loss:        0.491473
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.859460 loss:        0.409576
Test - acc:         0.824900 loss:        0.535463
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.859820 loss:        0.411462
Test - acc:         0.775700 loss:        0.700377
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.859800 loss:        0.410994
Test - acc:         0.762400 loss:        0.763880
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.859560 loss:        0.408570
Test - acc:         0.806400 loss:        0.582170
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.861440 loss:        0.402737
Test - acc:         0.830300 loss:        0.488864
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.857000 loss:        0.413490
Test - acc:         0.788800 loss:        0.616648
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.860140 loss:        0.405222
Test - acc:         0.830600 loss:        0.500886
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.861300 loss:        0.402088
Test - acc:         0.831700 loss:        0.499201
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.860700 loss:        0.408338
Test - acc:         0.781800 loss:        0.663493
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.864980 loss:        0.398256
Test - acc:         0.794000 loss:        0.648097
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.861000 loss:        0.404668
Test - acc:         0.822700 loss:        0.527349
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.691240 loss:        0.886738
Test - acc:         0.721600 loss:        0.830638
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.781800 loss:        0.631643
Test - acc:         0.746000 loss:        0.787640
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.801160 loss:        0.580945
Test - acc:         0.707400 loss:        0.928890
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.806600 loss:        0.558097
Test - acc:         0.759200 loss:        0.743694
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.815360 loss:        0.538462
Test - acc:         0.754500 loss:        0.732708
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.820800 loss:        0.518890
Test - acc:         0.779900 loss:        0.649864
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.822600 loss:        0.517953
Test - acc:         0.780800 loss:        0.667875
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.825880 loss:        0.507967
Test - acc:         0.792400 loss:        0.615784
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.821860 loss:        0.513407
Test - acc:         0.752900 loss:        0.720093
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.827280 loss:        0.500170
Test - acc:         0.800800 loss:        0.584870
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.828760 loss:        0.497717
Test - acc:         0.790300 loss:        0.632148
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.828260 loss:        0.499692
Test - acc:         0.789600 loss:        0.611306
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.829320 loss:        0.495527
Test - acc:         0.788200 loss:        0.632020
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.830820 loss:        0.490538
Test - acc:         0.752300 loss:        0.749917
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.829040 loss:        0.492157
Test - acc:         0.755900 loss:        0.746796
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.830680 loss:        0.489949
Test - acc:         0.776200 loss:        0.728231
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.832720 loss:        0.483581
Test - acc:         0.769000 loss:        0.719260
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.834160 loss:        0.484438
Test - acc:         0.808200 loss:        0.576926
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.835320 loss:        0.478576
Test - acc:         0.761700 loss:        0.727075
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.834800 loss:        0.478690
Test - acc:         0.781800 loss:        0.644884
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.835280 loss:        0.479060
Test - acc:         0.795600 loss:        0.609559
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.835380 loss:        0.477887
Test - acc:         0.747500 loss:        0.750908
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.885840 loss:        0.329591
Test - acc:         0.883000 loss:        0.333038
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.905920 loss:        0.278118
Test - acc:         0.890200 loss:        0.319301
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.910260 loss:        0.260234
Test - acc:         0.892700 loss:        0.311191
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.915520 loss:        0.245411
Test - acc:         0.896400 loss:        0.300732
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.919060 loss:        0.235795
Test - acc:         0.896300 loss:        0.294484
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.924020 loss:        0.224432
Test - acc:         0.900400 loss:        0.289666
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.925540 loss:        0.216387
Test - acc:         0.897400 loss:        0.298793
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.927060 loss:        0.211394
Test - acc:         0.902400 loss:        0.294427
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.929900 loss:        0.203716
Test - acc:         0.900700 loss:        0.289232
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.932320 loss:        0.198475
Test - acc:         0.900900 loss:        0.292797
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.534720 loss:        1.289526
Test - acc:         0.693100 loss:        0.868270
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.737260 loss:        0.752358
Test - acc:         0.754000 loss:        0.700173
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.787180 loss:        0.614382
Test - acc:         0.794100 loss:        0.602503
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.808380 loss:        0.551598
Test - acc:         0.805800 loss:        0.572614
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.823820 loss:        0.509699
Test - acc:         0.822500 loss:        0.517851
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.836040 loss:        0.476271
Test - acc:         0.816000 loss:        0.543115
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.841160 loss:        0.459923
Test - acc:         0.832400 loss:        0.483392
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.847060 loss:        0.439700
Test - acc:         0.816300 loss:        0.548137
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.850760 loss:        0.429870
Test - acc:         0.838100 loss:        0.483672
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.857220 loss:        0.411866
Test - acc:         0.810700 loss:        0.563749
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.859520 loss:        0.407300
Test - acc:         0.827000 loss:        0.499770
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.862460 loss:        0.396744
Test - acc:         0.849600 loss:        0.447736
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.864820 loss:        0.393061
Test - acc:         0.843800 loss:        0.448681
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.867020 loss:        0.382846
Test - acc:         0.828000 loss:        0.500445
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.869740 loss:        0.374418
Test - acc:         0.845100 loss:        0.470968
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.871660 loss:        0.369525
Test - acc:         0.840700 loss:        0.472253
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.870880 loss:        0.369538
Test - acc:         0.849700 loss:        0.446621
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.874920 loss:        0.358824
Test - acc:         0.845200 loss:        0.452388
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.879400 loss:        0.354251
Test - acc:         0.843900 loss:        0.470320
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.877280 loss:        0.356214
Test - acc:         0.832700 loss:        0.503996
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.877500 loss:        0.350454
Test - acc:         0.829300 loss:        0.509390
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.879140 loss:        0.347477
Test - acc:         0.848600 loss:        0.453197
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.880700 loss:        0.343066
Test - acc:         0.843800 loss:        0.456539
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.882100 loss:        0.338093
Test - acc:         0.847600 loss:        0.460624
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.883780 loss:        0.338620
Test - acc:         0.844400 loss:        0.459998
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.884140 loss:        0.331147
Test - acc:         0.837200 loss:        0.492282
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.884940 loss:        0.331035
Test - acc:         0.846800 loss:        0.466140
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.885180 loss:        0.328874
Test - acc:         0.848900 loss:        0.454440
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.884840 loss:        0.328226
Test - acc:         0.850700 loss:        0.444917
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.887220 loss:        0.323515
Test - acc:         0.852900 loss:        0.441604
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.888180 loss:        0.325149
Test - acc:         0.862700 loss:        0.409715
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.886180 loss:        0.325951
Test - acc:         0.846900 loss:        0.470005
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.389120 loss:        1.671852
Test - acc:         0.466700 loss:        1.474517
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.529200 loss:        1.318587
Test - acc:         0.552300 loss:        1.261227
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.594080 loss:        1.145269
Test - acc:         0.612500 loss:        1.077193
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.633700 loss:        1.032983
Test - acc:         0.613700 loss:        1.094160
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.659960 loss:        0.960651
Test - acc:         0.616100 loss:        1.136347
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.682480 loss:        0.899825
Test - acc:         0.691900 loss:        0.877960
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.696160 loss:        0.858599
Test - acc:         0.687200 loss:        0.889911
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.713040 loss:        0.818096
Test - acc:         0.716300 loss:        0.802239
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.725680 loss:        0.778625
Test - acc:         0.723600 loss:        0.789728
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.734940 loss:        0.754229
Test - acc:         0.708200 loss:        0.834590
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.747900 loss:        0.723595
Test - acc:         0.691300 loss:        0.913775
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.752140 loss:        0.703190
Test - acc:         0.720300 loss:        0.804813
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.758080 loss:        0.691292
Test - acc:         0.751600 loss:        0.716243
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.764320 loss:        0.672941
Test - acc:         0.747100 loss:        0.737858
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.772940 loss:        0.651038
Test - acc:         0.716700 loss:        0.848946
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.775060 loss:        0.646527
Test - acc:         0.751600 loss:        0.713216
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.780060 loss:        0.631094
Test - acc:         0.756900 loss:        0.690350
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.780940 loss:        0.626811
Test - acc:         0.746900 loss:        0.729458
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.786360 loss:        0.616157
Test - acc:         0.762900 loss:        0.692166
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.789920 loss:        0.604670
Test - acc:         0.768700 loss:        0.661678
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.789680 loss:        0.599233
Test - acc:         0.769200 loss:        0.657056
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.794340 loss:        0.590194
Test - acc:         0.773900 loss:        0.646842
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.797500 loss:        0.579818
Test - acc:         0.789300 loss:        0.613749
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.796280 loss:        0.577120
Test - acc:         0.766900 loss:        0.689315
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.800260 loss:        0.570777
Test - acc:         0.770300 loss:        0.666428
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.801340 loss:        0.568714
Test - acc:         0.784600 loss:        0.616097
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.806200 loss:        0.557947
Test - acc:         0.778000 loss:        0.650199
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.805640 loss:        0.552465
Test - acc:         0.786300 loss:        0.618473
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.807080 loss:        0.555710
Test - acc:         0.792100 loss:        0.598490
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.808280 loss:        0.548734
Test - acc:         0.790700 loss:        0.601942
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.811360 loss:        0.541621
Test - acc:         0.791100 loss:        0.611887
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.811640 loss:        0.541604
Test - acc:         0.797700 loss:        0.579643
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.401860 loss:        1.665962
Test - acc:         0.445000 loss:        1.543574
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.508280 loss:        1.367552
Test - acc:         0.275600 loss:        2.785733
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.552520 loss:        1.249783
Test - acc:         0.318100 loss:        2.665241
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.577740 loss:        1.175476
Test - acc:         0.390500 loss:        2.023039
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.596300 loss:        1.128839
Test - acc:         0.517400 loss:        1.399161
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.616480 loss:        1.079166
Test - acc:         0.591700 loss:        1.148910
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.626000 loss:        1.051492
Test - acc:         0.506700 loss:        1.453599
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.635300 loss:        1.021772
Test - acc:         0.570300 loss:        1.285956
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.644280 loss:        0.997460
Test - acc:         0.410500 loss:        2.668757
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.653460 loss:        0.978137
Test - acc:         0.487600 loss:        1.822815
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.659560 loss:        0.955511
Test - acc:         0.635100 loss:        1.028591
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.665440 loss:        0.942938
Test - acc:         0.634800 loss:        1.036053
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.669240 loss:        0.933460
Test - acc:         0.647800 loss:        1.013359
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.674540 loss:        0.922629
Test - acc:         0.503600 loss:        1.700276
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.678940 loss:        0.907955
Test - acc:         0.657400 loss:        0.975701
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.682920 loss:        0.892837
Test - acc:         0.549200 loss:        1.454507
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.689300 loss:        0.880784
Test - acc:         0.633800 loss:        1.063614
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.690280 loss:        0.875936
Test - acc:         0.663600 loss:        0.972702
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.694220 loss:        0.865889
Test - acc:         0.414600 loss:        2.358169
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.699140 loss:        0.853341
Test - acc:         0.632900 loss:        1.053946
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.702120 loss:        0.842256
Test - acc:         0.674800 loss:        0.914295
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.701620 loss:        0.842182
Test - acc:         0.626900 loss:        1.119496
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.710700 loss:        0.826296
Test - acc:         0.657300 loss:        0.984601
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.709140 loss:        0.823700
Test - acc:         0.386400 loss:        2.698946
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.711280 loss:        0.820461
Test - acc:         0.692100 loss:        0.873613
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.714080 loss:        0.811805
Test - acc:         0.697100 loss:        0.852129
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.741120 loss:        0.737064
Test - acc:         0.749800 loss:        0.710486
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.749620 loss:        0.717287
Test - acc:         0.754900 loss:        0.700270
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.752480 loss:        0.708271
Test - acc:         0.756000 loss:        0.694484
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.755000 loss:        0.699456
Test - acc:         0.755100 loss:        0.692174
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.757340 loss:        0.695748
Test - acc:         0.761200 loss:        0.687842
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.756240 loss:        0.693131
Test - acc:         0.760300 loss:        0.685445
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.150520 loss:        2.737678
Test - acc:         0.100000 loss:       17.850390
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.251000 loss:        1.981305
Test - acc:         0.143600 loss:        3.150935
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.320400 loss:        1.857786
Test - acc:         0.183200 loss:        2.419170
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.346820 loss:        1.793773
Test - acc:         0.334900 loss:        1.830637
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.364780 loss:        1.746775
Test - acc:         0.307100 loss:        1.933527
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.380820 loss:        1.709644
Test - acc:         0.350200 loss:        1.815303
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.389620 loss:        1.682159
Test - acc:         0.355200 loss:        1.773146
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.401980 loss:        1.652632
Test - acc:         0.340600 loss:        1.939168
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.412820 loss:        1.627956
Test - acc:         0.352800 loss:        1.759929
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.419500 loss:        1.608115
Test - acc:         0.389700 loss:        1.683088
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.430100 loss:        1.582528
Test - acc:         0.299600 loss:        2.017909
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.434440 loss:        1.567241
Test - acc:         0.346800 loss:        1.851700
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.439340 loss:        1.553437
Test - acc:         0.332100 loss:        1.895518
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.449400 loss:        1.530677
Test - acc:         0.304100 loss:        2.090033
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.454560 loss:        1.516769
Test - acc:         0.331400 loss:        1.931973
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.462060 loss:        1.494753
Test - acc:         0.334400 loss:        1.916436
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.467340 loss:        1.483505
Test - acc:         0.349900 loss:        1.914453
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.472220 loss:        1.472788
Test - acc:         0.337900 loss:        1.948324
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.479200 loss:        1.452699
Test - acc:         0.350000 loss:        1.933158
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.482820 loss:        1.443026
Test - acc:         0.346000 loss:        1.957973
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.491060 loss:        1.421807
Test - acc:         0.327700 loss:        2.096062
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.492140 loss:        1.413427
Test - acc:         0.381000 loss:        1.788184
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.493760 loss:        1.406681
Test - acc:         0.341600 loss:        2.061945
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.500840 loss:        1.393753
Test - acc:         0.338700 loss:        2.137429
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.504360 loss:        1.377386
Test - acc:         0.296500 loss:        2.519878
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.506920 loss:        1.373474
Test - acc:         0.354800 loss:        1.958388
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.512960 loss:        1.362234
Test - acc:         0.298900 loss:        2.416152
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.516280 loss:        1.352271
Test - acc:         0.332600 loss:        2.144203
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.517680 loss:        1.343678
Test - acc:         0.371100 loss:        1.816278
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.519500 loss:        1.342713
Test - acc:         0.318400 loss:        2.356040
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.522480 loss:        1.334407
Test - acc:         0.361300 loss:        1.915735
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.526780 loss:        1.324901
Test - acc:         0.399200 loss:        1.747582
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.135560 loss:        2.640297
Test - acc:         0.121000 loss:        2.496242
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.220920 loss:        2.143038
Test - acc:         0.154000 loss:        2.841865
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.248160 loss:        2.091330
Test - acc:         0.158900 loss:        2.978263
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.262180 loss:        2.062806
Test - acc:         0.209600 loss:        2.183399
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.268040 loss:        2.043677
Test - acc:         0.179000 loss:        2.414155
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.276680 loss:        2.024612
Test - acc:         0.159100 loss:        2.472884
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.285720 loss:        2.009993
Test - acc:         0.197500 loss:        2.204506
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.287600 loss:        1.998961
Test - acc:         0.207700 loss:        2.487527
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.294340 loss:        1.983067
Test - acc:         0.268100 loss:        2.041679
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.297920 loss:        1.973599
Test - acc:         0.237000 loss:        2.137670
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.302160 loss:        1.960721
Test - acc:         0.209800 loss:        2.272280
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.304400 loss:        1.954115
Test - acc:         0.247900 loss:        2.113808
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.311000 loss:        1.941377
Test - acc:         0.299000 loss:        1.943123
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.316580 loss:        1.928229
Test - acc:         0.259700 loss:        2.072161
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.318860 loss:        1.919438
Test - acc:         0.219300 loss:        2.224922
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.323700 loss:        1.908914
Test - acc:         0.240100 loss:        2.133418
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.323600 loss:        1.905601
Test - acc:         0.233500 loss:        2.203691
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.331720 loss:        1.896635
Test - acc:         0.198900 loss:        2.302334
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.332160 loss:        1.887420
Test - acc:         0.290800 loss:        2.018081
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.331340 loss:        1.884349
Test - acc:         0.255700 loss:        2.091103
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.335420 loss:        1.877132
Test - acc:         0.197200 loss:        2.355556
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.336900 loss:        1.870035
Test - acc:         0.205200 loss:        2.293983
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.340200 loss:        1.865018
Test - acc:         0.230200 loss:        2.221083
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.336600 loss:        1.860860
Test - acc:         0.242100 loss:        2.067791
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.340500 loss:        1.856446
Test - acc:         0.245400 loss:        2.138579
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.342660 loss:        1.848589
Test - acc:         0.229200 loss:        2.188186
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.347380 loss:        1.843356
Test - acc:         0.228000 loss:        2.170272
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.345480 loss:        1.838590
Test - acc:         0.287600 loss:        1.981844
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.347080 loss:        1.839334
Test - acc:         0.273100 loss:        2.017100
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.349560 loss:        1.833230
Test - acc:         0.245600 loss:        2.132224
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.352120 loss:        1.828025
Test - acc:         0.349900 loss:        1.836974
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.352060 loss:        1.822713
Test - acc:         0.325000 loss:        1.889566
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.101320 loss:        2.495590
Test - acc:         0.105900 loss:        2.435428
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.136660 loss:        2.310570
Test - acc:         0.128800 loss:        2.373891
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.168680 loss:        2.263659
Test - acc:         0.148400 loss:        2.302655
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.176880 loss:        2.241067
Test - acc:         0.154300 loss:        2.252734
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.182860 loss:        2.224215
Test - acc:         0.143600 loss:        2.287580
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.184580 loss:        2.213075
Test - acc:         0.150100 loss:        2.250100
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.183460 loss:        2.208687
Test - acc:         0.149900 loss:        2.275838
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.184640 loss:        2.206074
Test - acc:         0.106400 loss:        2.354514
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.186820 loss:        2.201355
Test - acc:         0.143400 loss:        2.334026
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.187080 loss:        2.199992
Test - acc:         0.154500 loss:        2.254011
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.185080 loss:        2.198505
Test - acc:         0.125200 loss:        2.284057
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.188160 loss:        2.195903
Test - acc:         0.169500 loss:        2.230316
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.186300 loss:        2.196276
Test - acc:         0.180500 loss:        2.200655
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.188520 loss:        2.194623
Test - acc:         0.158400 loss:        2.250846
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.190180 loss:        2.194325
Test - acc:         0.145800 loss:        2.244204
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.191000 loss:        2.191960
Test - acc:         0.184000 loss:        2.203674
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.192200 loss:        2.190550
Test - acc:         0.177500 loss:        2.235912
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.193000 loss:        2.188390
Test - acc:         0.176500 loss:        2.296222
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.192280 loss:        2.186895
Test - acc:         0.173900 loss:        2.225246
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.193940 loss:        2.187368
Test - acc:         0.182400 loss:        2.211097
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.191960 loss:        2.188165
Test - acc:         0.193000 loss:        2.190532
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.196860 loss:        2.184844
Test - acc:         0.182600 loss:        2.194464
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.198260 loss:        2.179938
Test - acc:         0.189200 loss:        2.205430
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.198200 loss:        2.180869
Test - acc:         0.181000 loss:        2.241010
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.198820 loss:        2.178846
Test - acc:         0.181000 loss:        2.247703
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.204340 loss:        2.176054
Test - acc:         0.173000 loss:        2.224231
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.202780 loss:        2.177303
Test - acc:         0.170500 loss:        2.257425
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.202580 loss:        2.176843
Test - acc:         0.204000 loss:        2.168900
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.202740 loss:        2.176053
Test - acc:         0.190100 loss:        2.197091
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.204740 loss:        2.175736
Test - acc:         0.209600 loss:        2.182371
Sparsity :          0.9990
Wdecay :        0.000500
