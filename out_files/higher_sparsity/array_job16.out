Running --model vgg19 --prune_criterion global_magnitude --seed 42 --prune_freq 32 --prune_rate 0.5 --comment=vgg19_crit=global_magnitude_pf=32_seed=42 --save_model=pre-finetune/vgg19_global_magnitude_pf32_s42 --logdir=criterion_experiment_no_bias/vgg19
******************************
Running
{
    "model": "vgg19",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 2000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "global_magnitude",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "criterion_experiment_no_bias/vgg19",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": false,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": "pre-finetune/vgg19_global_magnitude_pf32_s42",
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 20024000
Model has 20040522 total params.
num_weights=20029504
num_biases=11018
---Biases omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.103940 loss:        2.526355
Test - acc:         0.097900 loss:        2.302562
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.111000 loss:        2.296948
Test - acc:         0.135800 loss:        2.287671
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.185180 loss:        2.095249
Test - acc:         0.204100 loss:        1.924940
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.241660 loss:        1.887907
Test - acc:         0.264800 loss:        1.811172
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.326660 loss:        1.714085
Test - acc:         0.350900 loss:        1.675830
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.436780 loss:        1.479783
Test - acc:         0.408200 loss:        1.759592
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.525500 loss:        1.303626
Test - acc:         0.537000 loss:        1.378895
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.616800 loss:        1.083961
Test - acc:         0.590600 loss:        1.300951
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.669560 loss:        0.951760
Test - acc:         0.608900 loss:        1.110669
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.711520 loss:        0.844386
Test - acc:         0.650100 loss:        1.219713
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.741020 loss:        0.780390
Test - acc:         0.671600 loss:        1.031043
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.753580 loss:        0.740309
Test - acc:         0.644100 loss:        1.180330
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.772760 loss:        0.691439
Test - acc:         0.657200 loss:        1.078353
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.782560 loss:        0.664204
Test - acc:         0.571800 loss:        1.670740
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.784380 loss:        0.656515
Test - acc:         0.731100 loss:        0.940813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.795500 loss:        0.622433
Test - acc:         0.714400 loss:        0.970297
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.803000 loss:        0.609394
Test - acc:         0.781900 loss:        0.669261
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.807580 loss:        0.591968
Test - acc:         0.770600 loss:        0.724289
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.809360 loss:        0.589175
Test - acc:         0.803000 loss:        0.614238
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.814200 loss:        0.569209
Test - acc:         0.757800 loss:        0.758979
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.816600 loss:        0.563241
Test - acc:         0.763800 loss:        0.707813
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.821260 loss:        0.551170
Test - acc:         0.761900 loss:        0.738491
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.823380 loss:        0.544565
Test - acc:         0.785600 loss:        0.672582
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.824980 loss:        0.539750
Test - acc:         0.739100 loss:        0.799185
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.825620 loss:        0.537203
Test - acc:         0.794200 loss:        0.634343
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.831920 loss:        0.515494
Test - acc:         0.784400 loss:        0.658191
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.834500 loss:        0.515579
Test - acc:         0.769900 loss:        0.790124
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.836880 loss:        0.507073
Test - acc:         0.761500 loss:        0.783315
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.835440 loss:        0.506329
Test - acc:         0.799000 loss:        0.616728
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.834040 loss:        0.505033
Test - acc:         0.684600 loss:        1.254283
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.839060 loss:        0.497706
Test - acc:         0.683600 loss:        1.059633
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.843760 loss:        0.483917
Test - acc:         0.768400 loss:        0.772606
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.842980 loss:        0.480907
Test - acc:         0.741100 loss:        0.843525
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.844260 loss:        0.478610
Test - acc:         0.744500 loss:        0.847535
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.847800 loss:        0.468008
Test - acc:         0.753800 loss:        0.778204
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.848320 loss:        0.461876
Test - acc:         0.800300 loss:        0.618537
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.849780 loss:        0.462173
Test - acc:         0.667100 loss:        1.221256
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.848320 loss:        0.462012
Test - acc:         0.730800 loss:        0.884803
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.848800 loss:        0.465004
Test - acc:         0.782400 loss:        0.681410
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.853080 loss:        0.450737
Test - acc:         0.757800 loss:        0.769041
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.851220 loss:        0.454556
Test - acc:         0.750300 loss:        0.884189
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.854440 loss:        0.445935
Test - acc:         0.782700 loss:        0.696773
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.851260 loss:        0.447650
Test - acc:         0.792200 loss:        0.625771
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.859200 loss:        0.431710
Test - acc:         0.749100 loss:        0.865527
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.855200 loss:        0.441636
Test - acc:         0.815400 loss:        0.605546
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.859520 loss:        0.434188
Test - acc:         0.806100 loss:        0.603092
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.859960 loss:        0.427580
Test - acc:         0.804500 loss:        0.619323
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.857680 loss:        0.433960
Test - acc:         0.832800 loss:        0.525921
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.860140 loss:        0.424274
Test - acc:         0.824900 loss:        0.559197
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.857800 loss:        0.429840
Test - acc:         0.808000 loss:        0.612606
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.863340 loss:        0.416877
Test - acc:         0.737100 loss:        0.959990
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.860540 loss:        0.425821
Test - acc:         0.727100 loss:        0.938162
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.860280 loss:        0.421861
Test - acc:         0.787600 loss:        0.718784
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.862060 loss:        0.419654
Test - acc:         0.808600 loss:        0.589820
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.863120 loss:        0.417562
Test - acc:         0.783500 loss:        0.681589
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.859200 loss:        0.423442
Test - acc:         0.827800 loss:        0.554143
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.863920 loss:        0.411730
Test - acc:         0.827600 loss:        0.541658
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.864980 loss:        0.410348
Test - acc:         0.823900 loss:        0.552504
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.865660 loss:        0.407465
Test - acc:         0.806800 loss:        0.614640
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.863660 loss:        0.410182
Test - acc:         0.810600 loss:        0.579648
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.863200 loss:        0.410320
Test - acc:         0.766500 loss:        0.735427
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.866820 loss:        0.406893
Test - acc:         0.845500 loss:        0.477059
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.864180 loss:        0.410608
Test - acc:         0.766000 loss:        0.793003
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.870460 loss:        0.398221
Test - acc:         0.808200 loss:        0.614402
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.864640 loss:        0.407498
Test - acc:         0.777400 loss:        0.698829
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.869400 loss:        0.395641
Test - acc:         0.806200 loss:        0.580667
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.868400 loss:        0.397960
Test - acc:         0.774600 loss:        0.676103
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.866140 loss:        0.407124
Test - acc:         0.787700 loss:        0.662181
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.869800 loss:        0.397799
Test - acc:         0.835500 loss:        0.507820
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.868420 loss:        0.398923
Test - acc:         0.783300 loss:        0.744237
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.869880 loss:        0.395066
Test - acc:         0.814300 loss:        0.587512
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.869700 loss:        0.396407
Test - acc:         0.775600 loss:        0.813632
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.869500 loss:        0.389731
Test - acc:         0.791800 loss:        0.692584
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.868600 loss:        0.396114
Test - acc:         0.650300 loss:        1.096707
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.870440 loss:        0.392959
Test - acc:         0.785500 loss:        0.753815
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.869220 loss:        0.395935
Test - acc:         0.797100 loss:        0.597755
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.869140 loss:        0.396343
Test - acc:         0.813500 loss:        0.569318
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.869700 loss:        0.394514
Test - acc:         0.803900 loss:        0.612593
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.870180 loss:        0.390651
Test - acc:         0.844500 loss:        0.464979
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.871400 loss:        0.387475
Test - acc:         0.800200 loss:        0.618535
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.870160 loss:        0.388878
Test - acc:         0.801800 loss:        0.618505
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.872260 loss:        0.384685
Test - acc:         0.814900 loss:        0.573057
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.868780 loss:        0.390246
Test - acc:         0.803600 loss:        0.618015
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.872640 loss:        0.383529
Test - acc:         0.809700 loss:        0.625508
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.872380 loss:        0.386009
Test - acc:         0.782300 loss:        0.734121
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.874200 loss:        0.382569
Test - acc:         0.792400 loss:        0.736154
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.870180 loss:        0.390796
Test - acc:         0.771100 loss:        0.715080
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.872800 loss:        0.386579
Test - acc:         0.783700 loss:        0.687074
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.871380 loss:        0.387315
Test - acc:         0.794300 loss:        0.664635
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.873440 loss:        0.382774
Test - acc:         0.783800 loss:        0.741021
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.873500 loss:        0.387807
Test - acc:         0.837200 loss:        0.512876
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.871000 loss:        0.383621
Test - acc:         0.802400 loss:        0.666442
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.873060 loss:        0.385305
Test - acc:         0.754900 loss:        0.802813
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.871560 loss:        0.387170
Test - acc:         0.833300 loss:        0.537466
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.866980 loss:        0.411785
Test - acc:         0.764300 loss:        0.763507
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.872600 loss:        0.383632
Test - acc:         0.835800 loss:        0.502487
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.877100 loss:        0.369596
Test - acc:         0.787800 loss:        0.677322
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.878700 loss:        0.366332
Test - acc:         0.802100 loss:        0.627049
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.872000 loss:        0.386614
Test - acc:         0.815600 loss:        0.574212
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.877060 loss:        0.364401
Test - acc:         0.794300 loss:        0.646263
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.877880 loss:        0.368084
Test - acc:         0.842200 loss:        0.496717
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.874420 loss:        0.371068
Test - acc:         0.807900 loss:        0.592618
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.874620 loss:        0.376315
Test - acc:         0.786800 loss:        0.707295
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.879020 loss:        0.364176
Test - acc:         0.819500 loss:        0.531545
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.876300 loss:        0.366389
Test - acc:         0.827300 loss:        0.536095
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.873700 loss:        0.380415
Test - acc:         0.831900 loss:        0.487410
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.877360 loss:        0.365985
Test - acc:         0.820700 loss:        0.546697
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.879800 loss:        0.361776
Test - acc:         0.798300 loss:        0.691886
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.877100 loss:        0.367633
Test - acc:         0.822500 loss:        0.544915
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.878100 loss:        0.364666
Test - acc:         0.786700 loss:        0.690613
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.878920 loss:        0.364767
Test - acc:         0.810500 loss:        0.592789
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.877780 loss:        0.364450
Test - acc:         0.841600 loss:        0.509567
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.876760 loss:        0.370508
Test - acc:         0.777300 loss:        0.726989
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.878700 loss:        0.367558
Test - acc:         0.814500 loss:        0.562238
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.876960 loss:        0.368900
Test - acc:         0.801100 loss:        0.662325
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.879020 loss:        0.363552
Test - acc:         0.823700 loss:        0.558828
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.877640 loss:        0.363670
Test - acc:         0.799600 loss:        0.688231
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.879880 loss:        0.360703
Test - acc:         0.744600 loss:        0.857094
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.878880 loss:        0.363525
Test - acc:         0.836000 loss:        0.514582
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.879320 loss:        0.359114
Test - acc:         0.799700 loss:        0.615957
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.879400 loss:        0.364070
Test - acc:         0.830100 loss:        0.512181
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.879040 loss:        0.362586
Test - acc:         0.807900 loss:        0.675810
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.879740 loss:        0.365490
Test - acc:         0.798500 loss:        0.697997
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.879260 loss:        0.365499
Test - acc:         0.793700 loss:        0.672119
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.878940 loss:        0.362077
Test - acc:         0.775000 loss:        0.714407
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.878800 loss:        0.363037
Test - acc:         0.834200 loss:        0.503818
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.878940 loss:        0.362267
Test - acc:         0.803800 loss:        0.643440
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.870460 loss:        0.398327
Test - acc:         0.784300 loss:        0.636895
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.890260 loss:        0.329240
Test - acc:         0.867500 loss:        0.400163
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.885400 loss:        0.337205
Test - acc:         0.836000 loss:        0.510226
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.885500 loss:        0.344745
Test - acc:         0.736100 loss:        0.912900
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.887500 loss:        0.338418
Test - acc:         0.721800 loss:        0.974307
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.888040 loss:        0.336373
Test - acc:         0.795900 loss:        0.620160
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.885680 loss:        0.346626
Test - acc:         0.831600 loss:        0.515234
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.885240 loss:        0.340135
Test - acc:         0.782400 loss:        0.800237
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.882080 loss:        0.359780
Test - acc:         0.802100 loss:        0.624518
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.884500 loss:        0.346256
Test - acc:         0.853900 loss:        0.447605
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.884620 loss:        0.343607
Test - acc:         0.845000 loss:        0.464893
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.885900 loss:        0.338961
Test - acc:         0.830100 loss:        0.519261
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.886660 loss:        0.337384
Test - acc:         0.800600 loss:        0.647396
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.887840 loss:        0.335391
Test - acc:         0.844000 loss:        0.490382
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.885240 loss:        0.340492
Test - acc:         0.827000 loss:        0.546500
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.886440 loss:        0.338975
Test - acc:         0.846800 loss:        0.466086
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.886820 loss:        0.341105
Test - acc:         0.825700 loss:        0.519273
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.886360 loss:        0.337543
Test - acc:         0.811000 loss:        0.569791
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.889200 loss:        0.333094
Test - acc:         0.819500 loss:        0.580090
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.886000 loss:        0.340352
Test - acc:         0.841000 loss:        0.490544
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.888680 loss:        0.334906
Test - acc:         0.806400 loss:        0.622639
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.885200 loss:        0.340092
Test - acc:         0.847900 loss:        0.454174
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.887800 loss:        0.335641
Test - acc:         0.827800 loss:        0.529491
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.941500 loss:        0.178038
Test - acc:         0.920900 loss:        0.245512
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.956020 loss:        0.131112
Test - acc:         0.924600 loss:        0.237694
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.961420 loss:        0.111174
Test - acc:         0.925800 loss:        0.238293
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.966720 loss:        0.099287
Test - acc:         0.925600 loss:        0.242856
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.971580 loss:        0.084475
Test - acc:         0.927000 loss:        0.246190
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.972840 loss:        0.081905
Test - acc:         0.926500 loss:        0.250448
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.975440 loss:        0.073611
Test - acc:         0.927600 loss:        0.246417
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.977060 loss:        0.067658
Test - acc:         0.924900 loss:        0.255276
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.978560 loss:        0.064640
Test - acc:         0.927700 loss:        0.255116
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.981500 loss:        0.056888
Test - acc:         0.925300 loss:        0.263381
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.982220 loss:        0.052367
Test - acc:         0.927300 loss:        0.276505
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.983540 loss:        0.050878
Test - acc:         0.926100 loss:        0.276371
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.983900 loss:        0.047432
Test - acc:         0.924700 loss:        0.285871
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.985360 loss:        0.043097
Test - acc:         0.926500 loss:        0.276626
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.984640 loss:        0.044586
Test - acc:         0.922400 loss:        0.282545
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.985300 loss:        0.044745
Test - acc:         0.925100 loss:        0.279920
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.985380 loss:        0.042882
Test - acc:         0.925800 loss:        0.279692
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.986360 loss:        0.039984
Test - acc:         0.921500 loss:        0.288885
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.988040 loss:        0.036465
Test - acc:         0.921600 loss:        0.295791
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.985700 loss:        0.042342
Test - acc:         0.921400 loss:        0.293071
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.987520 loss:        0.037223
Test - acc:         0.924300 loss:        0.292587
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.986900 loss:        0.038710
Test - acc:         0.918600 loss:        0.306551
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.987120 loss:        0.038669
Test - acc:         0.923700 loss:        0.293584
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.985560 loss:        0.041526
Test - acc:         0.915400 loss:        0.344701
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.984540 loss:        0.043884
Test - acc:         0.916200 loss:        0.320851
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.984540 loss:        0.044911
Test - acc:         0.920200 loss:        0.296183
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.985820 loss:        0.042323
Test - acc:         0.923100 loss:        0.293158
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.986300 loss:        0.040360
Test - acc:         0.918500 loss:        0.316386
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.984560 loss:        0.044225
Test - acc:         0.921200 loss:        0.294813
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.985160 loss:        0.042298
Test - acc:         0.919200 loss:        0.313676
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.984940 loss:        0.045622
Test - acc:         0.915200 loss:        0.312704
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.984460 loss:        0.045549
Test - acc:         0.916100 loss:        0.301700
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.985900 loss:        0.043057
Test - acc:         0.918800 loss:        0.309264
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.985020 loss:        0.045915
Test - acc:         0.912000 loss:        0.329339
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.984380 loss:        0.047819
Test - acc:         0.912000 loss:        0.339562
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.983320 loss:        0.049896
Test - acc:         0.915900 loss:        0.316534
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.982700 loss:        0.051204
Test - acc:         0.914800 loss:        0.317168
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.983140 loss:        0.048875
Test - acc:         0.912900 loss:        0.328372
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.981920 loss:        0.054633
Test - acc:         0.924300 loss:        0.287408
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.982460 loss:        0.051304
Test - acc:         0.921700 loss:        0.277149
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.983460 loss:        0.050346
Test - acc:         0.917000 loss:        0.319460
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.982340 loss:        0.051556
Test - acc:         0.915100 loss:        0.324668
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.971020 loss:        0.083914
Test - acc:         0.911600 loss:        0.328949
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.977520 loss:        0.066046
Test - acc:         0.920200 loss:        0.308366
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.979420 loss:        0.060425
Test - acc:         0.915500 loss:        0.312346
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.980700 loss:        0.057717
Test - acc:         0.917100 loss:        0.317153
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.980760 loss:        0.057021
Test - acc:         0.914200 loss:        0.310880
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.982020 loss:        0.052966
Test - acc:         0.912900 loss:        0.322919
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.981660 loss:        0.054243
Test - acc:         0.909800 loss:        0.336197
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.982140 loss:        0.052825
Test - acc:         0.911200 loss:        0.337671
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.982120 loss:        0.054323
Test - acc:         0.919200 loss:        0.307584
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.981640 loss:        0.054250
Test - acc:         0.912700 loss:        0.328316
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.980540 loss:        0.057154
Test - acc:         0.912300 loss:        0.321838
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.981260 loss:        0.053712
Test - acc:         0.913600 loss:        0.328039
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.980180 loss:        0.057349
Test - acc:         0.905500 loss:        0.351587
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.981880 loss:        0.055064
Test - acc:         0.910800 loss:        0.333924
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.980040 loss:        0.058530
Test - acc:         0.906600 loss:        0.353812
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.980080 loss:        0.058255
Test - acc:         0.912400 loss:        0.321752
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.983460 loss:        0.051239
Test - acc:         0.916900 loss:        0.319351
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.982320 loss:        0.052397
Test - acc:         0.911000 loss:        0.332177
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.979980 loss:        0.058264
Test - acc:         0.916700 loss:        0.310924
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.981400 loss:        0.054042
Test - acc:         0.897500 loss:        0.386824
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.981620 loss:        0.054791
Test - acc:         0.911100 loss:        0.336472
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.982040 loss:        0.053724
Test - acc:         0.907300 loss:        0.357470
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.982800 loss:        0.051726
Test - acc:         0.908800 loss:        0.347963
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.980120 loss:        0.056692
Test - acc:         0.908200 loss:        0.359039
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.979420 loss:        0.059173
Test - acc:         0.909500 loss:        0.342231
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.981480 loss:        0.053579
Test - acc:         0.913500 loss:        0.309650
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.980000 loss:        0.059316
Test - acc:         0.909900 loss:        0.331480
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.980060 loss:        0.058129
Test - acc:         0.892500 loss:        0.422944
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.981200 loss:        0.055239
Test - acc:         0.910200 loss:        0.342242
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.981320 loss:        0.057141
Test - acc:         0.909200 loss:        0.340777
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.981400 loss:        0.055329
Test - acc:         0.910400 loss:        0.354003
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.980840 loss:        0.057129
Test - acc:         0.910300 loss:        0.335220
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.958060 loss:        0.120927
Test - acc:         0.895200 loss:        0.364041
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.968960 loss:        0.090784
Test - acc:         0.888200 loss:        0.401602
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.969140 loss:        0.088702
Test - acc:         0.914100 loss:        0.307976
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.972780 loss:        0.081254
Test - acc:         0.898600 loss:        0.367256
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.973240 loss:        0.078058
Test - acc:         0.908800 loss:        0.341587
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.974840 loss:        0.074620
Test - acc:         0.901400 loss:        0.373712
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.971840 loss:        0.081727
Test - acc:         0.909900 loss:        0.322513
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.975080 loss:        0.073660
Test - acc:         0.907000 loss:        0.350730
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.973760 loss:        0.077547
Test - acc:         0.899500 loss:        0.366896
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.972080 loss:        0.079037
Test - acc:         0.911500 loss:        0.319155
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.975220 loss:        0.072677
Test - acc:         0.904400 loss:        0.366756
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.975420 loss:        0.072542
Test - acc:         0.907800 loss:        0.333155
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.974700 loss:        0.074689
Test - acc:         0.908900 loss:        0.337109
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.973900 loss:        0.074032
Test - acc:         0.909000 loss:        0.326852
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.975820 loss:        0.069249
Test - acc:         0.907900 loss:        0.349726
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.976460 loss:        0.069992
Test - acc:         0.910000 loss:        0.329922
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.974420 loss:        0.071491
Test - acc:         0.906400 loss:        0.353725
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.976020 loss:        0.070489
Test - acc:         0.903600 loss:        0.350786
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.976920 loss:        0.066741
Test - acc:         0.914800 loss:        0.327666
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.975400 loss:        0.072315
Test - acc:         0.906400 loss:        0.365478
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.975900 loss:        0.070006
Test - acc:         0.906100 loss:        0.343410
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.978260 loss:        0.063246
Test - acc:         0.906700 loss:        0.348524
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.975780 loss:        0.070759
Test - acc:         0.907100 loss:        0.347546
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.976120 loss:        0.069941
Test - acc:         0.894500 loss:        0.397444
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.975500 loss:        0.072037
Test - acc:         0.909300 loss:        0.318061
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.977520 loss:        0.065761
Test - acc:         0.894800 loss:        0.405640
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.985820 loss:        0.043639
Test - acc:         0.923500 loss:        0.272425
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.991560 loss:        0.028625
Test - acc:         0.925800 loss:        0.266675
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.993240 loss:        0.024155
Test - acc:         0.928200 loss:        0.263372
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.993220 loss:        0.022728
Test - acc:         0.928600 loss:        0.266315
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.994340 loss:        0.019936
Test - acc:         0.929200 loss:        0.265160
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.994900 loss:        0.018446
Test - acc:         0.931000 loss:        0.265558
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.958180 loss:        0.124591
Test - acc:         0.907900 loss:        0.332037
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.966220 loss:        0.096929
Test - acc:         0.910300 loss:        0.318893
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.970600 loss:        0.084440
Test - acc:         0.910900 loss:        0.311966
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.973320 loss:        0.076902
Test - acc:         0.911700 loss:        0.310530
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.975640 loss:        0.072363
Test - acc:         0.913800 loss:        0.307408
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.976440 loss:        0.070753
Test - acc:         0.914700 loss:        0.301850
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.978400 loss:        0.064166
Test - acc:         0.914200 loss:        0.306353
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.979120 loss:        0.062441
Test - acc:         0.913800 loss:        0.304993
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.980640 loss:        0.058000
Test - acc:         0.915700 loss:        0.311386
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.980260 loss:        0.058110
Test - acc:         0.917400 loss:        0.305680
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.982040 loss:        0.053489
Test - acc:         0.915700 loss:        0.306901
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.982500 loss:        0.052811
Test - acc:         0.917000 loss:        0.306625
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.982120 loss:        0.052323
Test - acc:         0.916500 loss:        0.304816
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.982900 loss:        0.049596
Test - acc:         0.919500 loss:        0.304073
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.984460 loss:        0.047045
Test - acc:         0.918300 loss:        0.310433
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.983620 loss:        0.048534
Test - acc:         0.918500 loss:        0.305862
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.985340 loss:        0.046216
Test - acc:         0.918400 loss:        0.303549
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.984880 loss:        0.044750
Test - acc:         0.918300 loss:        0.307605
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.985440 loss:        0.044520
Test - acc:         0.919200 loss:        0.308500
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.986740 loss:        0.041485
Test - acc:         0.918100 loss:        0.310308
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.985380 loss:        0.042877
Test - acc:         0.918600 loss:        0.311690
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.986600 loss:        0.040936
Test - acc:         0.919200 loss:        0.308753
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.986800 loss:        0.040445
Test - acc:         0.920100 loss:        0.309545
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.986520 loss:        0.041449
Test - acc:         0.921400 loss:        0.309572
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.987220 loss:        0.038587
Test - acc:         0.919000 loss:        0.314605
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.987580 loss:        0.036844
Test - acc:         0.918800 loss:        0.314971
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.986100 loss:        0.040049
Test - acc:         0.919000 loss:        0.313584
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.987740 loss:        0.037512
Test - acc:         0.921500 loss:        0.307790
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.988180 loss:        0.037102
Test - acc:         0.919900 loss:        0.308790
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.988340 loss:        0.035877
Test - acc:         0.919000 loss:        0.311694
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.988900 loss:        0.034531
Test - acc:         0.918500 loss:        0.312950
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.988940 loss:        0.034526
Test - acc:         0.919600 loss:        0.315285
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.823640 loss:        0.552664
Test - acc:         0.835700 loss:        0.529232
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.867420 loss:        0.403761
Test - acc:         0.853700 loss:        0.476839
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.883220 loss:        0.354962
Test - acc:         0.861300 loss:        0.456358
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.891880 loss:        0.333047
Test - acc:         0.866300 loss:        0.441272
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.897280 loss:        0.309264
Test - acc:         0.869800 loss:        0.434134
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.900180 loss:        0.298293
Test - acc:         0.874900 loss:        0.418441
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.906780 loss:        0.285323
Test - acc:         0.873400 loss:        0.431593
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.910200 loss:        0.275017
Test - acc:         0.876200 loss:        0.421940
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.909820 loss:        0.271717
Test - acc:         0.876600 loss:        0.414477
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.911420 loss:        0.267328
Test - acc:         0.880400 loss:        0.406129
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.916980 loss:        0.255866
Test - acc:         0.875200 loss:        0.409417
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.917720 loss:        0.249683
Test - acc:         0.877500 loss:        0.406954
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.918620 loss:        0.244999
Test - acc:         0.880500 loss:        0.392648
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.920680 loss:        0.242729
Test - acc:         0.880800 loss:        0.392626
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.921340 loss:        0.239031
Test - acc:         0.883200 loss:        0.389622
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.921860 loss:        0.235881
Test - acc:         0.884800 loss:        0.387238
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.923680 loss:        0.233497
Test - acc:         0.885000 loss:        0.390618
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.923540 loss:        0.228438
Test - acc:         0.883500 loss:        0.392259
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.924220 loss:        0.227157
Test - acc:         0.883600 loss:        0.403142
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.925120 loss:        0.224746
Test - acc:         0.883300 loss:        0.401178
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.926380 loss:        0.222433
Test - acc:         0.887200 loss:        0.388691
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.926800 loss:        0.221871
Test - acc:         0.884800 loss:        0.392066
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.928080 loss:        0.217536
Test - acc:         0.882900 loss:        0.393587
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.928900 loss:        0.213792
Test - acc:         0.887200 loss:        0.375082
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.929320 loss:        0.214527
Test - acc:         0.885400 loss:        0.388540
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.930440 loss:        0.209820
Test - acc:         0.887400 loss:        0.383388
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.930700 loss:        0.206286
Test - acc:         0.887200 loss:        0.389370
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.930420 loss:        0.209385
Test - acc:         0.886400 loss:        0.387340
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.931040 loss:        0.209160
Test - acc:         0.888100 loss:        0.383258
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.932640 loss:        0.202870
Test - acc:         0.886300 loss:        0.385031
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.932800 loss:        0.203354
Test - acc:         0.886300 loss:        0.390094
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.933860 loss:        0.201782
Test - acc:         0.887900 loss:        0.385966
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.649300 loss:        1.098718
Test - acc:         0.715700 loss:        0.957668
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.734040 loss:        0.885020
Test - acc:         0.748600 loss:        0.857919
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.758860 loss:        0.806770
Test - acc:         0.762100 loss:        0.800378
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.770340 loss:        0.765039
Test - acc:         0.773800 loss:        0.765743
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.780420 loss:        0.727832
Test - acc:         0.777600 loss:        0.740140
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.787380 loss:        0.699053
Test - acc:         0.782100 loss:        0.736116
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.789280 loss:        0.685214
Test - acc:         0.781600 loss:        0.724311
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.792680 loss:        0.676569
Test - acc:         0.786600 loss:        0.705575
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.798380 loss:        0.658191
Test - acc:         0.790500 loss:        0.685765
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.802600 loss:        0.641909
Test - acc:         0.794800 loss:        0.674301
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.802920 loss:        0.635587
Test - acc:         0.793900 loss:        0.691484
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.805940 loss:        0.627419
Test - acc:         0.793900 loss:        0.679344
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.806200 loss:        0.620090
Test - acc:         0.802400 loss:        0.656947
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.808980 loss:        0.613161
Test - acc:         0.801600 loss:        0.659678
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.811040 loss:        0.605167
Test - acc:         0.800100 loss:        0.648939
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.810500 loss:        0.606081
Test - acc:         0.800300 loss:        0.655579
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.810000 loss:        0.605178
Test - acc:         0.810900 loss:        0.629086
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.817920 loss:        0.586364
Test - acc:         0.803900 loss:        0.650383
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.816000 loss:        0.592557
Test - acc:         0.798100 loss:        0.668994
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.816840 loss:        0.584559
Test - acc:         0.803200 loss:        0.634568
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.817680 loss:        0.579578
Test - acc:         0.809300 loss:        0.625707
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.817400 loss:        0.583303
Test - acc:         0.805900 loss:        0.633036
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.819860 loss:        0.576465
Test - acc:         0.805600 loss:        0.628266
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.819180 loss:        0.569455
Test - acc:         0.810300 loss:        0.630201
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.818740 loss:        0.572383
Test - acc:         0.809100 loss:        0.619299
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.821180 loss:        0.565426
Test - acc:         0.811400 loss:        0.623040
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.821560 loss:        0.566171
Test - acc:         0.808200 loss:        0.623657
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.823680 loss:        0.555523
Test - acc:         0.806000 loss:        0.637224
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.823360 loss:        0.563453
Test - acc:         0.810900 loss:        0.614186
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.822820 loss:        0.561407
Test - acc:         0.807000 loss:        0.628529
Sparsity :          0.9990
Wdecay :        0.000500
