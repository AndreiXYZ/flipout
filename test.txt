******************************
Running
{
    "model": "resnet18",
    "dataset": "cifar10",
    "batch_size": 128,
    "test_batch_size": 5000,
    "epochs": 350,
    "lr": 0.1,
    "device": "cuda",
    "seed": 42,
    "prune_criterion": "weight_squared_div_flips",
    "prune_freq": 32,
    "prune_rate": 0.5,
    "sensitivity": 0,
    "flip_threshold": 1,
    "stop_pruning_at": -1,
    "prune_bias": false,
    "prune_bnorm": false,
    "use_ema_flips": false,
    "beta_ema_flips": null,
    "reset_flip_cts": false,
    "normalize_magnitudes": false,
    "beta_ema_maghists": null,
    "logdir": "test",
    "opt": "sgd",
    "momentum": 0.9,
    "use_scheduler": true,
    "milestones": [
        150,
        250
    ],
    "reg_type": "wdecay",
    "lambda": 0.0005,
    "anneal_lambda": false,
    "anneal_lr": false,
    "add_noise": true,
    "scale_noise_by_lr": false,
    "stop_noise_at": -1,
    "noise_only_prunable": false,
    "noise_scale_factor": 1,
    "global_noise": true,
    "snip_sparsity": 0.0,
    "beta_ema": 0.999,
    "lambas": [
        1.0,
        1.0,
        1.0,
        1.0
    ],
    "local_rep": false,
    "temperature": 0.6666666666666666,
    "save_model": null,
    "load_model": null,
    "parallel": false
}
******************************
Files already downloaded and verified
Files already downloaded and verified
Total prunable params of model: 11164352
Model has 11173962 total params.
num_weights=11169152
num_biases=4810
num.prunable=11164352
---Biases omitted from pruning---
---Bnorm omitted from pruning---
========== Epoch 1 ==========
LR =  0.1
Train - acc:        0.326880 loss:        1.858766
Test - acc:         0.460600 loss:        1.451301
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 2 ==========
LR =  0.1
Train - acc:        0.504640 loss:        1.352020
Test - acc:         0.528500 loss:        1.304943
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 3 ==========
LR =  0.1
Train - acc:        0.620420 loss:        1.063897
Test - acc:         0.616500 loss:        1.085963
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 4 ==========
LR =  0.1
Train - acc:        0.694480 loss:        0.862878
Test - acc:         0.645100 loss:        1.075948
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 5 ==========
LR =  0.1
Train - acc:        0.749100 loss:        0.721626
Test - acc:         0.734600 loss:        0.788300
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 6 ==========
LR =  0.1
Train - acc:        0.782880 loss:        0.624070
Test - acc:         0.776800 loss:        0.651278
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 7 ==========
LR =  0.1
Train - acc:        0.803860 loss:        0.571190
Test - acc:         0.789600 loss:        0.617397
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 8 ==========
LR =  0.1
Train - acc:        0.816500 loss:        0.534659
Test - acc:         0.776100 loss:        0.686206
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 9 ==========
LR =  0.1
Train - acc:        0.824620 loss:        0.511252
Test - acc:         0.767300 loss:        0.697715
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 10 ==========
LR =  0.1
Train - acc:        0.830100 loss:        0.492953
Test - acc:         0.789200 loss:        0.623710
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 11 ==========
LR =  0.1
Train - acc:        0.836480 loss:        0.474511
Test - acc:         0.739700 loss:        0.813960
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 12 ==========
LR =  0.1
Train - acc:        0.843360 loss:        0.457772
Test - acc:         0.803000 loss:        0.599718
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 13 ==========
LR =  0.1
Train - acc:        0.849160 loss:        0.444143
Test - acc:         0.795300 loss:        0.598299
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 14 ==========
LR =  0.1
Train - acc:        0.849260 loss:        0.444741
Test - acc:         0.761500 loss:        0.709062
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 15 ==========
LR =  0.1
Train - acc:        0.852260 loss:        0.432352
Test - acc:         0.820300 loss:        0.539191
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 16 ==========
LR =  0.1
Train - acc:        0.854600 loss:        0.424793
Test - acc:         0.824300 loss:        0.532800
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 17 ==========
LR =  0.1
Train - acc:        0.860820 loss:        0.409515
Test - acc:         0.824000 loss:        0.528523
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 18 ==========
LR =  0.1
Train - acc:        0.859400 loss:        0.407545
Test - acc:         0.794700 loss:        0.630947
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 19 ==========
LR =  0.1
Train - acc:        0.861780 loss:        0.403872
Test - acc:         0.843400 loss:        0.454615
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 20 ==========
LR =  0.1
Train - acc:        0.863800 loss:        0.397893
Test - acc:         0.813800 loss:        0.562551
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 21 ==========
LR =  0.1
Train - acc:        0.866920 loss:        0.392618
Test - acc:         0.839200 loss:        0.470852
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 22 ==========
LR =  0.1
Train - acc:        0.867520 loss:        0.387200
Test - acc:         0.807700 loss:        0.610000
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 23 ==========
LR =  0.1
Train - acc:        0.868940 loss:        0.382494
Test - acc:         0.791900 loss:        0.619533
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 24 ==========
LR =  0.1
Train - acc:        0.870200 loss:        0.380269
Test - acc:         0.812700 loss:        0.596839
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 25 ==========
LR =  0.1
Train - acc:        0.871200 loss:        0.374233
Test - acc:         0.832300 loss:        0.505863
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 26 ==========
LR =  0.1
Train - acc:        0.871280 loss:        0.379795
Test - acc:         0.827900 loss:        0.516552
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 27 ==========
LR =  0.1
Train - acc:        0.875780 loss:        0.368313
Test - acc:         0.806600 loss:        0.591760
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 28 ==========
LR =  0.1
Train - acc:        0.872560 loss:        0.372255
Test - acc:         0.835600 loss:        0.471035
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 29 ==========
LR =  0.1
Train - acc:        0.876100 loss:        0.362688
Test - acc:         0.838100 loss:        0.486613
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 30 ==========
LR =  0.1
Train - acc:        0.876140 loss:        0.367302
Test - acc:         0.828300 loss:        0.527631
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 31 ==========
LR =  0.1
Train - acc:        0.873820 loss:        0.363960
Test - acc:         0.854100 loss:        0.441828
Sparsity :          0.0000
Wdecay :        0.000500
========== Epoch 32 ==========
LR =  0.1
Train - acc:        0.875980 loss:        0.363700
Test - acc:         0.824300 loss:        0.548676
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 33 ==========
LR =  0.1
Train - acc:        0.894400 loss:        0.308222
Test - acc:         0.842700 loss:        0.481054
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 34 ==========
LR =  0.1
Train - acc:        0.891120 loss:        0.319179
Test - acc:         0.851900 loss:        0.459765
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 35 ==========
LR =  0.1
Train - acc:        0.888340 loss:        0.324204
Test - acc:         0.804700 loss:        0.602540
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 36 ==========
LR =  0.1
Train - acc:        0.887860 loss:        0.322917
Test - acc:         0.832100 loss:        0.511038
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 37 ==========
LR =  0.1
Train - acc:        0.889060 loss:        0.326054
Test - acc:         0.818100 loss:        0.596852
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 38 ==========
LR =  0.1
Train - acc:        0.888800 loss:        0.327176
Test - acc:         0.867500 loss:        0.393795
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 39 ==========
LR =  0.1
Train - acc:        0.888100 loss:        0.326776
Test - acc:         0.861300 loss:        0.428557
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 40 ==========
LR =  0.1
Train - acc:        0.890400 loss:        0.323909
Test - acc:         0.861100 loss:        0.423783
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 41 ==========
LR =  0.1
Train - acc:        0.888020 loss:        0.329326
Test - acc:         0.823000 loss:        0.541547
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 42 ==========
LR =  0.1
Train - acc:        0.891940 loss:        0.317089
Test - acc:         0.828800 loss:        0.518166
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 43 ==========
LR =  0.1
Train - acc:        0.888320 loss:        0.326338
Test - acc:         0.851800 loss:        0.445582
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 44 ==========
LR =  0.1
Train - acc:        0.890380 loss:        0.324259
Test - acc:         0.822000 loss:        0.564256
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 45 ==========
LR =  0.1
Train - acc:        0.890860 loss:        0.322495
Test - acc:         0.848400 loss:        0.449552
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 46 ==========
LR =  0.1
Train - acc:        0.890500 loss:        0.324083
Test - acc:         0.814500 loss:        0.577783
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 47 ==========
LR =  0.1
Train - acc:        0.889200 loss:        0.323930
Test - acc:         0.828400 loss:        0.534652
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 48 ==========
LR =  0.1
Train - acc:        0.889780 loss:        0.324287
Test - acc:         0.819800 loss:        0.554986
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 49 ==========
LR =  0.1
Train - acc:        0.892160 loss:        0.318580
Test - acc:         0.817400 loss:        0.576814
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 50 ==========
LR =  0.1
Train - acc:        0.889180 loss:        0.324845
Test - acc:         0.856800 loss:        0.441299
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 51 ==========
LR =  0.1
Train - acc:        0.891540 loss:        0.316895
Test - acc:         0.824300 loss:        0.575817
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 52 ==========
LR =  0.1
Train - acc:        0.890120 loss:        0.322260
Test - acc:         0.859200 loss:        0.416052
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 53 ==========
LR =  0.1
Train - acc:        0.892020 loss:        0.316027
Test - acc:         0.850600 loss:        0.452093
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 54 ==========
LR =  0.1
Train - acc:        0.890700 loss:        0.319367
Test - acc:         0.824400 loss:        0.562389
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 55 ==========
LR =  0.1
Train - acc:        0.891440 loss:        0.317923
Test - acc:         0.840700 loss:        0.478969
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 56 ==========
LR =  0.1
Train - acc:        0.892100 loss:        0.318851
Test - acc:         0.801400 loss:        0.614856
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 57 ==========
LR =  0.1
Train - acc:        0.888060 loss:        0.323057
Test - acc:         0.852900 loss:        0.450537
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 58 ==========
LR =  0.1
Train - acc:        0.891920 loss:        0.316971
Test - acc:         0.844900 loss:        0.459546
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 59 ==========
LR =  0.1
Train - acc:        0.891380 loss:        0.321543
Test - acc:         0.864900 loss:        0.398546
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 60 ==========
LR =  0.1
Train - acc:        0.890200 loss:        0.324289
Test - acc:         0.823300 loss:        0.528725
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 61 ==========
LR =  0.1
Train - acc:        0.890160 loss:        0.318714
Test - acc:         0.836800 loss:        0.495465
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 62 ==========
LR =  0.1
Train - acc:        0.889580 loss:        0.321411
Test - acc:         0.837900 loss:        0.490247
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 63 ==========
LR =  0.1
Train - acc:        0.891360 loss:        0.320233
Test - acc:         0.829400 loss:        0.516048
Sparsity :          0.5000
Wdecay :        0.000500
========== Epoch 64 ==========
LR =  0.1
Train - acc:        0.891240 loss:        0.321269
Test - acc:         0.845900 loss:        0.474597
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 65 ==========
LR =  0.1
Train - acc:        0.904240 loss:        0.278951
Test - acc:         0.824600 loss:        0.566636
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 66 ==========
LR =  0.1
Train - acc:        0.901600 loss:        0.286300
Test - acc:         0.863500 loss:        0.409603
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 67 ==========
LR =  0.1
Train - acc:        0.902000 loss:        0.284768
Test - acc:         0.874800 loss:        0.389391
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 68 ==========
LR =  0.1
Train - acc:        0.899420 loss:        0.294639
Test - acc:         0.860500 loss:        0.428658
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 69 ==========
LR =  0.1
Train - acc:        0.901140 loss:        0.289930
Test - acc:         0.856800 loss:        0.443042
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 70 ==========
LR =  0.1
Train - acc:        0.900300 loss:        0.294979
Test - acc:         0.843600 loss:        0.465458
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 71 ==========
LR =  0.1
Train - acc:        0.899340 loss:        0.293111
Test - acc:         0.816400 loss:        0.591770
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 72 ==========
LR =  0.1
Train - acc:        0.897680 loss:        0.297942
Test - acc:         0.837700 loss:        0.503047
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 73 ==========
LR =  0.1
Train - acc:        0.901060 loss:        0.287825
Test - acc:         0.866000 loss:        0.404919
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 74 ==========
LR =  0.1
Train - acc:        0.901120 loss:        0.293213
Test - acc:         0.812900 loss:        0.561335
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 75 ==========
LR =  0.1
Train - acc:        0.900080 loss:        0.290780
Test - acc:         0.825800 loss:        0.564427
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 76 ==========
LR =  0.1
Train - acc:        0.900340 loss:        0.294283
Test - acc:         0.860800 loss:        0.424733
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 77 ==========
LR =  0.1
Train - acc:        0.899720 loss:        0.292877
Test - acc:         0.858700 loss:        0.427001
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 78 ==========
LR =  0.1
Train - acc:        0.901060 loss:        0.288638
Test - acc:         0.813200 loss:        0.561006
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 79 ==========
LR =  0.1
Train - acc:        0.898920 loss:        0.296759
Test - acc:         0.855000 loss:        0.450297
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 80 ==========
LR =  0.1
Train - acc:        0.900660 loss:        0.289607
Test - acc:         0.870300 loss:        0.406593
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 81 ==========
LR =  0.1
Train - acc:        0.897520 loss:        0.298672
Test - acc:         0.861300 loss:        0.428740
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 82 ==========
LR =  0.1
Train - acc:        0.900760 loss:        0.288718
Test - acc:         0.852400 loss:        0.469692
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 83 ==========
LR =  0.1
Train - acc:        0.900540 loss:        0.291405
Test - acc:         0.850800 loss:        0.450485
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 84 ==========
LR =  0.1
Train - acc:        0.901100 loss:        0.287330
Test - acc:         0.828700 loss:        0.547754
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 85 ==========
LR =  0.1
Train - acc:        0.899640 loss:        0.293709
Test - acc:         0.873300 loss:        0.386022
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 86 ==========
LR =  0.1
Train - acc:        0.899820 loss:        0.291816
Test - acc:         0.878600 loss:        0.366134
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 87 ==========
LR =  0.1
Train - acc:        0.901780 loss:        0.287440
Test - acc:         0.845300 loss:        0.465488
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 88 ==========
LR =  0.1
Train - acc:        0.903000 loss:        0.286764
Test - acc:         0.855100 loss:        0.448264
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 89 ==========
LR =  0.1
Train - acc:        0.899580 loss:        0.294337
Test - acc:         0.807800 loss:        0.620166
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 90 ==========
LR =  0.1
Train - acc:        0.900400 loss:        0.293954
Test - acc:         0.841300 loss:        0.516972
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 91 ==========
LR =  0.1
Train - acc:        0.898280 loss:        0.297021
Test - acc:         0.874600 loss:        0.379616
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 92 ==========
LR =  0.1
Train - acc:        0.900260 loss:        0.290757
Test - acc:         0.856100 loss:        0.446294
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 93 ==========
LR =  0.1
Train - acc:        0.903060 loss:        0.289154
Test - acc:         0.815100 loss:        0.597313
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 94 ==========
LR =  0.1
Train - acc:        0.897160 loss:        0.299830
Test - acc:         0.872600 loss:        0.396418
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 95 ==========
LR =  0.1
Train - acc:        0.899820 loss:        0.292953
Test - acc:         0.834300 loss:        0.501901
Sparsity :          0.7500
Wdecay :        0.000500
========== Epoch 96 ==========
LR =  0.1
Train - acc:        0.897020 loss:        0.296962
Test - acc:         0.829100 loss:        0.550603
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 97 ==========
LR =  0.1
Train - acc:        0.911180 loss:        0.257116
Test - acc:         0.843700 loss:        0.514254
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 98 ==========
LR =  0.1
Train - acc:        0.910160 loss:        0.262894
Test - acc:         0.827500 loss:        0.549959
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 99 ==========
LR =  0.1
Train - acc:        0.907160 loss:        0.268955
Test - acc:         0.853200 loss:        0.467001
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 100 ==========
LR =  0.1
Train - acc:        0.907080 loss:        0.272267
Test - acc:         0.876700 loss:        0.378215
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 101 ==========
LR =  0.1
Train - acc:        0.907100 loss:        0.271632
Test - acc:         0.875800 loss:        0.374421
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 102 ==========
LR =  0.1
Train - acc:        0.906560 loss:        0.269354
Test - acc:         0.841200 loss:        0.504122
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 103 ==========
LR =  0.1
Train - acc:        0.905140 loss:        0.275838
Test - acc:         0.852000 loss:        0.459303
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 104 ==========
LR =  0.1
Train - acc:        0.907900 loss:        0.271204
Test - acc:         0.876700 loss:        0.363421
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 105 ==========
LR =  0.1
Train - acc:        0.904760 loss:        0.272985
Test - acc:         0.842100 loss:        0.500835
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 106 ==========
LR =  0.1
Train - acc:        0.907660 loss:        0.272809
Test - acc:         0.839600 loss:        0.509141
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 107 ==========
LR =  0.1
Train - acc:        0.909620 loss:        0.264534
Test - acc:         0.847800 loss:        0.473048
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 108 ==========
LR =  0.1
Train - acc:        0.907440 loss:        0.272656
Test - acc:         0.836900 loss:        0.515447
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 109 ==========
LR =  0.1
Train - acc:        0.904600 loss:        0.274932
Test - acc:         0.875000 loss:        0.383247
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 110 ==========
LR =  0.1
Train - acc:        0.905040 loss:        0.274087
Test - acc:         0.864100 loss:        0.406947
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 111 ==========
LR =  0.1
Train - acc:        0.907000 loss:        0.268436
Test - acc:         0.873700 loss:        0.377103
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 112 ==========
LR =  0.1
Train - acc:        0.907480 loss:        0.270107
Test - acc:         0.874200 loss:        0.388792
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 113 ==========
LR =  0.1
Train - acc:        0.905540 loss:        0.273977
Test - acc:         0.874800 loss:        0.381722
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 114 ==========
LR =  0.1
Train - acc:        0.906000 loss:        0.273648
Test - acc:         0.875400 loss:        0.373841
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 115 ==========
LR =  0.1
Train - acc:        0.908480 loss:        0.265995
Test - acc:         0.860400 loss:        0.424422
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 116 ==========
LR =  0.1
Train - acc:        0.907500 loss:        0.271111
Test - acc:         0.846000 loss:        0.478921
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 117 ==========
LR =  0.1
Train - acc:        0.910940 loss:        0.264584
Test - acc:         0.874800 loss:        0.387162
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 118 ==========
LR =  0.1
Train - acc:        0.905660 loss:        0.273965
Test - acc:         0.838500 loss:        0.486640
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 119 ==========
LR =  0.1
Train - acc:        0.907600 loss:        0.268653
Test - acc:         0.850700 loss:        0.460513
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 120 ==========
LR =  0.1
Train - acc:        0.906600 loss:        0.271942
Test - acc:         0.845800 loss:        0.496868
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 121 ==========
LR =  0.1
Train - acc:        0.905540 loss:        0.273447
Test - acc:         0.866300 loss:        0.401882
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 122 ==========
LR =  0.1
Train - acc:        0.906540 loss:        0.274080
Test - acc:         0.846600 loss:        0.472245
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 123 ==========
LR =  0.1
Train - acc:        0.907640 loss:        0.270677
Test - acc:         0.857900 loss:        0.438259
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 124 ==========
LR =  0.1
Train - acc:        0.904960 loss:        0.274510
Test - acc:         0.839100 loss:        0.548948
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 125 ==========
LR =  0.1
Train - acc:        0.907080 loss:        0.272743
Test - acc:         0.846100 loss:        0.469649
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 126 ==========
LR =  0.1
Train - acc:        0.906780 loss:        0.270212
Test - acc:         0.868400 loss:        0.399762
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 127 ==========
LR =  0.1
Train - acc:        0.907760 loss:        0.268880
Test - acc:         0.839200 loss:        0.508436
Sparsity :          0.8750
Wdecay :        0.000500
========== Epoch 128 ==========
LR =  0.1
Train - acc:        0.907340 loss:        0.273769
Test - acc:         0.797600 loss:        0.633460
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 129 ==========
LR =  0.1
Train - acc:        0.916300 loss:        0.240812
Test - acc:         0.869500 loss:        0.409475
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 130 ==========
LR =  0.1
Train - acc:        0.914400 loss:        0.247734
Test - acc:         0.882900 loss:        0.357796
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 131 ==========
LR =  0.1
Train - acc:        0.914140 loss:        0.249901
Test - acc:         0.867000 loss:        0.410152
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 132 ==========
LR =  0.1
Train - acc:        0.915240 loss:        0.250118
Test - acc:         0.869700 loss:        0.390805
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 133 ==========
LR =  0.1
Train - acc:        0.911800 loss:        0.255820
Test - acc:         0.847200 loss:        0.457196
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 134 ==========
LR =  0.1
Train - acc:        0.912640 loss:        0.255598
Test - acc:         0.853800 loss:        0.473038
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 135 ==========
LR =  0.1
Train - acc:        0.912940 loss:        0.255119
Test - acc:         0.878900 loss:        0.357897
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 136 ==========
LR =  0.1
Train - acc:        0.913520 loss:        0.252130
Test - acc:         0.874900 loss:        0.385052
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 137 ==========
LR =  0.1
Train - acc:        0.912260 loss:        0.254526
Test - acc:         0.835400 loss:        0.515142
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 138 ==========
LR =  0.1
Train - acc:        0.912740 loss:        0.257146
Test - acc:         0.883300 loss:        0.350310
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 139 ==========
LR =  0.1
Train - acc:        0.908000 loss:        0.260633
Test - acc:         0.821300 loss:        0.560937
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 140 ==========
LR =  0.1
Train - acc:        0.911580 loss:        0.256261
Test - acc:         0.849100 loss:        0.453184
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 141 ==========
LR =  0.1
Train - acc:        0.911440 loss:        0.255808
Test - acc:         0.875300 loss:        0.389943
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 142 ==========
LR =  0.1
Train - acc:        0.912560 loss:        0.254494
Test - acc:         0.875400 loss:        0.377993
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 143 ==========
LR =  0.1
Train - acc:        0.912260 loss:        0.253854
Test - acc:         0.864300 loss:        0.399763
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 144 ==========
LR =  0.1
Train - acc:        0.913620 loss:        0.251782
Test - acc:         0.855300 loss:        0.475252
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 145 ==========
LR =  0.1
Train - acc:        0.910340 loss:        0.257876
Test - acc:         0.870800 loss:        0.406043
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 146 ==========
LR =  0.1
Train - acc:        0.911640 loss:        0.254798
Test - acc:         0.879200 loss:        0.367758
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 147 ==========
LR =  0.1
Train - acc:        0.911880 loss:        0.255412
Test - acc:         0.877200 loss:        0.375279
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 148 ==========
LR =  0.1
Train - acc:        0.912740 loss:        0.252686
Test - acc:         0.834700 loss:        0.522248
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 149 ==========
LR =  0.1
Train - acc:        0.914080 loss:        0.249508
Test - acc:         0.858300 loss:        0.434606
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 150 ==========
LR =  0.010000000000000002
Train - acc:        0.912000 loss:        0.252221
Test - acc:         0.860700 loss:        0.433667
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 151 ==========
LR =  0.010000000000000002
Train - acc:        0.951240 loss:        0.144469
Test - acc:         0.930400 loss:        0.200442
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 152 ==========
LR =  0.010000000000000002
Train - acc:        0.965040 loss:        0.107311
Test - acc:         0.935500 loss:        0.192312
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 153 ==========
LR =  0.010000000000000002
Train - acc:        0.969000 loss:        0.093977
Test - acc:         0.937600 loss:        0.188224
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 154 ==========
LR =  0.010000000000000002
Train - acc:        0.973020 loss:        0.080983
Test - acc:         0.937400 loss:        0.190244
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 155 ==========
LR =  0.010000000000000002
Train - acc:        0.975160 loss:        0.075504
Test - acc:         0.939400 loss:        0.190870
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 156 ==========
LR =  0.010000000000000002
Train - acc:        0.978400 loss:        0.066931
Test - acc:         0.938800 loss:        0.197604
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 157 ==========
LR =  0.010000000000000002
Train - acc:        0.979360 loss:        0.061725
Test - acc:         0.939300 loss:        0.190466
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 158 ==========
LR =  0.010000000000000002
Train - acc:        0.980700 loss:        0.058369
Test - acc:         0.940700 loss:        0.195910
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 159 ==========
LR =  0.010000000000000002
Train - acc:        0.982240 loss:        0.055510
Test - acc:         0.938800 loss:        0.197275
Sparsity :          0.9375
Wdecay :        0.000500
========== Epoch 160 ==========
LR =  0.010000000000000002
Train - acc:        0.983140 loss:        0.052497
Test - acc:         0.939400 loss:        0.195875
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 161 ==========
LR =  0.010000000000000002
Train - acc:        0.983440 loss:        0.050862
Test - acc:         0.938700 loss:        0.196507
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 162 ==========
LR =  0.010000000000000002
Train - acc:        0.985240 loss:        0.047213
Test - acc:         0.934900 loss:        0.209591
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 163 ==========
LR =  0.010000000000000002
Train - acc:        0.986340 loss:        0.044418
Test - acc:         0.941100 loss:        0.199325
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 164 ==========
LR =  0.010000000000000002
Train - acc:        0.986200 loss:        0.042740
Test - acc:         0.937100 loss:        0.209913
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 165 ==========
LR =  0.010000000000000002
Train - acc:        0.987640 loss:        0.039923
Test - acc:         0.939200 loss:        0.211636
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 166 ==========
LR =  0.010000000000000002
Train - acc:        0.987020 loss:        0.039447
Test - acc:         0.938200 loss:        0.208597
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 167 ==========
LR =  0.010000000000000002
Train - acc:        0.987620 loss:        0.039295
Test - acc:         0.937900 loss:        0.212290
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 168 ==========
LR =  0.010000000000000002
Train - acc:        0.988100 loss:        0.037308
Test - acc:         0.938200 loss:        0.217827
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 169 ==========
LR =  0.010000000000000002
Train - acc:        0.987820 loss:        0.038620
Test - acc:         0.940200 loss:        0.218386
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 170 ==========
LR =  0.010000000000000002
Train - acc:        0.987660 loss:        0.038504
Test - acc:         0.937200 loss:        0.213252
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 171 ==========
LR =  0.010000000000000002
Train - acc:        0.989180 loss:        0.036042
Test - acc:         0.941900 loss:        0.205913
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 172 ==========
LR =  0.010000000000000002
Train - acc:        0.989080 loss:        0.035163
Test - acc:         0.938700 loss:        0.216563
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 173 ==========
LR =  0.010000000000000002
Train - acc:        0.989400 loss:        0.034110
Test - acc:         0.936000 loss:        0.232010
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 174 ==========
LR =  0.010000000000000002
Train - acc:        0.989320 loss:        0.035272
Test - acc:         0.937600 loss:        0.219842
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 175 ==========
LR =  0.010000000000000002
Train - acc:        0.987040 loss:        0.038694
Test - acc:         0.935700 loss:        0.223647
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 176 ==========
LR =  0.010000000000000002
Train - acc:        0.987900 loss:        0.037780
Test - acc:         0.937100 loss:        0.225225
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 177 ==========
LR =  0.010000000000000002
Train - acc:        0.987360 loss:        0.038710
Test - acc:         0.939000 loss:        0.218904
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 178 ==========
LR =  0.010000000000000002
Train - acc:        0.988920 loss:        0.034943
Test - acc:         0.938200 loss:        0.229318
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 179 ==========
LR =  0.010000000000000002
Train - acc:        0.989340 loss:        0.035009
Test - acc:         0.936000 loss:        0.230253
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 180 ==========
LR =  0.010000000000000002
Train - acc:        0.987660 loss:        0.037563
Test - acc:         0.937800 loss:        0.226703
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 181 ==========
LR =  0.010000000000000002
Train - acc:        0.987860 loss:        0.037120
Test - acc:         0.935100 loss:        0.237186
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 182 ==========
LR =  0.010000000000000002
Train - acc:        0.988240 loss:        0.036668
Test - acc:         0.935000 loss:        0.243229
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 183 ==========
LR =  0.010000000000000002
Train - acc:        0.986960 loss:        0.040697
Test - acc:         0.933000 loss:        0.231995
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 184 ==========
LR =  0.010000000000000002
Train - acc:        0.987140 loss:        0.039687
Test - acc:         0.934300 loss:        0.235743
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 185 ==========
LR =  0.010000000000000002
Train - acc:        0.986700 loss:        0.040469
Test - acc:         0.932200 loss:        0.241107
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 186 ==========
LR =  0.010000000000000002
Train - acc:        0.986660 loss:        0.041405
Test - acc:         0.936500 loss:        0.227157
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 187 ==========
LR =  0.010000000000000002
Train - acc:        0.986440 loss:        0.042035
Test - acc:         0.929000 loss:        0.253575
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 188 ==========
LR =  0.010000000000000002
Train - acc:        0.985980 loss:        0.043233
Test - acc:         0.929200 loss:        0.252555
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 189 ==========
LR =  0.010000000000000002
Train - acc:        0.985880 loss:        0.043761
Test - acc:         0.930800 loss:        0.251561
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 190 ==========
LR =  0.010000000000000002
Train - acc:        0.985640 loss:        0.044404
Test - acc:         0.931700 loss:        0.246802
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 191 ==========
LR =  0.010000000000000002
Train - acc:        0.983500 loss:        0.048857
Test - acc:         0.935900 loss:        0.237175
Sparsity :          0.9688
Wdecay :        0.000500
========== Epoch 192 ==========
LR =  0.010000000000000002
Train - acc:        0.984660 loss:        0.046720
Test - acc:         0.929600 loss:        0.253307
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 193 ==========
LR =  0.010000000000000002
Train - acc:        0.971740 loss:        0.084283
Test - acc:         0.928100 loss:        0.243168
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 194 ==========
LR =  0.010000000000000002
Train - acc:        0.978020 loss:        0.066581
Test - acc:         0.924900 loss:        0.261043
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 195 ==========
LR =  0.010000000000000002
Train - acc:        0.977280 loss:        0.066705
Test - acc:         0.925200 loss:        0.253395
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 196 ==========
LR =  0.010000000000000002
Train - acc:        0.980580 loss:        0.060540
Test - acc:         0.928800 loss:        0.243398
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 197 ==========
LR =  0.010000000000000002
Train - acc:        0.978880 loss:        0.062414
Test - acc:         0.924800 loss:        0.246422
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 198 ==========
LR =  0.010000000000000002
Train - acc:        0.980600 loss:        0.059762
Test - acc:         0.923200 loss:        0.270858
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 199 ==========
LR =  0.010000000000000002
Train - acc:        0.980040 loss:        0.059539
Test - acc:         0.918100 loss:        0.286434
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 200 ==========
LR =  0.010000000000000002
Train - acc:        0.980760 loss:        0.058846
Test - acc:         0.928500 loss:        0.256521
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 201 ==========
LR =  0.010000000000000002
Train - acc:        0.982160 loss:        0.053336
Test - acc:         0.927600 loss:        0.254880
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 202 ==========
LR =  0.010000000000000002
Train - acc:        0.981140 loss:        0.056355
Test - acc:         0.930900 loss:        0.242371
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 203 ==========
LR =  0.010000000000000002
Train - acc:        0.980100 loss:        0.057362
Test - acc:         0.922800 loss:        0.290183
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 204 ==========
LR =  0.010000000000000002
Train - acc:        0.982200 loss:        0.056049
Test - acc:         0.929900 loss:        0.252654
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 205 ==========
LR =  0.010000000000000002
Train - acc:        0.980480 loss:        0.057128
Test - acc:         0.920000 loss:        0.282807
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 206 ==========
LR =  0.010000000000000002
Train - acc:        0.980560 loss:        0.058646
Test - acc:         0.929400 loss:        0.240627
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 207 ==========
LR =  0.010000000000000002
Train - acc:        0.979900 loss:        0.060184
Test - acc:         0.920600 loss:        0.282635
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 208 ==========
LR =  0.010000000000000002
Train - acc:        0.981320 loss:        0.055789
Test - acc:         0.924800 loss:        0.272190
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 209 ==========
LR =  0.010000000000000002
Train - acc:        0.980880 loss:        0.058217
Test - acc:         0.921900 loss:        0.269945
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 210 ==========
LR =  0.010000000000000002
Train - acc:        0.982500 loss:        0.053469
Test - acc:         0.929500 loss:        0.254657
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 211 ==========
LR =  0.010000000000000002
Train - acc:        0.979840 loss:        0.059727
Test - acc:         0.921500 loss:        0.280616
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 212 ==========
LR =  0.010000000000000002
Train - acc:        0.979780 loss:        0.060086
Test - acc:         0.927100 loss:        0.252819
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 213 ==========
LR =  0.010000000000000002
Train - acc:        0.981300 loss:        0.056945
Test - acc:         0.916100 loss:        0.292025
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 214 ==========
LR =  0.010000000000000002
Train - acc:        0.980400 loss:        0.057882
Test - acc:         0.923700 loss:        0.279132
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 215 ==========
LR =  0.010000000000000002
Train - acc:        0.980800 loss:        0.058706
Test - acc:         0.924600 loss:        0.266377
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 216 ==========
LR =  0.010000000000000002
Train - acc:        0.980400 loss:        0.056841
Test - acc:         0.926800 loss:        0.266615
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 217 ==========
LR =  0.010000000000000002
Train - acc:        0.980640 loss:        0.056901
Test - acc:         0.926600 loss:        0.255178
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 218 ==========
LR =  0.010000000000000002
Train - acc:        0.981420 loss:        0.055446
Test - acc:         0.928100 loss:        0.265010
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 219 ==========
LR =  0.010000000000000002
Train - acc:        0.981240 loss:        0.054403
Test - acc:         0.925100 loss:        0.266540
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 220 ==========
LR =  0.010000000000000002
Train - acc:        0.981160 loss:        0.056937
Test - acc:         0.924300 loss:        0.277121
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 221 ==========
LR =  0.010000000000000002
Train - acc:        0.981460 loss:        0.056196
Test - acc:         0.927300 loss:        0.264808
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 222 ==========
LR =  0.010000000000000002
Train - acc:        0.980940 loss:        0.056691
Test - acc:         0.924700 loss:        0.263033
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 223 ==========
LR =  0.010000000000000002
Train - acc:        0.982200 loss:        0.054227
Test - acc:         0.930700 loss:        0.240750
Sparsity :          0.9844
Wdecay :        0.000500
========== Epoch 224 ==========
LR =  0.010000000000000002
Train - acc:        0.981080 loss:        0.055828
Test - acc:         0.926400 loss:        0.265829
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 225 ==========
LR =  0.010000000000000002
Train - acc:        0.950720 loss:        0.147302
Test - acc:         0.915200 loss:        0.261932
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 226 ==========
LR =  0.010000000000000002
Train - acc:        0.959180 loss:        0.120591
Test - acc:         0.913700 loss:        0.281663
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 227 ==========
LR =  0.010000000000000002
Train - acc:        0.960140 loss:        0.115462
Test - acc:         0.915300 loss:        0.274315
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 228 ==========
LR =  0.010000000000000002
Train - acc:        0.962120 loss:        0.110937
Test - acc:         0.908700 loss:        0.307286
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 229 ==========
LR =  0.010000000000000002
Train - acc:        0.962960 loss:        0.107724
Test - acc:         0.917700 loss:        0.270720
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 230 ==========
LR =  0.010000000000000002
Train - acc:        0.963780 loss:        0.106563
Test - acc:         0.914000 loss:        0.265144
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 231 ==========
LR =  0.010000000000000002
Train - acc:        0.965860 loss:        0.101132
Test - acc:         0.916400 loss:        0.270248
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 232 ==========
LR =  0.010000000000000002
Train - acc:        0.967080 loss:        0.095770
Test - acc:         0.917100 loss:        0.276604
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 233 ==========
LR =  0.010000000000000002
Train - acc:        0.965420 loss:        0.101011
Test - acc:         0.917100 loss:        0.274435
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 234 ==========
LR =  0.010000000000000002
Train - acc:        0.965280 loss:        0.101208
Test - acc:         0.918000 loss:        0.267574
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 235 ==========
LR =  0.010000000000000002
Train - acc:        0.965120 loss:        0.099062
Test - acc:         0.923900 loss:        0.258745
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 236 ==========
LR =  0.010000000000000002
Train - acc:        0.964680 loss:        0.101553
Test - acc:         0.917600 loss:        0.283348
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 237 ==========
LR =  0.010000000000000002
Train - acc:        0.967100 loss:        0.096022
Test - acc:         0.924600 loss:        0.251520
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 238 ==========
LR =  0.010000000000000002
Train - acc:        0.968560 loss:        0.093325
Test - acc:         0.918600 loss:        0.273145
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 239 ==========
LR =  0.010000000000000002
Train - acc:        0.969640 loss:        0.089377
Test - acc:         0.918100 loss:        0.278030
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 240 ==========
LR =  0.010000000000000002
Train - acc:        0.966480 loss:        0.095246
Test - acc:         0.918500 loss:        0.269836
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 241 ==========
LR =  0.010000000000000002
Train - acc:        0.967160 loss:        0.094921
Test - acc:         0.913400 loss:        0.294403
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 242 ==========
LR =  0.010000000000000002
Train - acc:        0.967360 loss:        0.094947
Test - acc:         0.921600 loss:        0.273314
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 243 ==========
LR =  0.010000000000000002
Train - acc:        0.968360 loss:        0.090863
Test - acc:         0.920100 loss:        0.272086
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 244 ==========
LR =  0.010000000000000002
Train - acc:        0.966400 loss:        0.095931
Test - acc:         0.915500 loss:        0.284719
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 245 ==========
LR =  0.010000000000000002
Train - acc:        0.969220 loss:        0.088126
Test - acc:         0.925700 loss:        0.260153
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 246 ==========
LR =  0.010000000000000002
Train - acc:        0.969040 loss:        0.089372
Test - acc:         0.919600 loss:        0.268286
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 247 ==========
LR =  0.010000000000000002
Train - acc:        0.969480 loss:        0.090121
Test - acc:         0.913100 loss:        0.281256
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 248 ==========
LR =  0.010000000000000002
Train - acc:        0.969720 loss:        0.089485
Test - acc:         0.917500 loss:        0.278249
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 249 ==========
LR =  0.010000000000000002
Train - acc:        0.968560 loss:        0.091198
Test - acc:         0.920300 loss:        0.269422
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 250 ==========
LR =  0.0010000000000000002
Train - acc:        0.970060 loss:        0.088730
Test - acc:         0.912000 loss:        0.287088
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 251 ==========
LR =  0.0010000000000000002
Train - acc:        0.979540 loss:        0.064920
Test - acc:         0.934100 loss:        0.217561
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 252 ==========
LR =  0.0010000000000000002
Train - acc:        0.984400 loss:        0.050945
Test - acc:         0.936300 loss:        0.211083
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 253 ==========
LR =  0.0010000000000000002
Train - acc:        0.986340 loss:        0.045951
Test - acc:         0.936100 loss:        0.211326
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 254 ==========
LR =  0.0010000000000000002
Train - acc:        0.987380 loss:        0.043303
Test - acc:         0.935400 loss:        0.212689
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 255 ==========
LR =  0.0010000000000000002
Train - acc:        0.987920 loss:        0.041283
Test - acc:         0.937600 loss:        0.209273
Sparsity :          0.9922
Wdecay :        0.000500
========== Epoch 256 ==========
LR =  0.0010000000000000002
Train - acc:        0.989160 loss:        0.038466
Test - acc:         0.937500 loss:        0.208038
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 257 ==========
LR =  0.0010000000000000002
Train - acc:        0.916160 loss:        0.252329
Test - acc:         0.896500 loss:        0.308732
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 258 ==========
LR =  0.0010000000000000002
Train - acc:        0.934500 loss:        0.196899
Test - acc:         0.906000 loss:        0.288109
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 259 ==========
LR =  0.0010000000000000002
Train - acc:        0.940040 loss:        0.178939
Test - acc:         0.906500 loss:        0.280551
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 260 ==========
LR =  0.0010000000000000002
Train - acc:        0.944520 loss:        0.168956
Test - acc:         0.909900 loss:        0.277862
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 261 ==========
LR =  0.0010000000000000002
Train - acc:        0.946360 loss:        0.161371
Test - acc:         0.912000 loss:        0.267304
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 262 ==========
LR =  0.0010000000000000002
Train - acc:        0.949180 loss:        0.155594
Test - acc:         0.914300 loss:        0.263937
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 263 ==========
LR =  0.0010000000000000002
Train - acc:        0.950100 loss:        0.150480
Test - acc:         0.914300 loss:        0.263766
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 264 ==========
LR =  0.0010000000000000002
Train - acc:        0.951920 loss:        0.144003
Test - acc:         0.914400 loss:        0.263101
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 265 ==========
LR =  0.0010000000000000002
Train - acc:        0.951980 loss:        0.142299
Test - acc:         0.916700 loss:        0.260727
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 266 ==========
LR =  0.0010000000000000002
Train - acc:        0.954280 loss:        0.138506
Test - acc:         0.913400 loss:        0.259274
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 267 ==========
LR =  0.0010000000000000002
Train - acc:        0.952680 loss:        0.139335
Test - acc:         0.916600 loss:        0.255793
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 268 ==========
LR =  0.0010000000000000002
Train - acc:        0.956200 loss:        0.133727
Test - acc:         0.916400 loss:        0.254408
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 269 ==========
LR =  0.0010000000000000002
Train - acc:        0.955540 loss:        0.131583
Test - acc:         0.916200 loss:        0.257191
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 270 ==========
LR =  0.0010000000000000002
Train - acc:        0.957380 loss:        0.128655
Test - acc:         0.917300 loss:        0.254066
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 271 ==========
LR =  0.0010000000000000002
Train - acc:        0.958600 loss:        0.124311
Test - acc:         0.920400 loss:        0.253949
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 272 ==========
LR =  0.0010000000000000002
Train - acc:        0.957780 loss:        0.126881
Test - acc:         0.918200 loss:        0.254699
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 273 ==========
LR =  0.0010000000000000002
Train - acc:        0.957920 loss:        0.122938
Test - acc:         0.919500 loss:        0.253537
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 274 ==========
LR =  0.0010000000000000002
Train - acc:        0.960340 loss:        0.122066
Test - acc:         0.918700 loss:        0.253285
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 275 ==========
LR =  0.0010000000000000002
Train - acc:        0.960580 loss:        0.119573
Test - acc:         0.919700 loss:        0.251821
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 276 ==========
LR =  0.0010000000000000002
Train - acc:        0.960600 loss:        0.119869
Test - acc:         0.919200 loss:        0.253222
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 277 ==========
LR =  0.0010000000000000002
Train - acc:        0.961560 loss:        0.116316
Test - acc:         0.920600 loss:        0.251836
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 278 ==========
LR =  0.0010000000000000002
Train - acc:        0.960620 loss:        0.116238
Test - acc:         0.918700 loss:        0.249089
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 279 ==========
LR =  0.0010000000000000002
Train - acc:        0.961120 loss:        0.115656
Test - acc:         0.918900 loss:        0.251355
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 280 ==========
LR =  0.0010000000000000002
Train - acc:        0.961360 loss:        0.115190
Test - acc:         0.919500 loss:        0.250173
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 281 ==========
LR =  0.0010000000000000002
Train - acc:        0.963280 loss:        0.111488
Test - acc:         0.921200 loss:        0.249680
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 282 ==========
LR =  0.0010000000000000002
Train - acc:        0.962700 loss:        0.111134
Test - acc:         0.920800 loss:        0.253923
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 283 ==========
LR =  0.0010000000000000002
Train - acc:        0.962340 loss:        0.113417
Test - acc:         0.920100 loss:        0.253755
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 284 ==========
LR =  0.0010000000000000002
Train - acc:        0.962800 loss:        0.110879
Test - acc:         0.922900 loss:        0.250280
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 285 ==========
LR =  0.0010000000000000002
Train - acc:        0.963920 loss:        0.108301
Test - acc:         0.920400 loss:        0.250609
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 286 ==========
LR =  0.0010000000000000002
Train - acc:        0.963600 loss:        0.108856
Test - acc:         0.920400 loss:        0.254063
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 287 ==========
LR =  0.0010000000000000002
Train - acc:        0.964740 loss:        0.105461
Test - acc:         0.920100 loss:        0.251934
Sparsity :          0.9961
Wdecay :        0.000500
========== Epoch 288 ==========
LR =  0.0010000000000000002
Train - acc:        0.964960 loss:        0.106227
Test - acc:         0.922800 loss:        0.253197
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 289 ==========
LR =  0.0010000000000000002
Train - acc:        0.772660 loss:        0.666370
Test - acc:         0.818400 loss:        0.532517
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 290 ==========
LR =  0.0010000000000000002
Train - acc:        0.831600 loss:        0.497600
Test - acc:         0.840900 loss:        0.476997
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 291 ==========
LR =  0.0010000000000000002
Train - acc:        0.845300 loss:        0.459044
Test - acc:         0.846700 loss:        0.456036
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 292 ==========
LR =  0.0010000000000000002
Train - acc:        0.856140 loss:        0.431315
Test - acc:         0.855300 loss:        0.434452
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 293 ==========
LR =  0.0010000000000000002
Train - acc:        0.859260 loss:        0.418457
Test - acc:         0.861100 loss:        0.417531
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 294 ==========
LR =  0.0010000000000000002
Train - acc:        0.865000 loss:        0.401421
Test - acc:         0.861400 loss:        0.412380
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 295 ==========
LR =  0.0010000000000000002
Train - acc:        0.867880 loss:        0.394939
Test - acc:         0.865600 loss:        0.403904
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 296 ==========
LR =  0.0010000000000000002
Train - acc:        0.870720 loss:        0.381706
Test - acc:         0.867300 loss:        0.399485
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 297 ==========
LR =  0.0010000000000000002
Train - acc:        0.873160 loss:        0.374948
Test - acc:         0.867300 loss:        0.400837
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 298 ==========
LR =  0.0010000000000000002
Train - acc:        0.873980 loss:        0.370908
Test - acc:         0.870700 loss:        0.392037
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 299 ==========
LR =  0.0010000000000000002
Train - acc:        0.876560 loss:        0.365440
Test - acc:         0.871400 loss:        0.389283
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 300 ==========
LR =  0.0010000000000000002
Train - acc:        0.879180 loss:        0.360135
Test - acc:         0.871400 loss:        0.384910
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 301 ==========
LR =  0.0010000000000000002
Train - acc:        0.878340 loss:        0.355360
Test - acc:         0.876100 loss:        0.378262
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 302 ==========
LR =  0.0010000000000000002
Train - acc:        0.880820 loss:        0.349708
Test - acc:         0.873400 loss:        0.380658
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 303 ==========
LR =  0.0010000000000000002
Train - acc:        0.883520 loss:        0.344809
Test - acc:         0.874200 loss:        0.374104
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 304 ==========
LR =  0.0010000000000000002
Train - acc:        0.882980 loss:        0.345614
Test - acc:         0.874600 loss:        0.376899
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 305 ==========
LR =  0.0010000000000000002
Train - acc:        0.884060 loss:        0.341126
Test - acc:         0.874500 loss:        0.371736
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 306 ==========
LR =  0.0010000000000000002
Train - acc:        0.886720 loss:        0.336002
Test - acc:         0.875000 loss:        0.375529
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 307 ==========
LR =  0.0010000000000000002
Train - acc:        0.887080 loss:        0.337163
Test - acc:         0.876000 loss:        0.368095
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 308 ==========
LR =  0.0010000000000000002
Train - acc:        0.887460 loss:        0.333069
Test - acc:         0.875600 loss:        0.374981
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 309 ==========
LR =  0.0010000000000000002
Train - acc:        0.889720 loss:        0.326031
Test - acc:         0.873900 loss:        0.369967
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 310 ==========
LR =  0.0010000000000000002
Train - acc:        0.886860 loss:        0.331262
Test - acc:         0.871600 loss:        0.375134
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 311 ==========
LR =  0.0010000000000000002
Train - acc:        0.890980 loss:        0.324019
Test - acc:         0.874200 loss:        0.374105
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 312 ==========
LR =  0.0010000000000000002
Train - acc:        0.889320 loss:        0.323272
Test - acc:         0.878400 loss:        0.362994
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 313 ==========
LR =  0.0010000000000000002
Train - acc:        0.887880 loss:        0.325759
Test - acc:         0.877500 loss:        0.369312
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 314 ==========
LR =  0.0010000000000000002
Train - acc:        0.890620 loss:        0.320913
Test - acc:         0.875000 loss:        0.372655
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 315 ==========
LR =  0.0010000000000000002
Train - acc:        0.890740 loss:        0.321116
Test - acc:         0.874900 loss:        0.370241
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 316 ==========
LR =  0.0010000000000000002
Train - acc:        0.890520 loss:        0.316866
Test - acc:         0.880000 loss:        0.359560
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 317 ==========
LR =  0.0010000000000000002
Train - acc:        0.892920 loss:        0.318977
Test - acc:         0.877900 loss:        0.363908
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 318 ==========
LR =  0.0010000000000000002
Train - acc:        0.895100 loss:        0.313272
Test - acc:         0.877700 loss:        0.366289
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 319 ==========
LR =  0.0010000000000000002
Train - acc:        0.894160 loss:        0.313111
Test - acc:         0.878800 loss:        0.360047
Sparsity :          0.9980
Wdecay :        0.000500
========== Epoch 320 ==========
LR =  0.0010000000000000002
Train - acc:        0.892860 loss:        0.313765
Test - acc:         0.880500 loss:        0.356904
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 321 ==========
LR =  0.0010000000000000002
Train - acc:        0.614840 loss:        1.095297
Test - acc:         0.682800 loss:        0.898959
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 322 ==========
LR =  0.0010000000000000002
Train - acc:        0.693380 loss:        0.889104
Test - acc:         0.712700 loss:        0.823079
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 323 ==========
LR =  0.0010000000000000002
Train - acc:        0.717040 loss:        0.825931
Test - acc:         0.724300 loss:        0.789141
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 324 ==========
LR =  0.0010000000000000002
Train - acc:        0.728500 loss:        0.791225
Test - acc:         0.732400 loss:        0.763734
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 325 ==========
LR =  0.0010000000000000002
Train - acc:        0.740180 loss:        0.761493
Test - acc:         0.740900 loss:        0.745478
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 326 ==========
LR =  0.0010000000000000002
Train - acc:        0.745920 loss:        0.744525
Test - acc:         0.752900 loss:        0.709376
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 327 ==========
LR =  0.0010000000000000002
Train - acc:        0.750100 loss:        0.729673
Test - acc:         0.754200 loss:        0.700440
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 328 ==========
LR =  0.0010000000000000002
Train - acc:        0.757740 loss:        0.711268
Test - acc:         0.761400 loss:        0.689140
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 329 ==========
LR =  0.0010000000000000002
Train - acc:        0.760040 loss:        0.703933
Test - acc:         0.766300 loss:        0.678928
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 330 ==========
LR =  0.0010000000000000002
Train - acc:        0.765220 loss:        0.691423
Test - acc:         0.770200 loss:        0.669554
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 331 ==========
LR =  0.0010000000000000002
Train - acc:        0.765580 loss:        0.684981
Test - acc:         0.768500 loss:        0.672031
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 332 ==========
LR =  0.0010000000000000002
Train - acc:        0.768940 loss:        0.677544
Test - acc:         0.768000 loss:        0.667806
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 333 ==========
LR =  0.0010000000000000002
Train - acc:        0.769740 loss:        0.673974
Test - acc:         0.776600 loss:        0.656079
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 334 ==========
LR =  0.0010000000000000002
Train - acc:        0.774680 loss:        0.661573
Test - acc:         0.775100 loss:        0.659904
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 335 ==========
LR =  0.0010000000000000002
Train - acc:        0.774360 loss:        0.659496
Test - acc:         0.775300 loss:        0.648453
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 336 ==========
LR =  0.0010000000000000002
Train - acc:        0.777440 loss:        0.653214
Test - acc:         0.781900 loss:        0.641939
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 337 ==========
LR =  0.0010000000000000002
Train - acc:        0.780040 loss:        0.647783
Test - acc:         0.778200 loss:        0.637749
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 338 ==========
LR =  0.0010000000000000002
Train - acc:        0.779940 loss:        0.646837
Test - acc:         0.782700 loss:        0.634948
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 339 ==========
LR =  0.0010000000000000002
Train - acc:        0.779600 loss:        0.642756
Test - acc:         0.778500 loss:        0.636298
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 340 ==========
LR =  0.0010000000000000002
Train - acc:        0.781300 loss:        0.635649
Test - acc:         0.786600 loss:        0.627595
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 341 ==========
LR =  0.0010000000000000002
Train - acc:        0.783040 loss:        0.631746
Test - acc:         0.782900 loss:        0.626044
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 342 ==========
LR =  0.0010000000000000002
Train - acc:        0.784440 loss:        0.628787
Test - acc:         0.786500 loss:        0.623665
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 343 ==========
LR =  0.0010000000000000002
Train - acc:        0.787060 loss:        0.623561
Test - acc:         0.785700 loss:        0.624833
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 344 ==========
LR =  0.0010000000000000002
Train - acc:        0.784900 loss:        0.627326
Test - acc:         0.784000 loss:        0.622004
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 345 ==========
LR =  0.0010000000000000002
Train - acc:        0.786400 loss:        0.619732
Test - acc:         0.785300 loss:        0.612520
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 346 ==========
LR =  0.0010000000000000002
Train - acc:        0.788200 loss:        0.619547
Test - acc:         0.788600 loss:        0.616484
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 347 ==========
LR =  0.0010000000000000002
Train - acc:        0.787760 loss:        0.613956
Test - acc:         0.789500 loss:        0.605582
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 348 ==========
LR =  0.0010000000000000002
Train - acc:        0.791300 loss:        0.609820
Test - acc:         0.784100 loss:        0.611748
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 349 ==========
LR =  0.0010000000000000002
Train - acc:        0.790660 loss:        0.609957
Test - acc:         0.789700 loss:        0.605805
Sparsity :          0.9990
Wdecay :        0.000500
========== Epoch 350 ==========
LR =  0.0010000000000000002
Train - acc:        0.793000 loss:        0.607070
Test - acc:         0.795300 loss:        0.600221
Sparsity :          0.9990
Wdecay :        0.000500
